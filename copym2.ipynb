{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the requiered libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['EC'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/M2.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split the data into features (X) and target (y)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEC\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.8/site-packages/pandas/core/frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5121\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5260\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5264\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5265\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.8/site-packages/pandas/core/generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.8/site-packages/pandas/core/generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.8/site-packages/pandas/core/indexes/base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6700\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['EC'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_excel('data/M2.xlsx')\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop(columns=['EC']).values\n",
    "y = data['EC'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 274.68   ,  695.85412,  194.108  , 2111.033  ,   35.305  ],\n",
       "       [ 216.43   ,  440.69755,  129.725  , 1388.57   ,   23.656  ],\n",
       "       [ 205.56   ,  473.9763 ,  122.522  , 1394.069  ,   15.576  ],\n",
       "       [ 267.57   ,  600.802  ,  160.494  , 1796.989  ,   24.564  ],\n",
       "       [ 209.31   ,  401.088  ,  102.076  , 1121.01   ,   17.483  ],\n",
       "       [ 168.23   ,  836.5    ,  191.207  , 1493.9325 ,   35.394  ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Use only the last output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = LSTMRegressor(input_size, hidden_size, num_layers, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50000], Train Loss: 27737640.0000, Val Loss: 20690876.0000\n",
      "Epoch [2/50000], Train Loss: 27730658.0000, Val Loss: 20684650.0000\n",
      "Epoch [3/50000], Train Loss: 27723674.0000, Val Loss: 20678424.0000\n",
      "Epoch [4/50000], Train Loss: 27716694.0000, Val Loss: 20672200.0000\n",
      "Epoch [5/50000], Train Loss: 27709714.0000, Val Loss: 20665976.0000\n",
      "Epoch [6/50000], Train Loss: 27702740.0000, Val Loss: 20659752.0000\n",
      "Epoch [7/50000], Train Loss: 27695758.0000, Val Loss: 20653534.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50000], Train Loss: 27688782.0000, Val Loss: 20647316.0000\n",
      "Epoch [9/50000], Train Loss: 27681808.0000, Val Loss: 20641094.0000\n",
      "Epoch [10/50000], Train Loss: 27674836.0000, Val Loss: 20634876.0000\n",
      "Epoch [11/50000], Train Loss: 27667860.0000, Val Loss: 20628664.0000\n",
      "Epoch [12/50000], Train Loss: 27660892.0000, Val Loss: 20622450.0000\n",
      "Epoch [13/50000], Train Loss: 27653922.0000, Val Loss: 20616238.0000\n",
      "Epoch [14/50000], Train Loss: 27646956.0000, Val Loss: 20610026.0000\n",
      "Epoch [15/50000], Train Loss: 27639992.0000, Val Loss: 20603814.0000\n",
      "Epoch [16/50000], Train Loss: 27633024.0000, Val Loss: 20597608.0000\n",
      "Epoch [17/50000], Train Loss: 27626062.0000, Val Loss: 20591402.0000\n",
      "Epoch [18/50000], Train Loss: 27619096.0000, Val Loss: 20585198.0000\n",
      "Epoch [19/50000], Train Loss: 27612140.0000, Val Loss: 20578996.0000\n",
      "Epoch [20/50000], Train Loss: 27605180.0000, Val Loss: 20572792.0000\n",
      "Epoch [21/50000], Train Loss: 27598226.0000, Val Loss: 20566592.0000\n",
      "Epoch [22/50000], Train Loss: 27591272.0000, Val Loss: 20560396.0000\n",
      "Epoch [23/50000], Train Loss: 27584316.0000, Val Loss: 20554196.0000\n",
      "Epoch [24/50000], Train Loss: 27577362.0000, Val Loss: 20548002.0000\n",
      "Epoch [25/50000], Train Loss: 27570414.0000, Val Loss: 20541812.0000\n",
      "Epoch [26/50000], Train Loss: 27563466.0000, Val Loss: 20535616.0000\n",
      "Epoch [27/50000], Train Loss: 27556520.0000, Val Loss: 20529428.0000\n",
      "Epoch [28/50000], Train Loss: 27549576.0000, Val Loss: 20523238.0000\n",
      "Epoch [29/50000], Train Loss: 27542632.0000, Val Loss: 20517050.0000\n",
      "Epoch [30/50000], Train Loss: 27535688.0000, Val Loss: 20510868.0000\n",
      "Epoch [31/50000], Train Loss: 27528752.0000, Val Loss: 20504682.0000\n",
      "Epoch [32/50000], Train Loss: 27521812.0000, Val Loss: 20498500.0000\n",
      "Epoch [33/50000], Train Loss: 27514874.0000, Val Loss: 20492318.0000\n",
      "Epoch [34/50000], Train Loss: 27507942.0000, Val Loss: 20486142.0000\n",
      "Epoch [35/50000], Train Loss: 27501008.0000, Val Loss: 20479964.0000\n",
      "Epoch [36/50000], Train Loss: 27494074.0000, Val Loss: 20473790.0000\n",
      "Epoch [37/50000], Train Loss: 27487146.0000, Val Loss: 20467616.0000\n",
      "Epoch [38/50000], Train Loss: 27480218.0000, Val Loss: 20461446.0000\n",
      "Epoch [39/50000], Train Loss: 27473294.0000, Val Loss: 20455274.0000\n",
      "Epoch [40/50000], Train Loss: 27466368.0000, Val Loss: 20449108.0000\n",
      "Epoch [41/50000], Train Loss: 27459450.0000, Val Loss: 20442940.0000\n",
      "Epoch [42/50000], Train Loss: 27452528.0000, Val Loss: 20436778.0000\n",
      "Epoch [43/50000], Train Loss: 27445612.0000, Val Loss: 20430616.0000\n",
      "Epoch [44/50000], Train Loss: 27438698.0000, Val Loss: 20424452.0000\n",
      "Epoch [45/50000], Train Loss: 27431776.0000, Val Loss: 20418292.0000\n",
      "Epoch [46/50000], Train Loss: 27424866.0000, Val Loss: 20412136.0000\n",
      "Epoch [47/50000], Train Loss: 27417958.0000, Val Loss: 20405978.0000\n",
      "Epoch [48/50000], Train Loss: 27411048.0000, Val Loss: 20399826.0000\n",
      "Epoch [49/50000], Train Loss: 27404140.0000, Val Loss: 20393668.0000\n",
      "Epoch [50/50000], Train Loss: 27397230.0000, Val Loss: 20387520.0000\n",
      "Epoch [51/50000], Train Loss: 27390330.0000, Val Loss: 20381370.0000\n",
      "Epoch [52/50000], Train Loss: 27383422.0000, Val Loss: 20375226.0000\n",
      "Epoch [53/50000], Train Loss: 27376524.0000, Val Loss: 20369076.0000\n",
      "Epoch [54/50000], Train Loss: 27369624.0000, Val Loss: 20362934.0000\n",
      "Epoch [55/50000], Train Loss: 27362728.0000, Val Loss: 20356794.0000\n",
      "Epoch [56/50000], Train Loss: 27355834.0000, Val Loss: 20350650.0000\n",
      "Epoch [57/50000], Train Loss: 27348940.0000, Val Loss: 20344512.0000\n",
      "Epoch [58/50000], Train Loss: 27342048.0000, Val Loss: 20338374.0000\n",
      "Epoch [59/50000], Train Loss: 27335160.0000, Val Loss: 20332242.0000\n",
      "Epoch [60/50000], Train Loss: 27328272.0000, Val Loss: 20326108.0000\n",
      "Epoch [61/50000], Train Loss: 27321384.0000, Val Loss: 20319976.0000\n",
      "Epoch [62/50000], Train Loss: 27314500.0000, Val Loss: 20313848.0000\n",
      "Epoch [63/50000], Train Loss: 27307616.0000, Val Loss: 20307720.0000\n",
      "Epoch [64/50000], Train Loss: 27300738.0000, Val Loss: 20301592.0000\n",
      "Epoch [65/50000], Train Loss: 27293858.0000, Val Loss: 20295468.0000\n",
      "Epoch [66/50000], Train Loss: 27286986.0000, Val Loss: 20289344.0000\n",
      "Epoch [67/50000], Train Loss: 27280104.0000, Val Loss: 20283226.0000\n",
      "Epoch [68/50000], Train Loss: 27273236.0000, Val Loss: 20277108.0000\n",
      "Epoch [69/50000], Train Loss: 27266364.0000, Val Loss: 20270986.0000\n",
      "Epoch [70/50000], Train Loss: 27259488.0000, Val Loss: 20264874.0000\n",
      "Epoch [71/50000], Train Loss: 27252624.0000, Val Loss: 20258758.0000\n",
      "Epoch [72/50000], Train Loss: 27245758.0000, Val Loss: 20252646.0000\n",
      "Epoch [73/50000], Train Loss: 27238898.0000, Val Loss: 20246540.0000\n",
      "Epoch [74/50000], Train Loss: 27232036.0000, Val Loss: 20240430.0000\n",
      "Epoch [75/50000], Train Loss: 27225174.0000, Val Loss: 20234322.0000\n",
      "Epoch [76/50000], Train Loss: 27218312.0000, Val Loss: 20228220.0000\n",
      "Epoch [77/50000], Train Loss: 27211462.0000, Val Loss: 20222112.0000\n",
      "Epoch [78/50000], Train Loss: 27204604.0000, Val Loss: 20216010.0000\n",
      "Epoch [79/50000], Train Loss: 27197748.0000, Val Loss: 20209914.0000\n",
      "Epoch [80/50000], Train Loss: 27190898.0000, Val Loss: 20203814.0000\n",
      "Epoch [81/50000], Train Loss: 27184052.0000, Val Loss: 20197718.0000\n",
      "Epoch [82/50000], Train Loss: 27177202.0000, Val Loss: 20191624.0000\n",
      "Epoch [83/50000], Train Loss: 27170358.0000, Val Loss: 20185530.0000\n",
      "Epoch [84/50000], Train Loss: 27163514.0000, Val Loss: 20179438.0000\n",
      "Epoch [85/50000], Train Loss: 27156668.0000, Val Loss: 20173350.0000\n",
      "Epoch [86/50000], Train Loss: 27149830.0000, Val Loss: 20167262.0000\n",
      "Epoch [87/50000], Train Loss: 27142990.0000, Val Loss: 20161180.0000\n",
      "Epoch [88/50000], Train Loss: 27136154.0000, Val Loss: 20155096.0000\n",
      "Epoch [89/50000], Train Loss: 27129320.0000, Val Loss: 20149012.0000\n",
      "Epoch [90/50000], Train Loss: 27122490.0000, Val Loss: 20142934.0000\n",
      "Epoch [91/50000], Train Loss: 27115660.0000, Val Loss: 20136852.0000\n",
      "Epoch [92/50000], Train Loss: 27108828.0000, Val Loss: 20130776.0000\n",
      "Epoch [93/50000], Train Loss: 27102004.0000, Val Loss: 20124700.0000\n",
      "Epoch [94/50000], Train Loss: 27095172.0000, Val Loss: 20118628.0000\n",
      "Epoch [95/50000], Train Loss: 27088348.0000, Val Loss: 20112554.0000\n",
      "Epoch [96/50000], Train Loss: 27081526.0000, Val Loss: 20106488.0000\n",
      "Epoch [97/50000], Train Loss: 27074710.0000, Val Loss: 20100416.0000\n",
      "Epoch [98/50000], Train Loss: 27067888.0000, Val Loss: 20094350.0000\n",
      "Epoch [99/50000], Train Loss: 27061076.0000, Val Loss: 20088286.0000\n",
      "Epoch [100/50000], Train Loss: 27054258.0000, Val Loss: 20082222.0000\n",
      "Epoch [101/50000], Train Loss: 27047442.0000, Val Loss: 20076164.0000\n",
      "Epoch [102/50000], Train Loss: 27040634.0000, Val Loss: 20070102.0000\n",
      "Epoch [103/50000], Train Loss: 27033824.0000, Val Loss: 20064044.0000\n",
      "Epoch [104/50000], Train Loss: 27027014.0000, Val Loss: 20057988.0000\n",
      "Epoch [105/50000], Train Loss: 27020212.0000, Val Loss: 20051938.0000\n",
      "Epoch [106/50000], Train Loss: 27013408.0000, Val Loss: 20045882.0000\n",
      "Epoch [107/50000], Train Loss: 27006604.0000, Val Loss: 20039834.0000\n",
      "Epoch [108/50000], Train Loss: 26999808.0000, Val Loss: 20033784.0000\n",
      "Epoch [109/50000], Train Loss: 26993008.0000, Val Loss: 20027738.0000\n",
      "Epoch [110/50000], Train Loss: 26986210.0000, Val Loss: 20021694.0000\n",
      "Epoch [111/50000], Train Loss: 26979416.0000, Val Loss: 20015646.0000\n",
      "Epoch [112/50000], Train Loss: 26972622.0000, Val Loss: 20009606.0000\n",
      "Epoch [113/50000], Train Loss: 26965834.0000, Val Loss: 20003564.0000\n",
      "Epoch [114/50000], Train Loss: 26959042.0000, Val Loss: 19997526.0000\n",
      "Epoch [115/50000], Train Loss: 26952254.0000, Val Loss: 19991488.0000\n",
      "Epoch [116/50000], Train Loss: 26945468.0000, Val Loss: 19985452.0000\n",
      "Epoch [117/50000], Train Loss: 26938686.0000, Val Loss: 19979424.0000\n",
      "Epoch [118/50000], Train Loss: 26931906.0000, Val Loss: 19973390.0000\n",
      "Epoch [119/50000], Train Loss: 26925122.0000, Val Loss: 19967360.0000\n",
      "Epoch [120/50000], Train Loss: 26918346.0000, Val Loss: 19961334.0000\n",
      "Epoch [121/50000], Train Loss: 26911568.0000, Val Loss: 19955306.0000\n",
      "Epoch [122/50000], Train Loss: 26904794.0000, Val Loss: 19949282.0000\n",
      "Epoch [123/50000], Train Loss: 26898018.0000, Val Loss: 19943262.0000\n",
      "Epoch [124/50000], Train Loss: 26891248.0000, Val Loss: 19937240.0000\n",
      "Epoch [125/50000], Train Loss: 26884478.0000, Val Loss: 19931222.0000\n",
      "Epoch [126/50000], Train Loss: 26877714.0000, Val Loss: 19925204.0000\n",
      "Epoch [127/50000], Train Loss: 26870946.0000, Val Loss: 19919188.0000\n",
      "Epoch [128/50000], Train Loss: 26864182.0000, Val Loss: 19913174.0000\n",
      "Epoch [129/50000], Train Loss: 26857422.0000, Val Loss: 19907162.0000\n",
      "Epoch [130/50000], Train Loss: 26850662.0000, Val Loss: 19901154.0000\n",
      "Epoch [131/50000], Train Loss: 26843902.0000, Val Loss: 19895144.0000\n",
      "Epoch [132/50000], Train Loss: 26837144.0000, Val Loss: 19889134.0000\n",
      "Epoch [133/50000], Train Loss: 26830388.0000, Val Loss: 19883132.0000\n",
      "Epoch [134/50000], Train Loss: 26823636.0000, Val Loss: 19877130.0000\n",
      "Epoch [135/50000], Train Loss: 26816884.0000, Val Loss: 19871126.0000\n",
      "Epoch [136/50000], Train Loss: 26810134.0000, Val Loss: 19865126.0000\n",
      "Epoch [137/50000], Train Loss: 26803388.0000, Val Loss: 19859126.0000\n",
      "Epoch [138/50000], Train Loss: 26796638.0000, Val Loss: 19853132.0000\n",
      "Epoch [139/50000], Train Loss: 26789900.0000, Val Loss: 19847138.0000\n",
      "Epoch [140/50000], Train Loss: 26783156.0000, Val Loss: 19841144.0000\n",
      "Epoch [141/50000], Train Loss: 26776412.0000, Val Loss: 19835154.0000\n",
      "Epoch [142/50000], Train Loss: 26769676.0000, Val Loss: 19829162.0000\n",
      "Epoch [143/50000], Train Loss: 26762940.0000, Val Loss: 19823176.0000\n",
      "Epoch [144/50000], Train Loss: 26756204.0000, Val Loss: 19817188.0000\n",
      "Epoch [145/50000], Train Loss: 26749472.0000, Val Loss: 19811204.0000\n",
      "Epoch [146/50000], Train Loss: 26742740.0000, Val Loss: 19805220.0000\n",
      "Epoch [147/50000], Train Loss: 26736006.0000, Val Loss: 19799244.0000\n",
      "Epoch [148/50000], Train Loss: 26729282.0000, Val Loss: 19793262.0000\n",
      "Epoch [149/50000], Train Loss: 26722558.0000, Val Loss: 19787286.0000\n",
      "Epoch [150/50000], Train Loss: 26715834.0000, Val Loss: 19781312.0000\n",
      "Epoch [151/50000], Train Loss: 26709114.0000, Val Loss: 19775336.0000\n",
      "Epoch [152/50000], Train Loss: 26702390.0000, Val Loss: 19769366.0000\n",
      "Epoch [153/50000], Train Loss: 26695674.0000, Val Loss: 19763392.0000\n",
      "Epoch [154/50000], Train Loss: 26688954.0000, Val Loss: 19757426.0000\n",
      "Epoch [155/50000], Train Loss: 26682240.0000, Val Loss: 19751456.0000\n",
      "Epoch [156/50000], Train Loss: 26675522.0000, Val Loss: 19745492.0000\n",
      "Epoch [157/50000], Train Loss: 26668814.0000, Val Loss: 19739528.0000\n",
      "Epoch [158/50000], Train Loss: 26662104.0000, Val Loss: 19733564.0000\n",
      "Epoch [159/50000], Train Loss: 26655394.0000, Val Loss: 19727606.0000\n",
      "Epoch [160/50000], Train Loss: 26648688.0000, Val Loss: 19721646.0000\n",
      "Epoch [161/50000], Train Loss: 26641988.0000, Val Loss: 19715692.0000\n",
      "Epoch [162/50000], Train Loss: 26635282.0000, Val Loss: 19709736.0000\n",
      "Epoch [163/50000], Train Loss: 26628582.0000, Val Loss: 19703784.0000\n",
      "Epoch [164/50000], Train Loss: 26621882.0000, Val Loss: 19697834.0000\n",
      "Epoch [165/50000], Train Loss: 26615186.0000, Val Loss: 19691882.0000\n",
      "Epoch [166/50000], Train Loss: 26608488.0000, Val Loss: 19685932.0000\n",
      "Epoch [167/50000], Train Loss: 26601796.0000, Val Loss: 19679992.0000\n",
      "Epoch [168/50000], Train Loss: 26595104.0000, Val Loss: 19674044.0000\n",
      "Epoch [169/50000], Train Loss: 26588410.0000, Val Loss: 19668098.0000\n",
      "Epoch [170/50000], Train Loss: 26581724.0000, Val Loss: 19662156.0000\n",
      "Epoch [171/50000], Train Loss: 26575034.0000, Val Loss: 19656220.0000\n",
      "Epoch [172/50000], Train Loss: 26568354.0000, Val Loss: 19650280.0000\n",
      "Epoch [173/50000], Train Loss: 26561666.0000, Val Loss: 19644348.0000\n",
      "Epoch [174/50000], Train Loss: 26554988.0000, Val Loss: 19638410.0000\n",
      "Epoch [175/50000], Train Loss: 26548304.0000, Val Loss: 19632480.0000\n",
      "Epoch [176/50000], Train Loss: 26541630.0000, Val Loss: 19626552.0000\n",
      "Epoch [177/50000], Train Loss: 26534956.0000, Val Loss: 19620618.0000\n",
      "Epoch [178/50000], Train Loss: 26528280.0000, Val Loss: 19614692.0000\n",
      "Epoch [179/50000], Train Loss: 26521608.0000, Val Loss: 19608766.0000\n",
      "Epoch [180/50000], Train Loss: 26514940.0000, Val Loss: 19602842.0000\n",
      "Epoch [181/50000], Train Loss: 26508270.0000, Val Loss: 19596920.0000\n",
      "Epoch [182/50000], Train Loss: 26501602.0000, Val Loss: 19591000.0000\n",
      "Epoch [183/50000], Train Loss: 26494938.0000, Val Loss: 19585080.0000\n",
      "Epoch [184/50000], Train Loss: 26488270.0000, Val Loss: 19579164.0000\n",
      "Epoch [185/50000], Train Loss: 26481610.0000, Val Loss: 19573250.0000\n",
      "Epoch [186/50000], Train Loss: 26474952.0000, Val Loss: 19567334.0000\n",
      "Epoch [187/50000], Train Loss: 26468292.0000, Val Loss: 19561422.0000\n",
      "Epoch [188/50000], Train Loss: 26461636.0000, Val Loss: 19555512.0000\n",
      "Epoch [189/50000], Train Loss: 26454982.0000, Val Loss: 19549604.0000\n",
      "Epoch [190/50000], Train Loss: 26448328.0000, Val Loss: 19543694.0000\n",
      "Epoch [191/50000], Train Loss: 26441676.0000, Val Loss: 19537792.0000\n",
      "Epoch [192/50000], Train Loss: 26435028.0000, Val Loss: 19531890.0000\n",
      "Epoch [193/50000], Train Loss: 26428382.0000, Val Loss: 19525986.0000\n",
      "Epoch [194/50000], Train Loss: 26421736.0000, Val Loss: 19520088.0000\n",
      "Epoch [195/50000], Train Loss: 26415092.0000, Val Loss: 19514188.0000\n",
      "Epoch [196/50000], Train Loss: 26408450.0000, Val Loss: 19508292.0000\n",
      "Epoch [197/50000], Train Loss: 26401810.0000, Val Loss: 19502400.0000\n",
      "Epoch [198/50000], Train Loss: 26395170.0000, Val Loss: 19496504.0000\n",
      "Epoch [199/50000], Train Loss: 26388534.0000, Val Loss: 19490616.0000\n",
      "Epoch [200/50000], Train Loss: 26381900.0000, Val Loss: 19484726.0000\n",
      "Epoch [201/50000], Train Loss: 26375264.0000, Val Loss: 19478838.0000\n",
      "Epoch [202/50000], Train Loss: 26368638.0000, Val Loss: 19472950.0000\n",
      "Epoch [203/50000], Train Loss: 26362004.0000, Val Loss: 19467068.0000\n",
      "Epoch [204/50000], Train Loss: 26355378.0000, Val Loss: 19461182.0000\n",
      "Epoch [205/50000], Train Loss: 26348756.0000, Val Loss: 19455300.0000\n",
      "Epoch [206/50000], Train Loss: 26342126.0000, Val Loss: 19449422.0000\n",
      "Epoch [207/50000], Train Loss: 26335504.0000, Val Loss: 19443544.0000\n",
      "Epoch [208/50000], Train Loss: 26328886.0000, Val Loss: 19437666.0000\n",
      "Epoch [209/50000], Train Loss: 26322264.0000, Val Loss: 19431796.0000\n",
      "Epoch [210/50000], Train Loss: 26315650.0000, Val Loss: 19425920.0000\n",
      "Epoch [211/50000], Train Loss: 26309032.0000, Val Loss: 19420054.0000\n",
      "Epoch [212/50000], Train Loss: 26302420.0000, Val Loss: 19414182.0000\n",
      "Epoch [213/50000], Train Loss: 26295804.0000, Val Loss: 19408316.0000\n",
      "Epoch [214/50000], Train Loss: 26289196.0000, Val Loss: 19402450.0000\n",
      "Epoch [215/50000], Train Loss: 26282586.0000, Val Loss: 19396586.0000\n",
      "Epoch [216/50000], Train Loss: 26275980.0000, Val Loss: 19390722.0000\n",
      "Epoch [217/50000], Train Loss: 26269380.0000, Val Loss: 19384862.0000\n",
      "Epoch [218/50000], Train Loss: 26262772.0000, Val Loss: 19379002.0000\n",
      "Epoch [219/50000], Train Loss: 26256170.0000, Val Loss: 19373144.0000\n",
      "Epoch [220/50000], Train Loss: 26249570.0000, Val Loss: 19367292.0000\n",
      "Epoch [221/50000], Train Loss: 26242976.0000, Val Loss: 19361438.0000\n",
      "Epoch [222/50000], Train Loss: 26236380.0000, Val Loss: 19355584.0000\n",
      "Epoch [223/50000], Train Loss: 26229782.0000, Val Loss: 19349734.0000\n",
      "Epoch [224/50000], Train Loss: 26223194.0000, Val Loss: 19343890.0000\n",
      "Epoch [225/50000], Train Loss: 26216602.0000, Val Loss: 19338042.0000\n",
      "Epoch [226/50000], Train Loss: 26210012.0000, Val Loss: 19332194.0000\n",
      "Epoch [227/50000], Train Loss: 26203426.0000, Val Loss: 19326348.0000\n",
      "Epoch [228/50000], Train Loss: 26196838.0000, Val Loss: 19320510.0000\n",
      "Epoch [229/50000], Train Loss: 26190260.0000, Val Loss: 19314670.0000\n",
      "Epoch [230/50000], Train Loss: 26183674.0000, Val Loss: 19308828.0000\n",
      "Epoch [231/50000], Train Loss: 26177094.0000, Val Loss: 19302990.0000\n",
      "Epoch [232/50000], Train Loss: 26170516.0000, Val Loss: 19297156.0000\n",
      "Epoch [233/50000], Train Loss: 26163938.0000, Val Loss: 19291322.0000\n",
      "Epoch [234/50000], Train Loss: 26157362.0000, Val Loss: 19285494.0000\n",
      "Epoch [235/50000], Train Loss: 26150790.0000, Val Loss: 19279660.0000\n",
      "Epoch [236/50000], Train Loss: 26144216.0000, Val Loss: 19273832.0000\n",
      "Epoch [237/50000], Train Loss: 26137648.0000, Val Loss: 19268006.0000\n",
      "Epoch [238/50000], Train Loss: 26131078.0000, Val Loss: 19262184.0000\n",
      "Epoch [239/50000], Train Loss: 26124514.0000, Val Loss: 19256358.0000\n",
      "Epoch [240/50000], Train Loss: 26117954.0000, Val Loss: 19250538.0000\n",
      "Epoch [241/50000], Train Loss: 26111390.0000, Val Loss: 19244716.0000\n",
      "Epoch [242/50000], Train Loss: 26104826.0000, Val Loss: 19238900.0000\n",
      "Epoch [243/50000], Train Loss: 26098270.0000, Val Loss: 19233084.0000\n",
      "Epoch [244/50000], Train Loss: 26091714.0000, Val Loss: 19227268.0000\n",
      "Epoch [245/50000], Train Loss: 26085154.0000, Val Loss: 19221454.0000\n",
      "Epoch [246/50000], Train Loss: 26078604.0000, Val Loss: 19215644.0000\n",
      "Epoch [247/50000], Train Loss: 26072050.0000, Val Loss: 19209834.0000\n",
      "Epoch [248/50000], Train Loss: 26065502.0000, Val Loss: 19204024.0000\n",
      "Epoch [249/50000], Train Loss: 26058952.0000, Val Loss: 19198218.0000\n",
      "Epoch [250/50000], Train Loss: 26052404.0000, Val Loss: 19192414.0000\n",
      "Epoch [251/50000], Train Loss: 26045858.0000, Val Loss: 19186612.0000\n",
      "Epoch [252/50000], Train Loss: 26039314.0000, Val Loss: 19180812.0000\n",
      "Epoch [253/50000], Train Loss: 26032772.0000, Val Loss: 19175010.0000\n",
      "Epoch [254/50000], Train Loss: 26026236.0000, Val Loss: 19169214.0000\n",
      "Epoch [255/50000], Train Loss: 26019696.0000, Val Loss: 19163422.0000\n",
      "Epoch [256/50000], Train Loss: 26013164.0000, Val Loss: 19157622.0000\n",
      "Epoch [257/50000], Train Loss: 26006624.0000, Val Loss: 19151832.0000\n",
      "Epoch [258/50000], Train Loss: 26000094.0000, Val Loss: 19146042.0000\n",
      "Epoch [259/50000], Train Loss: 25993562.0000, Val Loss: 19140254.0000\n",
      "Epoch [260/50000], Train Loss: 25987032.0000, Val Loss: 19134464.0000\n",
      "Epoch [261/50000], Train Loss: 25980508.0000, Val Loss: 19128678.0000\n",
      "Epoch [262/50000], Train Loss: 25973984.0000, Val Loss: 19122892.0000\n",
      "Epoch [263/50000], Train Loss: 25967456.0000, Val Loss: 19117110.0000\n",
      "Epoch [264/50000], Train Loss: 25960934.0000, Val Loss: 19111328.0000\n",
      "Epoch [265/50000], Train Loss: 25954412.0000, Val Loss: 19105548.0000\n",
      "Epoch [266/50000], Train Loss: 25947896.0000, Val Loss: 19099774.0000\n",
      "Epoch [267/50000], Train Loss: 25941378.0000, Val Loss: 19093994.0000\n",
      "Epoch [268/50000], Train Loss: 25934860.0000, Val Loss: 19088220.0000\n",
      "Epoch [269/50000], Train Loss: 25928346.0000, Val Loss: 19082446.0000\n",
      "Epoch [270/50000], Train Loss: 25921836.0000, Val Loss: 19076678.0000\n",
      "Epoch [271/50000], Train Loss: 25915326.0000, Val Loss: 19070908.0000\n",
      "Epoch [272/50000], Train Loss: 25908820.0000, Val Loss: 19065140.0000\n",
      "Epoch [273/50000], Train Loss: 25902314.0000, Val Loss: 19059372.0000\n",
      "Epoch [274/50000], Train Loss: 25895806.0000, Val Loss: 19053610.0000\n",
      "Epoch [275/50000], Train Loss: 25889300.0000, Val Loss: 19047848.0000\n",
      "Epoch [276/50000], Train Loss: 25882804.0000, Val Loss: 19042086.0000\n",
      "Epoch [277/50000], Train Loss: 25876302.0000, Val Loss: 19036330.0000\n",
      "Epoch [278/50000], Train Loss: 25869808.0000, Val Loss: 19030570.0000\n",
      "Epoch [279/50000], Train Loss: 25863304.0000, Val Loss: 19024814.0000\n",
      "Epoch [280/50000], Train Loss: 25856812.0000, Val Loss: 19019062.0000\n",
      "Epoch [281/50000], Train Loss: 25850320.0000, Val Loss: 19013308.0000\n",
      "Epoch [282/50000], Train Loss: 25843834.0000, Val Loss: 19007560.0000\n",
      "Epoch [283/50000], Train Loss: 25837344.0000, Val Loss: 19001806.0000\n",
      "Epoch [284/50000], Train Loss: 25830854.0000, Val Loss: 18996062.0000\n",
      "Epoch [285/50000], Train Loss: 25824370.0000, Val Loss: 18990316.0000\n",
      "Epoch [286/50000], Train Loss: 25817882.0000, Val Loss: 18984572.0000\n",
      "Epoch [287/50000], Train Loss: 25811402.0000, Val Loss: 18978830.0000\n",
      "Epoch [288/50000], Train Loss: 25804924.0000, Val Loss: 18973086.0000\n",
      "Epoch [289/50000], Train Loss: 25798440.0000, Val Loss: 18967350.0000\n",
      "Epoch [290/50000], Train Loss: 25791966.0000, Val Loss: 18961610.0000\n",
      "Epoch [291/50000], Train Loss: 25785488.0000, Val Loss: 18955876.0000\n",
      "Epoch [292/50000], Train Loss: 25779016.0000, Val Loss: 18950140.0000\n",
      "Epoch [293/50000], Train Loss: 25772546.0000, Val Loss: 18944408.0000\n",
      "Epoch [294/50000], Train Loss: 25766074.0000, Val Loss: 18938678.0000\n",
      "Epoch [295/50000], Train Loss: 25759608.0000, Val Loss: 18932950.0000\n",
      "Epoch [296/50000], Train Loss: 25753138.0000, Val Loss: 18927220.0000\n",
      "Epoch [297/50000], Train Loss: 25746672.0000, Val Loss: 18921496.0000\n",
      "Epoch [298/50000], Train Loss: 25740212.0000, Val Loss: 18915772.0000\n",
      "Epoch [299/50000], Train Loss: 25733754.0000, Val Loss: 18910048.0000\n",
      "Epoch [300/50000], Train Loss: 25727290.0000, Val Loss: 18904330.0000\n",
      "Epoch [301/50000], Train Loss: 25720836.0000, Val Loss: 18898612.0000\n",
      "Epoch [302/50000], Train Loss: 25714378.0000, Val Loss: 18892892.0000\n",
      "Epoch [303/50000], Train Loss: 25707920.0000, Val Loss: 18887176.0000\n",
      "Epoch [304/50000], Train Loss: 25701468.0000, Val Loss: 18881462.0000\n",
      "Epoch [305/50000], Train Loss: 25695016.0000, Val Loss: 18875750.0000\n",
      "Epoch [306/50000], Train Loss: 25688570.0000, Val Loss: 18870040.0000\n",
      "Epoch [307/50000], Train Loss: 25682122.0000, Val Loss: 18864330.0000\n",
      "Epoch [308/50000], Train Loss: 25675674.0000, Val Loss: 18858620.0000\n",
      "Epoch [309/50000], Train Loss: 25669228.0000, Val Loss: 18852918.0000\n",
      "Epoch [310/50000], Train Loss: 25662792.0000, Val Loss: 18847214.0000\n",
      "Epoch [311/50000], Train Loss: 25656350.0000, Val Loss: 18841510.0000\n",
      "Epoch [312/50000], Train Loss: 25649912.0000, Val Loss: 18835808.0000\n",
      "Epoch [313/50000], Train Loss: 25643472.0000, Val Loss: 18830112.0000\n",
      "Epoch [314/50000], Train Loss: 25637036.0000, Val Loss: 18824414.0000\n",
      "Epoch [315/50000], Train Loss: 25630600.0000, Val Loss: 18818716.0000\n",
      "Epoch [316/50000], Train Loss: 25624168.0000, Val Loss: 18813024.0000\n",
      "Epoch [317/50000], Train Loss: 25617736.0000, Val Loss: 18807330.0000\n",
      "Epoch [318/50000], Train Loss: 25611312.0000, Val Loss: 18801638.0000\n",
      "Epoch [319/50000], Train Loss: 25604886.0000, Val Loss: 18795954.0000\n",
      "Epoch [320/50000], Train Loss: 25598458.0000, Val Loss: 18790266.0000\n",
      "Epoch [321/50000], Train Loss: 25592038.0000, Val Loss: 18784580.0000\n",
      "Epoch [322/50000], Train Loss: 25585614.0000, Val Loss: 18778894.0000\n",
      "Epoch [323/50000], Train Loss: 25579194.0000, Val Loss: 18773214.0000\n",
      "Epoch [324/50000], Train Loss: 25572774.0000, Val Loss: 18767530.0000\n",
      "Epoch [325/50000], Train Loss: 25566358.0000, Val Loss: 18761852.0000\n",
      "Epoch [326/50000], Train Loss: 25559942.0000, Val Loss: 18756178.0000\n",
      "Epoch [327/50000], Train Loss: 25553534.0000, Val Loss: 18750498.0000\n",
      "Epoch [328/50000], Train Loss: 25547118.0000, Val Loss: 18744824.0000\n",
      "Epoch [329/50000], Train Loss: 25540710.0000, Val Loss: 18739150.0000\n",
      "Epoch [330/50000], Train Loss: 25534298.0000, Val Loss: 18733482.0000\n",
      "Epoch [331/50000], Train Loss: 25527898.0000, Val Loss: 18727812.0000\n",
      "Epoch [332/50000], Train Loss: 25521494.0000, Val Loss: 18722144.0000\n",
      "Epoch [333/50000], Train Loss: 25515088.0000, Val Loss: 18716478.0000\n",
      "Epoch [334/50000], Train Loss: 25508686.0000, Val Loss: 18710814.0000\n",
      "Epoch [335/50000], Train Loss: 25502288.0000, Val Loss: 18705154.0000\n",
      "Epoch [336/50000], Train Loss: 25495892.0000, Val Loss: 18699490.0000\n",
      "Epoch [337/50000], Train Loss: 25489494.0000, Val Loss: 18693830.0000\n",
      "Epoch [338/50000], Train Loss: 25483100.0000, Val Loss: 18688170.0000\n",
      "Epoch [339/50000], Train Loss: 25476706.0000, Val Loss: 18682520.0000\n",
      "Epoch [340/50000], Train Loss: 25470316.0000, Val Loss: 18676862.0000\n",
      "Epoch [341/50000], Train Loss: 25463930.0000, Val Loss: 18671210.0000\n",
      "Epoch [342/50000], Train Loss: 25457542.0000, Val Loss: 18665560.0000\n",
      "Epoch [343/50000], Train Loss: 25451152.0000, Val Loss: 18659910.0000\n",
      "Epoch [344/50000], Train Loss: 25444770.0000, Val Loss: 18654260.0000\n",
      "Epoch [345/50000], Train Loss: 25438386.0000, Val Loss: 18648614.0000\n",
      "Epoch [346/50000], Train Loss: 25432008.0000, Val Loss: 18642974.0000\n",
      "Epoch [347/50000], Train Loss: 25425628.0000, Val Loss: 18637330.0000\n",
      "Epoch [348/50000], Train Loss: 25419250.0000, Val Loss: 18631686.0000\n",
      "Epoch [349/50000], Train Loss: 25412876.0000, Val Loss: 18626048.0000\n",
      "Epoch [350/50000], Train Loss: 25406502.0000, Val Loss: 18620410.0000\n",
      "Epoch [351/50000], Train Loss: 25400130.0000, Val Loss: 18614774.0000\n",
      "Epoch [352/50000], Train Loss: 25393756.0000, Val Loss: 18609142.0000\n",
      "Epoch [353/50000], Train Loss: 25387392.0000, Val Loss: 18603508.0000\n",
      "Epoch [354/50000], Train Loss: 25381024.0000, Val Loss: 18597874.0000\n",
      "Epoch [355/50000], Train Loss: 25374660.0000, Val Loss: 18592248.0000\n",
      "Epoch [356/50000], Train Loss: 25368296.0000, Val Loss: 18586622.0000\n",
      "Epoch [357/50000], Train Loss: 25361934.0000, Val Loss: 18580992.0000\n",
      "Epoch [358/50000], Train Loss: 25355576.0000, Val Loss: 18575368.0000\n",
      "Epoch [359/50000], Train Loss: 25349216.0000, Val Loss: 18569746.0000\n",
      "Epoch [360/50000], Train Loss: 25342862.0000, Val Loss: 18564122.0000\n",
      "Epoch [361/50000], Train Loss: 25336504.0000, Val Loss: 18558502.0000\n",
      "Epoch [362/50000], Train Loss: 25330152.0000, Val Loss: 18552886.0000\n",
      "Epoch [363/50000], Train Loss: 25323800.0000, Val Loss: 18547270.0000\n",
      "Epoch [364/50000], Train Loss: 25317452.0000, Val Loss: 18541656.0000\n",
      "Epoch [365/50000], Train Loss: 25311104.0000, Val Loss: 18536040.0000\n",
      "Epoch [366/50000], Train Loss: 25304756.0000, Val Loss: 18530430.0000\n",
      "Epoch [367/50000], Train Loss: 25298412.0000, Val Loss: 18524818.0000\n",
      "Epoch [368/50000], Train Loss: 25292068.0000, Val Loss: 18519212.0000\n",
      "Epoch [369/50000], Train Loss: 25285728.0000, Val Loss: 18513604.0000\n",
      "Epoch [370/50000], Train Loss: 25279386.0000, Val Loss: 18507998.0000\n",
      "Epoch [371/50000], Train Loss: 25273050.0000, Val Loss: 18502396.0000\n",
      "Epoch [372/50000], Train Loss: 25266710.0000, Val Loss: 18496796.0000\n",
      "Epoch [373/50000], Train Loss: 25260378.0000, Val Loss: 18491194.0000\n",
      "Epoch [374/50000], Train Loss: 25254046.0000, Val Loss: 18485596.0000\n",
      "Epoch [375/50000], Train Loss: 25247718.0000, Val Loss: 18480000.0000\n",
      "Epoch [376/50000], Train Loss: 25241384.0000, Val Loss: 18474402.0000\n",
      "Epoch [377/50000], Train Loss: 25235056.0000, Val Loss: 18468808.0000\n",
      "Epoch [378/50000], Train Loss: 25228730.0000, Val Loss: 18463214.0000\n",
      "Epoch [379/50000], Train Loss: 25222402.0000, Val Loss: 18457628.0000\n",
      "Epoch [380/50000], Train Loss: 25216086.0000, Val Loss: 18452038.0000\n",
      "Epoch [381/50000], Train Loss: 25209760.0000, Val Loss: 18446450.0000\n",
      "Epoch [382/50000], Train Loss: 25203442.0000, Val Loss: 18440862.0000\n",
      "Epoch [383/50000], Train Loss: 25197122.0000, Val Loss: 18435282.0000\n",
      "Epoch [384/50000], Train Loss: 25190804.0000, Val Loss: 18429700.0000\n",
      "Epoch [385/50000], Train Loss: 25184494.0000, Val Loss: 18424120.0000\n",
      "Epoch [386/50000], Train Loss: 25178182.0000, Val Loss: 18418538.0000\n",
      "Epoch [387/50000], Train Loss: 25171870.0000, Val Loss: 18412962.0000\n",
      "Epoch [388/50000], Train Loss: 25165564.0000, Val Loss: 18407386.0000\n",
      "Epoch [389/50000], Train Loss: 25159256.0000, Val Loss: 18401814.0000\n",
      "Epoch [390/50000], Train Loss: 25152948.0000, Val Loss: 18396242.0000\n",
      "Epoch [391/50000], Train Loss: 25146644.0000, Val Loss: 18390670.0000\n",
      "Epoch [392/50000], Train Loss: 25140346.0000, Val Loss: 18385100.0000\n",
      "Epoch [393/50000], Train Loss: 25134040.0000, Val Loss: 18379532.0000\n",
      "Epoch [394/50000], Train Loss: 25127742.0000, Val Loss: 18373970.0000\n",
      "Epoch [395/50000], Train Loss: 25121446.0000, Val Loss: 18368404.0000\n",
      "Epoch [396/50000], Train Loss: 25115150.0000, Val Loss: 18362844.0000\n",
      "Epoch [397/50000], Train Loss: 25108860.0000, Val Loss: 18357280.0000\n",
      "Epoch [398/50000], Train Loss: 25102566.0000, Val Loss: 18351720.0000\n",
      "Epoch [399/50000], Train Loss: 25096270.0000, Val Loss: 18346166.0000\n",
      "Epoch [400/50000], Train Loss: 25089988.0000, Val Loss: 18340608.0000\n",
      "Epoch [401/50000], Train Loss: 25083702.0000, Val Loss: 18335054.0000\n",
      "Epoch [402/50000], Train Loss: 25077414.0000, Val Loss: 18329502.0000\n",
      "Epoch [403/50000], Train Loss: 25071128.0000, Val Loss: 18323950.0000\n",
      "Epoch [404/50000], Train Loss: 25064850.0000, Val Loss: 18318400.0000\n",
      "Epoch [405/50000], Train Loss: 25058566.0000, Val Loss: 18312856.0000\n",
      "Epoch [406/50000], Train Loss: 25052292.0000, Val Loss: 18307308.0000\n",
      "Epoch [407/50000], Train Loss: 25046014.0000, Val Loss: 18301762.0000\n",
      "Epoch [408/50000], Train Loss: 25039738.0000, Val Loss: 18296220.0000\n",
      "Epoch [409/50000], Train Loss: 25033466.0000, Val Loss: 18290682.0000\n",
      "Epoch [410/50000], Train Loss: 25027194.0000, Val Loss: 18285138.0000\n",
      "Epoch [411/50000], Train Loss: 25020922.0000, Val Loss: 18279602.0000\n",
      "Epoch [412/50000], Train Loss: 25014656.0000, Val Loss: 18274068.0000\n",
      "Epoch [413/50000], Train Loss: 25008388.0000, Val Loss: 18268526.0000\n",
      "Epoch [414/50000], Train Loss: 25002124.0000, Val Loss: 18262998.0000\n",
      "Epoch [415/50000], Train Loss: 24995858.0000, Val Loss: 18257464.0000\n",
      "Epoch [416/50000], Train Loss: 24989598.0000, Val Loss: 18251934.0000\n",
      "Epoch [417/50000], Train Loss: 24983334.0000, Val Loss: 18246406.0000\n",
      "Epoch [418/50000], Train Loss: 24977080.0000, Val Loss: 18240878.0000\n",
      "Epoch [419/50000], Train Loss: 24970824.0000, Val Loss: 18235356.0000\n",
      "Epoch [420/50000], Train Loss: 24964566.0000, Val Loss: 18229830.0000\n",
      "Epoch [421/50000], Train Loss: 24958314.0000, Val Loss: 18224308.0000\n",
      "Epoch [422/50000], Train Loss: 24952060.0000, Val Loss: 18218786.0000\n",
      "Epoch [423/50000], Train Loss: 24945808.0000, Val Loss: 18213270.0000\n",
      "Epoch [424/50000], Train Loss: 24939560.0000, Val Loss: 18207754.0000\n",
      "Epoch [425/50000], Train Loss: 24933318.0000, Val Loss: 18202236.0000\n",
      "Epoch [426/50000], Train Loss: 24927068.0000, Val Loss: 18196724.0000\n",
      "Epoch [427/50000], Train Loss: 24920830.0000, Val Loss: 18191210.0000\n",
      "Epoch [428/50000], Train Loss: 24914582.0000, Val Loss: 18185700.0000\n",
      "Epoch [429/50000], Train Loss: 24908346.0000, Val Loss: 18180190.0000\n",
      "Epoch [430/50000], Train Loss: 24902108.0000, Val Loss: 18174686.0000\n",
      "Epoch [431/50000], Train Loss: 24895872.0000, Val Loss: 18169178.0000\n",
      "Epoch [432/50000], Train Loss: 24889634.0000, Val Loss: 18163674.0000\n",
      "Epoch [433/50000], Train Loss: 24883400.0000, Val Loss: 18158170.0000\n",
      "Epoch [434/50000], Train Loss: 24877168.0000, Val Loss: 18152670.0000\n",
      "Epoch [435/50000], Train Loss: 24870938.0000, Val Loss: 18147172.0000\n",
      "Epoch [436/50000], Train Loss: 24864714.0000, Val Loss: 18141672.0000\n",
      "Epoch [437/50000], Train Loss: 24858484.0000, Val Loss: 18136178.0000\n",
      "Epoch [438/50000], Train Loss: 24852260.0000, Val Loss: 18130682.0000\n",
      "Epoch [439/50000], Train Loss: 24846038.0000, Val Loss: 18125190.0000\n",
      "Epoch [440/50000], Train Loss: 24839816.0000, Val Loss: 18119698.0000\n",
      "Epoch [441/50000], Train Loss: 24833594.0000, Val Loss: 18114208.0000\n",
      "Epoch [442/50000], Train Loss: 24827376.0000, Val Loss: 18108722.0000\n",
      "Epoch [443/50000], Train Loss: 24821158.0000, Val Loss: 18103234.0000\n",
      "Epoch [444/50000], Train Loss: 24814942.0000, Val Loss: 18097750.0000\n",
      "Epoch [445/50000], Train Loss: 24808730.0000, Val Loss: 18092262.0000\n",
      "Epoch [446/50000], Train Loss: 24802520.0000, Val Loss: 18086782.0000\n",
      "Epoch [447/50000], Train Loss: 24796310.0000, Val Loss: 18081302.0000\n",
      "Epoch [448/50000], Train Loss: 24790100.0000, Val Loss: 18075826.0000\n",
      "Epoch [449/50000], Train Loss: 24783894.0000, Val Loss: 18070346.0000\n",
      "Epoch [450/50000], Train Loss: 24777686.0000, Val Loss: 18064870.0000\n",
      "Epoch [451/50000], Train Loss: 24771486.0000, Val Loss: 18059398.0000\n",
      "Epoch [452/50000], Train Loss: 24765282.0000, Val Loss: 18053928.0000\n",
      "Epoch [453/50000], Train Loss: 24759084.0000, Val Loss: 18048456.0000\n",
      "Epoch [454/50000], Train Loss: 24752882.0000, Val Loss: 18042984.0000\n",
      "Epoch [455/50000], Train Loss: 24746690.0000, Val Loss: 18037518.0000\n",
      "Epoch [456/50000], Train Loss: 24740492.0000, Val Loss: 18032056.0000\n",
      "Epoch [457/50000], Train Loss: 24734302.0000, Val Loss: 18026588.0000\n",
      "Epoch [458/50000], Train Loss: 24728106.0000, Val Loss: 18021126.0000\n",
      "Epoch [459/50000], Train Loss: 24721916.0000, Val Loss: 18015668.0000\n",
      "Epoch [460/50000], Train Loss: 24715728.0000, Val Loss: 18010208.0000\n",
      "Epoch [461/50000], Train Loss: 24709544.0000, Val Loss: 18004748.0000\n",
      "Epoch [462/50000], Train Loss: 24703358.0000, Val Loss: 17999292.0000\n",
      "Epoch [463/50000], Train Loss: 24697172.0000, Val Loss: 17993838.0000\n",
      "Epoch [464/50000], Train Loss: 24690992.0000, Val Loss: 17988386.0000\n",
      "Epoch [465/50000], Train Loss: 24684814.0000, Val Loss: 17982934.0000\n",
      "Epoch [466/50000], Train Loss: 24678634.0000, Val Loss: 17977482.0000\n",
      "Epoch [467/50000], Train Loss: 24672454.0000, Val Loss: 17972036.0000\n",
      "Epoch [468/50000], Train Loss: 24666282.0000, Val Loss: 17966592.0000\n",
      "Epoch [469/50000], Train Loss: 24660112.0000, Val Loss: 17961146.0000\n",
      "Epoch [470/50000], Train Loss: 24653936.0000, Val Loss: 17955702.0000\n",
      "Epoch [471/50000], Train Loss: 24647766.0000, Val Loss: 17950262.0000\n",
      "Epoch [472/50000], Train Loss: 24641602.0000, Val Loss: 17944822.0000\n",
      "Epoch [473/50000], Train Loss: 24635432.0000, Val Loss: 17939384.0000\n",
      "Epoch [474/50000], Train Loss: 24629266.0000, Val Loss: 17933946.0000\n",
      "Epoch [475/50000], Train Loss: 24623106.0000, Val Loss: 17928512.0000\n",
      "Epoch [476/50000], Train Loss: 24616942.0000, Val Loss: 17923076.0000\n",
      "Epoch [477/50000], Train Loss: 24610782.0000, Val Loss: 17917642.0000\n",
      "Epoch [478/50000], Train Loss: 24604620.0000, Val Loss: 17912214.0000\n",
      "Epoch [479/50000], Train Loss: 24598466.0000, Val Loss: 17906782.0000\n",
      "Epoch [480/50000], Train Loss: 24592312.0000, Val Loss: 17901356.0000\n",
      "Epoch [481/50000], Train Loss: 24586154.0000, Val Loss: 17895932.0000\n",
      "Epoch [482/50000], Train Loss: 24580002.0000, Val Loss: 17890506.0000\n",
      "Epoch [483/50000], Train Loss: 24573850.0000, Val Loss: 17885086.0000\n",
      "Epoch [484/50000], Train Loss: 24567704.0000, Val Loss: 17879662.0000\n",
      "Epoch [485/50000], Train Loss: 24561558.0000, Val Loss: 17874246.0000\n",
      "Epoch [486/50000], Train Loss: 24555414.0000, Val Loss: 17868826.0000\n",
      "Epoch [487/50000], Train Loss: 24549268.0000, Val Loss: 17863412.0000\n",
      "Epoch [488/50000], Train Loss: 24543128.0000, Val Loss: 17857994.0000\n",
      "Epoch [489/50000], Train Loss: 24536984.0000, Val Loss: 17852580.0000\n",
      "Epoch [490/50000], Train Loss: 24530848.0000, Val Loss: 17847170.0000\n",
      "Epoch [491/50000], Train Loss: 24524710.0000, Val Loss: 17841760.0000\n",
      "Epoch [492/50000], Train Loss: 24518576.0000, Val Loss: 17836354.0000\n",
      "Epoch [493/50000], Train Loss: 24512444.0000, Val Loss: 17830948.0000\n",
      "Epoch [494/50000], Train Loss: 24506310.0000, Val Loss: 17825542.0000\n",
      "Epoch [495/50000], Train Loss: 24500180.0000, Val Loss: 17820138.0000\n",
      "Epoch [496/50000], Train Loss: 24494048.0000, Val Loss: 17814738.0000\n",
      "Epoch [497/50000], Train Loss: 24487928.0000, Val Loss: 17809336.0000\n",
      "Epoch [498/50000], Train Loss: 24481796.0000, Val Loss: 17803938.0000\n",
      "Epoch [499/50000], Train Loss: 24475674.0000, Val Loss: 17798538.0000\n",
      "Epoch [500/50000], Train Loss: 24469552.0000, Val Loss: 17793146.0000\n",
      "Epoch [501/50000], Train Loss: 24463432.0000, Val Loss: 17787752.0000\n",
      "Epoch [502/50000], Train Loss: 24457314.0000, Val Loss: 17782360.0000\n",
      "Epoch [503/50000], Train Loss: 24451196.0000, Val Loss: 17776966.0000\n",
      "Epoch [504/50000], Train Loss: 24445080.0000, Val Loss: 17771578.0000\n",
      "Epoch [505/50000], Train Loss: 24438964.0000, Val Loss: 17766190.0000\n",
      "Epoch [506/50000], Train Loss: 24432852.0000, Val Loss: 17760804.0000\n",
      "Epoch [507/50000], Train Loss: 24426744.0000, Val Loss: 17755420.0000\n",
      "Epoch [508/50000], Train Loss: 24420632.0000, Val Loss: 17750038.0000\n",
      "Epoch [509/50000], Train Loss: 24414526.0000, Val Loss: 17744658.0000\n",
      "Epoch [510/50000], Train Loss: 24408418.0000, Val Loss: 17739278.0000\n",
      "Epoch [511/50000], Train Loss: 24402318.0000, Val Loss: 17733900.0000\n",
      "Epoch [512/50000], Train Loss: 24396214.0000, Val Loss: 17728524.0000\n",
      "Epoch [513/50000], Train Loss: 24390114.0000, Val Loss: 17723148.0000\n",
      "Epoch [514/50000], Train Loss: 24384016.0000, Val Loss: 17717776.0000\n",
      "Epoch [515/50000], Train Loss: 24377914.0000, Val Loss: 17712402.0000\n",
      "Epoch [516/50000], Train Loss: 24371822.0000, Val Loss: 17707034.0000\n",
      "Epoch [517/50000], Train Loss: 24365726.0000, Val Loss: 17701666.0000\n",
      "Epoch [518/50000], Train Loss: 24359638.0000, Val Loss: 17696298.0000\n",
      "Epoch [519/50000], Train Loss: 24353544.0000, Val Loss: 17690934.0000\n",
      "Epoch [520/50000], Train Loss: 24347452.0000, Val Loss: 17685570.0000\n",
      "Epoch [521/50000], Train Loss: 24341372.0000, Val Loss: 17680208.0000\n",
      "Epoch [522/50000], Train Loss: 24335280.0000, Val Loss: 17674846.0000\n",
      "Epoch [523/50000], Train Loss: 24329196.0000, Val Loss: 17669488.0000\n",
      "Epoch [524/50000], Train Loss: 24323114.0000, Val Loss: 17664130.0000\n",
      "Epoch [525/50000], Train Loss: 24317034.0000, Val Loss: 17658776.0000\n",
      "Epoch [526/50000], Train Loss: 24310952.0000, Val Loss: 17653420.0000\n",
      "Epoch [527/50000], Train Loss: 24304876.0000, Val Loss: 17648068.0000\n",
      "Epoch [528/50000], Train Loss: 24298800.0000, Val Loss: 17642720.0000\n",
      "Epoch [529/50000], Train Loss: 24292724.0000, Val Loss: 17637370.0000\n",
      "Epoch [530/50000], Train Loss: 24286654.0000, Val Loss: 17632020.0000\n",
      "Epoch [531/50000], Train Loss: 24280582.0000, Val Loss: 17626674.0000\n",
      "Epoch [532/50000], Train Loss: 24274512.0000, Val Loss: 17621330.0000\n",
      "Epoch [533/50000], Train Loss: 24268446.0000, Val Loss: 17615986.0000\n",
      "Epoch [534/50000], Train Loss: 24262380.0000, Val Loss: 17610644.0000\n",
      "Epoch [535/50000], Train Loss: 24256312.0000, Val Loss: 17605304.0000\n",
      "Epoch [536/50000], Train Loss: 24250248.0000, Val Loss: 17599964.0000\n",
      "Epoch [537/50000], Train Loss: 24244188.0000, Val Loss: 17594632.0000\n",
      "Epoch [538/50000], Train Loss: 24238130.0000, Val Loss: 17589294.0000\n",
      "Epoch [539/50000], Train Loss: 24232070.0000, Val Loss: 17583960.0000\n",
      "Epoch [540/50000], Train Loss: 24226018.0000, Val Loss: 17578630.0000\n",
      "Epoch [541/50000], Train Loss: 24219960.0000, Val Loss: 17573298.0000\n",
      "Epoch [542/50000], Train Loss: 24213908.0000, Val Loss: 17567966.0000\n",
      "Epoch [543/50000], Train Loss: 24207854.0000, Val Loss: 17562642.0000\n",
      "Epoch [544/50000], Train Loss: 24201806.0000, Val Loss: 17557314.0000\n",
      "Epoch [545/50000], Train Loss: 24195760.0000, Val Loss: 17551990.0000\n",
      "Epoch [546/50000], Train Loss: 24189710.0000, Val Loss: 17546670.0000\n",
      "Epoch [547/50000], Train Loss: 24183666.0000, Val Loss: 17541348.0000\n",
      "Epoch [548/50000], Train Loss: 24177624.0000, Val Loss: 17536030.0000\n",
      "Epoch [549/50000], Train Loss: 24171582.0000, Val Loss: 17530710.0000\n",
      "Epoch [550/50000], Train Loss: 24165542.0000, Val Loss: 17525396.0000\n",
      "Epoch [551/50000], Train Loss: 24159504.0000, Val Loss: 17520078.0000\n",
      "Epoch [552/50000], Train Loss: 24153464.0000, Val Loss: 17514770.0000\n",
      "Epoch [553/50000], Train Loss: 24147432.0000, Val Loss: 17509454.0000\n",
      "Epoch [554/50000], Train Loss: 24141398.0000, Val Loss: 17504144.0000\n",
      "Epoch [555/50000], Train Loss: 24135366.0000, Val Loss: 17498838.0000\n",
      "Epoch [556/50000], Train Loss: 24129338.0000, Val Loss: 17493530.0000\n",
      "Epoch [557/50000], Train Loss: 24123306.0000, Val Loss: 17488222.0000\n",
      "Epoch [558/50000], Train Loss: 24117280.0000, Val Loss: 17482924.0000\n",
      "Epoch [559/50000], Train Loss: 24111256.0000, Val Loss: 17477620.0000\n",
      "Epoch [560/50000], Train Loss: 24105230.0000, Val Loss: 17472316.0000\n",
      "Epoch [561/50000], Train Loss: 24099212.0000, Val Loss: 17467018.0000\n",
      "Epoch [562/50000], Train Loss: 24093188.0000, Val Loss: 17461722.0000\n",
      "Epoch [563/50000], Train Loss: 24087172.0000, Val Loss: 17456426.0000\n",
      "Epoch [564/50000], Train Loss: 24081150.0000, Val Loss: 17451134.0000\n",
      "Epoch [565/50000], Train Loss: 24075136.0000, Val Loss: 17445838.0000\n",
      "Epoch [566/50000], Train Loss: 24069124.0000, Val Loss: 17440548.0000\n",
      "Epoch [567/50000], Train Loss: 24063110.0000, Val Loss: 17435258.0000\n",
      "Epoch [568/50000], Train Loss: 24057102.0000, Val Loss: 17429972.0000\n",
      "Epoch [569/50000], Train Loss: 24051092.0000, Val Loss: 17424684.0000\n",
      "Epoch [570/50000], Train Loss: 24045086.0000, Val Loss: 17419398.0000\n",
      "Epoch [571/50000], Train Loss: 24039078.0000, Val Loss: 17414114.0000\n",
      "Epoch [572/50000], Train Loss: 24033076.0000, Val Loss: 17408830.0000\n",
      "Epoch [573/50000], Train Loss: 24027068.0000, Val Loss: 17403550.0000\n",
      "Epoch [574/50000], Train Loss: 24021070.0000, Val Loss: 17398272.0000\n",
      "Epoch [575/50000], Train Loss: 24015070.0000, Val Loss: 17392994.0000\n",
      "Epoch [576/50000], Train Loss: 24009072.0000, Val Loss: 17387718.0000\n",
      "Epoch [577/50000], Train Loss: 24003074.0000, Val Loss: 17382442.0000\n",
      "Epoch [578/50000], Train Loss: 23997078.0000, Val Loss: 17377172.0000\n",
      "Epoch [579/50000], Train Loss: 23991088.0000, Val Loss: 17371902.0000\n",
      "Epoch [580/50000], Train Loss: 23985096.0000, Val Loss: 17366630.0000\n",
      "Epoch [581/50000], Train Loss: 23979104.0000, Val Loss: 17361364.0000\n",
      "Epoch [582/50000], Train Loss: 23973116.0000, Val Loss: 17356096.0000\n",
      "Epoch [583/50000], Train Loss: 23967130.0000, Val Loss: 17350834.0000\n",
      "Epoch [584/50000], Train Loss: 23961146.0000, Val Loss: 17345568.0000\n",
      "Epoch [585/50000], Train Loss: 23955162.0000, Val Loss: 17340308.0000\n",
      "Epoch [586/50000], Train Loss: 23949182.0000, Val Loss: 17335048.0000\n",
      "Epoch [587/50000], Train Loss: 23943200.0000, Val Loss: 17329790.0000\n",
      "Epoch [588/50000], Train Loss: 23937222.0000, Val Loss: 17324530.0000\n",
      "Epoch [589/50000], Train Loss: 23931246.0000, Val Loss: 17319274.0000\n",
      "Epoch [590/50000], Train Loss: 23925268.0000, Val Loss: 17314020.0000\n",
      "Epoch [591/50000], Train Loss: 23919298.0000, Val Loss: 17308770.0000\n",
      "Epoch [592/50000], Train Loss: 23913322.0000, Val Loss: 17303520.0000\n",
      "Epoch [593/50000], Train Loss: 23907354.0000, Val Loss: 17298270.0000\n",
      "Epoch [594/50000], Train Loss: 23901384.0000, Val Loss: 17293022.0000\n",
      "Epoch [595/50000], Train Loss: 23895418.0000, Val Loss: 17287774.0000\n",
      "Epoch [596/50000], Train Loss: 23889452.0000, Val Loss: 17282532.0000\n",
      "Epoch [597/50000], Train Loss: 23883486.0000, Val Loss: 17277288.0000\n",
      "Epoch [598/50000], Train Loss: 23877526.0000, Val Loss: 17272044.0000\n",
      "Epoch [599/50000], Train Loss: 23871562.0000, Val Loss: 17266806.0000\n",
      "Epoch [600/50000], Train Loss: 23865606.0000, Val Loss: 17261570.0000\n",
      "Epoch [601/50000], Train Loss: 23859650.0000, Val Loss: 17256332.0000\n",
      "Epoch [602/50000], Train Loss: 23853692.0000, Val Loss: 17251096.0000\n",
      "Epoch [603/50000], Train Loss: 23847736.0000, Val Loss: 17245864.0000\n",
      "Epoch [604/50000], Train Loss: 23841788.0000, Val Loss: 17240634.0000\n",
      "Epoch [605/50000], Train Loss: 23835836.0000, Val Loss: 17235400.0000\n",
      "Epoch [606/50000], Train Loss: 23829886.0000, Val Loss: 17230172.0000\n",
      "Epoch [607/50000], Train Loss: 23823940.0000, Val Loss: 17224944.0000\n",
      "Epoch [608/50000], Train Loss: 23817992.0000, Val Loss: 17219718.0000\n",
      "Epoch [609/50000], Train Loss: 23812046.0000, Val Loss: 17214494.0000\n",
      "Epoch [610/50000], Train Loss: 23806102.0000, Val Loss: 17209270.0000\n",
      "Epoch [611/50000], Train Loss: 23800160.0000, Val Loss: 17204050.0000\n",
      "Epoch [612/50000], Train Loss: 23794224.0000, Val Loss: 17198830.0000\n",
      "Epoch [613/50000], Train Loss: 23788286.0000, Val Loss: 17193610.0000\n",
      "Epoch [614/50000], Train Loss: 23782350.0000, Val Loss: 17188390.0000\n",
      "Epoch [615/50000], Train Loss: 23776408.0000, Val Loss: 17183176.0000\n",
      "Epoch [616/50000], Train Loss: 23770476.0000, Val Loss: 17177962.0000\n",
      "Epoch [617/50000], Train Loss: 23764546.0000, Val Loss: 17172752.0000\n",
      "Epoch [618/50000], Train Loss: 23758614.0000, Val Loss: 17167540.0000\n",
      "Epoch [619/50000], Train Loss: 23752686.0000, Val Loss: 17162332.0000\n",
      "Epoch [620/50000], Train Loss: 23746762.0000, Val Loss: 17157124.0000\n",
      "Epoch [621/50000], Train Loss: 23740832.0000, Val Loss: 17151918.0000\n",
      "Epoch [622/50000], Train Loss: 23734908.0000, Val Loss: 17146714.0000\n",
      "Epoch [623/50000], Train Loss: 23728988.0000, Val Loss: 17141514.0000\n",
      "Epoch [624/50000], Train Loss: 23723068.0000, Val Loss: 17136312.0000\n",
      "Epoch [625/50000], Train Loss: 23717150.0000, Val Loss: 17131112.0000\n",
      "Epoch [626/50000], Train Loss: 23711232.0000, Val Loss: 17125912.0000\n",
      "Epoch [627/50000], Train Loss: 23705312.0000, Val Loss: 17120714.0000\n",
      "Epoch [628/50000], Train Loss: 23699400.0000, Val Loss: 17115520.0000\n",
      "Epoch [629/50000], Train Loss: 23693490.0000, Val Loss: 17110328.0000\n",
      "Epoch [630/50000], Train Loss: 23687576.0000, Val Loss: 17105134.0000\n",
      "Epoch [631/50000], Train Loss: 23681666.0000, Val Loss: 17099944.0000\n",
      "Epoch [632/50000], Train Loss: 23675760.0000, Val Loss: 17094754.0000\n",
      "Epoch [633/50000], Train Loss: 23669852.0000, Val Loss: 17089568.0000\n",
      "Epoch [634/50000], Train Loss: 23663946.0000, Val Loss: 17084384.0000\n",
      "Epoch [635/50000], Train Loss: 23658046.0000, Val Loss: 17079200.0000\n",
      "Epoch [636/50000], Train Loss: 23652148.0000, Val Loss: 17074016.0000\n",
      "Epoch [637/50000], Train Loss: 23646246.0000, Val Loss: 17068836.0000\n",
      "Epoch [638/50000], Train Loss: 23640346.0000, Val Loss: 17063654.0000\n",
      "Epoch [639/50000], Train Loss: 23634452.0000, Val Loss: 17058476.0000\n",
      "Epoch [640/50000], Train Loss: 23628558.0000, Val Loss: 17053302.0000\n",
      "Epoch [641/50000], Train Loss: 23622660.0000, Val Loss: 17048126.0000\n",
      "Epoch [642/50000], Train Loss: 23616770.0000, Val Loss: 17042948.0000\n",
      "Epoch [643/50000], Train Loss: 23610876.0000, Val Loss: 17037778.0000\n",
      "Epoch [644/50000], Train Loss: 23604988.0000, Val Loss: 17032606.0000\n",
      "Epoch [645/50000], Train Loss: 23599102.0000, Val Loss: 17027438.0000\n",
      "Epoch [646/50000], Train Loss: 23593216.0000, Val Loss: 17022268.0000\n",
      "Epoch [647/50000], Train Loss: 23587332.0000, Val Loss: 17017104.0000\n",
      "Epoch [648/50000], Train Loss: 23581448.0000, Val Loss: 17011942.0000\n",
      "Epoch [649/50000], Train Loss: 23575570.0000, Val Loss: 17006778.0000\n",
      "Epoch [650/50000], Train Loss: 23569690.0000, Val Loss: 17001614.0000\n",
      "Epoch [651/50000], Train Loss: 23563812.0000, Val Loss: 16996458.0000\n",
      "Epoch [652/50000], Train Loss: 23557938.0000, Val Loss: 16991298.0000\n",
      "Epoch [653/50000], Train Loss: 23552060.0000, Val Loss: 16986142.0000\n",
      "Epoch [654/50000], Train Loss: 23546192.0000, Val Loss: 16980988.0000\n",
      "Epoch [655/50000], Train Loss: 23540322.0000, Val Loss: 16975832.0000\n",
      "Epoch [656/50000], Train Loss: 23534448.0000, Val Loss: 16970680.0000\n",
      "Epoch [657/50000], Train Loss: 23528580.0000, Val Loss: 16965528.0000\n",
      "Epoch [658/50000], Train Loss: 23522718.0000, Val Loss: 16960380.0000\n",
      "Epoch [659/50000], Train Loss: 23516848.0000, Val Loss: 16955236.0000\n",
      "Epoch [660/50000], Train Loss: 23510988.0000, Val Loss: 16950088.0000\n",
      "Epoch [661/50000], Train Loss: 23505130.0000, Val Loss: 16944946.0000\n",
      "Epoch [662/50000], Train Loss: 23499264.0000, Val Loss: 16939800.0000\n",
      "Epoch [663/50000], Train Loss: 23493406.0000, Val Loss: 16934660.0000\n",
      "Epoch [664/50000], Train Loss: 23487550.0000, Val Loss: 16929518.0000\n",
      "Epoch [665/50000], Train Loss: 23481694.0000, Val Loss: 16924380.0000\n",
      "Epoch [666/50000], Train Loss: 23475840.0000, Val Loss: 16919242.0000\n",
      "Epoch [667/50000], Train Loss: 23469988.0000, Val Loss: 16914106.0000\n",
      "Epoch [668/50000], Train Loss: 23464136.0000, Val Loss: 16908972.0000\n",
      "Epoch [669/50000], Train Loss: 23458286.0000, Val Loss: 16903842.0000\n",
      "Epoch [670/50000], Train Loss: 23452436.0000, Val Loss: 16898710.0000\n",
      "Epoch [671/50000], Train Loss: 23446594.0000, Val Loss: 16893578.0000\n",
      "Epoch [672/50000], Train Loss: 23440748.0000, Val Loss: 16888452.0000\n",
      "Epoch [673/50000], Train Loss: 23434906.0000, Val Loss: 16883326.0000\n",
      "Epoch [674/50000], Train Loss: 23429066.0000, Val Loss: 16878204.0000\n",
      "Epoch [675/50000], Train Loss: 23423228.0000, Val Loss: 16873076.0000\n",
      "Epoch [676/50000], Train Loss: 23417386.0000, Val Loss: 16867958.0000\n",
      "Epoch [677/50000], Train Loss: 23411550.0000, Val Loss: 16862838.0000\n",
      "Epoch [678/50000], Train Loss: 23405714.0000, Val Loss: 16857720.0000\n",
      "Epoch [679/50000], Train Loss: 23399884.0000, Val Loss: 16852602.0000\n",
      "Epoch [680/50000], Train Loss: 23394052.0000, Val Loss: 16847484.0000\n",
      "Epoch [681/50000], Train Loss: 23388220.0000, Val Loss: 16842374.0000\n",
      "Epoch [682/50000], Train Loss: 23382396.0000, Val Loss: 16837258.0000\n",
      "Epoch [683/50000], Train Loss: 23376564.0000, Val Loss: 16832148.0000\n",
      "Epoch [684/50000], Train Loss: 23370740.0000, Val Loss: 16827042.0000\n",
      "Epoch [685/50000], Train Loss: 23364920.0000, Val Loss: 16821932.0000\n",
      "Epoch [686/50000], Train Loss: 23359098.0000, Val Loss: 16816826.0000\n",
      "Epoch [687/50000], Train Loss: 23353276.0000, Val Loss: 16811718.0000\n",
      "Epoch [688/50000], Train Loss: 23347458.0000, Val Loss: 16806618.0000\n",
      "Epoch [689/50000], Train Loss: 23341640.0000, Val Loss: 16801516.0000\n",
      "Epoch [690/50000], Train Loss: 23335824.0000, Val Loss: 16796414.0000\n",
      "Epoch [691/50000], Train Loss: 23330008.0000, Val Loss: 16791314.0000\n",
      "Epoch [692/50000], Train Loss: 23324192.0000, Val Loss: 16786220.0000\n",
      "Epoch [693/50000], Train Loss: 23318382.0000, Val Loss: 16781118.0000\n",
      "Epoch [694/50000], Train Loss: 23312570.0000, Val Loss: 16776026.0000\n",
      "Epoch [695/50000], Train Loss: 23306760.0000, Val Loss: 16770933.0000\n",
      "Epoch [696/50000], Train Loss: 23300958.0000, Val Loss: 16765843.0000\n",
      "Epoch [697/50000], Train Loss: 23295154.0000, Val Loss: 16760751.0000\n",
      "Epoch [698/50000], Train Loss: 23289350.0000, Val Loss: 16755665.0000\n",
      "Epoch [699/50000], Train Loss: 23283550.0000, Val Loss: 16750575.0000\n",
      "Epoch [700/50000], Train Loss: 23277748.0000, Val Loss: 16745494.0000\n",
      "Epoch [701/50000], Train Loss: 23271950.0000, Val Loss: 16740410.0000\n",
      "Epoch [702/50000], Train Loss: 23266156.0000, Val Loss: 16735328.0000\n",
      "Epoch [703/50000], Train Loss: 23260358.0000, Val Loss: 16730244.0000\n",
      "Epoch [704/50000], Train Loss: 23254564.0000, Val Loss: 16725170.0000\n",
      "Epoch [705/50000], Train Loss: 23248774.0000, Val Loss: 16720090.0000\n",
      "Epoch [706/50000], Train Loss: 23242982.0000, Val Loss: 16715015.0000\n",
      "Epoch [707/50000], Train Loss: 23237196.0000, Val Loss: 16709938.0000\n",
      "Epoch [708/50000], Train Loss: 23231402.0000, Val Loss: 16704867.0000\n",
      "Epoch [709/50000], Train Loss: 23225618.0000, Val Loss: 16699796.0000\n",
      "Epoch [710/50000], Train Loss: 23219838.0000, Val Loss: 16694723.0000\n",
      "Epoch [711/50000], Train Loss: 23214048.0000, Val Loss: 16689655.0000\n",
      "Epoch [712/50000], Train Loss: 23208268.0000, Val Loss: 16684590.0000\n",
      "Epoch [713/50000], Train Loss: 23202488.0000, Val Loss: 16679523.0000\n",
      "Epoch [714/50000], Train Loss: 23196712.0000, Val Loss: 16674460.0000\n",
      "Epoch [715/50000], Train Loss: 23190936.0000, Val Loss: 16669396.0000\n",
      "Epoch [716/50000], Train Loss: 23185160.0000, Val Loss: 16664335.0000\n",
      "Epoch [717/50000], Train Loss: 23179388.0000, Val Loss: 16659275.0000\n",
      "Epoch [718/50000], Train Loss: 23173614.0000, Val Loss: 16654219.0000\n",
      "Epoch [719/50000], Train Loss: 23167842.0000, Val Loss: 16649161.0000\n",
      "Epoch [720/50000], Train Loss: 23162074.0000, Val Loss: 16644106.0000\n",
      "Epoch [721/50000], Train Loss: 23156310.0000, Val Loss: 16639053.0000\n",
      "Epoch [722/50000], Train Loss: 23150542.0000, Val Loss: 16634000.0000\n",
      "Epoch [723/50000], Train Loss: 23144780.0000, Val Loss: 16628953.0000\n",
      "Epoch [724/50000], Train Loss: 23139018.0000, Val Loss: 16623904.0000\n",
      "Epoch [725/50000], Train Loss: 23133256.0000, Val Loss: 16618855.0000\n",
      "Epoch [726/50000], Train Loss: 23127498.0000, Val Loss: 16613807.0000\n",
      "Epoch [727/50000], Train Loss: 23121738.0000, Val Loss: 16608764.0000\n",
      "Epoch [728/50000], Train Loss: 23115984.0000, Val Loss: 16603718.0000\n",
      "Epoch [729/50000], Train Loss: 23110228.0000, Val Loss: 16598677.0000\n",
      "Epoch [730/50000], Train Loss: 23104474.0000, Val Loss: 16593638.0000\n",
      "Epoch [731/50000], Train Loss: 23098720.0000, Val Loss: 16588601.0000\n",
      "Epoch [732/50000], Train Loss: 23092972.0000, Val Loss: 16583564.0000\n",
      "Epoch [733/50000], Train Loss: 23087220.0000, Val Loss: 16578530.0000\n",
      "Epoch [734/50000], Train Loss: 23081478.0000, Val Loss: 16573497.0000\n",
      "Epoch [735/50000], Train Loss: 23075734.0000, Val Loss: 16568463.0000\n",
      "Epoch [736/50000], Train Loss: 23069988.0000, Val Loss: 16563429.0000\n",
      "Epoch [737/50000], Train Loss: 23064244.0000, Val Loss: 16558400.0000\n",
      "Epoch [738/50000], Train Loss: 23058504.0000, Val Loss: 16553374.0000\n",
      "Epoch [739/50000], Train Loss: 23052764.0000, Val Loss: 16548348.0000\n",
      "Epoch [740/50000], Train Loss: 23047032.0000, Val Loss: 16543319.0000\n",
      "Epoch [741/50000], Train Loss: 23041288.0000, Val Loss: 16538298.0000\n",
      "Epoch [742/50000], Train Loss: 23035556.0000, Val Loss: 16533274.0000\n",
      "Epoch [743/50000], Train Loss: 23029822.0000, Val Loss: 16528257.0000\n",
      "Epoch [744/50000], Train Loss: 23024092.0000, Val Loss: 16523237.0000\n",
      "Epoch [745/50000], Train Loss: 23018362.0000, Val Loss: 16518221.0000\n",
      "Epoch [746/50000], Train Loss: 23012634.0000, Val Loss: 16513204.0000\n",
      "Epoch [747/50000], Train Loss: 23006908.0000, Val Loss: 16508188.0000\n",
      "Epoch [748/50000], Train Loss: 23001180.0000, Val Loss: 16503179.0000\n",
      "Epoch [749/50000], Train Loss: 22995460.0000, Val Loss: 16498164.0000\n",
      "Epoch [750/50000], Train Loss: 22989738.0000, Val Loss: 16493154.0000\n",
      "Epoch [751/50000], Train Loss: 22984016.0000, Val Loss: 16488145.0000\n",
      "Epoch [752/50000], Train Loss: 22978296.0000, Val Loss: 16483139.0000\n",
      "Epoch [753/50000], Train Loss: 22972580.0000, Val Loss: 16478134.0000\n",
      "Epoch [754/50000], Train Loss: 22966864.0000, Val Loss: 16473129.0000\n",
      "Epoch [755/50000], Train Loss: 22961148.0000, Val Loss: 16468128.0000\n",
      "Epoch [756/50000], Train Loss: 22955436.0000, Val Loss: 16463126.0000\n",
      "Epoch [757/50000], Train Loss: 22949726.0000, Val Loss: 16458128.0000\n",
      "Epoch [758/50000], Train Loss: 22944016.0000, Val Loss: 16453131.0000\n",
      "Epoch [759/50000], Train Loss: 22938310.0000, Val Loss: 16448133.0000\n",
      "Epoch [760/50000], Train Loss: 22932600.0000, Val Loss: 16443136.0000\n",
      "Epoch [761/50000], Train Loss: 22926896.0000, Val Loss: 16438144.0000\n",
      "Epoch [762/50000], Train Loss: 22921194.0000, Val Loss: 16433152.0000\n",
      "Epoch [763/50000], Train Loss: 22915492.0000, Val Loss: 16428160.0000\n",
      "Epoch [764/50000], Train Loss: 22909790.0000, Val Loss: 16423170.0000\n",
      "Epoch [765/50000], Train Loss: 22904090.0000, Val Loss: 16418186.0000\n",
      "Epoch [766/50000], Train Loss: 22898396.0000, Val Loss: 16413199.0000\n",
      "Epoch [767/50000], Train Loss: 22892698.0000, Val Loss: 16408212.0000\n",
      "Epoch [768/50000], Train Loss: 22887002.0000, Val Loss: 16403231.0000\n",
      "Epoch [769/50000], Train Loss: 22881306.0000, Val Loss: 16398250.0000\n",
      "Epoch [770/50000], Train Loss: 22875622.0000, Val Loss: 16393270.0000\n",
      "Epoch [771/50000], Train Loss: 22869936.0000, Val Loss: 16388292.0000\n",
      "Epoch [772/50000], Train Loss: 22864244.0000, Val Loss: 16383311.0000\n",
      "Epoch [773/50000], Train Loss: 22858556.0000, Val Loss: 16378338.0000\n",
      "Epoch [774/50000], Train Loss: 22852876.0000, Val Loss: 16373362.0000\n",
      "Epoch [775/50000], Train Loss: 22847190.0000, Val Loss: 16368390.0000\n",
      "Epoch [776/50000], Train Loss: 22841506.0000, Val Loss: 16363419.0000\n",
      "Epoch [777/50000], Train Loss: 22835826.0000, Val Loss: 16358450.0000\n",
      "Epoch [778/50000], Train Loss: 22830150.0000, Val Loss: 16353483.0000\n",
      "Epoch [779/50000], Train Loss: 22824470.0000, Val Loss: 16348512.0000\n",
      "Epoch [780/50000], Train Loss: 22818794.0000, Val Loss: 16343548.0000\n",
      "Epoch [781/50000], Train Loss: 22813116.0000, Val Loss: 16338583.0000\n",
      "Epoch [782/50000], Train Loss: 22807444.0000, Val Loss: 16333621.0000\n",
      "Epoch [783/50000], Train Loss: 22801774.0000, Val Loss: 16328663.0000\n",
      "Epoch [784/50000], Train Loss: 22796108.0000, Val Loss: 16323705.0000\n",
      "Epoch [785/50000], Train Loss: 22790442.0000, Val Loss: 16318746.0000\n",
      "Epoch [786/50000], Train Loss: 22784772.0000, Val Loss: 16313789.0000\n",
      "Epoch [787/50000], Train Loss: 22779112.0000, Val Loss: 16308835.0000\n",
      "Epoch [788/50000], Train Loss: 22773448.0000, Val Loss: 16303878.0000\n",
      "Epoch [789/50000], Train Loss: 22767780.0000, Val Loss: 16298927.0000\n",
      "Epoch [790/50000], Train Loss: 22762124.0000, Val Loss: 16293977.0000\n",
      "Epoch [791/50000], Train Loss: 22756464.0000, Val Loss: 16289029.0000\n",
      "Epoch [792/50000], Train Loss: 22750808.0000, Val Loss: 16284079.0000\n",
      "Epoch [793/50000], Train Loss: 22745148.0000, Val Loss: 16279134.0000\n",
      "Epoch [794/50000], Train Loss: 22739496.0000, Val Loss: 16274191.0000\n",
      "Epoch [795/50000], Train Loss: 22733842.0000, Val Loss: 16269248.0000\n",
      "Epoch [796/50000], Train Loss: 22728192.0000, Val Loss: 16264304.0000\n",
      "Epoch [797/50000], Train Loss: 22722540.0000, Val Loss: 16259366.0000\n",
      "Epoch [798/50000], Train Loss: 22716896.0000, Val Loss: 16254427.0000\n",
      "Epoch [799/50000], Train Loss: 22711248.0000, Val Loss: 16249490.0000\n",
      "Epoch [800/50000], Train Loss: 22705604.0000, Val Loss: 16244556.0000\n",
      "Epoch [801/50000], Train Loss: 22699962.0000, Val Loss: 16239618.0000\n",
      "Epoch [802/50000], Train Loss: 22694314.0000, Val Loss: 16234687.0000\n",
      "Epoch [803/50000], Train Loss: 22688676.0000, Val Loss: 16229757.0000\n",
      "Epoch [804/50000], Train Loss: 22683040.0000, Val Loss: 16224830.0000\n",
      "Epoch [805/50000], Train Loss: 22677406.0000, Val Loss: 16219900.0000\n",
      "Epoch [806/50000], Train Loss: 22671766.0000, Val Loss: 16214970.0000\n",
      "Epoch [807/50000], Train Loss: 22666130.0000, Val Loss: 16210046.0000\n",
      "Epoch [808/50000], Train Loss: 22660502.0000, Val Loss: 16205122.0000\n",
      "Epoch [809/50000], Train Loss: 22654868.0000, Val Loss: 16200199.0000\n",
      "Epoch [810/50000], Train Loss: 22649236.0000, Val Loss: 16195279.0000\n",
      "Epoch [811/50000], Train Loss: 22643610.0000, Val Loss: 16190361.0000\n",
      "Epoch [812/50000], Train Loss: 22637986.0000, Val Loss: 16185444.0000\n",
      "Epoch [813/50000], Train Loss: 22632358.0000, Val Loss: 16180527.0000\n",
      "Epoch [814/50000], Train Loss: 22626738.0000, Val Loss: 16175613.0000\n",
      "Epoch [815/50000], Train Loss: 22621112.0000, Val Loss: 16170699.0000\n",
      "Epoch [816/50000], Train Loss: 22615494.0000, Val Loss: 16165787.0000\n",
      "Epoch [817/50000], Train Loss: 22609878.0000, Val Loss: 16160878.0000\n",
      "Epoch [818/50000], Train Loss: 22604258.0000, Val Loss: 16155967.0000\n",
      "Epoch [819/50000], Train Loss: 22598644.0000, Val Loss: 16151059.0000\n",
      "Epoch [820/50000], Train Loss: 22593028.0000, Val Loss: 16146151.0000\n",
      "Epoch [821/50000], Train Loss: 22587416.0000, Val Loss: 16141250.0000\n",
      "Epoch [822/50000], Train Loss: 22581804.0000, Val Loss: 16136347.0000\n",
      "Epoch [823/50000], Train Loss: 22576196.0000, Val Loss: 16131443.0000\n",
      "Epoch [824/50000], Train Loss: 22570586.0000, Val Loss: 16126542.0000\n",
      "Epoch [825/50000], Train Loss: 22564980.0000, Val Loss: 16121646.0000\n",
      "Epoch [826/50000], Train Loss: 22559376.0000, Val Loss: 16116748.0000\n",
      "Epoch [827/50000], Train Loss: 22553772.0000, Val Loss: 16111852.0000\n",
      "Epoch [828/50000], Train Loss: 22548172.0000, Val Loss: 16106957.0000\n",
      "Epoch [829/50000], Train Loss: 22542572.0000, Val Loss: 16102064.0000\n",
      "Epoch [830/50000], Train Loss: 22536972.0000, Val Loss: 16097173.0000\n",
      "Epoch [831/50000], Train Loss: 22531372.0000, Val Loss: 16092284.0000\n",
      "Epoch [832/50000], Train Loss: 22525776.0000, Val Loss: 16087396.0000\n",
      "Epoch [833/50000], Train Loss: 22520180.0000, Val Loss: 16082507.0000\n",
      "Epoch [834/50000], Train Loss: 22514586.0000, Val Loss: 16077621.0000\n",
      "Epoch [835/50000], Train Loss: 22508996.0000, Val Loss: 16072740.0000\n",
      "Epoch [836/50000], Train Loss: 22503410.0000, Val Loss: 16067857.0000\n",
      "Epoch [837/50000], Train Loss: 22497822.0000, Val Loss: 16062972.0000\n",
      "Epoch [838/50000], Train Loss: 22492232.0000, Val Loss: 16058097.0000\n",
      "Epoch [839/50000], Train Loss: 22486650.0000, Val Loss: 16053214.0000\n",
      "Epoch [840/50000], Train Loss: 22481064.0000, Val Loss: 16048341.0000\n",
      "Epoch [841/50000], Train Loss: 22475482.0000, Val Loss: 16043462.0000\n",
      "Epoch [842/50000], Train Loss: 22469900.0000, Val Loss: 16038590.0000\n",
      "Epoch [843/50000], Train Loss: 22464322.0000, Val Loss: 16033719.0000\n",
      "Epoch [844/50000], Train Loss: 22458746.0000, Val Loss: 16028850.0000\n",
      "Epoch [845/50000], Train Loss: 22453170.0000, Val Loss: 16023977.0000\n",
      "Epoch [846/50000], Train Loss: 22447592.0000, Val Loss: 16019113.0000\n",
      "Epoch [847/50000], Train Loss: 22442022.0000, Val Loss: 16014244.0000\n",
      "Epoch [848/50000], Train Loss: 22436454.0000, Val Loss: 16009377.0000\n",
      "Epoch [849/50000], Train Loss: 22430880.0000, Val Loss: 16004515.0000\n",
      "Epoch [850/50000], Train Loss: 22425312.0000, Val Loss: 15999652.0000\n",
      "Epoch [851/50000], Train Loss: 22419744.0000, Val Loss: 15994795.0000\n",
      "Epoch [852/50000], Train Loss: 22414178.0000, Val Loss: 15989934.0000\n",
      "Epoch [853/50000], Train Loss: 22408616.0000, Val Loss: 15985075.0000\n",
      "Epoch [854/50000], Train Loss: 22403052.0000, Val Loss: 15980218.0000\n",
      "Epoch [855/50000], Train Loss: 22397484.0000, Val Loss: 15975365.0000\n",
      "Epoch [856/50000], Train Loss: 22391930.0000, Val Loss: 15970511.0000\n",
      "Epoch [857/50000], Train Loss: 22386372.0000, Val Loss: 15965660.0000\n",
      "Epoch [858/50000], Train Loss: 22380816.0000, Val Loss: 15960811.0000\n",
      "Epoch [859/50000], Train Loss: 22375264.0000, Val Loss: 15955959.0000\n",
      "Epoch [860/50000], Train Loss: 22369706.0000, Val Loss: 15951113.0000\n",
      "Epoch [861/50000], Train Loss: 22364158.0000, Val Loss: 15946267.0000\n",
      "Epoch [862/50000], Train Loss: 22358606.0000, Val Loss: 15941423.0000\n",
      "Epoch [863/50000], Train Loss: 22353058.0000, Val Loss: 15936577.0000\n",
      "Epoch [864/50000], Train Loss: 22347506.0000, Val Loss: 15931738.0000\n",
      "Epoch [865/50000], Train Loss: 22341962.0000, Val Loss: 15926896.0000\n",
      "Epoch [866/50000], Train Loss: 22336418.0000, Val Loss: 15922059.0000\n",
      "Epoch [867/50000], Train Loss: 22330878.0000, Val Loss: 15917221.0000\n",
      "Epoch [868/50000], Train Loss: 22325336.0000, Val Loss: 15912384.0000\n",
      "Epoch [869/50000], Train Loss: 22319796.0000, Val Loss: 15907550.0000\n",
      "Epoch [870/50000], Train Loss: 22314258.0000, Val Loss: 15902720.0000\n",
      "Epoch [871/50000], Train Loss: 22308724.0000, Val Loss: 15897886.0000\n",
      "Epoch [872/50000], Train Loss: 22303184.0000, Val Loss: 15893057.0000\n",
      "Epoch [873/50000], Train Loss: 22297650.0000, Val Loss: 15888229.0000\n",
      "Epoch [874/50000], Train Loss: 22292120.0000, Val Loss: 15883405.0000\n",
      "Epoch [875/50000], Train Loss: 22286594.0000, Val Loss: 15878574.0000\n",
      "Epoch [876/50000], Train Loss: 22281062.0000, Val Loss: 15873755.0000\n",
      "Epoch [877/50000], Train Loss: 22275534.0000, Val Loss: 15868929.0000\n",
      "Epoch [878/50000], Train Loss: 22270008.0000, Val Loss: 15864108.0000\n",
      "Epoch [879/50000], Train Loss: 22264482.0000, Val Loss: 15859286.0000\n",
      "Epoch [880/50000], Train Loss: 22258958.0000, Val Loss: 15854470.0000\n",
      "Epoch [881/50000], Train Loss: 22253436.0000, Val Loss: 15849652.0000\n",
      "Epoch [882/50000], Train Loss: 22247914.0000, Val Loss: 15844837.0000\n",
      "Epoch [883/50000], Train Loss: 22242396.0000, Val Loss: 15840023.0000\n",
      "Epoch [884/50000], Train Loss: 22236880.0000, Val Loss: 15835210.0000\n",
      "Epoch [885/50000], Train Loss: 22231364.0000, Val Loss: 15830398.0000\n",
      "Epoch [886/50000], Train Loss: 22225850.0000, Val Loss: 15825591.0000\n",
      "Epoch [887/50000], Train Loss: 22220342.0000, Val Loss: 15820782.0000\n",
      "Epoch [888/50000], Train Loss: 22214826.0000, Val Loss: 15815977.0000\n",
      "Epoch [889/50000], Train Loss: 22209318.0000, Val Loss: 15811170.0000\n",
      "Epoch [890/50000], Train Loss: 22203810.0000, Val Loss: 15806366.0000\n",
      "Epoch [891/50000], Train Loss: 22198304.0000, Val Loss: 15801566.0000\n",
      "Epoch [892/50000], Train Loss: 22192800.0000, Val Loss: 15796763.0000\n",
      "Epoch [893/50000], Train Loss: 22187296.0000, Val Loss: 15791962.0000\n",
      "Epoch [894/50000], Train Loss: 22181792.0000, Val Loss: 15787167.0000\n",
      "Epoch [895/50000], Train Loss: 22176292.0000, Val Loss: 15782368.0000\n",
      "Epoch [896/50000], Train Loss: 22170792.0000, Val Loss: 15777573.0000\n",
      "Epoch [897/50000], Train Loss: 22165296.0000, Val Loss: 15772779.0000\n",
      "Epoch [898/50000], Train Loss: 22159798.0000, Val Loss: 15767987.0000\n",
      "Epoch [899/50000], Train Loss: 22154302.0000, Val Loss: 15763196.0000\n",
      "Epoch [900/50000], Train Loss: 22148810.0000, Val Loss: 15758409.0000\n",
      "Epoch [901/50000], Train Loss: 22143320.0000, Val Loss: 15753616.0000\n",
      "Epoch [902/50000], Train Loss: 22137826.0000, Val Loss: 15748834.0000\n",
      "Epoch [903/50000], Train Loss: 22132340.0000, Val Loss: 15744049.0000\n",
      "Epoch [904/50000], Train Loss: 22126854.0000, Val Loss: 15739264.0000\n",
      "Epoch [905/50000], Train Loss: 22121366.0000, Val Loss: 15734482.0000\n",
      "Epoch [906/50000], Train Loss: 22115884.0000, Val Loss: 15729700.0000\n",
      "Epoch [907/50000], Train Loss: 22110398.0000, Val Loss: 15724919.0000\n",
      "Epoch [908/50000], Train Loss: 22104920.0000, Val Loss: 15720144.0000\n",
      "Epoch [909/50000], Train Loss: 22099440.0000, Val Loss: 15715366.0000\n",
      "Epoch [910/50000], Train Loss: 22093964.0000, Val Loss: 15710592.0000\n",
      "Epoch [911/50000], Train Loss: 22088484.0000, Val Loss: 15705819.0000\n",
      "Epoch [912/50000], Train Loss: 22083012.0000, Val Loss: 15701049.0000\n",
      "Epoch [913/50000], Train Loss: 22077538.0000, Val Loss: 15696276.0000\n",
      "Epoch [914/50000], Train Loss: 22072066.0000, Val Loss: 15691507.0000\n",
      "Epoch [915/50000], Train Loss: 22066596.0000, Val Loss: 15686737.0000\n",
      "Epoch [916/50000], Train Loss: 22061126.0000, Val Loss: 15681971.0000\n",
      "Epoch [917/50000], Train Loss: 22055656.0000, Val Loss: 15677205.0000\n",
      "Epoch [918/50000], Train Loss: 22050188.0000, Val Loss: 15672445.0000\n",
      "Epoch [919/50000], Train Loss: 22044726.0000, Val Loss: 15667684.0000\n",
      "Epoch [920/50000], Train Loss: 22039264.0000, Val Loss: 15662921.0000\n",
      "Epoch [921/50000], Train Loss: 22033798.0000, Val Loss: 15658162.0000\n",
      "Epoch [922/50000], Train Loss: 22028342.0000, Val Loss: 15653405.0000\n",
      "Epoch [923/50000], Train Loss: 22022880.0000, Val Loss: 15648649.0000\n",
      "Epoch [924/50000], Train Loss: 22017424.0000, Val Loss: 15643893.0000\n",
      "Epoch [925/50000], Train Loss: 22011970.0000, Val Loss: 15639139.0000\n",
      "Epoch [926/50000], Train Loss: 22006514.0000, Val Loss: 15634391.0000\n",
      "Epoch [927/50000], Train Loss: 22001064.0000, Val Loss: 15629637.0000\n",
      "Epoch [928/50000], Train Loss: 21995610.0000, Val Loss: 15624891.0000\n",
      "Epoch [929/50000], Train Loss: 21990164.0000, Val Loss: 15620144.0000\n",
      "Epoch [930/50000], Train Loss: 21984714.0000, Val Loss: 15615394.0000\n",
      "Epoch [931/50000], Train Loss: 21979264.0000, Val Loss: 15610650.0000\n",
      "Epoch [932/50000], Train Loss: 21973820.0000, Val Loss: 15605905.0000\n",
      "Epoch [933/50000], Train Loss: 21968378.0000, Val Loss: 15601164.0000\n",
      "Epoch [934/50000], Train Loss: 21962936.0000, Val Loss: 15596426.0000\n",
      "Epoch [935/50000], Train Loss: 21957498.0000, Val Loss: 15591684.0000\n",
      "Epoch [936/50000], Train Loss: 21952054.0000, Val Loss: 15586950.0000\n",
      "Epoch [937/50000], Train Loss: 21946620.0000, Val Loss: 15582212.0000\n",
      "Epoch [938/50000], Train Loss: 21941180.0000, Val Loss: 15577477.0000\n",
      "Epoch [939/50000], Train Loss: 21935746.0000, Val Loss: 15572746.0000\n",
      "Epoch [940/50000], Train Loss: 21930314.0000, Val Loss: 15568016.0000\n",
      "Epoch [941/50000], Train Loss: 21924882.0000, Val Loss: 15563281.0000\n",
      "Epoch [942/50000], Train Loss: 21919450.0000, Val Loss: 15558554.0000\n",
      "Epoch [943/50000], Train Loss: 21914024.0000, Val Loss: 15553824.0000\n",
      "Epoch [944/50000], Train Loss: 21908594.0000, Val Loss: 15549099.0000\n",
      "Epoch [945/50000], Train Loss: 21903166.0000, Val Loss: 15544375.0000\n",
      "Epoch [946/50000], Train Loss: 21897746.0000, Val Loss: 15539650.0000\n",
      "Epoch [947/50000], Train Loss: 21892320.0000, Val Loss: 15534929.0000\n",
      "Epoch [948/50000], Train Loss: 21886898.0000, Val Loss: 15530210.0000\n",
      "Epoch [949/50000], Train Loss: 21881480.0000, Val Loss: 15525489.0000\n",
      "Epoch [950/50000], Train Loss: 21876060.0000, Val Loss: 15520773.0000\n",
      "Epoch [951/50000], Train Loss: 21870646.0000, Val Loss: 15516057.0000\n",
      "Epoch [952/50000], Train Loss: 21865228.0000, Val Loss: 15511343.0000\n",
      "Epoch [953/50000], Train Loss: 21859818.0000, Val Loss: 15506629.0000\n",
      "Epoch [954/50000], Train Loss: 21854402.0000, Val Loss: 15501919.0000\n",
      "Epoch [955/50000], Train Loss: 21848992.0000, Val Loss: 15497206.0000\n",
      "Epoch [956/50000], Train Loss: 21843582.0000, Val Loss: 15492497.0000\n",
      "Epoch [957/50000], Train Loss: 21838172.0000, Val Loss: 15487791.0000\n",
      "Epoch [958/50000], Train Loss: 21832764.0000, Val Loss: 15483085.0000\n",
      "Epoch [959/50000], Train Loss: 21827362.0000, Val Loss: 15478379.0000\n",
      "Epoch [960/50000], Train Loss: 21821956.0000, Val Loss: 15473676.0000\n",
      "Epoch [961/50000], Train Loss: 21816556.0000, Val Loss: 15468976.0000\n",
      "Epoch [962/50000], Train Loss: 21811156.0000, Val Loss: 15464273.0000\n",
      "Epoch [963/50000], Train Loss: 21805752.0000, Val Loss: 15459577.0000\n",
      "Epoch [964/50000], Train Loss: 21800358.0000, Val Loss: 15454880.0000\n",
      "Epoch [965/50000], Train Loss: 21794964.0000, Val Loss: 15450185.0000\n",
      "Epoch [966/50000], Train Loss: 21789570.0000, Val Loss: 15445486.0000\n",
      "Epoch [967/50000], Train Loss: 21784172.0000, Val Loss: 15440791.0000\n",
      "Epoch [968/50000], Train Loss: 21778780.0000, Val Loss: 15436100.0000\n",
      "Epoch [969/50000], Train Loss: 21773388.0000, Val Loss: 15431411.0000\n",
      "Epoch [970/50000], Train Loss: 21768004.0000, Val Loss: 15426723.0000\n",
      "Epoch [971/50000], Train Loss: 21762616.0000, Val Loss: 15422036.0000\n",
      "Epoch [972/50000], Train Loss: 21757232.0000, Val Loss: 15417348.0000\n",
      "Epoch [973/50000], Train Loss: 21751844.0000, Val Loss: 15412662.0000\n",
      "Epoch [974/50000], Train Loss: 21746462.0000, Val Loss: 15407983.0000\n",
      "Epoch [975/50000], Train Loss: 21741082.0000, Val Loss: 15403300.0000\n",
      "Epoch [976/50000], Train Loss: 21735700.0000, Val Loss: 15398619.0000\n",
      "Epoch [977/50000], Train Loss: 21730322.0000, Val Loss: 15393938.0000\n",
      "Epoch [978/50000], Train Loss: 21724944.0000, Val Loss: 15389260.0000\n",
      "Epoch [979/50000], Train Loss: 21719566.0000, Val Loss: 15384583.0000\n",
      "Epoch [980/50000], Train Loss: 21714192.0000, Val Loss: 15379911.0000\n",
      "Epoch [981/50000], Train Loss: 21708824.0000, Val Loss: 15375237.0000\n",
      "Epoch [982/50000], Train Loss: 21703454.0000, Val Loss: 15370565.0000\n",
      "Epoch [983/50000], Train Loss: 21698082.0000, Val Loss: 15365895.0000\n",
      "Epoch [984/50000], Train Loss: 21692712.0000, Val Loss: 15361227.0000\n",
      "Epoch [985/50000], Train Loss: 21687348.0000, Val Loss: 15356559.0000\n",
      "Epoch [986/50000], Train Loss: 21681980.0000, Val Loss: 15351890.0000\n",
      "Epoch [987/50000], Train Loss: 21676616.0000, Val Loss: 15347223.0000\n",
      "Epoch [988/50000], Train Loss: 21671252.0000, Val Loss: 15342565.0000\n",
      "Epoch [989/50000], Train Loss: 21665896.0000, Val Loss: 15337903.0000\n",
      "Epoch [990/50000], Train Loss: 21660538.0000, Val Loss: 15333242.0000\n",
      "Epoch [991/50000], Train Loss: 21655174.0000, Val Loss: 15328580.0000\n",
      "Epoch [992/50000], Train Loss: 21649822.0000, Val Loss: 15323924.0000\n",
      "Epoch [993/50000], Train Loss: 21644466.0000, Val Loss: 15319269.0000\n",
      "Epoch [994/50000], Train Loss: 21639112.0000, Val Loss: 15314612.0000\n",
      "Epoch [995/50000], Train Loss: 21633760.0000, Val Loss: 15309962.0000\n",
      "Epoch [996/50000], Train Loss: 21628414.0000, Val Loss: 15305308.0000\n",
      "Epoch [997/50000], Train Loss: 21623060.0000, Val Loss: 15300656.0000\n",
      "Epoch [998/50000], Train Loss: 21617714.0000, Val Loss: 15296006.0000\n",
      "Epoch [999/50000], Train Loss: 21612364.0000, Val Loss: 15291360.0000\n",
      "Epoch [1000/50000], Train Loss: 21607022.0000, Val Loss: 15286711.0000\n",
      "Epoch [1001/50000], Train Loss: 21601676.0000, Val Loss: 15282067.0000\n",
      "Epoch [1002/50000], Train Loss: 21596338.0000, Val Loss: 15277423.0000\n",
      "Epoch [1003/50000], Train Loss: 21590996.0000, Val Loss: 15272783.0000\n",
      "Epoch [1004/50000], Train Loss: 21585658.0000, Val Loss: 15268141.0000\n",
      "Epoch [1005/50000], Train Loss: 21580320.0000, Val Loss: 15263502.0000\n",
      "Epoch [1006/50000], Train Loss: 21574982.0000, Val Loss: 15258865.0000\n",
      "Epoch [1007/50000], Train Loss: 21569652.0000, Val Loss: 15254226.0000\n",
      "Epoch [1008/50000], Train Loss: 21564314.0000, Val Loss: 15249591.0000\n",
      "Epoch [1009/50000], Train Loss: 21558984.0000, Val Loss: 15244960.0000\n",
      "Epoch [1010/50000], Train Loss: 21553658.0000, Val Loss: 15240326.0000\n",
      "Epoch [1011/50000], Train Loss: 21548324.0000, Val Loss: 15235697.0000\n",
      "Epoch [1012/50000], Train Loss: 21543000.0000, Val Loss: 15231066.0000\n",
      "Epoch [1013/50000], Train Loss: 21537676.0000, Val Loss: 15226435.0000\n",
      "Epoch [1014/50000], Train Loss: 21532348.0000, Val Loss: 15221810.0000\n",
      "Epoch [1015/50000], Train Loss: 21527024.0000, Val Loss: 15217186.0000\n",
      "Epoch [1016/50000], Train Loss: 21521706.0000, Val Loss: 15212561.0000\n",
      "Epoch [1017/50000], Train Loss: 21516386.0000, Val Loss: 15207936.0000\n",
      "Epoch [1018/50000], Train Loss: 21511062.0000, Val Loss: 15203318.0000\n",
      "Epoch [1019/50000], Train Loss: 21505748.0000, Val Loss: 15198697.0000\n",
      "Epoch [1020/50000], Train Loss: 21500434.0000, Val Loss: 15194080.0000\n",
      "Epoch [1021/50000], Train Loss: 21495120.0000, Val Loss: 15189465.0000\n",
      "Epoch [1022/50000], Train Loss: 21489806.0000, Val Loss: 15184850.0000\n",
      "Epoch [1023/50000], Train Loss: 21484502.0000, Val Loss: 15180230.0000\n",
      "Epoch [1024/50000], Train Loss: 21479186.0000, Val Loss: 15175622.0000\n",
      "Epoch [1025/50000], Train Loss: 21473882.0000, Val Loss: 15171009.0000\n",
      "Epoch [1026/50000], Train Loss: 21468574.0000, Val Loss: 15166400.0000\n",
      "Epoch [1027/50000], Train Loss: 21463266.0000, Val Loss: 15161794.0000\n",
      "Epoch [1028/50000], Train Loss: 21457964.0000, Val Loss: 15157186.0000\n",
      "Epoch [1029/50000], Train Loss: 21452664.0000, Val Loss: 15152578.0000\n",
      "Epoch [1030/50000], Train Loss: 21447356.0000, Val Loss: 15147973.0000\n",
      "Epoch [1031/50000], Train Loss: 21442060.0000, Val Loss: 15143371.0000\n",
      "Epoch [1032/50000], Train Loss: 21436764.0000, Val Loss: 15138771.0000\n",
      "Epoch [1033/50000], Train Loss: 21431468.0000, Val Loss: 15134170.0000\n",
      "Epoch [1034/50000], Train Loss: 21426172.0000, Val Loss: 15129573.0000\n",
      "Epoch [1035/50000], Train Loss: 21420880.0000, Val Loss: 15124976.0000\n",
      "Epoch [1036/50000], Train Loss: 21415588.0000, Val Loss: 15120377.0000\n",
      "Epoch [1037/50000], Train Loss: 21410292.0000, Val Loss: 15115783.0000\n",
      "Epoch [1038/50000], Train Loss: 21405002.0000, Val Loss: 15111193.0000\n",
      "Epoch [1039/50000], Train Loss: 21399720.0000, Val Loss: 15106603.0000\n",
      "Epoch [1040/50000], Train Loss: 21394434.0000, Val Loss: 15102010.0000\n",
      "Epoch [1041/50000], Train Loss: 21389148.0000, Val Loss: 15097421.0000\n",
      "Epoch [1042/50000], Train Loss: 21383864.0000, Val Loss: 15092833.0000\n",
      "Epoch [1043/50000], Train Loss: 21378582.0000, Val Loss: 15088247.0000\n",
      "Epoch [1044/50000], Train Loss: 21373300.0000, Val Loss: 15083664.0000\n",
      "Epoch [1045/50000], Train Loss: 21368024.0000, Val Loss: 15079078.0000\n",
      "Epoch [1046/50000], Train Loss: 21362744.0000, Val Loss: 15074499.0000\n",
      "Epoch [1047/50000], Train Loss: 21357470.0000, Val Loss: 15069915.0000\n",
      "Epoch [1048/50000], Train Loss: 21352192.0000, Val Loss: 15065340.0000\n",
      "Epoch [1049/50000], Train Loss: 21346920.0000, Val Loss: 15060763.0000\n",
      "Epoch [1050/50000], Train Loss: 21341648.0000, Val Loss: 15056183.0000\n",
      "Epoch [1051/50000], Train Loss: 21336378.0000, Val Loss: 15051611.0000\n",
      "Epoch [1052/50000], Train Loss: 21331108.0000, Val Loss: 15047034.0000\n",
      "Epoch [1053/50000], Train Loss: 21325838.0000, Val Loss: 15042462.0000\n",
      "Epoch [1054/50000], Train Loss: 21320572.0000, Val Loss: 15037892.0000\n",
      "Epoch [1055/50000], Train Loss: 21315310.0000, Val Loss: 15033323.0000\n",
      "Epoch [1056/50000], Train Loss: 21310046.0000, Val Loss: 15028753.0000\n",
      "Epoch [1057/50000], Train Loss: 21304784.0000, Val Loss: 15024193.0000\n",
      "Epoch [1058/50000], Train Loss: 21299528.0000, Val Loss: 15019627.0000\n",
      "Epoch [1059/50000], Train Loss: 21294268.0000, Val Loss: 15015061.0000\n",
      "Epoch [1060/50000], Train Loss: 21289012.0000, Val Loss: 15010500.0000\n",
      "Epoch [1061/50000], Train Loss: 21283754.0000, Val Loss: 15005939.0000\n",
      "Epoch [1062/50000], Train Loss: 21278500.0000, Val Loss: 15001380.0000\n",
      "Epoch [1063/50000], Train Loss: 21273248.0000, Val Loss: 14996819.0000\n",
      "Epoch [1064/50000], Train Loss: 21267992.0000, Val Loss: 14992263.0000\n",
      "Epoch [1065/50000], Train Loss: 21262742.0000, Val Loss: 14987709.0000\n",
      "Epoch [1066/50000], Train Loss: 21257494.0000, Val Loss: 14983155.0000\n",
      "Epoch [1067/50000], Train Loss: 21252250.0000, Val Loss: 14978603.0000\n",
      "Epoch [1068/50000], Train Loss: 21247006.0000, Val Loss: 14974050.0000\n",
      "Epoch [1069/50000], Train Loss: 21241760.0000, Val Loss: 14969502.0000\n",
      "Epoch [1070/50000], Train Loss: 21236516.0000, Val Loss: 14964955.0000\n",
      "Epoch [1071/50000], Train Loss: 21231276.0000, Val Loss: 14960405.0000\n",
      "Epoch [1072/50000], Train Loss: 21226034.0000, Val Loss: 14955860.0000\n",
      "Epoch [1073/50000], Train Loss: 21220796.0000, Val Loss: 14951315.0000\n",
      "Epoch [1074/50000], Train Loss: 21215558.0000, Val Loss: 14946777.0000\n",
      "Epoch [1075/50000], Train Loss: 21210326.0000, Val Loss: 14942236.0000\n",
      "Epoch [1076/50000], Train Loss: 21205090.0000, Val Loss: 14937694.0000\n",
      "Epoch [1077/50000], Train Loss: 21199856.0000, Val Loss: 14933154.0000\n",
      "Epoch [1078/50000], Train Loss: 21194626.0000, Val Loss: 14928619.0000\n",
      "Epoch [1079/50000], Train Loss: 21189398.0000, Val Loss: 14924080.0000\n",
      "Epoch [1080/50000], Train Loss: 21184168.0000, Val Loss: 14919546.0000\n",
      "Epoch [1081/50000], Train Loss: 21178940.0000, Val Loss: 14915011.0000\n",
      "Epoch [1082/50000], Train Loss: 21173712.0000, Val Loss: 14910482.0000\n",
      "Epoch [1083/50000], Train Loss: 21168490.0000, Val Loss: 14905949.0000\n",
      "Epoch [1084/50000], Train Loss: 21163266.0000, Val Loss: 14901421.0000\n",
      "Epoch [1085/50000], Train Loss: 21158042.0000, Val Loss: 14896894.0000\n",
      "Epoch [1086/50000], Train Loss: 21152828.0000, Val Loss: 14892369.0000\n",
      "Epoch [1087/50000], Train Loss: 21147610.0000, Val Loss: 14887843.0000\n",
      "Epoch [1088/50000], Train Loss: 21142392.0000, Val Loss: 14883319.0000\n",
      "Epoch [1089/50000], Train Loss: 21137176.0000, Val Loss: 14878799.0000\n",
      "Epoch [1090/50000], Train Loss: 21131964.0000, Val Loss: 14874278.0000\n",
      "Epoch [1091/50000], Train Loss: 21126750.0000, Val Loss: 14869760.0000\n",
      "Epoch [1092/50000], Train Loss: 21121538.0000, Val Loss: 14865242.0000\n",
      "Epoch [1093/50000], Train Loss: 21116328.0000, Val Loss: 14860726.0000\n",
      "Epoch [1094/50000], Train Loss: 21111120.0000, Val Loss: 14856209.0000\n",
      "Epoch [1095/50000], Train Loss: 21105914.0000, Val Loss: 14851696.0000\n",
      "Epoch [1096/50000], Train Loss: 21100706.0000, Val Loss: 14847185.0000\n",
      "Epoch [1097/50000], Train Loss: 21095504.0000, Val Loss: 14842672.0000\n",
      "Epoch [1098/50000], Train Loss: 21090302.0000, Val Loss: 14838166.0000\n",
      "Epoch [1099/50000], Train Loss: 21085104.0000, Val Loss: 14833657.0000\n",
      "Epoch [1100/50000], Train Loss: 21079902.0000, Val Loss: 14829152.0000\n",
      "Epoch [1101/50000], Train Loss: 21074706.0000, Val Loss: 14824644.0000\n",
      "Epoch [1102/50000], Train Loss: 21069508.0000, Val Loss: 14820143.0000\n",
      "Epoch [1103/50000], Train Loss: 21064314.0000, Val Loss: 14815639.0000\n",
      "Epoch [1104/50000], Train Loss: 21059120.0000, Val Loss: 14811136.0000\n",
      "Epoch [1105/50000], Train Loss: 21053926.0000, Val Loss: 14806637.0000\n",
      "Epoch [1106/50000], Train Loss: 21048734.0000, Val Loss: 14802140.0000\n",
      "Epoch [1107/50000], Train Loss: 21043546.0000, Val Loss: 14797643.0000\n",
      "Epoch [1108/50000], Train Loss: 21038358.0000, Val Loss: 14793148.0000\n",
      "Epoch [1109/50000], Train Loss: 21033174.0000, Val Loss: 14788652.0000\n",
      "Epoch [1110/50000], Train Loss: 21027984.0000, Val Loss: 14784160.0000\n",
      "Epoch [1111/50000], Train Loss: 21022800.0000, Val Loss: 14779667.0000\n",
      "Epoch [1112/50000], Train Loss: 21017616.0000, Val Loss: 14775177.0000\n",
      "Epoch [1113/50000], Train Loss: 21012438.0000, Val Loss: 14770687.0000\n",
      "Epoch [1114/50000], Train Loss: 21007256.0000, Val Loss: 14766203.0000\n",
      "Epoch [1115/50000], Train Loss: 21002082.0000, Val Loss: 14761715.0000\n",
      "Epoch [1116/50000], Train Loss: 20996904.0000, Val Loss: 14757232.0000\n",
      "Epoch [1117/50000], Train Loss: 20991730.0000, Val Loss: 14752747.0000\n",
      "Epoch [1118/50000], Train Loss: 20986554.0000, Val Loss: 14748267.0000\n",
      "Epoch [1119/50000], Train Loss: 20981380.0000, Val Loss: 14743785.0000\n",
      "Epoch [1120/50000], Train Loss: 20976214.0000, Val Loss: 14739309.0000\n",
      "Epoch [1121/50000], Train Loss: 20971044.0000, Val Loss: 14734829.0000\n",
      "Epoch [1122/50000], Train Loss: 20965872.0000, Val Loss: 14730353.0000\n",
      "Epoch [1123/50000], Train Loss: 20960708.0000, Val Loss: 14725875.0000\n",
      "Epoch [1124/50000], Train Loss: 20955540.0000, Val Loss: 14721404.0000\n",
      "Epoch [1125/50000], Train Loss: 20950378.0000, Val Loss: 14716930.0000\n",
      "Epoch [1126/50000], Train Loss: 20945214.0000, Val Loss: 14712460.0000\n",
      "Epoch [1127/50000], Train Loss: 20940052.0000, Val Loss: 14707991.0000\n",
      "Epoch [1128/50000], Train Loss: 20934896.0000, Val Loss: 14703525.0000\n",
      "Epoch [1129/50000], Train Loss: 20929738.0000, Val Loss: 14699058.0000\n",
      "Epoch [1130/50000], Train Loss: 20924578.0000, Val Loss: 14694592.0000\n",
      "Epoch [1131/50000], Train Loss: 20919422.0000, Val Loss: 14690126.0000\n",
      "Epoch [1132/50000], Train Loss: 20914268.0000, Val Loss: 14685666.0000\n",
      "Epoch [1133/50000], Train Loss: 20909122.0000, Val Loss: 14681205.0000\n",
      "Epoch [1134/50000], Train Loss: 20903970.0000, Val Loss: 14676742.0000\n",
      "Epoch [1135/50000], Train Loss: 20898816.0000, Val Loss: 14672286.0000\n",
      "Epoch [1136/50000], Train Loss: 20893670.0000, Val Loss: 14667826.0000\n",
      "Epoch [1137/50000], Train Loss: 20888522.0000, Val Loss: 14663373.0000\n",
      "Epoch [1138/50000], Train Loss: 20883376.0000, Val Loss: 14658916.0000\n",
      "Epoch [1139/50000], Train Loss: 20878230.0000, Val Loss: 14654462.0000\n",
      "Epoch [1140/50000], Train Loss: 20873088.0000, Val Loss: 14650013.0000\n",
      "Epoch [1141/50000], Train Loss: 20867950.0000, Val Loss: 14645565.0000\n",
      "Epoch [1142/50000], Train Loss: 20862812.0000, Val Loss: 14641113.0000\n",
      "Epoch [1143/50000], Train Loss: 20857668.0000, Val Loss: 14636666.0000\n",
      "Epoch [1144/50000], Train Loss: 20852536.0000, Val Loss: 14632221.0000\n",
      "Epoch [1145/50000], Train Loss: 20847400.0000, Val Loss: 14627775.0000\n",
      "Epoch [1146/50000], Train Loss: 20842266.0000, Val Loss: 14623332.0000\n",
      "Epoch [1147/50000], Train Loss: 20837134.0000, Val Loss: 14618889.0000\n",
      "Epoch [1148/50000], Train Loss: 20832002.0000, Val Loss: 14614450.0000\n",
      "Epoch [1149/50000], Train Loss: 20826876.0000, Val Loss: 14610009.0000\n",
      "Epoch [1150/50000], Train Loss: 20821742.0000, Val Loss: 14605572.0000\n",
      "Epoch [1151/50000], Train Loss: 20816618.0000, Val Loss: 14601137.0000\n",
      "Epoch [1152/50000], Train Loss: 20811494.0000, Val Loss: 14596700.0000\n",
      "Epoch [1153/50000], Train Loss: 20806368.0000, Val Loss: 14592269.0000\n",
      "Epoch [1154/50000], Train Loss: 20801246.0000, Val Loss: 14587834.0000\n",
      "Epoch [1155/50000], Train Loss: 20796124.0000, Val Loss: 14583403.0000\n",
      "Epoch [1156/50000], Train Loss: 20791006.0000, Val Loss: 14578971.0000\n",
      "Epoch [1157/50000], Train Loss: 20785882.0000, Val Loss: 14574545.0000\n",
      "Epoch [1158/50000], Train Loss: 20780770.0000, Val Loss: 14570118.0000\n",
      "Epoch [1159/50000], Train Loss: 20775656.0000, Val Loss: 14565694.0000\n",
      "Epoch [1160/50000], Train Loss: 20770542.0000, Val Loss: 14561270.0000\n",
      "Epoch [1161/50000], Train Loss: 20765432.0000, Val Loss: 14556847.0000\n",
      "Epoch [1162/50000], Train Loss: 20760320.0000, Val Loss: 14552425.0000\n",
      "Epoch [1163/50000], Train Loss: 20755208.0000, Val Loss: 14548003.0000\n",
      "Epoch [1164/50000], Train Loss: 20750100.0000, Val Loss: 14543585.0000\n",
      "Epoch [1165/50000], Train Loss: 20744992.0000, Val Loss: 14539165.0000\n",
      "Epoch [1166/50000], Train Loss: 20739888.0000, Val Loss: 14534752.0000\n",
      "Epoch [1167/50000], Train Loss: 20734786.0000, Val Loss: 14530336.0000\n",
      "Epoch [1168/50000], Train Loss: 20729678.0000, Val Loss: 14525921.0000\n",
      "Epoch [1169/50000], Train Loss: 20724576.0000, Val Loss: 14521513.0000\n",
      "Epoch [1170/50000], Train Loss: 20719480.0000, Val Loss: 14517100.0000\n",
      "Epoch [1171/50000], Train Loss: 20714380.0000, Val Loss: 14512693.0000\n",
      "Epoch [1172/50000], Train Loss: 20709284.0000, Val Loss: 14508286.0000\n",
      "Epoch [1173/50000], Train Loss: 20704190.0000, Val Loss: 14503879.0000\n",
      "Epoch [1174/50000], Train Loss: 20699094.0000, Val Loss: 14499472.0000\n",
      "Epoch [1175/50000], Train Loss: 20693998.0000, Val Loss: 14495067.0000\n",
      "Epoch [1176/50000], Train Loss: 20688908.0000, Val Loss: 14490665.0000\n",
      "Epoch [1177/50000], Train Loss: 20683818.0000, Val Loss: 14486263.0000\n",
      "Epoch [1178/50000], Train Loss: 20678730.0000, Val Loss: 14481869.0000\n",
      "Epoch [1179/50000], Train Loss: 20673644.0000, Val Loss: 14477463.0000\n",
      "Epoch [1180/50000], Train Loss: 20668556.0000, Val Loss: 14473068.0000\n",
      "Epoch [1181/50000], Train Loss: 20663468.0000, Val Loss: 14468671.0000\n",
      "Epoch [1182/50000], Train Loss: 20658390.0000, Val Loss: 14464276.0000\n",
      "Epoch [1183/50000], Train Loss: 20653308.0000, Val Loss: 14459887.0000\n",
      "Epoch [1184/50000], Train Loss: 20648228.0000, Val Loss: 14455494.0000\n",
      "Epoch [1185/50000], Train Loss: 20643150.0000, Val Loss: 14451104.0000\n",
      "Epoch [1186/50000], Train Loss: 20638074.0000, Val Loss: 14446714.0000\n",
      "Epoch [1187/50000], Train Loss: 20632996.0000, Val Loss: 14442326.0000\n",
      "Epoch [1188/50000], Train Loss: 20627918.0000, Val Loss: 14437941.0000\n",
      "Epoch [1189/50000], Train Loss: 20622850.0000, Val Loss: 14433556.0000\n",
      "Epoch [1190/50000], Train Loss: 20617778.0000, Val Loss: 14429173.0000\n",
      "Epoch [1191/50000], Train Loss: 20612704.0000, Val Loss: 14424791.0000\n",
      "Epoch [1192/50000], Train Loss: 20607636.0000, Val Loss: 14420409.0000\n",
      "Epoch [1193/50000], Train Loss: 20602568.0000, Val Loss: 14416032.0000\n",
      "Epoch [1194/50000], Train Loss: 20597504.0000, Val Loss: 14411652.0000\n",
      "Epoch [1195/50000], Train Loss: 20592438.0000, Val Loss: 14407275.0000\n",
      "Epoch [1196/50000], Train Loss: 20587372.0000, Val Loss: 14402899.0000\n",
      "Epoch [1197/50000], Train Loss: 20582312.0000, Val Loss: 14398526.0000\n",
      "Epoch [1198/50000], Train Loss: 20577254.0000, Val Loss: 14394153.0000\n",
      "Epoch [1199/50000], Train Loss: 20572194.0000, Val Loss: 14389781.0000\n",
      "Epoch [1200/50000], Train Loss: 20567134.0000, Val Loss: 14385412.0000\n",
      "Epoch [1201/50000], Train Loss: 20562078.0000, Val Loss: 14381042.0000\n",
      "Epoch [1202/50000], Train Loss: 20557022.0000, Val Loss: 14376676.0000\n",
      "Epoch [1203/50000], Train Loss: 20551972.0000, Val Loss: 14372310.0000\n",
      "Epoch [1204/50000], Train Loss: 20546920.0000, Val Loss: 14367946.0000\n",
      "Epoch [1205/50000], Train Loss: 20541872.0000, Val Loss: 14363581.0000\n",
      "Epoch [1206/50000], Train Loss: 20536818.0000, Val Loss: 14359221.0000\n",
      "Epoch [1207/50000], Train Loss: 20531770.0000, Val Loss: 14354859.0000\n",
      "Epoch [1208/50000], Train Loss: 20526724.0000, Val Loss: 14350500.0000\n",
      "Epoch [1209/50000], Train Loss: 20521678.0000, Val Loss: 14346141.0000\n",
      "Epoch [1210/50000], Train Loss: 20516634.0000, Val Loss: 14341786.0000\n",
      "Epoch [1211/50000], Train Loss: 20511594.0000, Val Loss: 14337429.0000\n",
      "Epoch [1212/50000], Train Loss: 20506552.0000, Val Loss: 14333078.0000\n",
      "Epoch [1213/50000], Train Loss: 20501516.0000, Val Loss: 14328722.0000\n",
      "Epoch [1214/50000], Train Loss: 20496474.0000, Val Loss: 14324375.0000\n",
      "Epoch [1215/50000], Train Loss: 20491442.0000, Val Loss: 14320023.0000\n",
      "Epoch [1216/50000], Train Loss: 20486402.0000, Val Loss: 14315676.0000\n",
      "Epoch [1217/50000], Train Loss: 20481370.0000, Val Loss: 14311327.0000\n",
      "Epoch [1218/50000], Train Loss: 20476336.0000, Val Loss: 14306983.0000\n",
      "Epoch [1219/50000], Train Loss: 20471304.0000, Val Loss: 14302638.0000\n",
      "Epoch [1220/50000], Train Loss: 20466274.0000, Val Loss: 14298295.0000\n",
      "Epoch [1221/50000], Train Loss: 20461246.0000, Val Loss: 14293954.0000\n",
      "Epoch [1222/50000], Train Loss: 20456222.0000, Val Loss: 14289614.0000\n",
      "Epoch [1223/50000], Train Loss: 20451196.0000, Val Loss: 14285275.0000\n",
      "Epoch [1224/50000], Train Loss: 20446174.0000, Val Loss: 14280938.0000\n",
      "Epoch [1225/50000], Train Loss: 20441148.0000, Val Loss: 14276602.0000\n",
      "Epoch [1226/50000], Train Loss: 20436130.0000, Val Loss: 14272267.0000\n",
      "Epoch [1227/50000], Train Loss: 20431108.0000, Val Loss: 14267931.0000\n",
      "Epoch [1228/50000], Train Loss: 20426088.0000, Val Loss: 14263601.0000\n",
      "Epoch [1229/50000], Train Loss: 20421076.0000, Val Loss: 14259273.0000\n",
      "Epoch [1230/50000], Train Loss: 20416058.0000, Val Loss: 14254939.0000\n",
      "Epoch [1231/50000], Train Loss: 20411042.0000, Val Loss: 14250610.0000\n",
      "Epoch [1232/50000], Train Loss: 20406030.0000, Val Loss: 14246285.0000\n",
      "Epoch [1233/50000], Train Loss: 20401018.0000, Val Loss: 14241958.0000\n",
      "Epoch [1234/50000], Train Loss: 20396008.0000, Val Loss: 14237635.0000\n",
      "Epoch [1235/50000], Train Loss: 20391000.0000, Val Loss: 14233314.0000\n",
      "Epoch [1236/50000], Train Loss: 20385994.0000, Val Loss: 14228993.0000\n",
      "Epoch [1237/50000], Train Loss: 20380988.0000, Val Loss: 14224671.0000\n",
      "Epoch [1238/50000], Train Loss: 20375984.0000, Val Loss: 14220352.0000\n",
      "Epoch [1239/50000], Train Loss: 20370980.0000, Val Loss: 14216035.0000\n",
      "Epoch [1240/50000], Train Loss: 20365974.0000, Val Loss: 14211721.0000\n",
      "Epoch [1241/50000], Train Loss: 20360978.0000, Val Loss: 14207405.0000\n",
      "Epoch [1242/50000], Train Loss: 20355976.0000, Val Loss: 14203090.0000\n",
      "Epoch [1243/50000], Train Loss: 20350980.0000, Val Loss: 14198775.0000\n",
      "Epoch [1244/50000], Train Loss: 20345982.0000, Val Loss: 14194467.0000\n",
      "Epoch [1245/50000], Train Loss: 20340988.0000, Val Loss: 14190159.0000\n",
      "Epoch [1246/50000], Train Loss: 20335994.0000, Val Loss: 14185851.0000\n",
      "Epoch [1247/50000], Train Loss: 20331004.0000, Val Loss: 14181546.0000\n",
      "Epoch [1248/50000], Train Loss: 20326014.0000, Val Loss: 14177236.0000\n",
      "Epoch [1249/50000], Train Loss: 20321020.0000, Val Loss: 14172934.0000\n",
      "Epoch [1250/50000], Train Loss: 20316036.0000, Val Loss: 14168633.0000\n",
      "Epoch [1251/50000], Train Loss: 20311052.0000, Val Loss: 14164332.0000\n",
      "Epoch [1252/50000], Train Loss: 20306066.0000, Val Loss: 14160030.0000\n",
      "Epoch [1253/50000], Train Loss: 20301078.0000, Val Loss: 14155733.0000\n",
      "Epoch [1254/50000], Train Loss: 20296100.0000, Val Loss: 14151436.0000\n",
      "Epoch [1255/50000], Train Loss: 20291120.0000, Val Loss: 14147138.0000\n",
      "Epoch [1256/50000], Train Loss: 20286140.0000, Val Loss: 14142844.0000\n",
      "Epoch [1257/50000], Train Loss: 20281162.0000, Val Loss: 14138550.0000\n",
      "Epoch [1258/50000], Train Loss: 20276184.0000, Val Loss: 14134259.0000\n",
      "Epoch [1259/50000], Train Loss: 20271210.0000, Val Loss: 14129969.0000\n",
      "Epoch [1260/50000], Train Loss: 20266236.0000, Val Loss: 14125679.0000\n",
      "Epoch [1261/50000], Train Loss: 20261264.0000, Val Loss: 14121389.0000\n",
      "Epoch [1262/50000], Train Loss: 20256290.0000, Val Loss: 14117105.0000\n",
      "Epoch [1263/50000], Train Loss: 20251322.0000, Val Loss: 14112819.0000\n",
      "Epoch [1264/50000], Train Loss: 20246352.0000, Val Loss: 14108537.0000\n",
      "Epoch [1265/50000], Train Loss: 20241388.0000, Val Loss: 14104255.0000\n",
      "Epoch [1266/50000], Train Loss: 20236422.0000, Val Loss: 14099969.0000\n",
      "Epoch [1267/50000], Train Loss: 20231456.0000, Val Loss: 14095692.0000\n",
      "Epoch [1268/50000], Train Loss: 20226494.0000, Val Loss: 14091413.0000\n",
      "Epoch [1269/50000], Train Loss: 20221534.0000, Val Loss: 14087134.0000\n",
      "Epoch [1270/50000], Train Loss: 20216572.0000, Val Loss: 14082862.0000\n",
      "Epoch [1271/50000], Train Loss: 20211618.0000, Val Loss: 14078587.0000\n",
      "Epoch [1272/50000], Train Loss: 20206660.0000, Val Loss: 14074313.0000\n",
      "Epoch [1273/50000], Train Loss: 20201702.0000, Val Loss: 14070039.0000\n",
      "Epoch [1274/50000], Train Loss: 20196748.0000, Val Loss: 14065770.0000\n",
      "Epoch [1275/50000], Train Loss: 20191796.0000, Val Loss: 14061501.0000\n",
      "Epoch [1276/50000], Train Loss: 20186844.0000, Val Loss: 14057231.0000\n",
      "Epoch [1277/50000], Train Loss: 20181890.0000, Val Loss: 14052967.0000\n",
      "Epoch [1278/50000], Train Loss: 20176940.0000, Val Loss: 14048703.0000\n",
      "Epoch [1279/50000], Train Loss: 20171998.0000, Val Loss: 14044435.0000\n",
      "Epoch [1280/50000], Train Loss: 20167046.0000, Val Loss: 14040175.0000\n",
      "Epoch [1281/50000], Train Loss: 20162106.0000, Val Loss: 14035911.0000\n",
      "Epoch [1282/50000], Train Loss: 20157160.0000, Val Loss: 14031654.0000\n",
      "Epoch [1283/50000], Train Loss: 20152222.0000, Val Loss: 14027393.0000\n",
      "Epoch [1284/50000], Train Loss: 20147278.0000, Val Loss: 14023137.0000\n",
      "Epoch [1285/50000], Train Loss: 20142340.0000, Val Loss: 14018882.0000\n",
      "Epoch [1286/50000], Train Loss: 20137400.0000, Val Loss: 14014624.0000\n",
      "Epoch [1287/50000], Train Loss: 20132460.0000, Val Loss: 14010372.0000\n",
      "Epoch [1288/50000], Train Loss: 20127526.0000, Val Loss: 14006122.0000\n",
      "Epoch [1289/50000], Train Loss: 20122592.0000, Val Loss: 14001871.0000\n",
      "Epoch [1290/50000], Train Loss: 20117664.0000, Val Loss: 13997621.0000\n",
      "Epoch [1291/50000], Train Loss: 20112734.0000, Val Loss: 13993372.0000\n",
      "Epoch [1292/50000], Train Loss: 20107800.0000, Val Loss: 13989126.0000\n",
      "Epoch [1293/50000], Train Loss: 20102874.0000, Val Loss: 13984880.0000\n",
      "Epoch [1294/50000], Train Loss: 20097946.0000, Val Loss: 13980635.0000\n",
      "Epoch [1295/50000], Train Loss: 20093018.0000, Val Loss: 13976395.0000\n",
      "Epoch [1296/50000], Train Loss: 20088098.0000, Val Loss: 13972153.0000\n",
      "Epoch [1297/50000], Train Loss: 20083174.0000, Val Loss: 13967913.0000\n",
      "Epoch [1298/50000], Train Loss: 20078256.0000, Val Loss: 13963674.0000\n",
      "Epoch [1299/50000], Train Loss: 20073336.0000, Val Loss: 13959436.0000\n",
      "Epoch [1300/50000], Train Loss: 20068412.0000, Val Loss: 13955202.0000\n",
      "Epoch [1301/50000], Train Loss: 20063500.0000, Val Loss: 13950964.0000\n",
      "Epoch [1302/50000], Train Loss: 20058582.0000, Val Loss: 13946733.0000\n",
      "Epoch [1303/50000], Train Loss: 20053666.0000, Val Loss: 13942502.0000\n",
      "Epoch [1304/50000], Train Loss: 20048754.0000, Val Loss: 13938268.0000\n",
      "Epoch [1305/50000], Train Loss: 20043838.0000, Val Loss: 13934038.0000\n",
      "Epoch [1306/50000], Train Loss: 20038930.0000, Val Loss: 13929811.0000\n",
      "Epoch [1307/50000], Train Loss: 20034024.0000, Val Loss: 13925585.0000\n",
      "Epoch [1308/50000], Train Loss: 20029114.0000, Val Loss: 13921360.0000\n",
      "Epoch [1309/50000], Train Loss: 20024208.0000, Val Loss: 13917137.0000\n",
      "Epoch [1310/50000], Train Loss: 20019306.0000, Val Loss: 13912912.0000\n",
      "Epoch [1311/50000], Train Loss: 20014402.0000, Val Loss: 13908691.0000\n",
      "Epoch [1312/50000], Train Loss: 20009498.0000, Val Loss: 13904469.0000\n",
      "Epoch [1313/50000], Train Loss: 20004596.0000, Val Loss: 13900254.0000\n",
      "Epoch [1314/50000], Train Loss: 19999702.0000, Val Loss: 13896035.0000\n",
      "Epoch [1315/50000], Train Loss: 19994802.0000, Val Loss: 13891819.0000\n",
      "Epoch [1316/50000], Train Loss: 19989904.0000, Val Loss: 13887603.0000\n",
      "Epoch [1317/50000], Train Loss: 19985006.0000, Val Loss: 13883390.0000\n",
      "Epoch [1318/50000], Train Loss: 19980116.0000, Val Loss: 13879177.0000\n",
      "Epoch [1319/50000], Train Loss: 19975224.0000, Val Loss: 13874967.0000\n",
      "Epoch [1320/50000], Train Loss: 19970330.0000, Val Loss: 13870758.0000\n",
      "Epoch [1321/50000], Train Loss: 19965442.0000, Val Loss: 13866550.0000\n",
      "Epoch [1322/50000], Train Loss: 19960554.0000, Val Loss: 13862341.0000\n",
      "Epoch [1323/50000], Train Loss: 19955668.0000, Val Loss: 13858135.0000\n",
      "Epoch [1324/50000], Train Loss: 19950780.0000, Val Loss: 13853933.0000\n",
      "Epoch [1325/50000], Train Loss: 19945898.0000, Val Loss: 13849728.0000\n",
      "Epoch [1326/50000], Train Loss: 19941014.0000, Val Loss: 13845525.0000\n",
      "Epoch [1327/50000], Train Loss: 19936126.0000, Val Loss: 13841328.0000\n",
      "Epoch [1328/50000], Train Loss: 19931250.0000, Val Loss: 13837127.0000\n",
      "Epoch [1329/50000], Train Loss: 19926374.0000, Val Loss: 13832930.0000\n",
      "Epoch [1330/50000], Train Loss: 19921494.0000, Val Loss: 13828732.0000\n",
      "Epoch [1331/50000], Train Loss: 19916618.0000, Val Loss: 13824538.0000\n",
      "Epoch [1332/50000], Train Loss: 19911742.0000, Val Loss: 13820345.0000\n",
      "Epoch [1333/50000], Train Loss: 19906870.0000, Val Loss: 13816151.0000\n",
      "Epoch [1334/50000], Train Loss: 19902000.0000, Val Loss: 13811959.0000\n",
      "Epoch [1335/50000], Train Loss: 19897124.0000, Val Loss: 13807771.0000\n",
      "Epoch [1336/50000], Train Loss: 19892256.0000, Val Loss: 13803584.0000\n",
      "Epoch [1337/50000], Train Loss: 19887392.0000, Val Loss: 13799395.0000\n",
      "Epoch [1338/50000], Train Loss: 19882522.0000, Val Loss: 13795210.0000\n",
      "Epoch [1339/50000], Train Loss: 19877658.0000, Val Loss: 13791023.0000\n",
      "Epoch [1340/50000], Train Loss: 19872794.0000, Val Loss: 13786842.0000\n",
      "Epoch [1341/50000], Train Loss: 19867930.0000, Val Loss: 13782658.0000\n",
      "Epoch [1342/50000], Train Loss: 19863072.0000, Val Loss: 13778479.0000\n",
      "Epoch [1343/50000], Train Loss: 19858212.0000, Val Loss: 13774295.0000\n",
      "Epoch [1344/50000], Train Loss: 19853350.0000, Val Loss: 13770118.0000\n",
      "Epoch [1345/50000], Train Loss: 19848490.0000, Val Loss: 13765943.0000\n",
      "Epoch [1346/50000], Train Loss: 19843640.0000, Val Loss: 13761769.0000\n",
      "Epoch [1347/50000], Train Loss: 19838786.0000, Val Loss: 13757591.0000\n",
      "Epoch [1348/50000], Train Loss: 19833932.0000, Val Loss: 13753421.0000\n",
      "Epoch [1349/50000], Train Loss: 19829082.0000, Val Loss: 13749249.0000\n",
      "Epoch [1350/50000], Train Loss: 19824226.0000, Val Loss: 13745082.0000\n",
      "Epoch [1351/50000], Train Loss: 19819384.0000, Val Loss: 13740911.0000\n",
      "Epoch [1352/50000], Train Loss: 19814534.0000, Val Loss: 13736741.0000\n",
      "Epoch [1353/50000], Train Loss: 19809686.0000, Val Loss: 13732578.0000\n",
      "Epoch [1354/50000], Train Loss: 19804844.0000, Val Loss: 13728412.0000\n",
      "Epoch [1355/50000], Train Loss: 19799998.0000, Val Loss: 13724249.0000\n",
      "Epoch [1356/50000], Train Loss: 19795156.0000, Val Loss: 13720083.0000\n",
      "Epoch [1357/50000], Train Loss: 19790316.0000, Val Loss: 13715925.0000\n",
      "Epoch [1358/50000], Train Loss: 19785478.0000, Val Loss: 13711765.0000\n",
      "Epoch [1359/50000], Train Loss: 19780642.0000, Val Loss: 13707606.0000\n",
      "Epoch [1360/50000], Train Loss: 19775800.0000, Val Loss: 13703450.0000\n",
      "Epoch [1361/50000], Train Loss: 19770968.0000, Val Loss: 13699295.0000\n",
      "Epoch [1362/50000], Train Loss: 19766138.0000, Val Loss: 13695139.0000\n",
      "Epoch [1363/50000], Train Loss: 19761300.0000, Val Loss: 13690987.0000\n",
      "Epoch [1364/50000], Train Loss: 19756468.0000, Val Loss: 13686833.0000\n",
      "Epoch [1365/50000], Train Loss: 19751640.0000, Val Loss: 13682684.0000\n",
      "Epoch [1366/50000], Train Loss: 19746812.0000, Val Loss: 13678531.0000\n",
      "Epoch [1367/50000], Train Loss: 19741982.0000, Val Loss: 13674387.0000\n",
      "Epoch [1368/50000], Train Loss: 19737160.0000, Val Loss: 13670235.0000\n",
      "Epoch [1369/50000], Train Loss: 19732330.0000, Val Loss: 13666094.0000\n",
      "Epoch [1370/50000], Train Loss: 19727510.0000, Val Loss: 13661950.0000\n",
      "Epoch [1371/50000], Train Loss: 19722686.0000, Val Loss: 13657808.0000\n",
      "Epoch [1372/50000], Train Loss: 19717870.0000, Val Loss: 13653666.0000\n",
      "Epoch [1373/50000], Train Loss: 19713048.0000, Val Loss: 13649527.0000\n",
      "Epoch [1374/50000], Train Loss: 19708230.0000, Val Loss: 13645386.0000\n",
      "Epoch [1375/50000], Train Loss: 19703418.0000, Val Loss: 13641250.0000\n",
      "Epoch [1376/50000], Train Loss: 19698602.0000, Val Loss: 13637114.0000\n",
      "Epoch [1377/50000], Train Loss: 19693786.0000, Val Loss: 13632979.0000\n",
      "Epoch [1378/50000], Train Loss: 19688976.0000, Val Loss: 13628845.0000\n",
      "Epoch [1379/50000], Train Loss: 19684164.0000, Val Loss: 13624710.0000\n",
      "Epoch [1380/50000], Train Loss: 19679354.0000, Val Loss: 13620580.0000\n",
      "Epoch [1381/50000], Train Loss: 19674546.0000, Val Loss: 13616452.0000\n",
      "Epoch [1382/50000], Train Loss: 19669738.0000, Val Loss: 13612324.0000\n",
      "Epoch [1383/50000], Train Loss: 19664934.0000, Val Loss: 13608197.0000\n",
      "Epoch [1384/50000], Train Loss: 19660130.0000, Val Loss: 13604070.0000\n",
      "Epoch [1385/50000], Train Loss: 19655328.0000, Val Loss: 13599945.0000\n",
      "Epoch [1386/50000], Train Loss: 19650526.0000, Val Loss: 13595823.0000\n",
      "Epoch [1387/50000], Train Loss: 19645728.0000, Val Loss: 13591699.0000\n",
      "Epoch [1388/50000], Train Loss: 19640924.0000, Val Loss: 13587580.0000\n",
      "Epoch [1389/50000], Train Loss: 19636130.0000, Val Loss: 13583461.0000\n",
      "Epoch [1390/50000], Train Loss: 19631334.0000, Val Loss: 13579342.0000\n",
      "Epoch [1391/50000], Train Loss: 19626538.0000, Val Loss: 13575225.0000\n",
      "Epoch [1392/50000], Train Loss: 19621746.0000, Val Loss: 13571111.0000\n",
      "Epoch [1393/50000], Train Loss: 19616954.0000, Val Loss: 13566995.0000\n",
      "Epoch [1394/50000], Train Loss: 19612162.0000, Val Loss: 13562881.0000\n",
      "Epoch [1395/50000], Train Loss: 19607372.0000, Val Loss: 13558770.0000\n",
      "Epoch [1396/50000], Train Loss: 19602586.0000, Val Loss: 13554660.0000\n",
      "Epoch [1397/50000], Train Loss: 19597794.0000, Val Loss: 13550551.0000\n",
      "Epoch [1398/50000], Train Loss: 19593014.0000, Val Loss: 13546443.0000\n",
      "Epoch [1399/50000], Train Loss: 19588226.0000, Val Loss: 13542340.0000\n",
      "Epoch [1400/50000], Train Loss: 19583448.0000, Val Loss: 13538234.0000\n",
      "Epoch [1401/50000], Train Loss: 19578664.0000, Val Loss: 13534129.0000\n",
      "Epoch [1402/50000], Train Loss: 19573884.0000, Val Loss: 13530025.0000\n",
      "Epoch [1403/50000], Train Loss: 19569104.0000, Val Loss: 13525926.0000\n",
      "Epoch [1404/50000], Train Loss: 19564328.0000, Val Loss: 13521828.0000\n",
      "Epoch [1405/50000], Train Loss: 19559556.0000, Val Loss: 13517728.0000\n",
      "Epoch [1406/50000], Train Loss: 19554780.0000, Val Loss: 13513630.0000\n",
      "Epoch [1407/50000], Train Loss: 19550006.0000, Val Loss: 13509534.0000\n",
      "Epoch [1408/50000], Train Loss: 19545234.0000, Val Loss: 13505442.0000\n",
      "Epoch [1409/50000], Train Loss: 19540466.0000, Val Loss: 13501348.0000\n",
      "Epoch [1410/50000], Train Loss: 19535696.0000, Val Loss: 13497253.0000\n",
      "Epoch [1411/50000], Train Loss: 19530924.0000, Val Loss: 13493165.0000\n",
      "Epoch [1412/50000], Train Loss: 19526162.0000, Val Loss: 13489075.0000\n",
      "Epoch [1413/50000], Train Loss: 19521396.0000, Val Loss: 13484986.0000\n",
      "Epoch [1414/50000], Train Loss: 19516634.0000, Val Loss: 13480898.0000\n",
      "Epoch [1415/50000], Train Loss: 19511868.0000, Val Loss: 13476813.0000\n",
      "Epoch [1416/50000], Train Loss: 19507108.0000, Val Loss: 13472730.0000\n",
      "Epoch [1417/50000], Train Loss: 19502350.0000, Val Loss: 13468647.0000\n",
      "Epoch [1418/50000], Train Loss: 19497594.0000, Val Loss: 13464563.0000\n",
      "Epoch [1419/50000], Train Loss: 19492834.0000, Val Loss: 13460482.0000\n",
      "Epoch [1420/50000], Train Loss: 19488074.0000, Val Loss: 13456405.0000\n",
      "Epoch [1421/50000], Train Loss: 19483324.0000, Val Loss: 13452326.0000\n",
      "Epoch [1422/50000], Train Loss: 19478570.0000, Val Loss: 13448250.0000\n",
      "Epoch [1423/50000], Train Loss: 19473822.0000, Val Loss: 13444175.0000\n",
      "Epoch [1424/50000], Train Loss: 19469068.0000, Val Loss: 13440101.0000\n",
      "Epoch [1425/50000], Train Loss: 19464322.0000, Val Loss: 13436028.0000\n",
      "Epoch [1426/50000], Train Loss: 19459570.0000, Val Loss: 13431957.0000\n",
      "Epoch [1427/50000], Train Loss: 19454824.0000, Val Loss: 13427887.0000\n",
      "Epoch [1428/50000], Train Loss: 19450082.0000, Val Loss: 13423819.0000\n",
      "Epoch [1429/50000], Train Loss: 19445338.0000, Val Loss: 13419750.0000\n",
      "Epoch [1430/50000], Train Loss: 19440596.0000, Val Loss: 13415685.0000\n",
      "Epoch [1431/50000], Train Loss: 19435856.0000, Val Loss: 13411621.0000\n",
      "Epoch [1432/50000], Train Loss: 19431116.0000, Val Loss: 13407556.0000\n",
      "Epoch [1433/50000], Train Loss: 19426376.0000, Val Loss: 13403495.0000\n",
      "Epoch [1434/50000], Train Loss: 19421640.0000, Val Loss: 13399430.0000\n",
      "Epoch [1435/50000], Train Loss: 19416904.0000, Val Loss: 13395373.0000\n",
      "Epoch [1436/50000], Train Loss: 19412170.0000, Val Loss: 13391315.0000\n",
      "Epoch [1437/50000], Train Loss: 19407438.0000, Val Loss: 13387255.0000\n",
      "Epoch [1438/50000], Train Loss: 19402704.0000, Val Loss: 13383199.0000\n",
      "Epoch [1439/50000], Train Loss: 19397974.0000, Val Loss: 13379146.0000\n",
      "Epoch [1440/50000], Train Loss: 19393246.0000, Val Loss: 13375093.0000\n",
      "Epoch [1441/50000], Train Loss: 19388520.0000, Val Loss: 13371039.0000\n",
      "Epoch [1442/50000], Train Loss: 19383790.0000, Val Loss: 13366989.0000\n",
      "Epoch [1443/50000], Train Loss: 19379068.0000, Val Loss: 13362942.0000\n",
      "Epoch [1444/50000], Train Loss: 19374344.0000, Val Loss: 13358892.0000\n",
      "Epoch [1445/50000], Train Loss: 19369622.0000, Val Loss: 13354843.0000\n",
      "Epoch [1446/50000], Train Loss: 19364898.0000, Val Loss: 13350801.0000\n",
      "Epoch [1447/50000], Train Loss: 19360184.0000, Val Loss: 13346755.0000\n",
      "Epoch [1448/50000], Train Loss: 19355464.0000, Val Loss: 13342716.0000\n",
      "Epoch [1449/50000], Train Loss: 19350748.0000, Val Loss: 13338671.0000\n",
      "Epoch [1450/50000], Train Loss: 19346032.0000, Val Loss: 13334630.0000\n",
      "Epoch [1451/50000], Train Loss: 19341316.0000, Val Loss: 13330590.0000\n",
      "Epoch [1452/50000], Train Loss: 19336606.0000, Val Loss: 13326555.0000\n",
      "Epoch [1453/50000], Train Loss: 19331896.0000, Val Loss: 13322516.0000\n",
      "Epoch [1454/50000], Train Loss: 19327182.0000, Val Loss: 13318481.0000\n",
      "Epoch [1455/50000], Train Loss: 19322474.0000, Val Loss: 13314446.0000\n",
      "Epoch [1456/50000], Train Loss: 19317766.0000, Val Loss: 13310413.0000\n",
      "Epoch [1457/50000], Train Loss: 19313060.0000, Val Loss: 13306383.0000\n",
      "Epoch [1458/50000], Train Loss: 19308356.0000, Val Loss: 13302352.0000\n",
      "Epoch [1459/50000], Train Loss: 19303648.0000, Val Loss: 13298325.0000\n",
      "Epoch [1460/50000], Train Loss: 19298950.0000, Val Loss: 13294294.0000\n",
      "Epoch [1461/50000], Train Loss: 19294246.0000, Val Loss: 13290268.0000\n",
      "Epoch [1462/50000], Train Loss: 19289548.0000, Val Loss: 13286244.0000\n",
      "Epoch [1463/50000], Train Loss: 19284850.0000, Val Loss: 13282219.0000\n",
      "Epoch [1464/50000], Train Loss: 19280152.0000, Val Loss: 13278195.0000\n",
      "Epoch [1465/50000], Train Loss: 19275454.0000, Val Loss: 13274177.0000\n",
      "Epoch [1466/50000], Train Loss: 19270766.0000, Val Loss: 13270157.0000\n",
      "Epoch [1467/50000], Train Loss: 19266070.0000, Val Loss: 13266135.0000\n",
      "Epoch [1468/50000], Train Loss: 19261382.0000, Val Loss: 13262119.0000\n",
      "Epoch [1469/50000], Train Loss: 19256688.0000, Val Loss: 13258106.0000\n",
      "Epoch [1470/50000], Train Loss: 19252004.0000, Val Loss: 13254089.0000\n",
      "Epoch [1471/50000], Train Loss: 19247312.0000, Val Loss: 13250075.0000\n",
      "Epoch [1472/50000], Train Loss: 19242626.0000, Val Loss: 13246063.0000\n",
      "Epoch [1473/50000], Train Loss: 19237942.0000, Val Loss: 13242052.0000\n",
      "Epoch [1474/50000], Train Loss: 19233262.0000, Val Loss: 13238043.0000\n",
      "Epoch [1475/50000], Train Loss: 19228574.0000, Val Loss: 13234033.0000\n",
      "Epoch [1476/50000], Train Loss: 19223894.0000, Val Loss: 13230027.0000\n",
      "Epoch [1477/50000], Train Loss: 19219216.0000, Val Loss: 13226018.0000\n",
      "Epoch [1478/50000], Train Loss: 19214534.0000, Val Loss: 13222016.0000\n",
      "Epoch [1479/50000], Train Loss: 19209860.0000, Val Loss: 13218014.0000\n",
      "Epoch [1480/50000], Train Loss: 19205186.0000, Val Loss: 13214011.0000\n",
      "Epoch [1481/50000], Train Loss: 19200508.0000, Val Loss: 13210010.0000\n",
      "Epoch [1482/50000], Train Loss: 19195836.0000, Val Loss: 13206009.0000\n",
      "Epoch [1483/50000], Train Loss: 19191162.0000, Val Loss: 13202013.0000\n",
      "Epoch [1484/50000], Train Loss: 19186494.0000, Val Loss: 13198014.0000\n",
      "Epoch [1485/50000], Train Loss: 19181826.0000, Val Loss: 13194016.0000\n",
      "Epoch [1486/50000], Train Loss: 19177158.0000, Val Loss: 13190027.0000\n",
      "Epoch [1487/50000], Train Loss: 19172494.0000, Val Loss: 13186029.0000\n",
      "Epoch [1488/50000], Train Loss: 19167824.0000, Val Loss: 13182038.0000\n",
      "Epoch [1489/50000], Train Loss: 19163160.0000, Val Loss: 13178047.0000\n",
      "Epoch [1490/50000], Train Loss: 19158498.0000, Val Loss: 13174061.0000\n",
      "Epoch [1491/50000], Train Loss: 19153840.0000, Val Loss: 13170071.0000\n",
      "Epoch [1492/50000], Train Loss: 19149180.0000, Val Loss: 13166083.0000\n",
      "Epoch [1493/50000], Train Loss: 19144520.0000, Val Loss: 13162099.0000\n",
      "Epoch [1494/50000], Train Loss: 19139862.0000, Val Loss: 13158113.0000\n",
      "Epoch [1495/50000], Train Loss: 19135208.0000, Val Loss: 13154131.0000\n",
      "Epoch [1496/50000], Train Loss: 19130552.0000, Val Loss: 13150148.0000\n",
      "Epoch [1497/50000], Train Loss: 19125900.0000, Val Loss: 13146169.0000\n",
      "Epoch [1498/50000], Train Loss: 19121248.0000, Val Loss: 13142190.0000\n",
      "Epoch [1499/50000], Train Loss: 19116596.0000, Val Loss: 13138212.0000\n",
      "Epoch [1500/50000], Train Loss: 19111950.0000, Val Loss: 13134235.0000\n",
      "Epoch [1501/50000], Train Loss: 19107298.0000, Val Loss: 13130259.0000\n",
      "Epoch [1502/50000], Train Loss: 19102654.0000, Val Loss: 13126285.0000\n",
      "Epoch [1503/50000], Train Loss: 19098006.0000, Val Loss: 13122313.0000\n",
      "Epoch [1504/50000], Train Loss: 19093364.0000, Val Loss: 13118339.0000\n",
      "Epoch [1505/50000], Train Loss: 19088718.0000, Val Loss: 13114368.0000\n",
      "Epoch [1506/50000], Train Loss: 19084080.0000, Val Loss: 13110401.0000\n",
      "Epoch [1507/50000], Train Loss: 19079440.0000, Val Loss: 13106435.0000\n",
      "Epoch [1508/50000], Train Loss: 19074804.0000, Val Loss: 13102465.0000\n",
      "Epoch [1509/50000], Train Loss: 19070164.0000, Val Loss: 13098501.0000\n",
      "Epoch [1510/50000], Train Loss: 19065528.0000, Val Loss: 13094539.0000\n",
      "Epoch [1511/50000], Train Loss: 19060896.0000, Val Loss: 13090572.0000\n",
      "Epoch [1512/50000], Train Loss: 19056258.0000, Val Loss: 13086614.0000\n",
      "Epoch [1513/50000], Train Loss: 19051628.0000, Val Loss: 13082652.0000\n",
      "Epoch [1514/50000], Train Loss: 19046998.0000, Val Loss: 13078694.0000\n",
      "Epoch [1515/50000], Train Loss: 19042370.0000, Val Loss: 13074737.0000\n",
      "Epoch [1516/50000], Train Loss: 19037740.0000, Val Loss: 13070781.0000\n",
      "Epoch [1517/50000], Train Loss: 19033114.0000, Val Loss: 13066825.0000\n",
      "Epoch [1518/50000], Train Loss: 19028488.0000, Val Loss: 13062870.0000\n",
      "Epoch [1519/50000], Train Loss: 19023864.0000, Val Loss: 13058918.0000\n",
      "Epoch [1520/50000], Train Loss: 19019242.0000, Val Loss: 13054969.0000\n",
      "Epoch [1521/50000], Train Loss: 19014620.0000, Val Loss: 13051017.0000\n",
      "Epoch [1522/50000], Train Loss: 19009996.0000, Val Loss: 13047069.0000\n",
      "Epoch [1523/50000], Train Loss: 19005380.0000, Val Loss: 13043121.0000\n",
      "Epoch [1524/50000], Train Loss: 19000762.0000, Val Loss: 13039177.0000\n",
      "Epoch [1525/50000], Train Loss: 18996148.0000, Val Loss: 13035230.0000\n",
      "Epoch [1526/50000], Train Loss: 18991530.0000, Val Loss: 13031290.0000\n",
      "Epoch [1527/50000], Train Loss: 18986918.0000, Val Loss: 13027342.0000\n",
      "Epoch [1528/50000], Train Loss: 18982304.0000, Val Loss: 13023403.0000\n",
      "Epoch [1529/50000], Train Loss: 18977696.0000, Val Loss: 13019462.0000\n",
      "Epoch [1530/50000], Train Loss: 18973082.0000, Val Loss: 13015522.0000\n",
      "Epoch [1531/50000], Train Loss: 18968472.0000, Val Loss: 13011588.0000\n",
      "Epoch [1532/50000], Train Loss: 18963870.0000, Val Loss: 13007650.0000\n",
      "Epoch [1533/50000], Train Loss: 18959264.0000, Val Loss: 13003716.0000\n",
      "Epoch [1534/50000], Train Loss: 18954658.0000, Val Loss: 12999783.0000\n",
      "Epoch [1535/50000], Train Loss: 18950056.0000, Val Loss: 12995850.0000\n",
      "Epoch [1536/50000], Train Loss: 18945456.0000, Val Loss: 12991919.0000\n",
      "Epoch [1537/50000], Train Loss: 18940854.0000, Val Loss: 12987988.0000\n",
      "Epoch [1538/50000], Train Loss: 18936254.0000, Val Loss: 12984060.0000\n",
      "Epoch [1539/50000], Train Loss: 18931656.0000, Val Loss: 12980134.0000\n",
      "Epoch [1540/50000], Train Loss: 18927060.0000, Val Loss: 12976207.0000\n",
      "Epoch [1541/50000], Train Loss: 18922464.0000, Val Loss: 12972283.0000\n",
      "Epoch [1542/50000], Train Loss: 18917870.0000, Val Loss: 12968361.0000\n",
      "Epoch [1543/50000], Train Loss: 18913276.0000, Val Loss: 12964435.0000\n",
      "Epoch [1544/50000], Train Loss: 18908688.0000, Val Loss: 12960516.0000\n",
      "Epoch [1545/50000], Train Loss: 18904096.0000, Val Loss: 12956594.0000\n",
      "Epoch [1546/50000], Train Loss: 18899506.0000, Val Loss: 12952678.0000\n",
      "Epoch [1547/50000], Train Loss: 18894920.0000, Val Loss: 12948759.0000\n",
      "Epoch [1548/50000], Train Loss: 18890334.0000, Val Loss: 12944844.0000\n",
      "Epoch [1549/50000], Train Loss: 18885746.0000, Val Loss: 12940930.0000\n",
      "Epoch [1550/50000], Train Loss: 18881166.0000, Val Loss: 12937017.0000\n",
      "Epoch [1551/50000], Train Loss: 18876582.0000, Val Loss: 12933103.0000\n",
      "Epoch [1552/50000], Train Loss: 18872002.0000, Val Loss: 12929193.0000\n",
      "Epoch [1553/50000], Train Loss: 18867422.0000, Val Loss: 12925283.0000\n",
      "Epoch [1554/50000], Train Loss: 18862844.0000, Val Loss: 12921373.0000\n",
      "Epoch [1555/50000], Train Loss: 18858264.0000, Val Loss: 12917467.0000\n",
      "Epoch [1556/50000], Train Loss: 18853690.0000, Val Loss: 12913559.0000\n",
      "Epoch [1557/50000], Train Loss: 18849114.0000, Val Loss: 12909655.0000\n",
      "Epoch [1558/50000], Train Loss: 18844542.0000, Val Loss: 12905754.0000\n",
      "Epoch [1559/50000], Train Loss: 18839974.0000, Val Loss: 12901852.0000\n",
      "Epoch [1560/50000], Train Loss: 18835400.0000, Val Loss: 12897949.0000\n",
      "Epoch [1561/50000], Train Loss: 18830830.0000, Val Loss: 12894052.0000\n",
      "Epoch [1562/50000], Train Loss: 18826266.0000, Val Loss: 12890153.0000\n",
      "Epoch [1563/50000], Train Loss: 18821698.0000, Val Loss: 12886255.0000\n",
      "Epoch [1564/50000], Train Loss: 18817132.0000, Val Loss: 12882362.0000\n",
      "Epoch [1565/50000], Train Loss: 18812568.0000, Val Loss: 12878464.0000\n",
      "Epoch [1566/50000], Train Loss: 18808006.0000, Val Loss: 12874571.0000\n",
      "Epoch [1567/50000], Train Loss: 18803444.0000, Val Loss: 12870677.0000\n",
      "Epoch [1568/50000], Train Loss: 18798882.0000, Val Loss: 12866786.0000\n",
      "Epoch [1569/50000], Train Loss: 18794320.0000, Val Loss: 12862897.0000\n",
      "Epoch [1570/50000], Train Loss: 18789766.0000, Val Loss: 12859010.0000\n",
      "Epoch [1571/50000], Train Loss: 18785210.0000, Val Loss: 12855124.0000\n",
      "Epoch [1572/50000], Train Loss: 18780658.0000, Val Loss: 12851238.0000\n",
      "Epoch [1573/50000], Train Loss: 18776102.0000, Val Loss: 12847356.0000\n",
      "Epoch [1574/50000], Train Loss: 18771552.0000, Val Loss: 12843470.0000\n",
      "Epoch [1575/50000], Train Loss: 18767002.0000, Val Loss: 12839587.0000\n",
      "Epoch [1576/50000], Train Loss: 18762452.0000, Val Loss: 12835708.0000\n",
      "Epoch [1577/50000], Train Loss: 18757902.0000, Val Loss: 12831825.0000\n",
      "Epoch [1578/50000], Train Loss: 18753354.0000, Val Loss: 12827945.0000\n",
      "Epoch [1579/50000], Train Loss: 18748808.0000, Val Loss: 12824070.0000\n",
      "Epoch [1580/50000], Train Loss: 18744262.0000, Val Loss: 12820194.0000\n",
      "Epoch [1581/50000], Train Loss: 18739720.0000, Val Loss: 12816318.0000\n",
      "Epoch [1582/50000], Train Loss: 18735176.0000, Val Loss: 12812449.0000\n",
      "Epoch [1583/50000], Train Loss: 18730640.0000, Val Loss: 12808576.0000\n",
      "Epoch [1584/50000], Train Loss: 18726100.0000, Val Loss: 12804703.0000\n",
      "Epoch [1585/50000], Train Loss: 18721562.0000, Val Loss: 12800833.0000\n",
      "Epoch [1586/50000], Train Loss: 18717024.0000, Val Loss: 12796966.0000\n",
      "Epoch [1587/50000], Train Loss: 18712490.0000, Val Loss: 12793101.0000\n",
      "Epoch [1588/50000], Train Loss: 18707956.0000, Val Loss: 12789235.0000\n",
      "Epoch [1589/50000], Train Loss: 18703422.0000, Val Loss: 12785370.0000\n",
      "Epoch [1590/50000], Train Loss: 18698892.0000, Val Loss: 12781506.0000\n",
      "Epoch [1591/50000], Train Loss: 18694360.0000, Val Loss: 12777644.0000\n",
      "Epoch [1592/50000], Train Loss: 18689832.0000, Val Loss: 12773780.0000\n",
      "Epoch [1593/50000], Train Loss: 18685304.0000, Val Loss: 12769923.0000\n",
      "Epoch [1594/50000], Train Loss: 18680778.0000, Val Loss: 12766065.0000\n",
      "Epoch [1595/50000], Train Loss: 18676254.0000, Val Loss: 12762206.0000\n",
      "Epoch [1596/50000], Train Loss: 18671728.0000, Val Loss: 12758350.0000\n",
      "Epoch [1597/50000], Train Loss: 18667206.0000, Val Loss: 12754498.0000\n",
      "Epoch [1598/50000], Train Loss: 18662684.0000, Val Loss: 12750645.0000\n",
      "Epoch [1599/50000], Train Loss: 18658166.0000, Val Loss: 12746791.0000\n",
      "Epoch [1600/50000], Train Loss: 18653648.0000, Val Loss: 12742940.0000\n",
      "Epoch [1601/50000], Train Loss: 18649128.0000, Val Loss: 12739092.0000\n",
      "Epoch [1602/50000], Train Loss: 18644616.0000, Val Loss: 12735243.0000\n",
      "Epoch [1603/50000], Train Loss: 18640100.0000, Val Loss: 12731395.0000\n",
      "Epoch [1604/50000], Train Loss: 18635584.0000, Val Loss: 12727549.0000\n",
      "Epoch [1605/50000], Train Loss: 18631072.0000, Val Loss: 12723706.0000\n",
      "Epoch [1606/50000], Train Loss: 18626564.0000, Val Loss: 12719862.0000\n",
      "Epoch [1607/50000], Train Loss: 18622052.0000, Val Loss: 12716019.0000\n",
      "Epoch [1608/50000], Train Loss: 18617544.0000, Val Loss: 12712178.0000\n",
      "Epoch [1609/50000], Train Loss: 18613040.0000, Val Loss: 12708340.0000\n",
      "Epoch [1610/50000], Train Loss: 18608534.0000, Val Loss: 12704499.0000\n",
      "Epoch [1611/50000], Train Loss: 18604028.0000, Val Loss: 12700661.0000\n",
      "Epoch [1612/50000], Train Loss: 18599524.0000, Val Loss: 12696827.0000\n",
      "Epoch [1613/50000], Train Loss: 18595024.0000, Val Loss: 12692993.0000\n",
      "Epoch [1614/50000], Train Loss: 18590524.0000, Val Loss: 12689161.0000\n",
      "Epoch [1615/50000], Train Loss: 18586026.0000, Val Loss: 12685328.0000\n",
      "Epoch [1616/50000], Train Loss: 18581528.0000, Val Loss: 12681497.0000\n",
      "Epoch [1617/50000], Train Loss: 18577030.0000, Val Loss: 12677666.0000\n",
      "Epoch [1618/50000], Train Loss: 18572538.0000, Val Loss: 12673840.0000\n",
      "Epoch [1619/50000], Train Loss: 18568042.0000, Val Loss: 12670011.0000\n",
      "Epoch [1620/50000], Train Loss: 18563548.0000, Val Loss: 12666183.0000\n",
      "Epoch [1621/50000], Train Loss: 18559056.0000, Val Loss: 12662358.0000\n",
      "Epoch [1622/50000], Train Loss: 18554564.0000, Val Loss: 12658537.0000\n",
      "Epoch [1623/50000], Train Loss: 18550076.0000, Val Loss: 12654714.0000\n",
      "Epoch [1624/50000], Train Loss: 18545592.0000, Val Loss: 12650893.0000\n",
      "Epoch [1625/50000], Train Loss: 18541106.0000, Val Loss: 12647072.0000\n",
      "Epoch [1626/50000], Train Loss: 18536618.0000, Val Loss: 12643253.0000\n",
      "Epoch [1627/50000], Train Loss: 18532132.0000, Val Loss: 12639435.0000\n",
      "Epoch [1628/50000], Train Loss: 18527652.0000, Val Loss: 12635621.0000\n",
      "Epoch [1629/50000], Train Loss: 18523174.0000, Val Loss: 12631806.0000\n",
      "Epoch [1630/50000], Train Loss: 18518692.0000, Val Loss: 12627994.0000\n",
      "Epoch [1631/50000], Train Loss: 18514214.0000, Val Loss: 12624179.0000\n",
      "Epoch [1632/50000], Train Loss: 18509736.0000, Val Loss: 12620369.0000\n",
      "Epoch [1633/50000], Train Loss: 18505260.0000, Val Loss: 12616558.0000\n",
      "Epoch [1634/50000], Train Loss: 18500786.0000, Val Loss: 12612749.0000\n",
      "Epoch [1635/50000], Train Loss: 18496310.0000, Val Loss: 12608943.0000\n",
      "Epoch [1636/50000], Train Loss: 18491840.0000, Val Loss: 12605136.0000\n",
      "Epoch [1637/50000], Train Loss: 18487368.0000, Val Loss: 12601332.0000\n",
      "Epoch [1638/50000], Train Loss: 18482900.0000, Val Loss: 12597527.0000\n",
      "Epoch [1639/50000], Train Loss: 18478432.0000, Val Loss: 12593724.0000\n",
      "Epoch [1640/50000], Train Loss: 18473966.0000, Val Loss: 12589925.0000\n",
      "Epoch [1641/50000], Train Loss: 18469500.0000, Val Loss: 12586126.0000\n",
      "Epoch [1642/50000], Train Loss: 18465032.0000, Val Loss: 12582325.0000\n",
      "Epoch [1643/50000], Train Loss: 18460570.0000, Val Loss: 12578527.0000\n",
      "Epoch [1644/50000], Train Loss: 18456110.0000, Val Loss: 12574731.0000\n",
      "Epoch [1645/50000], Train Loss: 18451648.0000, Val Loss: 12570935.0000\n",
      "Epoch [1646/50000], Train Loss: 18447186.0000, Val Loss: 12567142.0000\n",
      "Epoch [1647/50000], Train Loss: 18442732.0000, Val Loss: 12563348.0000\n",
      "Epoch [1648/50000], Train Loss: 18438274.0000, Val Loss: 12559558.0000\n",
      "Epoch [1649/50000], Train Loss: 18433816.0000, Val Loss: 12555769.0000\n",
      "Epoch [1650/50000], Train Loss: 18429364.0000, Val Loss: 12551979.0000\n",
      "Epoch [1651/50000], Train Loss: 18424912.0000, Val Loss: 12548191.0000\n",
      "Epoch [1652/50000], Train Loss: 18420460.0000, Val Loss: 12544404.0000\n",
      "Epoch [1653/50000], Train Loss: 18416012.0000, Val Loss: 12540621.0000\n",
      "Epoch [1654/50000], Train Loss: 18411560.0000, Val Loss: 12536835.0000\n",
      "Epoch [1655/50000], Train Loss: 18407112.0000, Val Loss: 12533051.0000\n",
      "Epoch [1656/50000], Train Loss: 18402664.0000, Val Loss: 12529270.0000\n",
      "Epoch [1657/50000], Train Loss: 18398222.0000, Val Loss: 12525490.0000\n",
      "Epoch [1658/50000], Train Loss: 18393780.0000, Val Loss: 12521711.0000\n",
      "Epoch [1659/50000], Train Loss: 18389334.0000, Val Loss: 12517933.0000\n",
      "Epoch [1660/50000], Train Loss: 18384892.0000, Val Loss: 12514157.0000\n",
      "Epoch [1661/50000], Train Loss: 18380454.0000, Val Loss: 12510381.0000\n",
      "Epoch [1662/50000], Train Loss: 18376014.0000, Val Loss: 12506608.0000\n",
      "Epoch [1663/50000], Train Loss: 18371578.0000, Val Loss: 12502834.0000\n",
      "Epoch [1664/50000], Train Loss: 18367142.0000, Val Loss: 12499062.0000\n",
      "Epoch [1665/50000], Train Loss: 18362704.0000, Val Loss: 12495292.0000\n",
      "Epoch [1666/50000], Train Loss: 18358270.0000, Val Loss: 12491524.0000\n",
      "Epoch [1667/50000], Train Loss: 18353842.0000, Val Loss: 12487753.0000\n",
      "Epoch [1668/50000], Train Loss: 18349406.0000, Val Loss: 12483987.0000\n",
      "Epoch [1669/50000], Train Loss: 18344976.0000, Val Loss: 12480224.0000\n",
      "Epoch [1670/50000], Train Loss: 18340548.0000, Val Loss: 12476457.0000\n",
      "Epoch [1671/50000], Train Loss: 18336122.0000, Val Loss: 12472694.0000\n",
      "Epoch [1672/50000], Train Loss: 18331696.0000, Val Loss: 12468931.0000\n",
      "Epoch [1673/50000], Train Loss: 18327272.0000, Val Loss: 12465174.0000\n",
      "Epoch [1674/50000], Train Loss: 18322848.0000, Val Loss: 12461412.0000\n",
      "Epoch [1675/50000], Train Loss: 18318424.0000, Val Loss: 12457657.0000\n",
      "Epoch [1676/50000], Train Loss: 18314006.0000, Val Loss: 12453899.0000\n",
      "Epoch [1677/50000], Train Loss: 18309588.0000, Val Loss: 12450144.0000\n",
      "Epoch [1678/50000], Train Loss: 18305170.0000, Val Loss: 12446389.0000\n",
      "Epoch [1679/50000], Train Loss: 18300752.0000, Val Loss: 12442634.0000\n",
      "Epoch [1680/50000], Train Loss: 18296334.0000, Val Loss: 12438882.0000\n",
      "Epoch [1681/50000], Train Loss: 18291918.0000, Val Loss: 12435132.0000\n",
      "Epoch [1682/50000], Train Loss: 18287506.0000, Val Loss: 12431382.0000\n",
      "Epoch [1683/50000], Train Loss: 18283094.0000, Val Loss: 12427633.0000\n",
      "Epoch [1684/50000], Train Loss: 18278682.0000, Val Loss: 12423884.0000\n",
      "Epoch [1685/50000], Train Loss: 18274268.0000, Val Loss: 12420141.0000\n",
      "Epoch [1686/50000], Train Loss: 18269866.0000, Val Loss: 12416394.0000\n",
      "Epoch [1687/50000], Train Loss: 18265458.0000, Val Loss: 12412653.0000\n",
      "Epoch [1688/50000], Train Loss: 18261052.0000, Val Loss: 12408911.0000\n",
      "Epoch [1689/50000], Train Loss: 18256648.0000, Val Loss: 12405170.0000\n",
      "Epoch [1690/50000], Train Loss: 18252246.0000, Val Loss: 12401428.0000\n",
      "Epoch [1691/50000], Train Loss: 18247842.0000, Val Loss: 12397690.0000\n",
      "Epoch [1692/50000], Train Loss: 18243444.0000, Val Loss: 12393952.0000\n",
      "Epoch [1693/50000], Train Loss: 18239044.0000, Val Loss: 12390218.0000\n",
      "Epoch [1694/50000], Train Loss: 18234648.0000, Val Loss: 12386480.0000\n",
      "Epoch [1695/50000], Train Loss: 18230246.0000, Val Loss: 12382750.0000\n",
      "Epoch [1696/50000], Train Loss: 18225854.0000, Val Loss: 12379017.0000\n",
      "Epoch [1697/50000], Train Loss: 18221460.0000, Val Loss: 12375284.0000\n",
      "Epoch [1698/50000], Train Loss: 18217066.0000, Val Loss: 12371554.0000\n",
      "Epoch [1699/50000], Train Loss: 18212674.0000, Val Loss: 12367826.0000\n",
      "Epoch [1700/50000], Train Loss: 18208284.0000, Val Loss: 12364100.0000\n",
      "Epoch [1701/50000], Train Loss: 18203900.0000, Val Loss: 12360370.0000\n",
      "Epoch [1702/50000], Train Loss: 18199506.0000, Val Loss: 12356646.0000\n",
      "Epoch [1703/50000], Train Loss: 18195122.0000, Val Loss: 12352922.0000\n",
      "Epoch [1704/50000], Train Loss: 18190734.0000, Val Loss: 12349200.0000\n",
      "Epoch [1705/50000], Train Loss: 18186352.0000, Val Loss: 12345479.0000\n",
      "Epoch [1706/50000], Train Loss: 18181968.0000, Val Loss: 12341758.0000\n",
      "Epoch [1707/50000], Train Loss: 18177586.0000, Val Loss: 12338039.0000\n",
      "Epoch [1708/50000], Train Loss: 18173206.0000, Val Loss: 12334321.0000\n",
      "Epoch [1709/50000], Train Loss: 18168828.0000, Val Loss: 12330604.0000\n",
      "Epoch [1710/50000], Train Loss: 18164448.0000, Val Loss: 12326889.0000\n",
      "Epoch [1711/50000], Train Loss: 18160072.0000, Val Loss: 12323178.0000\n",
      "Epoch [1712/50000], Train Loss: 18155700.0000, Val Loss: 12319461.0000\n",
      "Epoch [1713/50000], Train Loss: 18151324.0000, Val Loss: 12315750.0000\n",
      "Epoch [1714/50000], Train Loss: 18146950.0000, Val Loss: 12312041.0000\n",
      "Epoch [1715/50000], Train Loss: 18142580.0000, Val Loss: 12308331.0000\n",
      "Epoch [1716/50000], Train Loss: 18138210.0000, Val Loss: 12304621.0000\n",
      "Epoch [1717/50000], Train Loss: 18133840.0000, Val Loss: 12300915.0000\n",
      "Epoch [1718/50000], Train Loss: 18129474.0000, Val Loss: 12297212.0000\n",
      "Epoch [1719/50000], Train Loss: 18125108.0000, Val Loss: 12293506.0000\n",
      "Epoch [1720/50000], Train Loss: 18120742.0000, Val Loss: 12289804.0000\n",
      "Epoch [1721/50000], Train Loss: 18116380.0000, Val Loss: 12286099.0000\n",
      "Epoch [1722/50000], Train Loss: 18112012.0000, Val Loss: 12282400.0000\n",
      "Epoch [1723/50000], Train Loss: 18107652.0000, Val Loss: 12278699.0000\n",
      "Epoch [1724/50000], Train Loss: 18103290.0000, Val Loss: 12274999.0000\n",
      "Epoch [1725/50000], Train Loss: 18098934.0000, Val Loss: 12271302.0000\n",
      "Epoch [1726/50000], Train Loss: 18094576.0000, Val Loss: 12267610.0000\n",
      "Epoch [1727/50000], Train Loss: 18090222.0000, Val Loss: 12263915.0000\n",
      "Epoch [1728/50000], Train Loss: 18085866.0000, Val Loss: 12260220.0000\n",
      "Epoch [1729/50000], Train Loss: 18081510.0000, Val Loss: 12256527.0000\n",
      "Epoch [1730/50000], Train Loss: 18077160.0000, Val Loss: 12252838.0000\n",
      "Epoch [1731/50000], Train Loss: 18072810.0000, Val Loss: 12249148.0000\n",
      "Epoch [1732/50000], Train Loss: 18068460.0000, Val Loss: 12245457.0000\n",
      "Epoch [1733/50000], Train Loss: 18064106.0000, Val Loss: 12241771.0000\n",
      "Epoch [1734/50000], Train Loss: 18059762.0000, Val Loss: 12238083.0000\n",
      "Epoch [1735/50000], Train Loss: 18055416.0000, Val Loss: 12234400.0000\n",
      "Epoch [1736/50000], Train Loss: 18051072.0000, Val Loss: 12230713.0000\n",
      "Epoch [1737/50000], Train Loss: 18046722.0000, Val Loss: 12227031.0000\n",
      "Epoch [1738/50000], Train Loss: 18042384.0000, Val Loss: 12223351.0000\n",
      "Epoch [1739/50000], Train Loss: 18038042.0000, Val Loss: 12219671.0000\n",
      "Epoch [1740/50000], Train Loss: 18033704.0000, Val Loss: 12215991.0000\n",
      "Epoch [1741/50000], Train Loss: 18029364.0000, Val Loss: 12212315.0000\n",
      "Epoch [1742/50000], Train Loss: 18025028.0000, Val Loss: 12208636.0000\n",
      "Epoch [1743/50000], Train Loss: 18020690.0000, Val Loss: 12204962.0000\n",
      "Epoch [1744/50000], Train Loss: 18016356.0000, Val Loss: 12201289.0000\n",
      "Epoch [1745/50000], Train Loss: 18012026.0000, Val Loss: 12197616.0000\n",
      "Epoch [1746/50000], Train Loss: 18007692.0000, Val Loss: 12193946.0000\n",
      "Epoch [1747/50000], Train Loss: 18003362.0000, Val Loss: 12190274.0000\n",
      "Epoch [1748/50000], Train Loss: 17999032.0000, Val Loss: 12186606.0000\n",
      "Epoch [1749/50000], Train Loss: 17994702.0000, Val Loss: 12182938.0000\n",
      "Epoch [1750/50000], Train Loss: 17990376.0000, Val Loss: 12179268.0000\n",
      "Epoch [1751/50000], Train Loss: 17986046.0000, Val Loss: 12175603.0000\n",
      "Epoch [1752/50000], Train Loss: 17981726.0000, Val Loss: 12171939.0000\n",
      "Epoch [1753/50000], Train Loss: 17977400.0000, Val Loss: 12168276.0000\n",
      "Epoch [1754/50000], Train Loss: 17973080.0000, Val Loss: 12164613.0000\n",
      "Epoch [1755/50000], Train Loss: 17968758.0000, Val Loss: 12160951.0000\n",
      "Epoch [1756/50000], Train Loss: 17964436.0000, Val Loss: 12157291.0000\n",
      "Epoch [1757/50000], Train Loss: 17960116.0000, Val Loss: 12153636.0000\n",
      "Epoch [1758/50000], Train Loss: 17955802.0000, Val Loss: 12149978.0000\n",
      "Epoch [1759/50000], Train Loss: 17951488.0000, Val Loss: 12146323.0000\n",
      "Epoch [1760/50000], Train Loss: 17947172.0000, Val Loss: 12142666.0000\n",
      "Epoch [1761/50000], Train Loss: 17942856.0000, Val Loss: 12139014.0000\n",
      "Epoch [1762/50000], Train Loss: 17938544.0000, Val Loss: 12135361.0000\n",
      "Epoch [1763/50000], Train Loss: 17934234.0000, Val Loss: 12131709.0000\n",
      "Epoch [1764/50000], Train Loss: 17929924.0000, Val Loss: 12128061.0000\n",
      "Epoch [1765/50000], Train Loss: 17925616.0000, Val Loss: 12124412.0000\n",
      "Epoch [1766/50000], Train Loss: 17921312.0000, Val Loss: 12120765.0000\n",
      "Epoch [1767/50000], Train Loss: 17917004.0000, Val Loss: 12117120.0000\n",
      "Epoch [1768/50000], Train Loss: 17912698.0000, Val Loss: 12113475.0000\n",
      "Epoch [1769/50000], Train Loss: 17908398.0000, Val Loss: 12109828.0000\n",
      "Epoch [1770/50000], Train Loss: 17904094.0000, Val Loss: 12106186.0000\n",
      "Epoch [1771/50000], Train Loss: 17899790.0000, Val Loss: 12102547.0000\n",
      "Epoch [1772/50000], Train Loss: 17895494.0000, Val Loss: 12098903.0000\n",
      "Epoch [1773/50000], Train Loss: 17891192.0000, Val Loss: 12095265.0000\n",
      "Epoch [1774/50000], Train Loss: 17886896.0000, Val Loss: 12091627.0000\n",
      "Epoch [1775/50000], Train Loss: 17882598.0000, Val Loss: 12087991.0000\n",
      "Epoch [1776/50000], Train Loss: 17878304.0000, Val Loss: 12084353.0000\n",
      "Epoch [1777/50000], Train Loss: 17874008.0000, Val Loss: 12080719.0000\n",
      "Epoch [1778/50000], Train Loss: 17869718.0000, Val Loss: 12077085.0000\n",
      "Epoch [1779/50000], Train Loss: 17865424.0000, Val Loss: 12073456.0000\n",
      "Epoch [1780/50000], Train Loss: 17861140.0000, Val Loss: 12069824.0000\n",
      "Epoch [1781/50000], Train Loss: 17856852.0000, Val Loss: 12066195.0000\n",
      "Epoch [1782/50000], Train Loss: 17852562.0000, Val Loss: 12062567.0000\n",
      "Epoch [1783/50000], Train Loss: 17848278.0000, Val Loss: 12058940.0000\n",
      "Epoch [1784/50000], Train Loss: 17843992.0000, Val Loss: 12055313.0000\n",
      "Epoch [1785/50000], Train Loss: 17839708.0000, Val Loss: 12051689.0000\n",
      "Epoch [1786/50000], Train Loss: 17835426.0000, Val Loss: 12048066.0000\n",
      "Epoch [1787/50000], Train Loss: 17831146.0000, Val Loss: 12044442.0000\n",
      "Epoch [1788/50000], Train Loss: 17826866.0000, Val Loss: 12040823.0000\n",
      "Epoch [1789/50000], Train Loss: 17822590.0000, Val Loss: 12037199.0000\n",
      "Epoch [1790/50000], Train Loss: 17818308.0000, Val Loss: 12033585.0000\n",
      "Epoch [1791/50000], Train Loss: 17814036.0000, Val Loss: 12029967.0000\n",
      "Epoch [1792/50000], Train Loss: 17809762.0000, Val Loss: 12026350.0000\n",
      "Epoch [1793/50000], Train Loss: 17805484.0000, Val Loss: 12022734.0000\n",
      "Epoch [1794/50000], Train Loss: 17801212.0000, Val Loss: 12019119.0000\n",
      "Epoch [1795/50000], Train Loss: 17796940.0000, Val Loss: 12015506.0000\n",
      "Epoch [1796/50000], Train Loss: 17792670.0000, Val Loss: 12011897.0000\n",
      "Epoch [1797/50000], Train Loss: 17788404.0000, Val Loss: 12008284.0000\n",
      "Epoch [1798/50000], Train Loss: 17784136.0000, Val Loss: 12004676.0000\n",
      "Epoch [1799/50000], Train Loss: 17779870.0000, Val Loss: 12001067.0000\n",
      "Epoch [1800/50000], Train Loss: 17775604.0000, Val Loss: 11997460.0000\n",
      "Epoch [1801/50000], Train Loss: 17771340.0000, Val Loss: 11993855.0000\n",
      "Epoch [1802/50000], Train Loss: 17767078.0000, Val Loss: 11990253.0000\n",
      "Epoch [1803/50000], Train Loss: 17762820.0000, Val Loss: 11986650.0000\n",
      "Epoch [1804/50000], Train Loss: 17758560.0000, Val Loss: 11983046.0000\n",
      "Epoch [1805/50000], Train Loss: 17754302.0000, Val Loss: 11979449.0000\n",
      "Epoch [1806/50000], Train Loss: 17750042.0000, Val Loss: 11975847.0000\n",
      "Epoch [1807/50000], Train Loss: 17745788.0000, Val Loss: 11972250.0000\n",
      "Epoch [1808/50000], Train Loss: 17741532.0000, Val Loss: 11968651.0000\n",
      "Epoch [1809/50000], Train Loss: 17737278.0000, Val Loss: 11965056.0000\n",
      "Epoch [1810/50000], Train Loss: 17733026.0000, Val Loss: 11961461.0000\n",
      "Epoch [1811/50000], Train Loss: 17728776.0000, Val Loss: 11957867.0000\n",
      "Epoch [1812/50000], Train Loss: 17724524.0000, Val Loss: 11954274.0000\n",
      "Epoch [1813/50000], Train Loss: 17720276.0000, Val Loss: 11950684.0000\n",
      "Epoch [1814/50000], Train Loss: 17716028.0000, Val Loss: 11947095.0000\n",
      "Epoch [1815/50000], Train Loss: 17711784.0000, Val Loss: 11943505.0000\n",
      "Epoch [1816/50000], Train Loss: 17707540.0000, Val Loss: 11939915.0000\n",
      "Epoch [1817/50000], Train Loss: 17703294.0000, Val Loss: 11936331.0000\n",
      "Epoch [1818/50000], Train Loss: 17699052.0000, Val Loss: 11932745.0000\n",
      "Epoch [1819/50000], Train Loss: 17694810.0000, Val Loss: 11929162.0000\n",
      "Epoch [1820/50000], Train Loss: 17690574.0000, Val Loss: 11925578.0000\n",
      "Epoch [1821/50000], Train Loss: 17686330.0000, Val Loss: 11921996.0000\n",
      "Epoch [1822/50000], Train Loss: 17682092.0000, Val Loss: 11918417.0000\n",
      "Epoch [1823/50000], Train Loss: 17677862.0000, Val Loss: 11914836.0000\n",
      "Epoch [1824/50000], Train Loss: 17673622.0000, Val Loss: 11911260.0000\n",
      "Epoch [1825/50000], Train Loss: 17669392.0000, Val Loss: 11907681.0000\n",
      "Epoch [1826/50000], Train Loss: 17665154.0000, Val Loss: 11904106.0000\n",
      "Epoch [1827/50000], Train Loss: 17660926.0000, Val Loss: 11900530.0000\n",
      "Epoch [1828/50000], Train Loss: 17656692.0000, Val Loss: 11896959.0000\n",
      "Epoch [1829/50000], Train Loss: 17652466.0000, Val Loss: 11893388.0000\n",
      "Epoch [1830/50000], Train Loss: 17648240.0000, Val Loss: 11889818.0000\n",
      "Epoch [1831/50000], Train Loss: 17644012.0000, Val Loss: 11886246.0000\n",
      "Epoch [1832/50000], Train Loss: 17639788.0000, Val Loss: 11882679.0000\n",
      "Epoch [1833/50000], Train Loss: 17635568.0000, Val Loss: 11879113.0000\n",
      "Epoch [1834/50000], Train Loss: 17631342.0000, Val Loss: 11875543.0000\n",
      "Epoch [1835/50000], Train Loss: 17627118.0000, Val Loss: 11871979.0000\n",
      "Epoch [1836/50000], Train Loss: 17622900.0000, Val Loss: 11868417.0000\n",
      "Epoch [1837/50000], Train Loss: 17618684.0000, Val Loss: 11864852.0000\n",
      "Epoch [1838/50000], Train Loss: 17614464.0000, Val Loss: 11861293.0000\n",
      "Epoch [1839/50000], Train Loss: 17610248.0000, Val Loss: 11857730.0000\n",
      "Epoch [1840/50000], Train Loss: 17606032.0000, Val Loss: 11854174.0000\n",
      "Epoch [1841/50000], Train Loss: 17601818.0000, Val Loss: 11850615.0000\n",
      "Epoch [1842/50000], Train Loss: 17597606.0000, Val Loss: 11847058.0000\n",
      "Epoch [1843/50000], Train Loss: 17593394.0000, Val Loss: 11843504.0000\n",
      "Epoch [1844/50000], Train Loss: 17589184.0000, Val Loss: 11839946.0000\n",
      "Epoch [1845/50000], Train Loss: 17584974.0000, Val Loss: 11836394.0000\n",
      "Epoch [1846/50000], Train Loss: 17580766.0000, Val Loss: 11832843.0000\n",
      "Epoch [1847/50000], Train Loss: 17576560.0000, Val Loss: 11829293.0000\n",
      "Epoch [1848/50000], Train Loss: 17572354.0000, Val Loss: 11825742.0000\n",
      "Epoch [1849/50000], Train Loss: 17568152.0000, Val Loss: 11822195.0000\n",
      "Epoch [1850/50000], Train Loss: 17563948.0000, Val Loss: 11818647.0000\n",
      "Epoch [1851/50000], Train Loss: 17559746.0000, Val Loss: 11815103.0000\n",
      "Epoch [1852/50000], Train Loss: 17555552.0000, Val Loss: 11811556.0000\n",
      "Epoch [1853/50000], Train Loss: 17551346.0000, Val Loss: 11808014.0000\n",
      "Epoch [1854/50000], Train Loss: 17547148.0000, Val Loss: 11804470.0000\n",
      "Epoch [1855/50000], Train Loss: 17542950.0000, Val Loss: 11800929.0000\n",
      "Epoch [1856/50000], Train Loss: 17538756.0000, Val Loss: 11797391.0000\n",
      "Epoch [1857/50000], Train Loss: 17534566.0000, Val Loss: 11793852.0000\n",
      "Epoch [1858/50000], Train Loss: 17530372.0000, Val Loss: 11790313.0000\n",
      "Epoch [1859/50000], Train Loss: 17526176.0000, Val Loss: 11786778.0000\n",
      "Epoch [1860/50000], Train Loss: 17521988.0000, Val Loss: 11783243.0000\n",
      "Epoch [1861/50000], Train Loss: 17517802.0000, Val Loss: 11779708.0000\n",
      "Epoch [1862/50000], Train Loss: 17513612.0000, Val Loss: 11776175.0000\n",
      "Epoch [1863/50000], Train Loss: 17509424.0000, Val Loss: 11772642.0000\n",
      "Epoch [1864/50000], Train Loss: 17505236.0000, Val Loss: 11769113.0000\n",
      "Epoch [1865/50000], Train Loss: 17501054.0000, Val Loss: 11765581.0000\n",
      "Epoch [1866/50000], Train Loss: 17496870.0000, Val Loss: 11762053.0000\n",
      "Epoch [1867/50000], Train Loss: 17492688.0000, Val Loss: 11758527.0000\n",
      "Epoch [1868/50000], Train Loss: 17488508.0000, Val Loss: 11754999.0000\n",
      "Epoch [1869/50000], Train Loss: 17484326.0000, Val Loss: 11751475.0000\n",
      "Epoch [1870/50000], Train Loss: 17480150.0000, Val Loss: 11747951.0000\n",
      "Epoch [1871/50000], Train Loss: 17475970.0000, Val Loss: 11744431.0000\n",
      "Epoch [1872/50000], Train Loss: 17471798.0000, Val Loss: 11740909.0000\n",
      "Epoch [1873/50000], Train Loss: 17467624.0000, Val Loss: 11737391.0000\n",
      "Epoch [1874/50000], Train Loss: 17463448.0000, Val Loss: 11733871.0000\n",
      "Epoch [1875/50000], Train Loss: 17459276.0000, Val Loss: 11730352.0000\n",
      "Epoch [1876/50000], Train Loss: 17455102.0000, Val Loss: 11726836.0000\n",
      "Epoch [1877/50000], Train Loss: 17450936.0000, Val Loss: 11723322.0000\n",
      "Epoch [1878/50000], Train Loss: 17446766.0000, Val Loss: 11719809.0000\n",
      "Epoch [1879/50000], Train Loss: 17442602.0000, Val Loss: 11716294.0000\n",
      "Epoch [1880/50000], Train Loss: 17438434.0000, Val Loss: 11712785.0000\n",
      "Epoch [1881/50000], Train Loss: 17434272.0000, Val Loss: 11709270.0000\n",
      "Epoch [1882/50000], Train Loss: 17430104.0000, Val Loss: 11705764.0000\n",
      "Epoch [1883/50000], Train Loss: 17425944.0000, Val Loss: 11702257.0000\n",
      "Epoch [1884/50000], Train Loss: 17421784.0000, Val Loss: 11698748.0000\n",
      "Epoch [1885/50000], Train Loss: 17417624.0000, Val Loss: 11695245.0000\n",
      "Epoch [1886/50000], Train Loss: 17413466.0000, Val Loss: 11691738.0000\n",
      "Epoch [1887/50000], Train Loss: 17409306.0000, Val Loss: 11688235.0000\n",
      "Epoch [1888/50000], Train Loss: 17405152.0000, Val Loss: 11684733.0000\n",
      "Epoch [1889/50000], Train Loss: 17400998.0000, Val Loss: 11681233.0000\n",
      "Epoch [1890/50000], Train Loss: 17396842.0000, Val Loss: 11677732.0000\n",
      "Epoch [1891/50000], Train Loss: 17392692.0000, Val Loss: 11674233.0000\n",
      "Epoch [1892/50000], Train Loss: 17388538.0000, Val Loss: 11670736.0000\n",
      "Epoch [1893/50000], Train Loss: 17384390.0000, Val Loss: 11667237.0000\n",
      "Epoch [1894/50000], Train Loss: 17380238.0000, Val Loss: 11663745.0000\n",
      "Epoch [1895/50000], Train Loss: 17376094.0000, Val Loss: 11660250.0000\n",
      "Epoch [1896/50000], Train Loss: 17371946.0000, Val Loss: 11656755.0000\n",
      "Epoch [1897/50000], Train Loss: 17367800.0000, Val Loss: 11653264.0000\n",
      "Epoch [1898/50000], Train Loss: 17363656.0000, Val Loss: 11649774.0000\n",
      "Epoch [1899/50000], Train Loss: 17359512.0000, Val Loss: 11646282.0000\n",
      "Epoch [1900/50000], Train Loss: 17355370.0000, Val Loss: 11642796.0000\n",
      "Epoch [1901/50000], Train Loss: 17351234.0000, Val Loss: 11639311.0000\n",
      "Epoch [1902/50000], Train Loss: 17347094.0000, Val Loss: 11635823.0000\n",
      "Epoch [1903/50000], Train Loss: 17342952.0000, Val Loss: 11632339.0000\n",
      "Epoch [1904/50000], Train Loss: 17338818.0000, Val Loss: 11628854.0000\n",
      "Epoch [1905/50000], Train Loss: 17334680.0000, Val Loss: 11625375.0000\n",
      "Epoch [1906/50000], Train Loss: 17330548.0000, Val Loss: 11621893.0000\n",
      "Epoch [1907/50000], Train Loss: 17326414.0000, Val Loss: 11618412.0000\n",
      "Epoch [1908/50000], Train Loss: 17322282.0000, Val Loss: 11614933.0000\n",
      "Epoch [1909/50000], Train Loss: 17318150.0000, Val Loss: 11611456.0000\n",
      "Epoch [1910/50000], Train Loss: 17314024.0000, Val Loss: 11607979.0000\n",
      "Epoch [1911/50000], Train Loss: 17309896.0000, Val Loss: 11604506.0000\n",
      "Epoch [1912/50000], Train Loss: 17305768.0000, Val Loss: 11601029.0000\n",
      "Epoch [1913/50000], Train Loss: 17301644.0000, Val Loss: 11597559.0000\n",
      "Epoch [1914/50000], Train Loss: 17297520.0000, Val Loss: 11594085.0000\n",
      "Epoch [1915/50000], Train Loss: 17293396.0000, Val Loss: 11590614.0000\n",
      "Epoch [1916/50000], Train Loss: 17289274.0000, Val Loss: 11587148.0000\n",
      "Epoch [1917/50000], Train Loss: 17285154.0000, Val Loss: 11583678.0000\n",
      "Epoch [1918/50000], Train Loss: 17281032.0000, Val Loss: 11580211.0000\n",
      "Epoch [1919/50000], Train Loss: 17276914.0000, Val Loss: 11576745.0000\n",
      "Epoch [1920/50000], Train Loss: 17272798.0000, Val Loss: 11573280.0000\n",
      "Epoch [1921/50000], Train Loss: 17268684.0000, Val Loss: 11569817.0000\n",
      "Epoch [1922/50000], Train Loss: 17264566.0000, Val Loss: 11566355.0000\n",
      "Epoch [1923/50000], Train Loss: 17260454.0000, Val Loss: 11562891.0000\n",
      "Epoch [1924/50000], Train Loss: 17256340.0000, Val Loss: 11559433.0000\n",
      "Epoch [1925/50000], Train Loss: 17252228.0000, Val Loss: 11555974.0000\n",
      "Epoch [1926/50000], Train Loss: 17248120.0000, Val Loss: 11552514.0000\n",
      "Epoch [1927/50000], Train Loss: 17244012.0000, Val Loss: 11549059.0000\n",
      "Epoch [1928/50000], Train Loss: 17239904.0000, Val Loss: 11545604.0000\n",
      "Epoch [1929/50000], Train Loss: 17235798.0000, Val Loss: 11542150.0000\n",
      "Epoch [1930/50000], Train Loss: 17231692.0000, Val Loss: 11538698.0000\n",
      "Epoch [1931/50000], Train Loss: 17227590.0000, Val Loss: 11535243.0000\n",
      "Epoch [1932/50000], Train Loss: 17223484.0000, Val Loss: 11531795.0000\n",
      "Epoch [1933/50000], Train Loss: 17219386.0000, Val Loss: 11528346.0000\n",
      "Epoch [1934/50000], Train Loss: 17215286.0000, Val Loss: 11524896.0000\n",
      "Epoch [1935/50000], Train Loss: 17211186.0000, Val Loss: 11521447.0000\n",
      "Epoch [1936/50000], Train Loss: 17207088.0000, Val Loss: 11518002.0000\n",
      "Epoch [1937/50000], Train Loss: 17202988.0000, Val Loss: 11514558.0000\n",
      "Epoch [1938/50000], Train Loss: 17198896.0000, Val Loss: 11511115.0000\n",
      "Epoch [1939/50000], Train Loss: 17194802.0000, Val Loss: 11507673.0000\n",
      "Epoch [1940/50000], Train Loss: 17190710.0000, Val Loss: 11504231.0000\n",
      "Epoch [1941/50000], Train Loss: 17186618.0000, Val Loss: 11500789.0000\n",
      "Epoch [1942/50000], Train Loss: 17182526.0000, Val Loss: 11497353.0000\n",
      "Epoch [1943/50000], Train Loss: 17178440.0000, Val Loss: 11493914.0000\n",
      "Epoch [1944/50000], Train Loss: 17174352.0000, Val Loss: 11490477.0000\n",
      "Epoch [1945/50000], Train Loss: 17170264.0000, Val Loss: 11487041.0000\n",
      "Epoch [1946/50000], Train Loss: 17166176.0000, Val Loss: 11483607.0000\n",
      "Epoch [1947/50000], Train Loss: 17162094.0000, Val Loss: 11480175.0000\n",
      "Epoch [1948/50000], Train Loss: 17158012.0000, Val Loss: 11476741.0000\n",
      "Epoch [1949/50000], Train Loss: 17153926.0000, Val Loss: 11473314.0000\n",
      "Epoch [1950/50000], Train Loss: 17149850.0000, Val Loss: 11469883.0000\n",
      "Epoch [1951/50000], Train Loss: 17145768.0000, Val Loss: 11466453.0000\n",
      "Epoch [1952/50000], Train Loss: 17141692.0000, Val Loss: 11463026.0000\n",
      "Epoch [1953/50000], Train Loss: 17137612.0000, Val Loss: 11459599.0000\n",
      "Epoch [1954/50000], Train Loss: 17133536.0000, Val Loss: 11456174.0000\n",
      "Epoch [1955/50000], Train Loss: 17129460.0000, Val Loss: 11452750.0000\n",
      "Epoch [1956/50000], Train Loss: 17125390.0000, Val Loss: 11449326.0000\n",
      "Epoch [1957/50000], Train Loss: 17121314.0000, Val Loss: 11445905.0000\n",
      "Epoch [1958/50000], Train Loss: 17117246.0000, Val Loss: 11442485.0000\n",
      "Epoch [1959/50000], Train Loss: 17113174.0000, Val Loss: 11439066.0000\n",
      "Epoch [1960/50000], Train Loss: 17109106.0000, Val Loss: 11435648.0000\n",
      "Epoch [1961/50000], Train Loss: 17105040.0000, Val Loss: 11432230.0000\n",
      "Epoch [1962/50000], Train Loss: 17100972.0000, Val Loss: 11428816.0000\n",
      "Epoch [1963/50000], Train Loss: 17096908.0000, Val Loss: 11425398.0000\n",
      "Epoch [1964/50000], Train Loss: 17092842.0000, Val Loss: 11421985.0000\n",
      "Epoch [1965/50000], Train Loss: 17088782.0000, Val Loss: 11418575.0000\n",
      "Epoch [1966/50000], Train Loss: 17084720.0000, Val Loss: 11415164.0000\n",
      "Epoch [1967/50000], Train Loss: 17080662.0000, Val Loss: 11411756.0000\n",
      "Epoch [1968/50000], Train Loss: 17076604.0000, Val Loss: 11408343.0000\n",
      "Epoch [1969/50000], Train Loss: 17072544.0000, Val Loss: 11404938.0000\n",
      "Epoch [1970/50000], Train Loss: 17068486.0000, Val Loss: 11401531.0000\n",
      "Epoch [1971/50000], Train Loss: 17064432.0000, Val Loss: 11398125.0000\n",
      "Epoch [1972/50000], Train Loss: 17060374.0000, Val Loss: 11394720.0000\n",
      "Epoch [1973/50000], Train Loss: 17056322.0000, Val Loss: 11391319.0000\n",
      "Epoch [1974/50000], Train Loss: 17052274.0000, Val Loss: 11387916.0000\n",
      "Epoch [1975/50000], Train Loss: 17048222.0000, Val Loss: 11384514.0000\n",
      "Epoch [1976/50000], Train Loss: 17044170.0000, Val Loss: 11381117.0000\n",
      "Epoch [1977/50000], Train Loss: 17040126.0000, Val Loss: 11377717.0000\n",
      "Epoch [1978/50000], Train Loss: 17036078.0000, Val Loss: 11374319.0000\n",
      "Epoch [1979/50000], Train Loss: 17032030.0000, Val Loss: 11370925.0000\n",
      "Epoch [1980/50000], Train Loss: 17027986.0000, Val Loss: 11367530.0000\n",
      "Epoch [1981/50000], Train Loss: 17023944.0000, Val Loss: 11364137.0000\n",
      "Epoch [1982/50000], Train Loss: 17019900.0000, Val Loss: 11360742.0000\n",
      "Epoch [1983/50000], Train Loss: 17015862.0000, Val Loss: 11357350.0000\n",
      "Epoch [1984/50000], Train Loss: 17011820.0000, Val Loss: 11353962.0000\n",
      "Epoch [1985/50000], Train Loss: 17007782.0000, Val Loss: 11350571.0000\n",
      "Epoch [1986/50000], Train Loss: 17003744.0000, Val Loss: 11347184.0000\n",
      "Epoch [1987/50000], Train Loss: 16999710.0000, Val Loss: 11343796.0000\n",
      "Epoch [1988/50000], Train Loss: 16995672.0000, Val Loss: 11340411.0000\n",
      "Epoch [1989/50000], Train Loss: 16991642.0000, Val Loss: 11337027.0000\n",
      "Epoch [1990/50000], Train Loss: 16987608.0000, Val Loss: 11333643.0000\n",
      "Epoch [1991/50000], Train Loss: 16983576.0000, Val Loss: 11330259.0000\n",
      "Epoch [1992/50000], Train Loss: 16979544.0000, Val Loss: 11326879.0000\n",
      "Epoch [1993/50000], Train Loss: 16975516.0000, Val Loss: 11323499.0000\n",
      "Epoch [1994/50000], Train Loss: 16971488.0000, Val Loss: 11320118.0000\n",
      "Epoch [1995/50000], Train Loss: 16967460.0000, Val Loss: 11316745.0000\n",
      "Epoch [1996/50000], Train Loss: 16963438.0000, Val Loss: 11313366.0000\n",
      "Epoch [1997/50000], Train Loss: 16959410.0000, Val Loss: 11309990.0000\n",
      "Epoch [1998/50000], Train Loss: 16955388.0000, Val Loss: 11306619.0000\n",
      "Epoch [1999/50000], Train Loss: 16951368.0000, Val Loss: 11303244.0000\n",
      "Epoch [2000/50000], Train Loss: 16947346.0000, Val Loss: 11299874.0000\n",
      "Epoch [2001/50000], Train Loss: 16943328.0000, Val Loss: 11296500.0000\n",
      "Epoch [2002/50000], Train Loss: 16939308.0000, Val Loss: 11293131.0000\n",
      "Epoch [2003/50000], Train Loss: 16935292.0000, Val Loss: 11289764.0000\n",
      "Epoch [2004/50000], Train Loss: 16931276.0000, Val Loss: 11286396.0000\n",
      "Epoch [2005/50000], Train Loss: 16927262.0000, Val Loss: 11283028.0000\n",
      "Epoch [2006/50000], Train Loss: 16923248.0000, Val Loss: 11279665.0000\n",
      "Epoch [2007/50000], Train Loss: 16919236.0000, Val Loss: 11276301.0000\n",
      "Epoch [2008/50000], Train Loss: 16915226.0000, Val Loss: 11272940.0000\n",
      "Epoch [2009/50000], Train Loss: 16911216.0000, Val Loss: 11269578.0000\n",
      "Epoch [2010/50000], Train Loss: 16907206.0000, Val Loss: 11266219.0000\n",
      "Epoch [2011/50000], Train Loss: 16903200.0000, Val Loss: 11262860.0000\n",
      "Epoch [2012/50000], Train Loss: 16899194.0000, Val Loss: 11259502.0000\n",
      "Epoch [2013/50000], Train Loss: 16895188.0000, Val Loss: 11256143.0000\n",
      "Epoch [2014/50000], Train Loss: 16891186.0000, Val Loss: 11252788.0000\n",
      "Epoch [2015/50000], Train Loss: 16887184.0000, Val Loss: 11249433.0000\n",
      "Epoch [2016/50000], Train Loss: 16883184.0000, Val Loss: 11246080.0000\n",
      "Epoch [2017/50000], Train Loss: 16879184.0000, Val Loss: 11242729.0000\n",
      "Epoch [2018/50000], Train Loss: 16875184.0000, Val Loss: 11239375.0000\n",
      "Epoch [2019/50000], Train Loss: 16871184.0000, Val Loss: 11236025.0000\n",
      "Epoch [2020/50000], Train Loss: 16867186.0000, Val Loss: 11232678.0000\n",
      "Epoch [2021/50000], Train Loss: 16863192.0000, Val Loss: 11229331.0000\n",
      "Epoch [2022/50000], Train Loss: 16859198.0000, Val Loss: 11225983.0000\n",
      "Epoch [2023/50000], Train Loss: 16855204.0000, Val Loss: 11222639.0000\n",
      "Epoch [2024/50000], Train Loss: 16851214.0000, Val Loss: 11219295.0000\n",
      "Epoch [2025/50000], Train Loss: 16847226.0000, Val Loss: 11215951.0000\n",
      "Epoch [2026/50000], Train Loss: 16843234.0000, Val Loss: 11212610.0000\n",
      "Epoch [2027/50000], Train Loss: 16839246.0000, Val Loss: 11209269.0000\n",
      "Epoch [2028/50000], Train Loss: 16835260.0000, Val Loss: 11205929.0000\n",
      "Epoch [2029/50000], Train Loss: 16831272.0000, Val Loss: 11202590.0000\n",
      "Epoch [2030/50000], Train Loss: 16827286.0000, Val Loss: 11199250.0000\n",
      "Epoch [2031/50000], Train Loss: 16823304.0000, Val Loss: 11195917.0000\n",
      "Epoch [2032/50000], Train Loss: 16819322.0000, Val Loss: 11192580.0000\n",
      "Epoch [2033/50000], Train Loss: 16815338.0000, Val Loss: 11189249.0000\n",
      "Epoch [2034/50000], Train Loss: 16811362.0000, Val Loss: 11185916.0000\n",
      "Epoch [2035/50000], Train Loss: 16807382.0000, Val Loss: 11182583.0000\n",
      "Epoch [2036/50000], Train Loss: 16803404.0000, Val Loss: 11179252.0000\n",
      "Epoch [2037/50000], Train Loss: 16799428.0000, Val Loss: 11175921.0000\n",
      "Epoch [2038/50000], Train Loss: 16795450.0000, Val Loss: 11172595.0000\n",
      "Epoch [2039/50000], Train Loss: 16791478.0000, Val Loss: 11169268.0000\n",
      "Epoch [2040/50000], Train Loss: 16787506.0000, Val Loss: 11165942.0000\n",
      "Epoch [2041/50000], Train Loss: 16783536.0000, Val Loss: 11162615.0000\n",
      "Epoch [2042/50000], Train Loss: 16779564.0000, Val Loss: 11159294.0000\n",
      "Epoch [2043/50000], Train Loss: 16775595.0000, Val Loss: 11155971.0000\n",
      "Epoch [2044/50000], Train Loss: 16771626.0000, Val Loss: 11152649.0000\n",
      "Epoch [2045/50000], Train Loss: 16767660.0000, Val Loss: 11149330.0000\n",
      "Epoch [2046/50000], Train Loss: 16763695.0000, Val Loss: 11146007.0000\n",
      "Epoch [2047/50000], Train Loss: 16759727.0000, Val Loss: 11142690.0000\n",
      "Epoch [2048/50000], Train Loss: 16755766.0000, Val Loss: 11139375.0000\n",
      "Epoch [2049/50000], Train Loss: 16751805.0000, Val Loss: 11136060.0000\n",
      "Epoch [2050/50000], Train Loss: 16747843.0000, Val Loss: 11132746.0000\n",
      "Epoch [2051/50000], Train Loss: 16743884.0000, Val Loss: 11129431.0000\n",
      "Epoch [2052/50000], Train Loss: 16739925.0000, Val Loss: 11126119.0000\n",
      "Epoch [2053/50000], Train Loss: 16735967.0000, Val Loss: 11122807.0000\n",
      "Epoch [2054/50000], Train Loss: 16732011.0000, Val Loss: 11119497.0000\n",
      "Epoch [2055/50000], Train Loss: 16728055.0000, Val Loss: 11116188.0000\n",
      "Epoch [2056/50000], Train Loss: 16724099.0000, Val Loss: 11112882.0000\n",
      "Epoch [2057/50000], Train Loss: 16720150.0000, Val Loss: 11109575.0000\n",
      "Epoch [2058/50000], Train Loss: 16716199.0000, Val Loss: 11106273.0000\n",
      "Epoch [2059/50000], Train Loss: 16712253.0000, Val Loss: 11102967.0000\n",
      "Epoch [2060/50000], Train Loss: 16708299.0000, Val Loss: 11099662.0000\n",
      "Epoch [2061/50000], Train Loss: 16704351.0000, Val Loss: 11096361.0000\n",
      "Epoch [2062/50000], Train Loss: 16700406.0000, Val Loss: 11093061.0000\n",
      "Epoch [2063/50000], Train Loss: 16696463.0000, Val Loss: 11089761.0000\n",
      "Epoch [2064/50000], Train Loss: 16692517.0000, Val Loss: 11086460.0000\n",
      "Epoch [2065/50000], Train Loss: 16688570.0000, Val Loss: 11083164.0000\n",
      "Epoch [2066/50000], Train Loss: 16684630.0000, Val Loss: 11079868.0000\n",
      "Epoch [2067/50000], Train Loss: 16680691.0000, Val Loss: 11076574.0000\n",
      "Epoch [2068/50000], Train Loss: 16676751.0000, Val Loss: 11073279.0000\n",
      "Epoch [2069/50000], Train Loss: 16672813.0000, Val Loss: 11069986.0000\n",
      "Epoch [2070/50000], Train Loss: 16668875.0000, Val Loss: 11066699.0000\n",
      "Epoch [2071/50000], Train Loss: 16664942.0000, Val Loss: 11063405.0000\n",
      "Epoch [2072/50000], Train Loss: 16661005.0000, Val Loss: 11060115.0000\n",
      "Epoch [2073/50000], Train Loss: 16657072.0000, Val Loss: 11056827.0000\n",
      "Epoch [2074/50000], Train Loss: 16653139.0000, Val Loss: 11053541.0000\n",
      "Epoch [2075/50000], Train Loss: 16649210.0000, Val Loss: 11050255.0000\n",
      "Epoch [2076/50000], Train Loss: 16645278.0000, Val Loss: 11046969.0000\n",
      "Epoch [2077/50000], Train Loss: 16641347.0000, Val Loss: 11043686.0000\n",
      "Epoch [2078/50000], Train Loss: 16637421.0000, Val Loss: 11040402.0000\n",
      "Epoch [2079/50000], Train Loss: 16633493.0000, Val Loss: 11037121.0000\n",
      "Epoch [2080/50000], Train Loss: 16629569.0000, Val Loss: 11033841.0000\n",
      "Epoch [2081/50000], Train Loss: 16625645.0000, Val Loss: 11030561.0000\n",
      "Epoch [2082/50000], Train Loss: 16621723.0000, Val Loss: 11027284.0000\n",
      "Epoch [2083/50000], Train Loss: 16617799.0000, Val Loss: 11024006.0000\n",
      "Epoch [2084/50000], Train Loss: 16613879.0000, Val Loss: 11020732.0000\n",
      "Epoch [2085/50000], Train Loss: 16609962.0000, Val Loss: 11017456.0000\n",
      "Epoch [2086/50000], Train Loss: 16606041.0000, Val Loss: 11014182.0000\n",
      "Epoch [2087/50000], Train Loss: 16602125.0000, Val Loss: 11010911.0000\n",
      "Epoch [2088/50000], Train Loss: 16598209.0000, Val Loss: 11007636.0000\n",
      "Epoch [2089/50000], Train Loss: 16594291.0000, Val Loss: 11004367.0000\n",
      "Epoch [2090/50000], Train Loss: 16590379.0000, Val Loss: 11001099.0000\n",
      "Epoch [2091/50000], Train Loss: 16586466.0000, Val Loss: 10997830.0000\n",
      "Epoch [2092/50000], Train Loss: 16582555.0000, Val Loss: 10994564.0000\n",
      "Epoch [2093/50000], Train Loss: 16578645.0000, Val Loss: 10991298.0000\n",
      "Epoch [2094/50000], Train Loss: 16574737.0000, Val Loss: 10988032.0000\n",
      "Epoch [2095/50000], Train Loss: 16570829.0000, Val Loss: 10984770.0000\n",
      "Epoch [2096/50000], Train Loss: 16566923.0000, Val Loss: 10981507.0000\n",
      "Epoch [2097/50000], Train Loss: 16563016.0000, Val Loss: 10978247.0000\n",
      "Epoch [2098/50000], Train Loss: 16559113.0000, Val Loss: 10974987.0000\n",
      "Epoch [2099/50000], Train Loss: 16555210.0000, Val Loss: 10971728.0000\n",
      "Epoch [2100/50000], Train Loss: 16551308.0000, Val Loss: 10968471.0000\n",
      "Epoch [2101/50000], Train Loss: 16547408.0000, Val Loss: 10965214.0000\n",
      "Epoch [2102/50000], Train Loss: 16543510.0000, Val Loss: 10961959.0000\n",
      "Epoch [2103/50000], Train Loss: 16539610.0000, Val Loss: 10958705.0000\n",
      "Epoch [2104/50000], Train Loss: 16535713.0000, Val Loss: 10955452.0000\n",
      "Epoch [2105/50000], Train Loss: 16531818.0000, Val Loss: 10952201.0000\n",
      "Epoch [2106/50000], Train Loss: 16527925.0000, Val Loss: 10948951.0000\n",
      "Epoch [2107/50000], Train Loss: 16524032.0000, Val Loss: 10945698.0000\n",
      "Epoch [2108/50000], Train Loss: 16520137.0000, Val Loss: 10942451.0000\n",
      "Epoch [2109/50000], Train Loss: 16516245.0000, Val Loss: 10939204.0000\n",
      "Epoch [2110/50000], Train Loss: 16512357.0000, Val Loss: 10935959.0000\n",
      "Epoch [2111/50000], Train Loss: 16508469.0000, Val Loss: 10932713.0000\n",
      "Epoch [2112/50000], Train Loss: 16504580.0000, Val Loss: 10929470.0000\n",
      "Epoch [2113/50000], Train Loss: 16500696.0000, Val Loss: 10926225.0000\n",
      "Epoch [2114/50000], Train Loss: 16496810.0000, Val Loss: 10922985.0000\n",
      "Epoch [2115/50000], Train Loss: 16492926.0000, Val Loss: 10919742.0000\n",
      "Epoch [2116/50000], Train Loss: 16489041.0000, Val Loss: 10916502.0000\n",
      "Epoch [2117/50000], Train Loss: 16485158.0000, Val Loss: 10913264.0000\n",
      "Epoch [2118/50000], Train Loss: 16481280.0000, Val Loss: 10910028.0000\n",
      "Epoch [2119/50000], Train Loss: 16477400.0000, Val Loss: 10906790.0000\n",
      "Epoch [2120/50000], Train Loss: 16473523.0000, Val Loss: 10903559.0000\n",
      "Epoch [2121/50000], Train Loss: 16469647.0000, Val Loss: 10900322.0000\n",
      "Epoch [2122/50000], Train Loss: 16465770.0000, Val Loss: 10897088.0000\n",
      "Epoch [2123/50000], Train Loss: 16461892.0000, Val Loss: 10893858.0000\n",
      "Epoch [2124/50000], Train Loss: 16458021.0000, Val Loss: 10890628.0000\n",
      "Epoch [2125/50000], Train Loss: 16454151.0000, Val Loss: 10887399.0000\n",
      "Epoch [2126/50000], Train Loss: 16450279.0000, Val Loss: 10884170.0000\n",
      "Epoch [2127/50000], Train Loss: 16446408.0000, Val Loss: 10880941.0000\n",
      "Epoch [2128/50000], Train Loss: 16442536.0000, Val Loss: 10877716.0000\n",
      "Epoch [2129/50000], Train Loss: 16438672.0000, Val Loss: 10874490.0000\n",
      "Epoch [2130/50000], Train Loss: 16434804.0000, Val Loss: 10871268.0000\n",
      "Epoch [2131/50000], Train Loss: 16430938.0000, Val Loss: 10868045.0000\n",
      "Epoch [2132/50000], Train Loss: 16427078.0000, Val Loss: 10864823.0000\n",
      "Epoch [2133/50000], Train Loss: 16423212.0000, Val Loss: 10861604.0000\n",
      "Epoch [2134/50000], Train Loss: 16419350.0000, Val Loss: 10858384.0000\n",
      "Epoch [2135/50000], Train Loss: 16415489.0000, Val Loss: 10855165.0000\n",
      "Epoch [2136/50000], Train Loss: 16411629.0000, Val Loss: 10851949.0000\n",
      "Epoch [2137/50000], Train Loss: 16407773.0000, Val Loss: 10848734.0000\n",
      "Epoch [2138/50000], Train Loss: 16403916.0000, Val Loss: 10845519.0000\n",
      "Epoch [2139/50000], Train Loss: 16400058.0000, Val Loss: 10842304.0000\n",
      "Epoch [2140/50000], Train Loss: 16396203.0000, Val Loss: 10839091.0000\n",
      "Epoch [2141/50000], Train Loss: 16392350.0000, Val Loss: 10835878.0000\n",
      "Epoch [2142/50000], Train Loss: 16388497.0000, Val Loss: 10832669.0000\n",
      "Epoch [2143/50000], Train Loss: 16384646.0000, Val Loss: 10829460.0000\n",
      "Epoch [2144/50000], Train Loss: 16380796.0000, Val Loss: 10826251.0000\n",
      "Epoch [2145/50000], Train Loss: 16376945.0000, Val Loss: 10823045.0000\n",
      "Epoch [2146/50000], Train Loss: 16373099.0000, Val Loss: 10819837.0000\n",
      "Epoch [2147/50000], Train Loss: 16369250.0000, Val Loss: 10816634.0000\n",
      "Epoch [2148/50000], Train Loss: 16365405.0000, Val Loss: 10813429.0000\n",
      "Epoch [2149/50000], Train Loss: 16361563.0000, Val Loss: 10810228.0000\n",
      "Epoch [2150/50000], Train Loss: 16357720.0000, Val Loss: 10807026.0000\n",
      "Epoch [2151/50000], Train Loss: 16353876.0000, Val Loss: 10803825.0000\n",
      "Epoch [2152/50000], Train Loss: 16350034.0000, Val Loss: 10800627.0000\n",
      "Epoch [2153/50000], Train Loss: 16346196.0000, Val Loss: 10797428.0000\n",
      "Epoch [2154/50000], Train Loss: 16342357.0000, Val Loss: 10794230.0000\n",
      "Epoch [2155/50000], Train Loss: 16338518.0000, Val Loss: 10791035.0000\n",
      "Epoch [2156/50000], Train Loss: 16334683.0000, Val Loss: 10787843.0000\n",
      "Epoch [2157/50000], Train Loss: 16330851.0000, Val Loss: 10784647.0000\n",
      "Epoch [2158/50000], Train Loss: 16327013.0000, Val Loss: 10781455.0000\n",
      "Epoch [2159/50000], Train Loss: 16323183.0000, Val Loss: 10778262.0000\n",
      "Epoch [2160/50000], Train Loss: 16319351.0000, Val Loss: 10775075.0000\n",
      "Epoch [2161/50000], Train Loss: 16315522.0000, Val Loss: 10771885.0000\n",
      "Epoch [2162/50000], Train Loss: 16311691.0000, Val Loss: 10768695.0000\n",
      "Epoch [2163/50000], Train Loss: 16307861.0000, Val Loss: 10765510.0000\n",
      "Epoch [2164/50000], Train Loss: 16304037.0000, Val Loss: 10762322.0000\n",
      "Epoch [2165/50000], Train Loss: 16300209.0000, Val Loss: 10759140.0000\n",
      "Epoch [2166/50000], Train Loss: 16296385.0000, Val Loss: 10755955.0000\n",
      "Epoch [2167/50000], Train Loss: 16292561.0000, Val Loss: 10752773.0000\n",
      "Epoch [2168/50000], Train Loss: 16288739.0000, Val Loss: 10749591.0000\n",
      "Epoch [2169/50000], Train Loss: 16284917.0000, Val Loss: 10746414.0000\n",
      "Epoch [2170/50000], Train Loss: 16281100.0000, Val Loss: 10743232.0000\n",
      "Epoch [2171/50000], Train Loss: 16277281.0000, Val Loss: 10740054.0000\n",
      "Epoch [2172/50000], Train Loss: 16273464.0000, Val Loss: 10736877.0000\n",
      "Epoch [2173/50000], Train Loss: 16269644.0000, Val Loss: 10733702.0000\n",
      "Epoch [2174/50000], Train Loss: 16265829.0000, Val Loss: 10730528.0000\n",
      "Epoch [2175/50000], Train Loss: 16262016.0000, Val Loss: 10727354.0000\n",
      "Epoch [2176/50000], Train Loss: 16258203.0000, Val Loss: 10724186.0000\n",
      "Epoch [2177/50000], Train Loss: 16254397.0000, Val Loss: 10721013.0000\n",
      "Epoch [2178/50000], Train Loss: 16250584.0000, Val Loss: 10717842.0000\n",
      "Epoch [2179/50000], Train Loss: 16246775.0000, Val Loss: 10714673.0000\n",
      "Epoch [2180/50000], Train Loss: 16242963.0000, Val Loss: 10711504.0000\n",
      "Epoch [2181/50000], Train Loss: 16239156.0000, Val Loss: 10708337.0000\n",
      "Epoch [2182/50000], Train Loss: 16235352.0000, Val Loss: 10705172.0000\n",
      "Epoch [2183/50000], Train Loss: 16231546.0000, Val Loss: 10702009.0000\n",
      "Epoch [2184/50000], Train Loss: 16227741.0000, Val Loss: 10698846.0000\n",
      "Epoch [2185/50000], Train Loss: 16223939.0000, Val Loss: 10695684.0000\n",
      "Epoch [2186/50000], Train Loss: 16220141.0000, Val Loss: 10692521.0000\n",
      "Epoch [2187/50000], Train Loss: 16216338.0000, Val Loss: 10689362.0000\n",
      "Epoch [2188/50000], Train Loss: 16212541.0000, Val Loss: 10686202.0000\n",
      "Epoch [2189/50000], Train Loss: 16208742.0000, Val Loss: 10683044.0000\n",
      "Epoch [2190/50000], Train Loss: 16204946.0000, Val Loss: 10679888.0000\n",
      "Epoch [2191/50000], Train Loss: 16201150.0000, Val Loss: 10676732.0000\n",
      "Epoch [2192/50000], Train Loss: 16197357.0000, Val Loss: 10673579.0000\n",
      "Epoch [2193/50000], Train Loss: 16193565.0000, Val Loss: 10670426.0000\n",
      "Epoch [2194/50000], Train Loss: 16189772.0000, Val Loss: 10667273.0000\n",
      "Epoch [2195/50000], Train Loss: 16185982.0000, Val Loss: 10664123.0000\n",
      "Epoch [2196/50000], Train Loss: 16182192.0000, Val Loss: 10660970.0000\n",
      "Epoch [2197/50000], Train Loss: 16178402.0000, Val Loss: 10657822.0000\n",
      "Epoch [2198/50000], Train Loss: 16174616.0000, Val Loss: 10654674.0000\n",
      "Epoch [2199/50000], Train Loss: 16170827.0000, Val Loss: 10651531.0000\n",
      "Epoch [2200/50000], Train Loss: 16167046.0000, Val Loss: 10648384.0000\n",
      "Epoch [2201/50000], Train Loss: 16163261.0000, Val Loss: 10645241.0000\n",
      "Epoch [2202/50000], Train Loss: 16159481.0000, Val Loss: 10642095.0000\n",
      "Epoch [2203/50000], Train Loss: 16155697.0000, Val Loss: 10638953.0000\n",
      "Epoch [2204/50000], Train Loss: 16151915.0000, Val Loss: 10635810.0000\n",
      "Epoch [2205/50000], Train Loss: 16148134.0000, Val Loss: 10632672.0000\n",
      "Epoch [2206/50000], Train Loss: 16144358.0000, Val Loss: 10629533.0000\n",
      "Epoch [2207/50000], Train Loss: 16140581.0000, Val Loss: 10626398.0000\n",
      "Epoch [2208/50000], Train Loss: 16136806.0000, Val Loss: 10623260.0000\n",
      "Epoch [2209/50000], Train Loss: 16133032.0000, Val Loss: 10620124.0000\n",
      "Epoch [2210/50000], Train Loss: 16129258.0000, Val Loss: 10616990.0000\n",
      "Epoch [2211/50000], Train Loss: 16125486.0000, Val Loss: 10613854.0000\n",
      "Epoch [2212/50000], Train Loss: 16121712.0000, Val Loss: 10610723.0000\n",
      "Epoch [2213/50000], Train Loss: 16117944.0000, Val Loss: 10607593.0000\n",
      "Epoch [2214/50000], Train Loss: 16114176.0000, Val Loss: 10604462.0000\n",
      "Epoch [2215/50000], Train Loss: 16110407.0000, Val Loss: 10601333.0000\n",
      "Epoch [2216/50000], Train Loss: 16106640.0000, Val Loss: 10598206.0000\n",
      "Epoch [2217/50000], Train Loss: 16102875.0000, Val Loss: 10595078.0000\n",
      "Epoch [2218/50000], Train Loss: 16099110.0000, Val Loss: 10591952.0000\n",
      "Epoch [2219/50000], Train Loss: 16095347.0000, Val Loss: 10588830.0000\n",
      "Epoch [2220/50000], Train Loss: 16091586.0000, Val Loss: 10585703.0000\n",
      "Epoch [2221/50000], Train Loss: 16087823.0000, Val Loss: 10582583.0000\n",
      "Epoch [2222/50000], Train Loss: 16084065.0000, Val Loss: 10579460.0000\n",
      "Epoch [2223/50000], Train Loss: 16080305.0000, Val Loss: 10576342.0000\n",
      "Epoch [2224/50000], Train Loss: 16076550.0000, Val Loss: 10573222.0000\n",
      "Epoch [2225/50000], Train Loss: 16072794.0000, Val Loss: 10570103.0000\n",
      "Epoch [2226/50000], Train Loss: 16069038.0000, Val Loss: 10566987.0000\n",
      "Epoch [2227/50000], Train Loss: 16065283.0000, Val Loss: 10563872.0000\n",
      "Epoch [2228/50000], Train Loss: 16061533.0000, Val Loss: 10560756.0000\n",
      "Epoch [2229/50000], Train Loss: 16057777.0000, Val Loss: 10557644.0000\n",
      "Epoch [2230/50000], Train Loss: 16054027.0000, Val Loss: 10554532.0000\n",
      "Epoch [2231/50000], Train Loss: 16050280.0000, Val Loss: 10551420.0000\n",
      "Epoch [2232/50000], Train Loss: 16046531.0000, Val Loss: 10548308.0000\n",
      "Epoch [2233/50000], Train Loss: 16042783.0000, Val Loss: 10545200.0000\n",
      "Epoch [2234/50000], Train Loss: 16039039.0000, Val Loss: 10542092.0000\n",
      "Epoch [2235/50000], Train Loss: 16035292.0000, Val Loss: 10538986.0000\n",
      "Epoch [2236/50000], Train Loss: 16031550.0000, Val Loss: 10535879.0000\n",
      "Epoch [2237/50000], Train Loss: 16027806.0000, Val Loss: 10532774.0000\n",
      "Epoch [2238/50000], Train Loss: 16024065.0000, Val Loss: 10529671.0000\n",
      "Epoch [2239/50000], Train Loss: 16020324.0000, Val Loss: 10526571.0000\n",
      "Epoch [2240/50000], Train Loss: 16016587.0000, Val Loss: 10523467.0000\n",
      "Epoch [2241/50000], Train Loss: 16012849.0000, Val Loss: 10520369.0000\n",
      "Epoch [2242/50000], Train Loss: 16009113.0000, Val Loss: 10517267.0000\n",
      "Epoch [2243/50000], Train Loss: 16005374.0000, Val Loss: 10514171.0000\n",
      "Epoch [2244/50000], Train Loss: 16001642.0000, Val Loss: 10511073.0000\n",
      "Epoch [2245/50000], Train Loss: 15997908.0000, Val Loss: 10507977.0000\n",
      "Epoch [2246/50000], Train Loss: 15994174.0000, Val Loss: 10504883.0000\n",
      "Epoch [2247/50000], Train Loss: 15990446.0000, Val Loss: 10501790.0000\n",
      "Epoch [2248/50000], Train Loss: 15986716.0000, Val Loss: 10498697.0000\n",
      "Epoch [2249/50000], Train Loss: 15982988.0000, Val Loss: 10495607.0000\n",
      "Epoch [2250/50000], Train Loss: 15979260.0000, Val Loss: 10492516.0000\n",
      "Epoch [2251/50000], Train Loss: 15975533.0000, Val Loss: 10489425.0000\n",
      "Epoch [2252/50000], Train Loss: 15971806.0000, Val Loss: 10486336.0000\n",
      "Epoch [2253/50000], Train Loss: 15968083.0000, Val Loss: 10483251.0000\n",
      "Epoch [2254/50000], Train Loss: 15964361.0000, Val Loss: 10480164.0000\n",
      "Epoch [2255/50000], Train Loss: 15960639.0000, Val Loss: 10477079.0000\n",
      "Epoch [2256/50000], Train Loss: 15956918.0000, Val Loss: 10473995.0000\n",
      "Epoch [2257/50000], Train Loss: 15953199.0000, Val Loss: 10470914.0000\n",
      "Epoch [2258/50000], Train Loss: 15949478.0000, Val Loss: 10467829.0000\n",
      "Epoch [2259/50000], Train Loss: 15945759.0000, Val Loss: 10464749.0000\n",
      "Epoch [2260/50000], Train Loss: 15942042.0000, Val Loss: 10461669.0000\n",
      "Epoch [2261/50000], Train Loss: 15938330.0000, Val Loss: 10458593.0000\n",
      "Epoch [2262/50000], Train Loss: 15934617.0000, Val Loss: 10455515.0000\n",
      "Epoch [2263/50000], Train Loss: 15930905.0000, Val Loss: 10452442.0000\n",
      "Epoch [2264/50000], Train Loss: 15927195.0000, Val Loss: 10449365.0000\n",
      "Epoch [2265/50000], Train Loss: 15923483.0000, Val Loss: 10446291.0000\n",
      "Epoch [2266/50000], Train Loss: 15919772.0000, Val Loss: 10443219.0000\n",
      "Epoch [2267/50000], Train Loss: 15916066.0000, Val Loss: 10440145.0000\n",
      "Epoch [2268/50000], Train Loss: 15912356.0000, Val Loss: 10437072.0000\n",
      "Epoch [2269/50000], Train Loss: 15908648.0000, Val Loss: 10434007.0000\n",
      "Epoch [2270/50000], Train Loss: 15904947.0000, Val Loss: 10430938.0000\n",
      "Epoch [2271/50000], Train Loss: 15901242.0000, Val Loss: 10427871.0000\n",
      "Epoch [2272/50000], Train Loss: 15897542.0000, Val Loss: 10424806.0000\n",
      "Epoch [2273/50000], Train Loss: 15893841.0000, Val Loss: 10421739.0000\n",
      "Epoch [2274/50000], Train Loss: 15890138.0000, Val Loss: 10418675.0000\n",
      "Epoch [2275/50000], Train Loss: 15886439.0000, Val Loss: 10415612.0000\n",
      "Epoch [2276/50000], Train Loss: 15882741.0000, Val Loss: 10412550.0000\n",
      "Epoch [2277/50000], Train Loss: 15879044.0000, Val Loss: 10409489.0000\n",
      "Epoch [2278/50000], Train Loss: 15875350.0000, Val Loss: 10406429.0000\n",
      "Epoch [2279/50000], Train Loss: 15871656.0000, Val Loss: 10403371.0000\n",
      "Epoch [2280/50000], Train Loss: 15867960.0000, Val Loss: 10400315.0000\n",
      "Epoch [2281/50000], Train Loss: 15864272.0000, Val Loss: 10397255.0000\n",
      "Epoch [2282/50000], Train Loss: 15860577.0000, Val Loss: 10394204.0000\n",
      "Epoch [2283/50000], Train Loss: 15856890.0000, Val Loss: 10391147.0000\n",
      "Epoch [2284/50000], Train Loss: 15853199.0000, Val Loss: 10388094.0000\n",
      "Epoch [2285/50000], Train Loss: 15849511.0000, Val Loss: 10385044.0000\n",
      "Epoch [2286/50000], Train Loss: 15845827.0000, Val Loss: 10381995.0000\n",
      "Epoch [2287/50000], Train Loss: 15842143.0000, Val Loss: 10378944.0000\n",
      "Epoch [2288/50000], Train Loss: 15838460.0000, Val Loss: 10375898.0000\n",
      "Epoch [2289/50000], Train Loss: 15834778.0000, Val Loss: 10372847.0000\n",
      "Epoch [2290/50000], Train Loss: 15831093.0000, Val Loss: 10369801.0000\n",
      "Epoch [2291/50000], Train Loss: 15827412.0000, Val Loss: 10366754.0000\n",
      "Epoch [2292/50000], Train Loss: 15823732.0000, Val Loss: 10363711.0000\n",
      "Epoch [2293/50000], Train Loss: 15820054.0000, Val Loss: 10360669.0000\n",
      "Epoch [2294/50000], Train Loss: 15816377.0000, Val Loss: 10357628.0000\n",
      "Epoch [2295/50000], Train Loss: 15812702.0000, Val Loss: 10354588.0000\n",
      "Epoch [2296/50000], Train Loss: 15809027.0000, Val Loss: 10351545.0000\n",
      "Epoch [2297/50000], Train Loss: 15805352.0000, Val Loss: 10348509.0000\n",
      "Epoch [2298/50000], Train Loss: 15801683.0000, Val Loss: 10345469.0000\n",
      "Epoch [2299/50000], Train Loss: 15798007.0000, Val Loss: 10342433.0000\n",
      "Epoch [2300/50000], Train Loss: 15794339.0000, Val Loss: 10339398.0000\n",
      "Epoch [2301/50000], Train Loss: 15790669.0000, Val Loss: 10336365.0000\n",
      "Epoch [2302/50000], Train Loss: 15787002.0000, Val Loss: 10333331.0000\n",
      "Epoch [2303/50000], Train Loss: 15783336.0000, Val Loss: 10330300.0000\n",
      "Epoch [2304/50000], Train Loss: 15779670.0000, Val Loss: 10327267.0000\n",
      "Epoch [2305/50000], Train Loss: 15776004.0000, Val Loss: 10324236.0000\n",
      "Epoch [2306/50000], Train Loss: 15772340.0000, Val Loss: 10321209.0000\n",
      "Epoch [2307/50000], Train Loss: 15768679.0000, Val Loss: 10318179.0000\n",
      "Epoch [2308/50000], Train Loss: 15765014.0000, Val Loss: 10315154.0000\n",
      "Epoch [2309/50000], Train Loss: 15761357.0000, Val Loss: 10312127.0000\n",
      "Epoch [2310/50000], Train Loss: 15757697.0000, Val Loss: 10309104.0000\n",
      "Epoch [2311/50000], Train Loss: 15754040.0000, Val Loss: 10306080.0000\n",
      "Epoch [2312/50000], Train Loss: 15750382.0000, Val Loss: 10303056.0000\n",
      "Epoch [2313/50000], Train Loss: 15746726.0000, Val Loss: 10300036.0000\n",
      "Epoch [2314/50000], Train Loss: 15743071.0000, Val Loss: 10297017.0000\n",
      "Epoch [2315/50000], Train Loss: 15739419.0000, Val Loss: 10293996.0000\n",
      "Epoch [2316/50000], Train Loss: 15735766.0000, Val Loss: 10290978.0000\n",
      "Epoch [2317/50000], Train Loss: 15732116.0000, Val Loss: 10287959.0000\n",
      "Epoch [2318/50000], Train Loss: 15728464.0000, Val Loss: 10284948.0000\n",
      "Epoch [2319/50000], Train Loss: 15724819.0000, Val Loss: 10281933.0000\n",
      "Epoch [2320/50000], Train Loss: 15721170.0000, Val Loss: 10278919.0000\n",
      "Epoch [2321/50000], Train Loss: 15717523.0000, Val Loss: 10275905.0000\n",
      "Epoch [2322/50000], Train Loss: 15713878.0000, Val Loss: 10272893.0000\n",
      "Epoch [2323/50000], Train Loss: 15710233.0000, Val Loss: 10269882.0000\n",
      "Epoch [2324/50000], Train Loss: 15706588.0000, Val Loss: 10266874.0000\n",
      "Epoch [2325/50000], Train Loss: 15702946.0000, Val Loss: 10263866.0000\n",
      "Epoch [2326/50000], Train Loss: 15699307.0000, Val Loss: 10260859.0000\n",
      "Epoch [2327/50000], Train Loss: 15695667.0000, Val Loss: 10257852.0000\n",
      "Epoch [2328/50000], Train Loss: 15692029.0000, Val Loss: 10254848.0000\n",
      "Epoch [2329/50000], Train Loss: 15688391.0000, Val Loss: 10251843.0000\n",
      "Epoch [2330/50000], Train Loss: 15684755.0000, Val Loss: 10248838.0000\n",
      "Epoch [2331/50000], Train Loss: 15681118.0000, Val Loss: 10245838.0000\n",
      "Epoch [2332/50000], Train Loss: 15677484.0000, Val Loss: 10242836.0000\n",
      "Epoch [2333/50000], Train Loss: 15673849.0000, Val Loss: 10239839.0000\n",
      "Epoch [2334/50000], Train Loss: 15670221.0000, Val Loss: 10236838.0000\n",
      "Epoch [2335/50000], Train Loss: 15666589.0000, Val Loss: 10233842.0000\n",
      "Epoch [2336/50000], Train Loss: 15662960.0000, Val Loss: 10230846.0000\n",
      "Epoch [2337/50000], Train Loss: 15659331.0000, Val Loss: 10227852.0000\n",
      "Epoch [2338/50000], Train Loss: 15655706.0000, Val Loss: 10224858.0000\n",
      "Epoch [2339/50000], Train Loss: 15652078.0000, Val Loss: 10221862.0000\n",
      "Epoch [2340/50000], Train Loss: 15648450.0000, Val Loss: 10218870.0000\n",
      "Epoch [2341/50000], Train Loss: 15644827.0000, Val Loss: 10215879.0000\n",
      "Epoch [2342/50000], Train Loss: 15641204.0000, Val Loss: 10212891.0000\n",
      "Epoch [2343/50000], Train Loss: 15637583.0000, Val Loss: 10209903.0000\n",
      "Epoch [2344/50000], Train Loss: 15633965.0000, Val Loss: 10206916.0000\n",
      "Epoch [2345/50000], Train Loss: 15630346.0000, Val Loss: 10203927.0000\n",
      "Epoch [2346/50000], Train Loss: 15626725.0000, Val Loss: 10200941.0000\n",
      "Epoch [2347/50000], Train Loss: 15623106.0000, Val Loss: 10197956.0000\n",
      "Epoch [2348/50000], Train Loss: 15619489.0000, Val Loss: 10194974.0000\n",
      "Epoch [2349/50000], Train Loss: 15615875.0000, Val Loss: 10191991.0000\n",
      "Epoch [2350/50000], Train Loss: 15612263.0000, Val Loss: 10189010.0000\n",
      "Epoch [2351/50000], Train Loss: 15608649.0000, Val Loss: 10186031.0000\n",
      "Epoch [2352/50000], Train Loss: 15605039.0000, Val Loss: 10183052.0000\n",
      "Epoch [2353/50000], Train Loss: 15601429.0000, Val Loss: 10180074.0000\n",
      "Epoch [2354/50000], Train Loss: 15597817.0000, Val Loss: 10177098.0000\n",
      "Epoch [2355/50000], Train Loss: 15594212.0000, Val Loss: 10174121.0000\n",
      "Epoch [2356/50000], Train Loss: 15590605.0000, Val Loss: 10171145.0000\n",
      "Epoch [2357/50000], Train Loss: 15586997.0000, Val Loss: 10168172.0000\n",
      "Epoch [2358/50000], Train Loss: 15583393.0000, Val Loss: 10165199.0000\n",
      "Epoch [2359/50000], Train Loss: 15579788.0000, Val Loss: 10162229.0000\n",
      "Epoch [2360/50000], Train Loss: 15576185.0000, Val Loss: 10159258.0000\n",
      "Epoch [2361/50000], Train Loss: 15572584.0000, Val Loss: 10156288.0000\n",
      "Epoch [2362/50000], Train Loss: 15568984.0000, Val Loss: 10153319.0000\n",
      "Epoch [2363/50000], Train Loss: 15565383.0000, Val Loss: 10150353.0000\n",
      "Epoch [2364/50000], Train Loss: 15561786.0000, Val Loss: 10147388.0000\n",
      "Epoch [2365/50000], Train Loss: 15558191.0000, Val Loss: 10144422.0000\n",
      "Epoch [2366/50000], Train Loss: 15554594.0000, Val Loss: 10141458.0000\n",
      "Epoch [2367/50000], Train Loss: 15551000.0000, Val Loss: 10138496.0000\n",
      "Epoch [2368/50000], Train Loss: 15547408.0000, Val Loss: 10135533.0000\n",
      "Epoch [2369/50000], Train Loss: 15543814.0000, Val Loss: 10132572.0000\n",
      "Epoch [2370/50000], Train Loss: 15540223.0000, Val Loss: 10129613.0000\n",
      "Epoch [2371/50000], Train Loss: 15536631.0000, Val Loss: 10126654.0000\n",
      "Epoch [2372/50000], Train Loss: 15533040.0000, Val Loss: 10123695.0000\n",
      "Epoch [2373/50000], Train Loss: 15529453.0000, Val Loss: 10120740.0000\n",
      "Epoch [2374/50000], Train Loss: 15525868.0000, Val Loss: 10117787.0000\n",
      "Epoch [2375/50000], Train Loss: 15522284.0000, Val Loss: 10114832.0000\n",
      "Epoch [2376/50000], Train Loss: 15518699.0000, Val Loss: 10111878.0000\n",
      "Epoch [2377/50000], Train Loss: 15515112.0000, Val Loss: 10108926.0000\n",
      "Epoch [2378/50000], Train Loss: 15511532.0000, Val Loss: 10105974.0000\n",
      "Epoch [2379/50000], Train Loss: 15507948.0000, Val Loss: 10103024.0000\n",
      "Epoch [2380/50000], Train Loss: 15504367.0000, Val Loss: 10100076.0000\n",
      "Epoch [2381/50000], Train Loss: 15500792.0000, Val Loss: 10097129.0000\n",
      "Epoch [2382/50000], Train Loss: 15497214.0000, Val Loss: 10094182.0000\n",
      "Epoch [2383/50000], Train Loss: 15493635.0000, Val Loss: 10091236.0000\n",
      "Epoch [2384/50000], Train Loss: 15490059.0000, Val Loss: 10088290.0000\n",
      "Epoch [2385/50000], Train Loss: 15486484.0000, Val Loss: 10085348.0000\n",
      "Epoch [2386/50000], Train Loss: 15482914.0000, Val Loss: 10082406.0000\n",
      "Epoch [2387/50000], Train Loss: 15479341.0000, Val Loss: 10079463.0000\n",
      "Epoch [2388/50000], Train Loss: 15475768.0000, Val Loss: 10076523.0000\n",
      "Epoch [2389/50000], Train Loss: 15472196.0000, Val Loss: 10073585.0000\n",
      "Epoch [2390/50000], Train Loss: 15468629.0000, Val Loss: 10070644.0000\n",
      "Epoch [2391/50000], Train Loss: 15465060.0000, Val Loss: 10067710.0000\n",
      "Epoch [2392/50000], Train Loss: 15461495.0000, Val Loss: 10064773.0000\n",
      "Epoch [2393/50000], Train Loss: 15457930.0000, Val Loss: 10061839.0000\n",
      "Epoch [2394/50000], Train Loss: 15454363.0000, Val Loss: 10058901.0000\n",
      "Epoch [2395/50000], Train Loss: 15450799.0000, Val Loss: 10055970.0000\n",
      "Epoch [2396/50000], Train Loss: 15447235.0000, Val Loss: 10053039.0000\n",
      "Epoch [2397/50000], Train Loss: 15443677.0000, Val Loss: 10050110.0000\n",
      "Epoch [2398/50000], Train Loss: 15440118.0000, Val Loss: 10047181.0000\n",
      "Epoch [2399/50000], Train Loss: 15436560.0000, Val Loss: 10044254.0000\n",
      "Epoch [2400/50000], Train Loss: 15433002.0000, Val Loss: 10041324.0000\n",
      "Epoch [2401/50000], Train Loss: 15429446.0000, Val Loss: 10038398.0000\n",
      "Epoch [2402/50000], Train Loss: 15425889.0000, Val Loss: 10035473.0000\n",
      "Epoch [2403/50000], Train Loss: 15422334.0000, Val Loss: 10032546.0000\n",
      "Epoch [2404/50000], Train Loss: 15418777.0000, Val Loss: 10029625.0000\n",
      "Epoch [2405/50000], Train Loss: 15415228.0000, Val Loss: 10026703.0000\n",
      "Epoch [2406/50000], Train Loss: 15411677.0000, Val Loss: 10023783.0000\n",
      "Epoch [2407/50000], Train Loss: 15408128.0000, Val Loss: 10020862.0000\n",
      "Epoch [2408/50000], Train Loss: 15404577.0000, Val Loss: 10017943.0000\n",
      "Epoch [2409/50000], Train Loss: 15401031.0000, Val Loss: 10015026.0000\n",
      "Epoch [2410/50000], Train Loss: 15397486.0000, Val Loss: 10012108.0000\n",
      "Epoch [2411/50000], Train Loss: 15393938.0000, Val Loss: 10009193.0000\n",
      "Epoch [2412/50000], Train Loss: 15390392.0000, Val Loss: 10006277.0000\n",
      "Epoch [2413/50000], Train Loss: 15386847.0000, Val Loss: 10003364.0000\n",
      "Epoch [2414/50000], Train Loss: 15383306.0000, Val Loss: 10000452.0000\n",
      "Epoch [2415/50000], Train Loss: 15379768.0000, Val Loss: 9997542.0000\n",
      "Epoch [2416/50000], Train Loss: 15376227.0000, Val Loss: 9994631.0000\n",
      "Epoch [2417/50000], Train Loss: 15372690.0000, Val Loss: 9991721.0000\n",
      "Epoch [2418/50000], Train Loss: 15369151.0000, Val Loss: 9988812.0000\n",
      "Epoch [2419/50000], Train Loss: 15365611.0000, Val Loss: 9985904.0000\n",
      "Epoch [2420/50000], Train Loss: 15362077.0000, Val Loss: 9982999.0000\n",
      "Epoch [2421/50000], Train Loss: 15358543.0000, Val Loss: 9980094.0000\n",
      "Epoch [2422/50000], Train Loss: 15355011.0000, Val Loss: 9977193.0000\n",
      "Epoch [2423/50000], Train Loss: 15351480.0000, Val Loss: 9974289.0000\n",
      "Epoch [2424/50000], Train Loss: 15347948.0000, Val Loss: 9971388.0000\n",
      "Epoch [2425/50000], Train Loss: 15344419.0000, Val Loss: 9968486.0000\n",
      "Epoch [2426/50000], Train Loss: 15340890.0000, Val Loss: 9965586.0000\n",
      "Epoch [2427/50000], Train Loss: 15337359.0000, Val Loss: 9962686.0000\n",
      "Epoch [2428/50000], Train Loss: 15333832.0000, Val Loss: 9959790.0000\n",
      "Epoch [2429/50000], Train Loss: 15330308.0000, Val Loss: 9956894.0000\n",
      "Epoch [2430/50000], Train Loss: 15326784.0000, Val Loss: 9953997.0000\n",
      "Epoch [2431/50000], Train Loss: 15323260.0000, Val Loss: 9951105.0000\n",
      "Epoch [2432/50000], Train Loss: 15319740.0000, Val Loss: 9948210.0000\n",
      "Epoch [2433/50000], Train Loss: 15316215.0000, Val Loss: 9945318.0000\n",
      "Epoch [2434/50000], Train Loss: 15312697.0000, Val Loss: 9942427.0000\n",
      "Epoch [2435/50000], Train Loss: 15309176.0000, Val Loss: 9939536.0000\n",
      "Epoch [2436/50000], Train Loss: 15305661.0000, Val Loss: 9936646.0000\n",
      "Epoch [2437/50000], Train Loss: 15302143.0000, Val Loss: 9933759.0000\n",
      "Epoch [2438/50000], Train Loss: 15298626.0000, Val Loss: 9930873.0000\n",
      "Epoch [2439/50000], Train Loss: 15295112.0000, Val Loss: 9927987.0000\n",
      "Epoch [2440/50000], Train Loss: 15291601.0000, Val Loss: 9925103.0000\n",
      "Epoch [2441/50000], Train Loss: 15288088.0000, Val Loss: 9922221.0000\n",
      "Epoch [2442/50000], Train Loss: 15284578.0000, Val Loss: 9919338.0000\n",
      "Epoch [2443/50000], Train Loss: 15281068.0000, Val Loss: 9916452.0000\n",
      "Epoch [2444/50000], Train Loss: 15277555.0000, Val Loss: 9913574.0000\n",
      "Epoch [2445/50000], Train Loss: 15274051.0000, Val Loss: 9910695.0000\n",
      "Epoch [2446/50000], Train Loss: 15270543.0000, Val Loss: 9907818.0000\n",
      "Epoch [2447/50000], Train Loss: 15267042.0000, Val Loss: 9904939.0000\n",
      "Epoch [2448/50000], Train Loss: 15263532.0000, Val Loss: 9902065.0000\n",
      "Epoch [2449/50000], Train Loss: 15260031.0000, Val Loss: 9899189.0000\n",
      "Epoch [2450/50000], Train Loss: 15256529.0000, Val Loss: 9896314.0000\n",
      "Epoch [2451/50000], Train Loss: 15253026.0000, Val Loss: 9893441.0000\n",
      "Epoch [2452/50000], Train Loss: 15249525.0000, Val Loss: 9890569.0000\n",
      "Epoch [2453/50000], Train Loss: 15246028.0000, Val Loss: 9887699.0000\n",
      "Epoch [2454/50000], Train Loss: 15242531.0000, Val Loss: 9884828.0000\n",
      "Epoch [2455/50000], Train Loss: 15239033.0000, Val Loss: 9881961.0000\n",
      "Epoch [2456/50000], Train Loss: 15235538.0000, Val Loss: 9879093.0000\n",
      "Epoch [2457/50000], Train Loss: 15232044.0000, Val Loss: 9876225.0000\n",
      "Epoch [2458/50000], Train Loss: 15228551.0000, Val Loss: 9873357.0000\n",
      "Epoch [2459/50000], Train Loss: 15225055.0000, Val Loss: 9870493.0000\n",
      "Epoch [2460/50000], Train Loss: 15221564.0000, Val Loss: 9867630.0000\n",
      "Epoch [2461/50000], Train Loss: 15218075.0000, Val Loss: 9864770.0000\n",
      "Epoch [2462/50000], Train Loss: 15214589.0000, Val Loss: 9861906.0000\n",
      "Epoch [2463/50000], Train Loss: 15211097.0000, Val Loss: 9859049.0000\n",
      "Epoch [2464/50000], Train Loss: 15207614.0000, Val Loss: 9856188.0000\n",
      "Epoch [2465/50000], Train Loss: 15204125.0000, Val Loss: 9853329.0000\n",
      "Epoch [2466/50000], Train Loss: 15200640.0000, Val Loss: 9850470.0000\n",
      "Epoch [2467/50000], Train Loss: 15197157.0000, Val Loss: 9847615.0000\n",
      "Epoch [2468/50000], Train Loss: 15193673.0000, Val Loss: 9844759.0000\n",
      "Epoch [2469/50000], Train Loss: 15190193.0000, Val Loss: 9841907.0000\n",
      "Epoch [2470/50000], Train Loss: 15186716.0000, Val Loss: 9839053.0000\n",
      "Epoch [2471/50000], Train Loss: 15183235.0000, Val Loss: 9836203.0000\n",
      "Epoch [2472/50000], Train Loss: 15179759.0000, Val Loss: 9833351.0000\n",
      "Epoch [2473/50000], Train Loss: 15176281.0000, Val Loss: 9830503.0000\n",
      "Epoch [2474/50000], Train Loss: 15172808.0000, Val Loss: 9827653.0000\n",
      "Epoch [2475/50000], Train Loss: 15169331.0000, Val Loss: 9824805.0000\n",
      "Epoch [2476/50000], Train Loss: 15165854.0000, Val Loss: 9821958.0000\n",
      "Epoch [2477/50000], Train Loss: 15162383.0000, Val Loss: 9819115.0000\n",
      "Epoch [2478/50000], Train Loss: 15158913.0000, Val Loss: 9816270.0000\n",
      "Epoch [2479/50000], Train Loss: 15155445.0000, Val Loss: 9813427.0000\n",
      "Epoch [2480/50000], Train Loss: 15151975.0000, Val Loss: 9810583.0000\n",
      "Epoch [2481/50000], Train Loss: 15148504.0000, Val Loss: 9807743.0000\n",
      "Epoch [2482/50000], Train Loss: 15145040.0000, Val Loss: 9804901.0000\n",
      "Epoch [2483/50000], Train Loss: 15141574.0000, Val Loss: 9802062.0000\n",
      "Epoch [2484/50000], Train Loss: 15138106.0000, Val Loss: 9799226.0000\n",
      "Epoch [2485/50000], Train Loss: 15134645.0000, Val Loss: 9796388.0000\n",
      "Epoch [2486/50000], Train Loss: 15131182.0000, Val Loss: 9793552.0000\n",
      "Epoch [2487/50000], Train Loss: 15127722.0000, Val Loss: 9790717.0000\n",
      "Epoch [2488/50000], Train Loss: 15124261.0000, Val Loss: 9787883.0000\n",
      "Epoch [2489/50000], Train Loss: 15120802.0000, Val Loss: 9785051.0000\n",
      "Epoch [2490/50000], Train Loss: 15117342.0000, Val Loss: 9782219.0000\n",
      "Epoch [2491/50000], Train Loss: 15113887.0000, Val Loss: 9779387.0000\n",
      "Epoch [2492/50000], Train Loss: 15110430.0000, Val Loss: 9776556.0000\n",
      "Epoch [2493/50000], Train Loss: 15106974.0000, Val Loss: 9773729.0000\n",
      "Epoch [2494/50000], Train Loss: 15103522.0000, Val Loss: 9770902.0000\n",
      "Epoch [2495/50000], Train Loss: 15100071.0000, Val Loss: 9768076.0000\n",
      "Epoch [2496/50000], Train Loss: 15096618.0000, Val Loss: 9765250.0000\n",
      "Epoch [2497/50000], Train Loss: 15093167.0000, Val Loss: 9762426.0000\n",
      "Epoch [2498/50000], Train Loss: 15089718.0000, Val Loss: 9759601.0000\n",
      "Epoch [2499/50000], Train Loss: 15086270.0000, Val Loss: 9756779.0000\n",
      "Epoch [2500/50000], Train Loss: 15082822.0000, Val Loss: 9753958.0000\n",
      "Epoch [2501/50000], Train Loss: 15079376.0000, Val Loss: 9751138.0000\n",
      "Epoch [2502/50000], Train Loss: 15075932.0000, Val Loss: 9748319.0000\n",
      "Epoch [2503/50000], Train Loss: 15072488.0000, Val Loss: 9745502.0000\n",
      "Epoch [2504/50000], Train Loss: 15069047.0000, Val Loss: 9742685.0000\n",
      "Epoch [2505/50000], Train Loss: 15065603.0000, Val Loss: 9739867.0000\n",
      "Epoch [2506/50000], Train Loss: 15062162.0000, Val Loss: 9737052.0000\n",
      "Epoch [2507/50000], Train Loss: 15058723.0000, Val Loss: 9734238.0000\n",
      "Epoch [2508/50000], Train Loss: 15055285.0000, Val Loss: 9731426.0000\n",
      "Epoch [2509/50000], Train Loss: 15051848.0000, Val Loss: 9728614.0000\n",
      "Epoch [2510/50000], Train Loss: 15048411.0000, Val Loss: 9725805.0000\n",
      "Epoch [2511/50000], Train Loss: 15044980.0000, Val Loss: 9722995.0000\n",
      "Epoch [2512/50000], Train Loss: 15041545.0000, Val Loss: 9720186.0000\n",
      "Epoch [2513/50000], Train Loss: 15038110.0000, Val Loss: 9717376.0000\n",
      "Epoch [2514/50000], Train Loss: 15034676.0000, Val Loss: 9714570.0000\n",
      "Epoch [2515/50000], Train Loss: 15031244.0000, Val Loss: 9711765.0000\n",
      "Epoch [2516/50000], Train Loss: 15027815.0000, Val Loss: 9708960.0000\n",
      "Epoch [2517/50000], Train Loss: 15024387.0000, Val Loss: 9706157.0000\n",
      "Epoch [2518/50000], Train Loss: 15020959.0000, Val Loss: 9703356.0000\n",
      "Epoch [2519/50000], Train Loss: 15017536.0000, Val Loss: 9700556.0000\n",
      "Epoch [2520/50000], Train Loss: 15014111.0000, Val Loss: 9697755.0000\n",
      "Epoch [2521/50000], Train Loss: 15010687.0000, Val Loss: 9694955.0000\n",
      "Epoch [2522/50000], Train Loss: 15007262.0000, Val Loss: 9692156.0000\n",
      "Epoch [2523/50000], Train Loss: 15003841.0000, Val Loss: 9689358.0000\n",
      "Epoch [2524/50000], Train Loss: 15000420.0000, Val Loss: 9686563.0000\n",
      "Epoch [2525/50000], Train Loss: 14997000.0000, Val Loss: 9683770.0000\n",
      "Epoch [2526/50000], Train Loss: 14993583.0000, Val Loss: 9680976.0000\n",
      "Epoch [2527/50000], Train Loss: 14990166.0000, Val Loss: 9678182.0000\n",
      "Epoch [2528/50000], Train Loss: 14986749.0000, Val Loss: 9675390.0000\n",
      "Epoch [2529/50000], Train Loss: 14983334.0000, Val Loss: 9672599.0000\n",
      "Epoch [2530/50000], Train Loss: 14979919.0000, Val Loss: 9669809.0000\n",
      "Epoch [2531/50000], Train Loss: 14976503.0000, Val Loss: 9667019.0000\n",
      "Epoch [2532/50000], Train Loss: 14973091.0000, Val Loss: 9664231.0000\n",
      "Epoch [2533/50000], Train Loss: 14969681.0000, Val Loss: 9661446.0000\n",
      "Epoch [2534/50000], Train Loss: 14966272.0000, Val Loss: 9658658.0000\n",
      "Epoch [2535/50000], Train Loss: 14962862.0000, Val Loss: 9655876.0000\n",
      "Epoch [2536/50000], Train Loss: 14959454.0000, Val Loss: 9653091.0000\n",
      "Epoch [2537/50000], Train Loss: 14956048.0000, Val Loss: 9650310.0000\n",
      "Epoch [2538/50000], Train Loss: 14952643.0000, Val Loss: 9647527.0000\n",
      "Epoch [2539/50000], Train Loss: 14949237.0000, Val Loss: 9644747.0000\n",
      "Epoch [2540/50000], Train Loss: 14945832.0000, Val Loss: 9641969.0000\n",
      "Epoch [2541/50000], Train Loss: 14942434.0000, Val Loss: 9639190.0000\n",
      "Epoch [2542/50000], Train Loss: 14939032.0000, Val Loss: 9636414.0000\n",
      "Epoch [2543/50000], Train Loss: 14935633.0000, Val Loss: 9633636.0000\n",
      "Epoch [2544/50000], Train Loss: 14932232.0000, Val Loss: 9630863.0000\n",
      "Epoch [2545/50000], Train Loss: 14928834.0000, Val Loss: 9628089.0000\n",
      "Epoch [2546/50000], Train Loss: 14925439.0000, Val Loss: 9625315.0000\n",
      "Epoch [2547/50000], Train Loss: 14922042.0000, Val Loss: 9622544.0000\n",
      "Epoch [2548/50000], Train Loss: 14918646.0000, Val Loss: 9619772.0000\n",
      "Epoch [2549/50000], Train Loss: 14915255.0000, Val Loss: 9617003.0000\n",
      "Epoch [2550/50000], Train Loss: 14911861.0000, Val Loss: 9614234.0000\n",
      "Epoch [2551/50000], Train Loss: 14908469.0000, Val Loss: 9611466.0000\n",
      "Epoch [2552/50000], Train Loss: 14905080.0000, Val Loss: 9608700.0000\n",
      "Epoch [2553/50000], Train Loss: 14901692.0000, Val Loss: 9605933.0000\n",
      "Epoch [2554/50000], Train Loss: 14898303.0000, Val Loss: 9603167.0000\n",
      "Epoch [2555/50000], Train Loss: 14894914.0000, Val Loss: 9600403.0000\n",
      "Epoch [2556/50000], Train Loss: 14891527.0000, Val Loss: 9597642.0000\n",
      "Epoch [2557/50000], Train Loss: 14888144.0000, Val Loss: 9594882.0000\n",
      "Epoch [2558/50000], Train Loss: 14884763.0000, Val Loss: 9592118.0000\n",
      "Epoch [2559/50000], Train Loss: 14881378.0000, Val Loss: 9589361.0000\n",
      "Epoch [2560/50000], Train Loss: 14877997.0000, Val Loss: 9586604.0000\n",
      "Epoch [2561/50000], Train Loss: 14874617.0000, Val Loss: 9583844.0000\n",
      "Epoch [2562/50000], Train Loss: 14871236.0000, Val Loss: 9581088.0000\n",
      "Epoch [2563/50000], Train Loss: 14867858.0000, Val Loss: 9578332.0000\n",
      "Epoch [2564/50000], Train Loss: 14864481.0000, Val Loss: 9575578.0000\n",
      "Epoch [2565/50000], Train Loss: 14861103.0000, Val Loss: 9572826.0000\n",
      "Epoch [2566/50000], Train Loss: 14857730.0000, Val Loss: 9570071.0000\n",
      "Epoch [2567/50000], Train Loss: 14854355.0000, Val Loss: 9567324.0000\n",
      "Epoch [2568/50000], Train Loss: 14850983.0000, Val Loss: 9564574.0000\n",
      "Epoch [2569/50000], Train Loss: 14847613.0000, Val Loss: 9561824.0000\n",
      "Epoch [2570/50000], Train Loss: 14844239.0000, Val Loss: 9559072.0000\n",
      "Epoch [2571/50000], Train Loss: 14840869.0000, Val Loss: 9556327.0000\n",
      "Epoch [2572/50000], Train Loss: 14837501.0000, Val Loss: 9553583.0000\n",
      "Epoch [2573/50000], Train Loss: 14834136.0000, Val Loss: 9550836.0000\n",
      "Epoch [2574/50000], Train Loss: 14830767.0000, Val Loss: 9548096.0000\n",
      "Epoch [2575/50000], Train Loss: 14827405.0000, Val Loss: 9545353.0000\n",
      "Epoch [2576/50000], Train Loss: 14824039.0000, Val Loss: 9542609.0000\n",
      "Epoch [2577/50000], Train Loss: 14820678.0000, Val Loss: 9539869.0000\n",
      "Epoch [2578/50000], Train Loss: 14817314.0000, Val Loss: 9537127.0000\n",
      "Epoch [2579/50000], Train Loss: 14813952.0000, Val Loss: 9534390.0000\n",
      "Epoch [2580/50000], Train Loss: 14810593.0000, Val Loss: 9531652.0000\n",
      "Epoch [2581/50000], Train Loss: 14807233.0000, Val Loss: 9528916.0000\n",
      "Epoch [2582/50000], Train Loss: 14803876.0000, Val Loss: 9526181.0000\n",
      "Epoch [2583/50000], Train Loss: 14800520.0000, Val Loss: 9523446.0000\n",
      "Epoch [2584/50000], Train Loss: 14797166.0000, Val Loss: 9520713.0000\n",
      "Epoch [2585/50000], Train Loss: 14793810.0000, Val Loss: 9517980.0000\n",
      "Epoch [2586/50000], Train Loss: 14790456.0000, Val Loss: 9515248.0000\n",
      "Epoch [2587/50000], Train Loss: 14787101.0000, Val Loss: 9512518.0000\n",
      "Epoch [2588/50000], Train Loss: 14783751.0000, Val Loss: 9509788.0000\n",
      "Epoch [2589/50000], Train Loss: 14780401.0000, Val Loss: 9507061.0000\n",
      "Epoch [2590/50000], Train Loss: 14777052.0000, Val Loss: 9504332.0000\n",
      "Epoch [2591/50000], Train Loss: 14773705.0000, Val Loss: 9501607.0000\n",
      "Epoch [2592/50000], Train Loss: 14770360.0000, Val Loss: 9498883.0000\n",
      "Epoch [2593/50000], Train Loss: 14767013.0000, Val Loss: 9496157.0000\n",
      "Epoch [2594/50000], Train Loss: 14763668.0000, Val Loss: 9493434.0000\n",
      "Epoch [2595/50000], Train Loss: 14760323.0000, Val Loss: 9490711.0000\n",
      "Epoch [2596/50000], Train Loss: 14756980.0000, Val Loss: 9487990.0000\n",
      "Epoch [2597/50000], Train Loss: 14753639.0000, Val Loss: 9485271.0000\n",
      "Epoch [2598/50000], Train Loss: 14750298.0000, Val Loss: 9482554.0000\n",
      "Epoch [2599/50000], Train Loss: 14746962.0000, Val Loss: 9479835.0000\n",
      "Epoch [2600/50000], Train Loss: 14743622.0000, Val Loss: 9477118.0000\n",
      "Epoch [2601/50000], Train Loss: 14740286.0000, Val Loss: 9474400.0000\n",
      "Epoch [2602/50000], Train Loss: 14736947.0000, Val Loss: 9471686.0000\n",
      "Epoch [2603/50000], Train Loss: 14733614.0000, Val Loss: 9468971.0000\n",
      "Epoch [2604/50000], Train Loss: 14730276.0000, Val Loss: 9466258.0000\n",
      "Epoch [2605/50000], Train Loss: 14726947.0000, Val Loss: 9463548.0000\n",
      "Epoch [2606/50000], Train Loss: 14723614.0000, Val Loss: 9460837.0000\n",
      "Epoch [2607/50000], Train Loss: 14720283.0000, Val Loss: 9458128.0000\n",
      "Epoch [2608/50000], Train Loss: 14716954.0000, Val Loss: 9455420.0000\n",
      "Epoch [2609/50000], Train Loss: 14713626.0000, Val Loss: 9452711.0000\n",
      "Epoch [2610/50000], Train Loss: 14710298.0000, Val Loss: 9450004.0000\n",
      "Epoch [2611/50000], Train Loss: 14706970.0000, Val Loss: 9447299.0000\n",
      "Epoch [2612/50000], Train Loss: 14703646.0000, Val Loss: 9444595.0000\n",
      "Epoch [2613/50000], Train Loss: 14700322.0000, Val Loss: 9441892.0000\n",
      "Epoch [2614/50000], Train Loss: 14696999.0000, Val Loss: 9439190.0000\n",
      "Epoch [2615/50000], Train Loss: 14693678.0000, Val Loss: 9436489.0000\n",
      "Epoch [2616/50000], Train Loss: 14690356.0000, Val Loss: 9433787.0000\n",
      "Epoch [2617/50000], Train Loss: 14687037.0000, Val Loss: 9431087.0000\n",
      "Epoch [2618/50000], Train Loss: 14683716.0000, Val Loss: 9428389.0000\n",
      "Epoch [2619/50000], Train Loss: 14680400.0000, Val Loss: 9425690.0000\n",
      "Epoch [2620/50000], Train Loss: 14677080.0000, Val Loss: 9422995.0000\n",
      "Epoch [2621/50000], Train Loss: 14673766.0000, Val Loss: 9420302.0000\n",
      "Epoch [2622/50000], Train Loss: 14670453.0000, Val Loss: 9417605.0000\n",
      "Epoch [2623/50000], Train Loss: 14667137.0000, Val Loss: 9414914.0000\n",
      "Epoch [2624/50000], Train Loss: 14663826.0000, Val Loss: 9412221.0000\n",
      "Epoch [2625/50000], Train Loss: 14660514.0000, Val Loss: 9409529.0000\n",
      "Epoch [2626/50000], Train Loss: 14657204.0000, Val Loss: 9406839.0000\n",
      "Epoch [2627/50000], Train Loss: 14653895.0000, Val Loss: 9404150.0000\n",
      "Epoch [2628/50000], Train Loss: 14650588.0000, Val Loss: 9401462.0000\n",
      "Epoch [2629/50000], Train Loss: 14647279.0000, Val Loss: 9398775.0000\n",
      "Epoch [2630/50000], Train Loss: 14643974.0000, Val Loss: 9396090.0000\n",
      "Epoch [2631/50000], Train Loss: 14640669.0000, Val Loss: 9393404.0000\n",
      "Epoch [2632/50000], Train Loss: 14637365.0000, Val Loss: 9390720.0000\n",
      "Epoch [2633/50000], Train Loss: 14634061.0000, Val Loss: 9388036.0000\n",
      "Epoch [2634/50000], Train Loss: 14630760.0000, Val Loss: 9385357.0000\n",
      "Epoch [2635/50000], Train Loss: 14627463.0000, Val Loss: 9382674.0000\n",
      "Epoch [2636/50000], Train Loss: 14624159.0000, Val Loss: 9379996.0000\n",
      "Epoch [2637/50000], Train Loss: 14620863.0000, Val Loss: 9377315.0000\n",
      "Epoch [2638/50000], Train Loss: 14617564.0000, Val Loss: 9374640.0000\n",
      "Epoch [2639/50000], Train Loss: 14614270.0000, Val Loss: 9371964.0000\n",
      "Epoch [2640/50000], Train Loss: 14610974.0000, Val Loss: 9369287.0000\n",
      "Epoch [2641/50000], Train Loss: 14607681.0000, Val Loss: 9366611.0000\n",
      "Epoch [2642/50000], Train Loss: 14604386.0000, Val Loss: 9363938.0000\n",
      "Epoch [2643/50000], Train Loss: 14601095.0000, Val Loss: 9361264.0000\n",
      "Epoch [2644/50000], Train Loss: 14597800.0000, Val Loss: 9358594.0000\n",
      "Epoch [2645/50000], Train Loss: 14594513.0000, Val Loss: 9355924.0000\n",
      "Epoch [2646/50000], Train Loss: 14591225.0000, Val Loss: 9353253.0000\n",
      "Epoch [2647/50000], Train Loss: 14587935.0000, Val Loss: 9350583.0000\n",
      "Epoch [2648/50000], Train Loss: 14584649.0000, Val Loss: 9347919.0000\n",
      "Epoch [2649/50000], Train Loss: 14581365.0000, Val Loss: 9345250.0000\n",
      "Epoch [2650/50000], Train Loss: 14578079.0000, Val Loss: 9342586.0000\n",
      "Epoch [2651/50000], Train Loss: 14574796.0000, Val Loss: 9339921.0000\n",
      "Epoch [2652/50000], Train Loss: 14571513.0000, Val Loss: 9337258.0000\n",
      "Epoch [2653/50000], Train Loss: 14568231.0000, Val Loss: 9334596.0000\n",
      "Epoch [2654/50000], Train Loss: 14564954.0000, Val Loss: 9331934.0000\n",
      "Epoch [2655/50000], Train Loss: 14561673.0000, Val Loss: 9329275.0000\n",
      "Epoch [2656/50000], Train Loss: 14558396.0000, Val Loss: 9326615.0000\n",
      "Epoch [2657/50000], Train Loss: 14555118.0000, Val Loss: 9323956.0000\n",
      "Epoch [2658/50000], Train Loss: 14551841.0000, Val Loss: 9321299.0000\n",
      "Epoch [2659/50000], Train Loss: 14548565.0000, Val Loss: 9318643.0000\n",
      "Epoch [2660/50000], Train Loss: 14545292.0000, Val Loss: 9315987.0000\n",
      "Epoch [2661/50000], Train Loss: 14542020.0000, Val Loss: 9313333.0000\n",
      "Epoch [2662/50000], Train Loss: 14538749.0000, Val Loss: 9310682.0000\n",
      "Epoch [2663/50000], Train Loss: 14535480.0000, Val Loss: 9308028.0000\n",
      "Epoch [2664/50000], Train Loss: 14532208.0000, Val Loss: 9305377.0000\n",
      "Epoch [2665/50000], Train Loss: 14528941.0000, Val Loss: 9302726.0000\n",
      "Epoch [2666/50000], Train Loss: 14525671.0000, Val Loss: 9300078.0000\n",
      "Epoch [2667/50000], Train Loss: 14522405.0000, Val Loss: 9297429.0000\n",
      "Epoch [2668/50000], Train Loss: 14519140.0000, Val Loss: 9294782.0000\n",
      "Epoch [2669/50000], Train Loss: 14515874.0000, Val Loss: 9292139.0000\n",
      "Epoch [2670/50000], Train Loss: 14512614.0000, Val Loss: 9289493.0000\n",
      "Epoch [2671/50000], Train Loss: 14509352.0000, Val Loss: 9286850.0000\n",
      "Epoch [2672/50000], Train Loss: 14506091.0000, Val Loss: 9284206.0000\n",
      "Epoch [2673/50000], Train Loss: 14502832.0000, Val Loss: 9281564.0000\n",
      "Epoch [2674/50000], Train Loss: 14499572.0000, Val Loss: 9278923.0000\n",
      "Epoch [2675/50000], Train Loss: 14496313.0000, Val Loss: 9276283.0000\n",
      "Epoch [2676/50000], Train Loss: 14493056.0000, Val Loss: 9273643.0000\n",
      "Epoch [2677/50000], Train Loss: 14489800.0000, Val Loss: 9271006.0000\n",
      "Epoch [2678/50000], Train Loss: 14486546.0000, Val Loss: 9268370.0000\n",
      "Epoch [2679/50000], Train Loss: 14483292.0000, Val Loss: 9265733.0000\n",
      "Epoch [2680/50000], Train Loss: 14480038.0000, Val Loss: 9263100.0000\n",
      "Epoch [2681/50000], Train Loss: 14476791.0000, Val Loss: 9260465.0000\n",
      "Epoch [2682/50000], Train Loss: 14473537.0000, Val Loss: 9257833.0000\n",
      "Epoch [2683/50000], Train Loss: 14470289.0000, Val Loss: 9255200.0000\n",
      "Epoch [2684/50000], Train Loss: 14467038.0000, Val Loss: 9252569.0000\n",
      "Epoch [2685/50000], Train Loss: 14463793.0000, Val Loss: 9249940.0000\n",
      "Epoch [2686/50000], Train Loss: 14460547.0000, Val Loss: 9247311.0000\n",
      "Epoch [2687/50000], Train Loss: 14457302.0000, Val Loss: 9244685.0000\n",
      "Epoch [2688/50000], Train Loss: 14454061.0000, Val Loss: 9242059.0000\n",
      "Epoch [2689/50000], Train Loss: 14450818.0000, Val Loss: 9239433.0000\n",
      "Epoch [2690/50000], Train Loss: 14447573.0000, Val Loss: 9236806.0000\n",
      "Epoch [2691/50000], Train Loss: 14444331.0000, Val Loss: 9234182.0000\n",
      "Epoch [2692/50000], Train Loss: 14441092.0000, Val Loss: 9231561.0000\n",
      "Epoch [2693/50000], Train Loss: 14437853.0000, Val Loss: 9228941.0000\n",
      "Epoch [2694/50000], Train Loss: 14434618.0000, Val Loss: 9226321.0000\n",
      "Epoch [2695/50000], Train Loss: 14431384.0000, Val Loss: 9223700.0000\n",
      "Epoch [2696/50000], Train Loss: 14428144.0000, Val Loss: 9221082.0000\n",
      "Epoch [2697/50000], Train Loss: 14424913.0000, Val Loss: 9218463.0000\n",
      "Epoch [2698/50000], Train Loss: 14421678.0000, Val Loss: 9215846.0000\n",
      "Epoch [2699/50000], Train Loss: 14418444.0000, Val Loss: 9213230.0000\n",
      "Epoch [2700/50000], Train Loss: 14415211.0000, Val Loss: 9210617.0000\n",
      "Epoch [2701/50000], Train Loss: 14411983.0000, Val Loss: 9208004.0000\n",
      "Epoch [2702/50000], Train Loss: 14408756.0000, Val Loss: 9205392.0000\n",
      "Epoch [2703/50000], Train Loss: 14405526.0000, Val Loss: 9202779.0000\n",
      "Epoch [2704/50000], Train Loss: 14402297.0000, Val Loss: 9200170.0000\n",
      "Epoch [2705/50000], Train Loss: 14399074.0000, Val Loss: 9197559.0000\n",
      "Epoch [2706/50000], Train Loss: 14395846.0000, Val Loss: 9194951.0000\n",
      "Epoch [2707/50000], Train Loss: 14392624.0000, Val Loss: 9192345.0000\n",
      "Epoch [2708/50000], Train Loss: 14389403.0000, Val Loss: 9189738.0000\n",
      "Epoch [2709/50000], Train Loss: 14386180.0000, Val Loss: 9187134.0000\n",
      "Epoch [2710/50000], Train Loss: 14382958.0000, Val Loss: 9184529.0000\n",
      "Epoch [2711/50000], Train Loss: 14379741.0000, Val Loss: 9181925.0000\n",
      "Epoch [2712/50000], Train Loss: 14376521.0000, Val Loss: 9179324.0000\n",
      "Epoch [2713/50000], Train Loss: 14373303.0000, Val Loss: 9176723.0000\n",
      "Epoch [2714/50000], Train Loss: 14370087.0000, Val Loss: 9174122.0000\n",
      "Epoch [2715/50000], Train Loss: 14366873.0000, Val Loss: 9171522.0000\n",
      "Epoch [2716/50000], Train Loss: 14363656.0000, Val Loss: 9168925.0000\n",
      "Epoch [2717/50000], Train Loss: 14360447.0000, Val Loss: 9166330.0000\n",
      "Epoch [2718/50000], Train Loss: 14357235.0000, Val Loss: 9163732.0000\n",
      "Epoch [2719/50000], Train Loss: 14354021.0000, Val Loss: 9161138.0000\n",
      "Epoch [2720/50000], Train Loss: 14350814.0000, Val Loss: 9158544.0000\n",
      "Epoch [2721/50000], Train Loss: 14347604.0000, Val Loss: 9155949.0000\n",
      "Epoch [2722/50000], Train Loss: 14344394.0000, Val Loss: 9153356.0000\n",
      "Epoch [2723/50000], Train Loss: 14341186.0000, Val Loss: 9150765.0000\n",
      "Epoch [2724/50000], Train Loss: 14337978.0000, Val Loss: 9148175.0000\n",
      "Epoch [2725/50000], Train Loss: 14334775.0000, Val Loss: 9145587.0000\n",
      "Epoch [2726/50000], Train Loss: 14331574.0000, Val Loss: 9143001.0000\n",
      "Epoch [2727/50000], Train Loss: 14328372.0000, Val Loss: 9140413.0000\n",
      "Epoch [2728/50000], Train Loss: 14325169.0000, Val Loss: 9137826.0000\n",
      "Epoch [2729/50000], Train Loss: 14321968.0000, Val Loss: 9135241.0000\n",
      "Epoch [2730/50000], Train Loss: 14318768.0000, Val Loss: 9132655.0000\n",
      "Epoch [2731/50000], Train Loss: 14315567.0000, Val Loss: 9130073.0000\n",
      "Epoch [2732/50000], Train Loss: 14312372.0000, Val Loss: 9127490.0000\n",
      "Epoch [2733/50000], Train Loss: 14309176.0000, Val Loss: 9124909.0000\n",
      "Epoch [2734/50000], Train Loss: 14305982.0000, Val Loss: 9122332.0000\n",
      "Epoch [2735/50000], Train Loss: 14302787.0000, Val Loss: 9119753.0000\n",
      "Epoch [2736/50000], Train Loss: 14299593.0000, Val Loss: 9117173.0000\n",
      "Epoch [2737/50000], Train Loss: 14296402.0000, Val Loss: 9114596.0000\n",
      "Epoch [2738/50000], Train Loss: 14293209.0000, Val Loss: 9112021.0000\n",
      "Epoch [2739/50000], Train Loss: 14290021.0000, Val Loss: 9109444.0000\n",
      "Epoch [2740/50000], Train Loss: 14286830.0000, Val Loss: 9106873.0000\n",
      "Epoch [2741/50000], Train Loss: 14283644.0000, Val Loss: 9104299.0000\n",
      "Epoch [2742/50000], Train Loss: 14280459.0000, Val Loss: 9101726.0000\n",
      "Epoch [2743/50000], Train Loss: 14277273.0000, Val Loss: 9099156.0000\n",
      "Epoch [2744/50000], Train Loss: 14274085.0000, Val Loss: 9096587.0000\n",
      "Epoch [2745/50000], Train Loss: 14270903.0000, Val Loss: 9094019.0000\n",
      "Epoch [2746/50000], Train Loss: 14267722.0000, Val Loss: 9091450.0000\n",
      "Epoch [2747/50000], Train Loss: 14264539.0000, Val Loss: 9088881.0000\n",
      "Epoch [2748/50000], Train Loss: 14261356.0000, Val Loss: 9086316.0000\n",
      "Epoch [2749/50000], Train Loss: 14258177.0000, Val Loss: 9083751.0000\n",
      "Epoch [2750/50000], Train Loss: 14255001.0000, Val Loss: 9081188.0000\n",
      "Epoch [2751/50000], Train Loss: 14251823.0000, Val Loss: 9078626.0000\n",
      "Epoch [2752/50000], Train Loss: 14248648.0000, Val Loss: 9076064.0000\n",
      "Epoch [2753/50000], Train Loss: 14245470.0000, Val Loss: 9073504.0000\n",
      "Epoch [2754/50000], Train Loss: 14242297.0000, Val Loss: 9070942.0000\n",
      "Epoch [2755/50000], Train Loss: 14239122.0000, Val Loss: 9068382.0000\n",
      "Epoch [2756/50000], Train Loss: 14235950.0000, Val Loss: 9065826.0000\n",
      "Epoch [2757/50000], Train Loss: 14232779.0000, Val Loss: 9063270.0000\n",
      "Epoch [2758/50000], Train Loss: 14229611.0000, Val Loss: 9060716.0000\n",
      "Epoch [2759/50000], Train Loss: 14226443.0000, Val Loss: 9058159.0000\n",
      "Epoch [2760/50000], Train Loss: 14223274.0000, Val Loss: 9055606.0000\n",
      "Epoch [2761/50000], Train Loss: 14220108.0000, Val Loss: 9053053.0000\n",
      "Epoch [2762/50000], Train Loss: 14216942.0000, Val Loss: 9050500.0000\n",
      "Epoch [2763/50000], Train Loss: 14213777.0000, Val Loss: 9047949.0000\n",
      "Epoch [2764/50000], Train Loss: 14210612.0000, Val Loss: 9045402.0000\n",
      "Epoch [2765/50000], Train Loss: 14207452.0000, Val Loss: 9042852.0000\n",
      "Epoch [2766/50000], Train Loss: 14204288.0000, Val Loss: 9040305.0000\n",
      "Epoch [2767/50000], Train Loss: 14201130.0000, Val Loss: 9037759.0000\n",
      "Epoch [2768/50000], Train Loss: 14197971.0000, Val Loss: 9035211.0000\n",
      "Epoch [2769/50000], Train Loss: 14194809.0000, Val Loss: 9032666.0000\n",
      "Epoch [2770/50000], Train Loss: 14191651.0000, Val Loss: 9030123.0000\n",
      "Epoch [2771/50000], Train Loss: 14188496.0000, Val Loss: 9027579.0000\n",
      "Epoch [2772/50000], Train Loss: 14185340.0000, Val Loss: 9025038.0000\n",
      "Epoch [2773/50000], Train Loss: 14182186.0000, Val Loss: 9022498.0000\n",
      "Epoch [2774/50000], Train Loss: 14179033.0000, Val Loss: 9019957.0000\n",
      "Epoch [2775/50000], Train Loss: 14175882.0000, Val Loss: 9017421.0000\n",
      "Epoch [2776/50000], Train Loss: 14172730.0000, Val Loss: 9014882.0000\n",
      "Epoch [2777/50000], Train Loss: 14169580.0000, Val Loss: 9012345.0000\n",
      "Epoch [2778/50000], Train Loss: 14166430.0000, Val Loss: 9009809.0000\n",
      "Epoch [2779/50000], Train Loss: 14163283.0000, Val Loss: 9007274.0000\n",
      "Epoch [2780/50000], Train Loss: 14160135.0000, Val Loss: 9004740.0000\n",
      "Epoch [2781/50000], Train Loss: 14156990.0000, Val Loss: 9002209.0000\n",
      "Epoch [2782/50000], Train Loss: 14153846.0000, Val Loss: 8999676.0000\n",
      "Epoch [2783/50000], Train Loss: 14150699.0000, Val Loss: 8997148.0000\n",
      "Epoch [2784/50000], Train Loss: 14147560.0000, Val Loss: 8994617.0000\n",
      "Epoch [2785/50000], Train Loss: 14144419.0000, Val Loss: 8992085.0000\n",
      "Epoch [2786/50000], Train Loss: 14141275.0000, Val Loss: 8989559.0000\n",
      "Epoch [2787/50000], Train Loss: 14138138.0000, Val Loss: 8987030.0000\n",
      "Epoch [2788/50000], Train Loss: 14134996.0000, Val Loss: 8984507.0000\n",
      "Epoch [2789/50000], Train Loss: 14131860.0000, Val Loss: 8981984.0000\n",
      "Epoch [2790/50000], Train Loss: 14128725.0000, Val Loss: 8979460.0000\n",
      "Epoch [2791/50000], Train Loss: 14125592.0000, Val Loss: 8976937.0000\n",
      "Epoch [2792/50000], Train Loss: 14122454.0000, Val Loss: 8974415.0000\n",
      "Epoch [2793/50000], Train Loss: 14119323.0000, Val Loss: 8971894.0000\n",
      "Epoch [2794/50000], Train Loss: 14116189.0000, Val Loss: 8969376.0000\n",
      "Epoch [2795/50000], Train Loss: 14113059.0000, Val Loss: 8966854.0000\n",
      "Epoch [2796/50000], Train Loss: 14109927.0000, Val Loss: 8964339.0000\n",
      "Epoch [2797/50000], Train Loss: 14106800.0000, Val Loss: 8961822.0000\n",
      "Epoch [2798/50000], Train Loss: 14103671.0000, Val Loss: 8959306.0000\n",
      "Epoch [2799/50000], Train Loss: 14100545.0000, Val Loss: 8956790.0000\n",
      "Epoch [2800/50000], Train Loss: 14097418.0000, Val Loss: 8954278.0000\n",
      "Epoch [2801/50000], Train Loss: 14094294.0000, Val Loss: 8951763.0000\n",
      "Epoch [2802/50000], Train Loss: 14091167.0000, Val Loss: 8949254.0000\n",
      "Epoch [2803/50000], Train Loss: 14088047.0000, Val Loss: 8946741.0000\n",
      "Epoch [2804/50000], Train Loss: 14084924.0000, Val Loss: 8944232.0000\n",
      "Epoch [2805/50000], Train Loss: 14081803.0000, Val Loss: 8941723.0000\n",
      "Epoch [2806/50000], Train Loss: 14078684.0000, Val Loss: 8939215.0000\n",
      "Epoch [2807/50000], Train Loss: 14075565.0000, Val Loss: 8936709.0000\n",
      "Epoch [2808/50000], Train Loss: 14072447.0000, Val Loss: 8934204.0000\n",
      "Epoch [2809/50000], Train Loss: 14069331.0000, Val Loss: 8931699.0000\n",
      "Epoch [2810/50000], Train Loss: 14066214.0000, Val Loss: 8929194.0000\n",
      "Epoch [2811/50000], Train Loss: 14063101.0000, Val Loss: 8926691.0000\n",
      "Epoch [2812/50000], Train Loss: 14059985.0000, Val Loss: 8924190.0000\n",
      "Epoch [2813/50000], Train Loss: 14056874.0000, Val Loss: 8921690.0000\n",
      "Epoch [2814/50000], Train Loss: 14053763.0000, Val Loss: 8919190.0000\n",
      "Epoch [2815/50000], Train Loss: 14050653.0000, Val Loss: 8916692.0000\n",
      "Epoch [2816/50000], Train Loss: 14047544.0000, Val Loss: 8914194.0000\n",
      "Epoch [2817/50000], Train Loss: 14044435.0000, Val Loss: 8911696.0000\n",
      "Epoch [2818/50000], Train Loss: 14041326.0000, Val Loss: 8909202.0000\n",
      "Epoch [2819/50000], Train Loss: 14038223.0000, Val Loss: 8906706.0000\n",
      "Epoch [2820/50000], Train Loss: 14035118.0000, Val Loss: 8904213.0000\n",
      "Epoch [2821/50000], Train Loss: 14032012.0000, Val Loss: 8901721.0000\n",
      "Epoch [2822/50000], Train Loss: 14028911.0000, Val Loss: 8899230.0000\n",
      "Epoch [2823/50000], Train Loss: 14025810.0000, Val Loss: 8896738.0000\n",
      "Epoch [2824/50000], Train Loss: 14022707.0000, Val Loss: 8894249.0000\n",
      "Epoch [2825/50000], Train Loss: 14019609.0000, Val Loss: 8891761.0000\n",
      "Epoch [2826/50000], Train Loss: 14016509.0000, Val Loss: 8889271.0000\n",
      "Epoch [2827/50000], Train Loss: 14013410.0000, Val Loss: 8886786.0000\n",
      "Epoch [2828/50000], Train Loss: 14010315.0000, Val Loss: 8884301.0000\n",
      "Epoch [2829/50000], Train Loss: 14007220.0000, Val Loss: 8881815.0000\n",
      "Epoch [2830/50000], Train Loss: 14004123.0000, Val Loss: 8879333.0000\n",
      "Epoch [2831/50000], Train Loss: 14001031.0000, Val Loss: 8876851.0000\n",
      "Epoch [2832/50000], Train Loss: 13997940.0000, Val Loss: 8874367.0000\n",
      "Epoch [2833/50000], Train Loss: 13994845.0000, Val Loss: 8871888.0000\n",
      "Epoch [2834/50000], Train Loss: 13991756.0000, Val Loss: 8869407.0000\n",
      "Epoch [2835/50000], Train Loss: 13988666.0000, Val Loss: 8866929.0000\n",
      "Epoch [2836/50000], Train Loss: 13985579.0000, Val Loss: 8864450.0000\n",
      "Epoch [2837/50000], Train Loss: 13982489.0000, Val Loss: 8861975.0000\n",
      "Epoch [2838/50000], Train Loss: 13979407.0000, Val Loss: 8859499.0000\n",
      "Epoch [2839/50000], Train Loss: 13976321.0000, Val Loss: 8857023.0000\n",
      "Epoch [2840/50000], Train Loss: 13973237.0000, Val Loss: 8854549.0000\n",
      "Epoch [2841/50000], Train Loss: 13970152.0000, Val Loss: 8852076.0000\n",
      "Epoch [2842/50000], Train Loss: 13967070.0000, Val Loss: 8849605.0000\n",
      "Epoch [2843/50000], Train Loss: 13963989.0000, Val Loss: 8847135.0000\n",
      "Epoch [2844/50000], Train Loss: 13960909.0000, Val Loss: 8844665.0000\n",
      "Epoch [2845/50000], Train Loss: 13957830.0000, Val Loss: 8842195.0000\n",
      "Epoch [2846/50000], Train Loss: 13954753.0000, Val Loss: 8839728.0000\n",
      "Epoch [2847/50000], Train Loss: 13951674.0000, Val Loss: 8837261.0000\n",
      "Epoch [2848/50000], Train Loss: 13948598.0000, Val Loss: 8834796.0000\n",
      "Epoch [2849/50000], Train Loss: 13945523.0000, Val Loss: 8832332.0000\n",
      "Epoch [2850/50000], Train Loss: 13942452.0000, Val Loss: 8829866.0000\n",
      "Epoch [2851/50000], Train Loss: 13939375.0000, Val Loss: 8827404.0000\n",
      "Epoch [2852/50000], Train Loss: 13936307.0000, Val Loss: 8824943.0000\n",
      "Epoch [2853/50000], Train Loss: 13933236.0000, Val Loss: 8822480.0000\n",
      "Epoch [2854/50000], Train Loss: 13930164.0000, Val Loss: 8820021.0000\n",
      "Epoch [2855/50000], Train Loss: 13927098.0000, Val Loss: 8817562.0000\n",
      "Epoch [2856/50000], Train Loss: 13924028.0000, Val Loss: 8815105.0000\n",
      "Epoch [2857/50000], Train Loss: 13920963.0000, Val Loss: 8812647.0000\n",
      "Epoch [2858/50000], Train Loss: 13917897.0000, Val Loss: 8810191.0000\n",
      "Epoch [2859/50000], Train Loss: 13914833.0000, Val Loss: 8807737.0000\n",
      "Epoch [2860/50000], Train Loss: 13911769.0000, Val Loss: 8805282.0000\n",
      "Epoch [2861/50000], Train Loss: 13908706.0000, Val Loss: 8802830.0000\n",
      "Epoch [2862/50000], Train Loss: 13905645.0000, Val Loss: 8800380.0000\n",
      "Epoch [2863/50000], Train Loss: 13902586.0000, Val Loss: 8797927.0000\n",
      "Epoch [2864/50000], Train Loss: 13899524.0000, Val Loss: 8795476.0000\n",
      "Epoch [2865/50000], Train Loss: 13896465.0000, Val Loss: 8793027.0000\n",
      "Epoch [2866/50000], Train Loss: 13893410.0000, Val Loss: 8790579.0000\n",
      "Epoch [2867/50000], Train Loss: 13890352.0000, Val Loss: 8788133.0000\n",
      "Epoch [2868/50000], Train Loss: 13887297.0000, Val Loss: 8785686.0000\n",
      "Epoch [2869/50000], Train Loss: 13884243.0000, Val Loss: 8783243.0000\n",
      "Epoch [2870/50000], Train Loss: 13881192.0000, Val Loss: 8780797.0000\n",
      "Epoch [2871/50000], Train Loss: 13878138.0000, Val Loss: 8778355.0000\n",
      "Epoch [2872/50000], Train Loss: 13875089.0000, Val Loss: 8775912.0000\n",
      "Epoch [2873/50000], Train Loss: 13872037.0000, Val Loss: 8773471.0000\n",
      "Epoch [2874/50000], Train Loss: 13868988.0000, Val Loss: 8771030.0000\n",
      "Epoch [2875/50000], Train Loss: 13865940.0000, Val Loss: 8768591.0000\n",
      "Epoch [2876/50000], Train Loss: 13862892.0000, Val Loss: 8766153.0000\n",
      "Epoch [2877/50000], Train Loss: 13859844.0000, Val Loss: 8763717.0000\n",
      "Epoch [2878/50000], Train Loss: 13856803.0000, Val Loss: 8761282.0000\n",
      "Epoch [2879/50000], Train Loss: 13853760.0000, Val Loss: 8758846.0000\n",
      "Epoch [2880/50000], Train Loss: 13850716.0000, Val Loss: 8756411.0000\n",
      "Epoch [2881/50000], Train Loss: 13847673.0000, Val Loss: 8753975.0000\n",
      "Epoch [2882/50000], Train Loss: 13844630.0000, Val Loss: 8751545.0000\n",
      "Epoch [2883/50000], Train Loss: 13841591.0000, Val Loss: 8749114.0000\n",
      "Epoch [2884/50000], Train Loss: 13838552.0000, Val Loss: 8746684.0000\n",
      "Epoch [2885/50000], Train Loss: 13835516.0000, Val Loss: 8744255.0000\n",
      "Epoch [2886/50000], Train Loss: 13832478.0000, Val Loss: 8741825.0000\n",
      "Epoch [2887/50000], Train Loss: 13829443.0000, Val Loss: 8739397.0000\n",
      "Epoch [2888/50000], Train Loss: 13826407.0000, Val Loss: 8736972.0000\n",
      "Epoch [2889/50000], Train Loss: 13823373.0000, Val Loss: 8734546.0000\n",
      "Epoch [2890/50000], Train Loss: 13820341.0000, Val Loss: 8732121.0000\n",
      "Epoch [2891/50000], Train Loss: 13817309.0000, Val Loss: 8729699.0000\n",
      "Epoch [2892/50000], Train Loss: 13814280.0000, Val Loss: 8727274.0000\n",
      "Epoch [2893/50000], Train Loss: 13811248.0000, Val Loss: 8724854.0000\n",
      "Epoch [2894/50000], Train Loss: 13808222.0000, Val Loss: 8722435.0000\n",
      "Epoch [2895/50000], Train Loss: 13805195.0000, Val Loss: 8720015.0000\n",
      "Epoch [2896/50000], Train Loss: 13802170.0000, Val Loss: 8717597.0000\n",
      "Epoch [2897/50000], Train Loss: 13799142.0000, Val Loss: 8715177.0000\n",
      "Epoch [2898/50000], Train Loss: 13796117.0000, Val Loss: 8712762.0000\n",
      "Epoch [2899/50000], Train Loss: 13793094.0000, Val Loss: 8710345.0000\n",
      "Epoch [2900/50000], Train Loss: 13790071.0000, Val Loss: 8707931.0000\n",
      "Epoch [2901/50000], Train Loss: 13787051.0000, Val Loss: 8705518.0000\n",
      "Epoch [2902/50000], Train Loss: 13784030.0000, Val Loss: 8703106.0000\n",
      "Epoch [2903/50000], Train Loss: 13781012.0000, Val Loss: 8700693.0000\n",
      "Epoch [2904/50000], Train Loss: 13777993.0000, Val Loss: 8698283.0000\n",
      "Epoch [2905/50000], Train Loss: 13774976.0000, Val Loss: 8695872.0000\n",
      "Epoch [2906/50000], Train Loss: 13771959.0000, Val Loss: 8693462.0000\n",
      "Epoch [2907/50000], Train Loss: 13768941.0000, Val Loss: 8691057.0000\n",
      "Epoch [2908/50000], Train Loss: 13765931.0000, Val Loss: 8688650.0000\n",
      "Epoch [2909/50000], Train Loss: 13762915.0000, Val Loss: 8686245.0000\n",
      "Epoch [2910/50000], Train Loss: 13759906.0000, Val Loss: 8683840.0000\n",
      "Epoch [2911/50000], Train Loss: 13756893.0000, Val Loss: 8681436.0000\n",
      "Epoch [2912/50000], Train Loss: 13753884.0000, Val Loss: 8679033.0000\n",
      "Epoch [2913/50000], Train Loss: 13750874.0000, Val Loss: 8676630.0000\n",
      "Epoch [2914/50000], Train Loss: 13747865.0000, Val Loss: 8674230.0000\n",
      "Epoch [2915/50000], Train Loss: 13744859.0000, Val Loss: 8671829.0000\n",
      "Epoch [2916/50000], Train Loss: 13741852.0000, Val Loss: 8669429.0000\n",
      "Epoch [2917/50000], Train Loss: 13738846.0000, Val Loss: 8667034.0000\n",
      "Epoch [2918/50000], Train Loss: 13735844.0000, Val Loss: 8664636.0000\n",
      "Epoch [2919/50000], Train Loss: 13732843.0000, Val Loss: 8662239.0000\n",
      "Epoch [2920/50000], Train Loss: 13729838.0000, Val Loss: 8659843.0000\n",
      "Epoch [2921/50000], Train Loss: 13726837.0000, Val Loss: 8657451.0000\n",
      "Epoch [2922/50000], Train Loss: 13723838.0000, Val Loss: 8655056.0000\n",
      "Epoch [2923/50000], Train Loss: 13720839.0000, Val Loss: 8652664.0000\n",
      "Epoch [2924/50000], Train Loss: 13717841.0000, Val Loss: 8650274.0000\n",
      "Epoch [2925/50000], Train Loss: 13714845.0000, Val Loss: 8647884.0000\n",
      "Epoch [2926/50000], Train Loss: 13711851.0000, Val Loss: 8645495.0000\n",
      "Epoch [2927/50000], Train Loss: 13708855.0000, Val Loss: 8643107.0000\n",
      "Epoch [2928/50000], Train Loss: 13705862.0000, Val Loss: 8640719.0000\n",
      "Epoch [2929/50000], Train Loss: 13702867.0000, Val Loss: 8638332.0000\n",
      "Epoch [2930/50000], Train Loss: 13699876.0000, Val Loss: 8635948.0000\n",
      "Epoch [2931/50000], Train Loss: 13696887.0000, Val Loss: 8633565.0000\n",
      "Epoch [2932/50000], Train Loss: 13693898.0000, Val Loss: 8631182.0000\n",
      "Epoch [2933/50000], Train Loss: 13690910.0000, Val Loss: 8628799.0000\n",
      "Epoch [2934/50000], Train Loss: 13687923.0000, Val Loss: 8626419.0000\n",
      "Epoch [2935/50000], Train Loss: 13684937.0000, Val Loss: 8624037.0000\n",
      "Epoch [2936/50000], Train Loss: 13681949.0000, Val Loss: 8621658.0000\n",
      "Epoch [2937/50000], Train Loss: 13678966.0000, Val Loss: 8619279.0000\n",
      "Epoch [2938/50000], Train Loss: 13675980.0000, Val Loss: 8616903.0000\n",
      "Epoch [2939/50000], Train Loss: 13672999.0000, Val Loss: 8614526.0000\n",
      "Epoch [2940/50000], Train Loss: 13670019.0000, Val Loss: 8612150.0000\n",
      "Epoch [2941/50000], Train Loss: 13667036.0000, Val Loss: 8609775.0000\n",
      "Epoch [2942/50000], Train Loss: 13664057.0000, Val Loss: 8607405.0000\n",
      "Epoch [2943/50000], Train Loss: 13661082.0000, Val Loss: 8605029.0000\n",
      "Epoch [2944/50000], Train Loss: 13658103.0000, Val Loss: 8602659.0000\n",
      "Epoch [2945/50000], Train Loss: 13655125.0000, Val Loss: 8600288.0000\n",
      "Epoch [2946/50000], Train Loss: 13652150.0000, Val Loss: 8597919.0000\n",
      "Epoch [2947/50000], Train Loss: 13649178.0000, Val Loss: 8595550.0000\n",
      "Epoch [2948/50000], Train Loss: 13646202.0000, Val Loss: 8593182.0000\n",
      "Epoch [2949/50000], Train Loss: 13643233.0000, Val Loss: 8590817.0000\n",
      "Epoch [2950/50000], Train Loss: 13640263.0000, Val Loss: 8588450.0000\n",
      "Epoch [2951/50000], Train Loss: 13637293.0000, Val Loss: 8586085.0000\n",
      "Epoch [2952/50000], Train Loss: 13634323.0000, Val Loss: 8583721.0000\n",
      "Epoch [2953/50000], Train Loss: 13631353.0000, Val Loss: 8581358.0000\n",
      "Epoch [2954/50000], Train Loss: 13628386.0000, Val Loss: 8578997.0000\n",
      "Epoch [2955/50000], Train Loss: 13625419.0000, Val Loss: 8576636.0000\n",
      "Epoch [2956/50000], Train Loss: 13622457.0000, Val Loss: 8574277.0000\n",
      "Epoch [2957/50000], Train Loss: 13619492.0000, Val Loss: 8571919.0000\n",
      "Epoch [2958/50000], Train Loss: 13616531.0000, Val Loss: 8569560.0000\n",
      "Epoch [2959/50000], Train Loss: 13613566.0000, Val Loss: 8567202.0000\n",
      "Epoch [2960/50000], Train Loss: 13610606.0000, Val Loss: 8564846.0000\n",
      "Epoch [2961/50000], Train Loss: 13607645.0000, Val Loss: 8562491.0000\n",
      "Epoch [2962/50000], Train Loss: 13604684.0000, Val Loss: 8560137.0000\n",
      "Epoch [2963/50000], Train Loss: 13601729.0000, Val Loss: 8557785.0000\n",
      "Epoch [2964/50000], Train Loss: 13598772.0000, Val Loss: 8555434.0000\n",
      "Epoch [2965/50000], Train Loss: 13595819.0000, Val Loss: 8553082.0000\n",
      "Epoch [2966/50000], Train Loss: 13592861.0000, Val Loss: 8550733.0000\n",
      "Epoch [2967/50000], Train Loss: 13589909.0000, Val Loss: 8548382.0000\n",
      "Epoch [2968/50000], Train Loss: 13586953.0000, Val Loss: 8546034.0000\n",
      "Epoch [2969/50000], Train Loss: 13584003.0000, Val Loss: 8543686.0000\n",
      "Epoch [2970/50000], Train Loss: 13581052.0000, Val Loss: 8541341.0000\n",
      "Epoch [2971/50000], Train Loss: 13578104.0000, Val Loss: 8538995.0000\n",
      "Epoch [2972/50000], Train Loss: 13575152.0000, Val Loss: 8536651.0000\n",
      "Epoch [2973/50000], Train Loss: 13572206.0000, Val Loss: 8534308.0000\n",
      "Epoch [2974/50000], Train Loss: 13569260.0000, Val Loss: 8531965.0000\n",
      "Epoch [2975/50000], Train Loss: 13566312.0000, Val Loss: 8529623.0000\n",
      "Epoch [2976/50000], Train Loss: 13563367.0000, Val Loss: 8527284.0000\n",
      "Epoch [2977/50000], Train Loss: 13560424.0000, Val Loss: 8524943.0000\n",
      "Epoch [2978/50000], Train Loss: 13557482.0000, Val Loss: 8522605.0000\n",
      "Epoch [2979/50000], Train Loss: 13554540.0000, Val Loss: 8520268.0000\n",
      "Epoch [2980/50000], Train Loss: 13551598.0000, Val Loss: 8517931.0000\n",
      "Epoch [2981/50000], Train Loss: 13548661.0000, Val Loss: 8515595.0000\n",
      "Epoch [2982/50000], Train Loss: 13545721.0000, Val Loss: 8513261.0000\n",
      "Epoch [2983/50000], Train Loss: 13542784.0000, Val Loss: 8510927.0000\n",
      "Epoch [2984/50000], Train Loss: 13539847.0000, Val Loss: 8508594.0000\n",
      "Epoch [2985/50000], Train Loss: 13536909.0000, Val Loss: 8506261.0000\n",
      "Epoch [2986/50000], Train Loss: 13533976.0000, Val Loss: 8503931.0000\n",
      "Epoch [2987/50000], Train Loss: 13531043.0000, Val Loss: 8501602.0000\n",
      "Epoch [2988/50000], Train Loss: 13528110.0000, Val Loss: 8499273.0000\n",
      "Epoch [2989/50000], Train Loss: 13525178.0000, Val Loss: 8496945.0000\n",
      "Epoch [2990/50000], Train Loss: 13522249.0000, Val Loss: 8494619.0000\n",
      "Epoch [2991/50000], Train Loss: 13519318.0000, Val Loss: 8492292.0000\n",
      "Epoch [2992/50000], Train Loss: 13516391.0000, Val Loss: 8489967.0000\n",
      "Epoch [2993/50000], Train Loss: 13513463.0000, Val Loss: 8487641.0000\n",
      "Epoch [2994/50000], Train Loss: 13510535.0000, Val Loss: 8485319.0000\n",
      "Epoch [2995/50000], Train Loss: 13507609.0000, Val Loss: 8482997.0000\n",
      "Epoch [2996/50000], Train Loss: 13504687.0000, Val Loss: 8480675.0000\n",
      "Epoch [2997/50000], Train Loss: 13501761.0000, Val Loss: 8478355.0000\n",
      "Epoch [2998/50000], Train Loss: 13498841.0000, Val Loss: 8476036.0000\n",
      "Epoch [2999/50000], Train Loss: 13495919.0000, Val Loss: 8473717.0000\n",
      "Epoch [3000/50000], Train Loss: 13492996.0000, Val Loss: 8471401.0000\n",
      "Epoch [3001/50000], Train Loss: 13490077.0000, Val Loss: 8469085.0000\n",
      "Epoch [3002/50000], Train Loss: 13487160.0000, Val Loss: 8466771.0000\n",
      "Epoch [3003/50000], Train Loss: 13484243.0000, Val Loss: 8464455.0000\n",
      "Epoch [3004/50000], Train Loss: 13481327.0000, Val Loss: 8462142.0000\n",
      "Epoch [3005/50000], Train Loss: 13478411.0000, Val Loss: 8459831.0000\n",
      "Epoch [3006/50000], Train Loss: 13475498.0000, Val Loss: 8457519.0000\n",
      "Epoch [3007/50000], Train Loss: 13472584.0000, Val Loss: 8455209.0000\n",
      "Epoch [3008/50000], Train Loss: 13469674.0000, Val Loss: 8452898.0000\n",
      "Epoch [3009/50000], Train Loss: 13466759.0000, Val Loss: 8450589.0000\n",
      "Epoch [3010/50000], Train Loss: 13463848.0000, Val Loss: 8448283.0000\n",
      "Epoch [3011/50000], Train Loss: 13460941.0000, Val Loss: 8445977.0000\n",
      "Epoch [3012/50000], Train Loss: 13458035.0000, Val Loss: 8443671.0000\n",
      "Epoch [3013/50000], Train Loss: 13455126.0000, Val Loss: 8441367.0000\n",
      "Epoch [3014/50000], Train Loss: 13452221.0000, Val Loss: 8439062.0000\n",
      "Epoch [3015/50000], Train Loss: 13449314.0000, Val Loss: 8436759.0000\n",
      "Epoch [3016/50000], Train Loss: 13446410.0000, Val Loss: 8434455.0000\n",
      "Epoch [3017/50000], Train Loss: 13443504.0000, Val Loss: 8432155.0000\n",
      "Epoch [3018/50000], Train Loss: 13440603.0000, Val Loss: 8429857.0000\n",
      "Epoch [3019/50000], Train Loss: 13437704.0000, Val Loss: 8427558.0000\n",
      "Epoch [3020/50000], Train Loss: 13434803.0000, Val Loss: 8425261.0000\n",
      "Epoch [3021/50000], Train Loss: 13431905.0000, Val Loss: 8422964.0000\n",
      "Epoch [3022/50000], Train Loss: 13429004.0000, Val Loss: 8420667.0000\n",
      "Epoch [3023/50000], Train Loss: 13426109.0000, Val Loss: 8418371.0000\n",
      "Epoch [3024/50000], Train Loss: 13423211.0000, Val Loss: 8416078.0000\n",
      "Epoch [3025/50000], Train Loss: 13420316.0000, Val Loss: 8413783.0000\n",
      "Epoch [3026/50000], Train Loss: 13417421.0000, Val Loss: 8411494.0000\n",
      "Epoch [3027/50000], Train Loss: 13414530.0000, Val Loss: 8409202.0000\n",
      "Epoch [3028/50000], Train Loss: 13411639.0000, Val Loss: 8406911.0000\n",
      "Epoch [3029/50000], Train Loss: 13408747.0000, Val Loss: 8404622.0000\n",
      "Epoch [3030/50000], Train Loss: 13405858.0000, Val Loss: 8402333.0000\n",
      "Epoch [3031/50000], Train Loss: 13402966.0000, Val Loss: 8400045.0000\n",
      "Epoch [3032/50000], Train Loss: 13400079.0000, Val Loss: 8397758.0000\n",
      "Epoch [3033/50000], Train Loss: 13397190.0000, Val Loss: 8395474.0000\n",
      "Epoch [3034/50000], Train Loss: 13394305.0000, Val Loss: 8393188.0000\n",
      "Epoch [3035/50000], Train Loss: 13391421.0000, Val Loss: 8390905.0000\n",
      "Epoch [3036/50000], Train Loss: 13388535.0000, Val Loss: 8388623.0000\n",
      "Epoch [3037/50000], Train Loss: 13385654.0000, Val Loss: 8386342.0000\n",
      "Epoch [3038/50000], Train Loss: 13382772.0000, Val Loss: 8384059.0000\n",
      "Epoch [3039/50000], Train Loss: 13379888.0000, Val Loss: 8381780.5000\n",
      "Epoch [3040/50000], Train Loss: 13377008.0000, Val Loss: 8379501.0000\n",
      "Epoch [3041/50000], Train Loss: 13374129.0000, Val Loss: 8377223.0000\n",
      "Epoch [3042/50000], Train Loss: 13371251.0000, Val Loss: 8374946.5000\n",
      "Epoch [3043/50000], Train Loss: 13368374.0000, Val Loss: 8372670.5000\n",
      "Epoch [3044/50000], Train Loss: 13365497.0000, Val Loss: 8370396.5000\n",
      "Epoch [3045/50000], Train Loss: 13362623.0000, Val Loss: 8368121.5000\n",
      "Epoch [3046/50000], Train Loss: 13359748.0000, Val Loss: 8365847.5000\n",
      "Epoch [3047/50000], Train Loss: 13356876.0000, Val Loss: 8363575.5000\n",
      "Epoch [3048/50000], Train Loss: 13354002.0000, Val Loss: 8361303.5000\n",
      "Epoch [3049/50000], Train Loss: 13351130.0000, Val Loss: 8359033.5000\n",
      "Epoch [3050/50000], Train Loss: 13348262.0000, Val Loss: 8356766.0000\n",
      "Epoch [3051/50000], Train Loss: 13345392.0000, Val Loss: 8354497.0000\n",
      "Epoch [3052/50000], Train Loss: 13342527.0000, Val Loss: 8352229.5000\n",
      "Epoch [3053/50000], Train Loss: 13339658.0000, Val Loss: 8349964.5000\n",
      "Epoch [3054/50000], Train Loss: 13336793.0000, Val Loss: 8347697.0000\n",
      "Epoch [3055/50000], Train Loss: 13333926.0000, Val Loss: 8345434.0000\n",
      "Epoch [3056/50000], Train Loss: 13331063.0000, Val Loss: 8343172.5000\n",
      "Epoch [3057/50000], Train Loss: 13328202.0000, Val Loss: 8340907.0000\n",
      "Epoch [3058/50000], Train Loss: 13325336.0000, Val Loss: 8338646.0000\n",
      "Epoch [3059/50000], Train Loss: 13322477.0000, Val Loss: 8336386.0000\n",
      "Epoch [3060/50000], Train Loss: 13319617.0000, Val Loss: 8334126.5000\n",
      "Epoch [3061/50000], Train Loss: 13316758.0000, Val Loss: 8331869.0000\n",
      "Epoch [3062/50000], Train Loss: 13313900.0000, Val Loss: 8329610.5000\n",
      "Epoch [3063/50000], Train Loss: 13311043.0000, Val Loss: 8327353.0000\n",
      "Epoch [3064/50000], Train Loss: 13308188.0000, Val Loss: 8325097.5000\n",
      "Epoch [3065/50000], Train Loss: 13305333.0000, Val Loss: 8322842.5000\n",
      "Epoch [3066/50000], Train Loss: 13302480.0000, Val Loss: 8320589.0000\n",
      "Epoch [3067/50000], Train Loss: 13299623.0000, Val Loss: 8318335.0000\n",
      "Epoch [3068/50000], Train Loss: 13296774.0000, Val Loss: 8316084.5000\n",
      "Epoch [3069/50000], Train Loss: 13293923.0000, Val Loss: 8313832.5000\n",
      "Epoch [3070/50000], Train Loss: 13291072.0000, Val Loss: 8311582.0000\n",
      "Epoch [3071/50000], Train Loss: 13288222.0000, Val Loss: 8309331.0000\n",
      "Epoch [3072/50000], Train Loss: 13285372.0000, Val Loss: 8307085.0000\n",
      "Epoch [3073/50000], Train Loss: 13282529.0000, Val Loss: 8304839.5000\n",
      "Epoch [3074/50000], Train Loss: 13279683.0000, Val Loss: 8302591.5000\n",
      "Epoch [3075/50000], Train Loss: 13276835.0000, Val Loss: 8300347.0000\n",
      "Epoch [3076/50000], Train Loss: 13273993.0000, Val Loss: 8298102.5000\n",
      "Epoch [3077/50000], Train Loss: 13271148.0000, Val Loss: 8295858.5000\n",
      "Epoch [3078/50000], Train Loss: 13268307.0000, Val Loss: 8293616.0000\n",
      "Epoch [3079/50000], Train Loss: 13265468.0000, Val Loss: 8291374.0000\n",
      "Epoch [3080/50000], Train Loss: 13262625.0000, Val Loss: 8289134.0000\n",
      "Epoch [3081/50000], Train Loss: 13259787.0000, Val Loss: 8286893.5000\n",
      "Epoch [3082/50000], Train Loss: 13256948.0000, Val Loss: 8284654.0000\n",
      "Epoch [3083/50000], Train Loss: 13254111.0000, Val Loss: 8282418.0000\n",
      "Epoch [3084/50000], Train Loss: 13251275.0000, Val Loss: 8280181.5000\n",
      "Epoch [3085/50000], Train Loss: 13248442.0000, Val Loss: 8277945.5000\n",
      "Epoch [3086/50000], Train Loss: 13245606.0000, Val Loss: 8275710.0000\n",
      "Epoch [3087/50000], Train Loss: 13242772.0000, Val Loss: 8273475.0000\n",
      "Epoch [3088/50000], Train Loss: 13239939.0000, Val Loss: 8271243.5000\n",
      "Epoch [3089/50000], Train Loss: 13237111.0000, Val Loss: 8269012.5000\n",
      "Epoch [3090/50000], Train Loss: 13234280.0000, Val Loss: 8266781.0000\n",
      "Epoch [3091/50000], Train Loss: 13231453.0000, Val Loss: 8264551.0000\n",
      "Epoch [3092/50000], Train Loss: 13228625.0000, Val Loss: 8262321.0000\n",
      "Epoch [3093/50000], Train Loss: 13225795.0000, Val Loss: 8260092.5000\n",
      "Epoch [3094/50000], Train Loss: 13222970.0000, Val Loss: 8257864.0000\n",
      "Epoch [3095/50000], Train Loss: 13220143.0000, Val Loss: 8255637.0000\n",
      "Epoch [3096/50000], Train Loss: 13217318.0000, Val Loss: 8253413.5000\n",
      "Epoch [3097/50000], Train Loss: 13214497.0000, Val Loss: 8251188.5000\n",
      "Epoch [3098/50000], Train Loss: 13211674.0000, Val Loss: 8248965.5000\n",
      "Epoch [3099/50000], Train Loss: 13208853.0000, Val Loss: 8246743.5000\n",
      "Epoch [3100/50000], Train Loss: 13206034.0000, Val Loss: 8244522.5000\n",
      "Epoch [3101/50000], Train Loss: 13203216.0000, Val Loss: 8242299.0000\n",
      "Epoch [3102/50000], Train Loss: 13200394.0000, Val Loss: 8240080.0000\n",
      "Epoch [3103/50000], Train Loss: 13197578.0000, Val Loss: 8237861.5000\n",
      "Epoch [3104/50000], Train Loss: 13194762.0000, Val Loss: 8235645.0000\n",
      "Epoch [3105/50000], Train Loss: 13191948.0000, Val Loss: 8233429.5000\n",
      "Epoch [3106/50000], Train Loss: 13189136.0000, Val Loss: 8231213.0000\n",
      "Epoch [3107/50000], Train Loss: 13186322.0000, Val Loss: 8228998.0000\n",
      "Epoch [3108/50000], Train Loss: 13183508.0000, Val Loss: 8226782.5000\n",
      "Epoch [3109/50000], Train Loss: 13180696.0000, Val Loss: 8224568.5000\n",
      "Epoch [3110/50000], Train Loss: 13177885.0000, Val Loss: 8222355.0000\n",
      "Epoch [3111/50000], Train Loss: 13175076.0000, Val Loss: 8220146.5000\n",
      "Epoch [3112/50000], Train Loss: 13172269.0000, Val Loss: 8217936.0000\n",
      "Epoch [3113/50000], Train Loss: 13169462.0000, Val Loss: 8215727.5000\n",
      "Epoch [3114/50000], Train Loss: 13166655.0000, Val Loss: 8213519.5000\n",
      "Epoch [3115/50000], Train Loss: 13163852.0000, Val Loss: 8211312.0000\n",
      "Epoch [3116/50000], Train Loss: 13161046.0000, Val Loss: 8209105.0000\n",
      "Epoch [3117/50000], Train Loss: 13158245.0000, Val Loss: 8206898.5000\n",
      "Epoch [3118/50000], Train Loss: 13155439.0000, Val Loss: 8204694.5000\n",
      "Epoch [3119/50000], Train Loss: 13152640.0000, Val Loss: 8202490.0000\n",
      "Epoch [3120/50000], Train Loss: 13149839.0000, Val Loss: 8200289.5000\n",
      "Epoch [3121/50000], Train Loss: 13147042.0000, Val Loss: 8198088.5000\n",
      "Epoch [3122/50000], Train Loss: 13144245.0000, Val Loss: 8195888.0000\n",
      "Epoch [3123/50000], Train Loss: 13141447.0000, Val Loss: 8193687.0000\n",
      "Epoch [3124/50000], Train Loss: 13138648.0000, Val Loss: 8191487.0000\n",
      "Epoch [3125/50000], Train Loss: 13135851.0000, Val Loss: 8189288.0000\n",
      "Epoch [3126/50000], Train Loss: 13133056.0000, Val Loss: 8187090.5000\n",
      "Epoch [3127/50000], Train Loss: 13130262.0000, Val Loss: 8184894.0000\n",
      "Epoch [3128/50000], Train Loss: 13127470.0000, Val Loss: 8182701.5000\n",
      "Epoch [3129/50000], Train Loss: 13124682.0000, Val Loss: 8180506.5000\n",
      "Epoch [3130/50000], Train Loss: 13121891.0000, Val Loss: 8178313.5000\n",
      "Epoch [3131/50000], Train Loss: 13119102.0000, Val Loss: 8176121.5000\n",
      "Epoch [3132/50000], Train Loss: 13116313.0000, Val Loss: 8173929.5000\n",
      "Epoch [3133/50000], Train Loss: 13113525.0000, Val Loss: 8171739.5000\n",
      "Epoch [3134/50000], Train Loss: 13110738.0000, Val Loss: 8169550.0000\n",
      "Epoch [3135/50000], Train Loss: 13107953.0000, Val Loss: 8167363.0000\n",
      "Epoch [3136/50000], Train Loss: 13105171.0000, Val Loss: 8165173.5000\n",
      "Epoch [3137/50000], Train Loss: 13102384.0000, Val Loss: 8162987.5000\n",
      "Epoch [3138/50000], Train Loss: 13099604.0000, Val Loss: 8160801.0000\n",
      "Epoch [3139/50000], Train Loss: 13096821.0000, Val Loss: 8158616.5000\n",
      "Epoch [3140/50000], Train Loss: 13094038.0000, Val Loss: 8156432.0000\n",
      "Epoch [3141/50000], Train Loss: 13091261.0000, Val Loss: 8154249.5000\n",
      "Epoch [3142/50000], Train Loss: 13088483.0000, Val Loss: 8152068.5000\n",
      "Epoch [3143/50000], Train Loss: 13085706.0000, Val Loss: 8149888.0000\n",
      "Epoch [3144/50000], Train Loss: 13082929.0000, Val Loss: 8147708.5000\n",
      "Epoch [3145/50000], Train Loss: 13080154.0000, Val Loss: 8145528.5000\n",
      "Epoch [3146/50000], Train Loss: 13077379.0000, Val Loss: 8143350.5000\n",
      "Epoch [3147/50000], Train Loss: 13074606.0000, Val Loss: 8141172.5000\n",
      "Epoch [3148/50000], Train Loss: 13071832.0000, Val Loss: 8138995.5000\n",
      "Epoch [3149/50000], Train Loss: 13069059.0000, Val Loss: 8136819.0000\n",
      "Epoch [3150/50000], Train Loss: 13066288.0000, Val Loss: 8134647.0000\n",
      "Epoch [3151/50000], Train Loss: 13063521.0000, Val Loss: 8132472.5000\n",
      "Epoch [3152/50000], Train Loss: 13060753.0000, Val Loss: 8130301.5000\n",
      "Epoch [3153/50000], Train Loss: 13057985.0000, Val Loss: 8128129.0000\n",
      "Epoch [3154/50000], Train Loss: 13055219.0000, Val Loss: 8125960.0000\n",
      "Epoch [3155/50000], Train Loss: 13052454.0000, Val Loss: 8123789.0000\n",
      "Epoch [3156/50000], Train Loss: 13049688.0000, Val Loss: 8121620.5000\n",
      "Epoch [3157/50000], Train Loss: 13046924.0000, Val Loss: 8119451.5000\n",
      "Epoch [3158/50000], Train Loss: 13044159.0000, Val Loss: 8117285.5000\n",
      "Epoch [3159/50000], Train Loss: 13041399.0000, Val Loss: 8115121.0000\n",
      "Epoch [3160/50000], Train Loss: 13038639.0000, Val Loss: 8112955.0000\n",
      "Epoch [3161/50000], Train Loss: 13035879.0000, Val Loss: 8110792.0000\n",
      "Epoch [3162/50000], Train Loss: 13033122.0000, Val Loss: 8108627.0000\n",
      "Epoch [3163/50000], Train Loss: 13030362.0000, Val Loss: 8106466.0000\n",
      "Epoch [3164/50000], Train Loss: 13027605.0000, Val Loss: 8104304.0000\n",
      "Epoch [3165/50000], Train Loss: 13024848.0000, Val Loss: 8102143.0000\n",
      "Epoch [3166/50000], Train Loss: 13022093.0000, Val Loss: 8099983.0000\n",
      "Epoch [3167/50000], Train Loss: 13019338.0000, Val Loss: 8097825.0000\n",
      "Epoch [3168/50000], Train Loss: 13016586.0000, Val Loss: 8095669.0000\n",
      "Epoch [3169/50000], Train Loss: 13013837.0000, Val Loss: 8093511.5000\n",
      "Epoch [3170/50000], Train Loss: 13011085.0000, Val Loss: 8091355.5000\n",
      "Epoch [3171/50000], Train Loss: 13008335.0000, Val Loss: 8089201.0000\n",
      "Epoch [3172/50000], Train Loss: 13005585.0000, Val Loss: 8087046.5000\n",
      "Epoch [3173/50000], Train Loss: 13002837.0000, Val Loss: 8084894.0000\n",
      "Epoch [3174/50000], Train Loss: 13000090.0000, Val Loss: 8082743.5000\n",
      "Epoch [3175/50000], Train Loss: 12997346.0000, Val Loss: 8080592.0000\n",
      "Epoch [3176/50000], Train Loss: 12994599.0000, Val Loss: 8078442.5000\n",
      "Epoch [3177/50000], Train Loss: 12991856.0000, Val Loss: 8076294.5000\n",
      "Epoch [3178/50000], Train Loss: 12989114.0000, Val Loss: 8074145.0000\n",
      "Epoch [3179/50000], Train Loss: 12986369.0000, Val Loss: 8071998.0000\n",
      "Epoch [3180/50000], Train Loss: 12983629.0000, Val Loss: 8069850.5000\n",
      "Epoch [3181/50000], Train Loss: 12980888.0000, Val Loss: 8067707.5000\n",
      "Epoch [3182/50000], Train Loss: 12978152.0000, Val Loss: 8065563.5000\n",
      "Epoch [3183/50000], Train Loss: 12975414.0000, Val Loss: 8063419.0000\n",
      "Epoch [3184/50000], Train Loss: 12972674.0000, Val Loss: 8061276.5000\n",
      "Epoch [3185/50000], Train Loss: 12969939.0000, Val Loss: 8059136.0000\n",
      "Epoch [3186/50000], Train Loss: 12967205.0000, Val Loss: 8056994.5000\n",
      "Epoch [3187/50000], Train Loss: 12964471.0000, Val Loss: 8054854.5000\n",
      "Epoch [3188/50000], Train Loss: 12961737.0000, Val Loss: 8052715.5000\n",
      "Epoch [3189/50000], Train Loss: 12959004.0000, Val Loss: 8050577.0000\n",
      "Epoch [3190/50000], Train Loss: 12956272.0000, Val Loss: 8048440.5000\n",
      "Epoch [3191/50000], Train Loss: 12953543.0000, Val Loss: 8046305.5000\n",
      "Epoch [3192/50000], Train Loss: 12950814.0000, Val Loss: 8044170.5000\n",
      "Epoch [3193/50000], Train Loss: 12948087.0000, Val Loss: 8042037.0000\n",
      "Epoch [3194/50000], Train Loss: 12945360.0000, Val Loss: 8039903.5000\n",
      "Epoch [3195/50000], Train Loss: 12942631.0000, Val Loss: 8037769.0000\n",
      "Epoch [3196/50000], Train Loss: 12939904.0000, Val Loss: 8035638.5000\n",
      "Epoch [3197/50000], Train Loss: 12937181.0000, Val Loss: 8033507.0000\n",
      "Epoch [3198/50000], Train Loss: 12934456.0000, Val Loss: 8031379.0000\n",
      "Epoch [3199/50000], Train Loss: 12931736.0000, Val Loss: 8029249.0000\n",
      "Epoch [3200/50000], Train Loss: 12929012.0000, Val Loss: 8027123.0000\n",
      "Epoch [3201/50000], Train Loss: 12926293.0000, Val Loss: 8024996.5000\n",
      "Epoch [3202/50000], Train Loss: 12923574.0000, Val Loss: 8022868.5000\n",
      "Epoch [3203/50000], Train Loss: 12920852.0000, Val Loss: 8020745.5000\n",
      "Epoch [3204/50000], Train Loss: 12918137.0000, Val Loss: 8018622.0000\n",
      "Epoch [3205/50000], Train Loss: 12915420.0000, Val Loss: 8016498.5000\n",
      "Epoch [3206/50000], Train Loss: 12912705.0000, Val Loss: 8014376.5000\n",
      "Epoch [3207/50000], Train Loss: 12909990.0000, Val Loss: 8012256.5000\n",
      "Epoch [3208/50000], Train Loss: 12907277.0000, Val Loss: 8010136.5000\n",
      "Epoch [3209/50000], Train Loss: 12904565.0000, Val Loss: 8008016.0000\n",
      "Epoch [3210/50000], Train Loss: 12901851.0000, Val Loss: 8005898.5000\n",
      "Epoch [3211/50000], Train Loss: 12899142.0000, Val Loss: 8003779.0000\n",
      "Epoch [3212/50000], Train Loss: 12896430.0000, Val Loss: 8001664.0000\n",
      "Epoch [3213/50000], Train Loss: 12893722.0000, Val Loss: 7999550.0000\n",
      "Epoch [3214/50000], Train Loss: 12891018.0000, Val Loss: 7997433.5000\n",
      "Epoch [3215/50000], Train Loss: 12888308.0000, Val Loss: 7995321.5000\n",
      "Epoch [3216/50000], Train Loss: 12885604.0000, Val Loss: 7993207.5000\n",
      "Epoch [3217/50000], Train Loss: 12882897.0000, Val Loss: 7991097.5000\n",
      "Epoch [3218/50000], Train Loss: 12880195.0000, Val Loss: 7988985.0000\n",
      "Epoch [3219/50000], Train Loss: 12877492.0000, Val Loss: 7986874.5000\n",
      "Epoch [3220/50000], Train Loss: 12874787.0000, Val Loss: 7984766.0000\n",
      "Epoch [3221/50000], Train Loss: 12872089.0000, Val Loss: 7982659.0000\n",
      "Epoch [3222/50000], Train Loss: 12869388.0000, Val Loss: 7980551.5000\n",
      "Epoch [3223/50000], Train Loss: 12866689.0000, Val Loss: 7978447.0000\n",
      "Epoch [3224/50000], Train Loss: 12863993.0000, Val Loss: 7976339.0000\n",
      "Epoch [3225/50000], Train Loss: 12861293.0000, Val Loss: 7974234.5000\n",
      "Epoch [3226/50000], Train Loss: 12858596.0000, Val Loss: 7972132.5000\n",
      "Epoch [3227/50000], Train Loss: 12855901.0000, Val Loss: 7970031.0000\n",
      "Epoch [3228/50000], Train Loss: 12853210.0000, Val Loss: 7967928.5000\n",
      "Epoch [3229/50000], Train Loss: 12850516.0000, Val Loss: 7965829.5000\n",
      "Epoch [3230/50000], Train Loss: 12847825.0000, Val Loss: 7963729.0000\n",
      "Epoch [3231/50000], Train Loss: 12845135.0000, Val Loss: 7961630.5000\n",
      "Epoch [3232/50000], Train Loss: 12842442.0000, Val Loss: 7959531.5000\n",
      "Epoch [3233/50000], Train Loss: 12839752.0000, Val Loss: 7957435.5000\n",
      "Epoch [3234/50000], Train Loss: 12837066.0000, Val Loss: 7955338.0000\n",
      "Epoch [3235/50000], Train Loss: 12834375.0000, Val Loss: 7953244.5000\n",
      "Epoch [3236/50000], Train Loss: 12831693.0000, Val Loss: 7951150.0000\n",
      "Epoch [3237/50000], Train Loss: 12829005.0000, Val Loss: 7949056.0000\n",
      "Epoch [3238/50000], Train Loss: 12826323.0000, Val Loss: 7946965.5000\n",
      "Epoch [3239/50000], Train Loss: 12823640.0000, Val Loss: 7944873.0000\n",
      "Epoch [3240/50000], Train Loss: 12820957.0000, Val Loss: 7942783.0000\n",
      "Epoch [3241/50000], Train Loss: 12818276.0000, Val Loss: 7940692.5000\n",
      "Epoch [3242/50000], Train Loss: 12815595.0000, Val Loss: 7938603.0000\n",
      "Epoch [3243/50000], Train Loss: 12812912.0000, Val Loss: 7936517.5000\n",
      "Epoch [3244/50000], Train Loss: 12810237.0000, Val Loss: 7934431.5000\n",
      "Epoch [3245/50000], Train Loss: 12807556.0000, Val Loss: 7932344.5000\n",
      "Epoch [3246/50000], Train Loss: 12804883.0000, Val Loss: 7930260.5000\n",
      "Epoch [3247/50000], Train Loss: 12802206.0000, Val Loss: 7928175.0000\n",
      "Epoch [3248/50000], Train Loss: 12799532.0000, Val Loss: 7926091.5000\n",
      "Epoch [3249/50000], Train Loss: 12796856.0000, Val Loss: 7924008.5000\n",
      "Epoch [3250/50000], Train Loss: 12794184.0000, Val Loss: 7921929.5000\n",
      "Epoch [3251/50000], Train Loss: 12791514.0000, Val Loss: 7919848.5000\n",
      "Epoch [3252/50000], Train Loss: 12788843.0000, Val Loss: 7917771.5000\n",
      "Epoch [3253/50000], Train Loss: 12786175.0000, Val Loss: 7915693.0000\n",
      "Epoch [3254/50000], Train Loss: 12783504.0000, Val Loss: 7913616.0000\n",
      "Epoch [3255/50000], Train Loss: 12780838.0000, Val Loss: 7911537.0000\n",
      "Epoch [3256/50000], Train Loss: 12778169.0000, Val Loss: 7909461.5000\n",
      "Epoch [3257/50000], Train Loss: 12775501.0000, Val Loss: 7907386.5000\n",
      "Epoch [3258/50000], Train Loss: 12772838.0000, Val Loss: 7905314.0000\n",
      "Epoch [3259/50000], Train Loss: 12770174.0000, Val Loss: 7903242.5000\n",
      "Epoch [3260/50000], Train Loss: 12767513.0000, Val Loss: 7901169.0000\n",
      "Epoch [3261/50000], Train Loss: 12764851.0000, Val Loss: 7899098.5000\n",
      "Epoch [3262/50000], Train Loss: 12762188.0000, Val Loss: 7897028.5000\n",
      "Epoch [3263/50000], Train Loss: 12759528.0000, Val Loss: 7894959.0000\n",
      "Epoch [3264/50000], Train Loss: 12756870.0000, Val Loss: 7892890.0000\n",
      "Epoch [3265/50000], Train Loss: 12754210.0000, Val Loss: 7890822.5000\n",
      "Epoch [3266/50000], Train Loss: 12751553.0000, Val Loss: 7888756.5000\n",
      "Epoch [3267/50000], Train Loss: 12748897.0000, Val Loss: 7886689.5000\n",
      "Epoch [3268/50000], Train Loss: 12746241.0000, Val Loss: 7884627.0000\n",
      "Epoch [3269/50000], Train Loss: 12743590.0000, Val Loss: 7882563.5000\n",
      "Epoch [3270/50000], Train Loss: 12740937.0000, Val Loss: 7880500.5000\n",
      "Epoch [3271/50000], Train Loss: 12738282.0000, Val Loss: 7878439.5000\n",
      "Epoch [3272/50000], Train Loss: 12735632.0000, Val Loss: 7876377.5000\n",
      "Epoch [3273/50000], Train Loss: 12732980.0000, Val Loss: 7874317.0000\n",
      "Epoch [3274/50000], Train Loss: 12730331.0000, Val Loss: 7872259.5000\n",
      "Epoch [3275/50000], Train Loss: 12727684.0000, Val Loss: 7870201.5000\n",
      "Epoch [3276/50000], Train Loss: 12725037.0000, Val Loss: 7868144.0000\n",
      "Epoch [3277/50000], Train Loss: 12722389.0000, Val Loss: 7866087.5000\n",
      "Epoch [3278/50000], Train Loss: 12719743.0000, Val Loss: 7864032.0000\n",
      "Epoch [3279/50000], Train Loss: 12717098.0000, Val Loss: 7861977.5000\n",
      "Epoch [3280/50000], Train Loss: 12714454.0000, Val Loss: 7859924.5000\n",
      "Epoch [3281/50000], Train Loss: 12711812.0000, Val Loss: 7857871.5000\n",
      "Epoch [3282/50000], Train Loss: 12709169.0000, Val Loss: 7855822.0000\n",
      "Epoch [3283/50000], Train Loss: 12706532.0000, Val Loss: 7853770.5000\n",
      "Epoch [3284/50000], Train Loss: 12703892.0000, Val Loss: 7851721.0000\n",
      "Epoch [3285/50000], Train Loss: 12701254.0000, Val Loss: 7849671.0000\n",
      "Epoch [3286/50000], Train Loss: 12698614.0000, Val Loss: 7847622.5000\n",
      "Epoch [3287/50000], Train Loss: 12695978.0000, Val Loss: 7845576.0000\n",
      "Epoch [3288/50000], Train Loss: 12693342.0000, Val Loss: 7843528.0000\n",
      "Epoch [3289/50000], Train Loss: 12690705.0000, Val Loss: 7841484.5000\n",
      "Epoch [3290/50000], Train Loss: 12688072.0000, Val Loss: 7839440.0000\n",
      "Epoch [3291/50000], Train Loss: 12685440.0000, Val Loss: 7837397.5000\n",
      "Epoch [3292/50000], Train Loss: 12682808.0000, Val Loss: 7835354.0000\n",
      "Epoch [3293/50000], Train Loss: 12680176.0000, Val Loss: 7833313.0000\n",
      "Epoch [3294/50000], Train Loss: 12677548.0000, Val Loss: 7831271.5000\n",
      "Epoch [3295/50000], Train Loss: 12674916.0000, Val Loss: 7829230.0000\n",
      "Epoch [3296/50000], Train Loss: 12672285.0000, Val Loss: 7827192.5000\n",
      "Epoch [3297/50000], Train Loss: 12669661.0000, Val Loss: 7825156.5000\n",
      "Epoch [3298/50000], Train Loss: 12667035.0000, Val Loss: 7823119.5000\n",
      "Epoch [3299/50000], Train Loss: 12664411.0000, Val Loss: 7821085.0000\n",
      "Epoch [3300/50000], Train Loss: 12661788.0000, Val Loss: 7819047.0000\n",
      "Epoch [3301/50000], Train Loss: 12659162.0000, Val Loss: 7817013.5000\n",
      "Epoch [3302/50000], Train Loss: 12656541.0000, Val Loss: 7814980.5000\n",
      "Epoch [3303/50000], Train Loss: 12653918.0000, Val Loss: 7812947.0000\n",
      "Epoch [3304/50000], Train Loss: 12651299.0000, Val Loss: 7810916.5000\n",
      "Epoch [3305/50000], Train Loss: 12648679.0000, Val Loss: 7808885.5000\n",
      "Epoch [3306/50000], Train Loss: 12646061.0000, Val Loss: 7806855.5000\n",
      "Epoch [3307/50000], Train Loss: 12643443.0000, Val Loss: 7804826.5000\n",
      "Epoch [3308/50000], Train Loss: 12640825.0000, Val Loss: 7802798.0000\n",
      "Epoch [3309/50000], Train Loss: 12638208.0000, Val Loss: 7800773.0000\n",
      "Epoch [3310/50000], Train Loss: 12635596.0000, Val Loss: 7798745.5000\n",
      "Epoch [3311/50000], Train Loss: 12632981.0000, Val Loss: 7796720.0000\n",
      "Epoch [3312/50000], Train Loss: 12630369.0000, Val Loss: 7794697.5000\n",
      "Epoch [3313/50000], Train Loss: 12627758.0000, Val Loss: 7792673.5000\n",
      "Epoch [3314/50000], Train Loss: 12625145.0000, Val Loss: 7790649.5000\n",
      "Epoch [3315/50000], Train Loss: 12622535.0000, Val Loss: 7788629.5000\n",
      "Epoch [3316/50000], Train Loss: 12619926.0000, Val Loss: 7786609.0000\n",
      "Epoch [3317/50000], Train Loss: 12617318.0000, Val Loss: 7784590.0000\n",
      "Epoch [3318/50000], Train Loss: 12614713.0000, Val Loss: 7782570.5000\n",
      "Epoch [3319/50000], Train Loss: 12612104.0000, Val Loss: 7780554.5000\n",
      "Epoch [3320/50000], Train Loss: 12609500.0000, Val Loss: 7778537.5000\n",
      "Epoch [3321/50000], Train Loss: 12606897.0000, Val Loss: 7776521.0000\n",
      "Epoch [3322/50000], Train Loss: 12604292.0000, Val Loss: 7774505.5000\n",
      "Epoch [3323/50000], Train Loss: 12601691.0000, Val Loss: 7772490.5000\n",
      "Epoch [3324/50000], Train Loss: 12599091.0000, Val Loss: 7770477.0000\n",
      "Epoch [3325/50000], Train Loss: 12596488.0000, Val Loss: 7768465.0000\n",
      "Epoch [3326/50000], Train Loss: 12593890.0000, Val Loss: 7766453.5000\n",
      "Epoch [3327/50000], Train Loss: 12591289.0000, Val Loss: 7764445.0000\n",
      "Epoch [3328/50000], Train Loss: 12588694.0000, Val Loss: 7762434.0000\n",
      "Epoch [3329/50000], Train Loss: 12586097.0000, Val Loss: 7760424.5000\n",
      "Epoch [3330/50000], Train Loss: 12583503.0000, Val Loss: 7758419.0000\n",
      "Epoch [3331/50000], Train Loss: 12580909.0000, Val Loss: 7756411.0000\n",
      "Epoch [3332/50000], Train Loss: 12578313.0000, Val Loss: 7754405.0000\n",
      "Epoch [3333/50000], Train Loss: 12575719.0000, Val Loss: 7752399.5000\n",
      "Epoch [3334/50000], Train Loss: 12573129.0000, Val Loss: 7750394.5000\n",
      "Epoch [3335/50000], Train Loss: 12570538.0000, Val Loss: 7748389.5000\n",
      "Epoch [3336/50000], Train Loss: 12567947.0000, Val Loss: 7746389.5000\n",
      "Epoch [3337/50000], Train Loss: 12565359.0000, Val Loss: 7744388.5000\n",
      "Epoch [3338/50000], Train Loss: 12562771.0000, Val Loss: 7742389.0000\n",
      "Epoch [3339/50000], Train Loss: 12560185.0000, Val Loss: 7740389.5000\n",
      "Epoch [3340/50000], Train Loss: 12557598.0000, Val Loss: 7738389.0000\n",
      "Epoch [3341/50000], Train Loss: 12555014.0000, Val Loss: 7736391.5000\n",
      "Epoch [3342/50000], Train Loss: 12552430.0000, Val Loss: 7734394.0000\n",
      "Epoch [3343/50000], Train Loss: 12549846.0000, Val Loss: 7732400.0000\n",
      "Epoch [3344/50000], Train Loss: 12547265.0000, Val Loss: 7730404.5000\n",
      "Epoch [3345/50000], Train Loss: 12544684.0000, Val Loss: 7728410.5000\n",
      "Epoch [3346/50000], Train Loss: 12542105.0000, Val Loss: 7726417.5000\n",
      "Epoch [3347/50000], Train Loss: 12539527.0000, Val Loss: 7724423.5000\n",
      "Epoch [3348/50000], Train Loss: 12536944.0000, Val Loss: 7722432.0000\n",
      "Epoch [3349/50000], Train Loss: 12534366.0000, Val Loss: 7720440.5000\n",
      "Epoch [3350/50000], Train Loss: 12531788.0000, Val Loss: 7718453.5000\n",
      "Epoch [3351/50000], Train Loss: 12529217.0000, Val Loss: 7716463.5000\n",
      "Epoch [3352/50000], Train Loss: 12526639.0000, Val Loss: 7714477.0000\n",
      "Epoch [3353/50000], Train Loss: 12524069.0000, Val Loss: 7712490.5000\n",
      "Epoch [3354/50000], Train Loss: 12521498.0000, Val Loss: 7710504.5000\n",
      "Epoch [3355/50000], Train Loss: 12518923.0000, Val Loss: 7708518.0000\n",
      "Epoch [3356/50000], Train Loss: 12516352.0000, Val Loss: 7706535.5000\n",
      "Epoch [3357/50000], Train Loss: 12513784.0000, Val Loss: 7704550.5000\n",
      "Epoch [3358/50000], Train Loss: 12511213.0000, Val Loss: 7702569.5000\n",
      "Epoch [3359/50000], Train Loss: 12508648.0000, Val Loss: 7700587.5000\n",
      "Epoch [3360/50000], Train Loss: 12506079.0000, Val Loss: 7698608.0000\n",
      "Epoch [3361/50000], Train Loss: 12503514.0000, Val Loss: 7696628.5000\n",
      "Epoch [3362/50000], Train Loss: 12500949.0000, Val Loss: 7694649.0000\n",
      "Epoch [3363/50000], Train Loss: 12498386.0000, Val Loss: 7692672.0000\n",
      "Epoch [3364/50000], Train Loss: 12495821.0000, Val Loss: 7690693.0000\n",
      "Epoch [3365/50000], Train Loss: 12493260.0000, Val Loss: 7688718.5000\n",
      "Epoch [3366/50000], Train Loss: 12490700.0000, Val Loss: 7686743.5000\n",
      "Epoch [3367/50000], Train Loss: 12488138.0000, Val Loss: 7684769.0000\n",
      "Epoch [3368/50000], Train Loss: 12485578.0000, Val Loss: 7682798.0000\n",
      "Epoch [3369/50000], Train Loss: 12483024.0000, Val Loss: 7680824.0000\n",
      "Epoch [3370/50000], Train Loss: 12480466.0000, Val Loss: 7678852.5000\n",
      "Epoch [3371/50000], Train Loss: 12477908.0000, Val Loss: 7676880.0000\n",
      "Epoch [3372/50000], Train Loss: 12475350.0000, Val Loss: 7674912.0000\n",
      "Epoch [3373/50000], Train Loss: 12472798.0000, Val Loss: 7672944.0000\n",
      "Epoch [3374/50000], Train Loss: 12470244.0000, Val Loss: 7670975.5000\n",
      "Epoch [3375/50000], Train Loss: 12467693.0000, Val Loss: 7669010.0000\n",
      "Epoch [3376/50000], Train Loss: 12465143.0000, Val Loss: 7667043.0000\n",
      "Epoch [3377/50000], Train Loss: 12462592.0000, Val Loss: 7665078.5000\n",
      "Epoch [3378/50000], Train Loss: 12460042.0000, Val Loss: 7663113.5000\n",
      "Epoch [3379/50000], Train Loss: 12457494.0000, Val Loss: 7661150.0000\n",
      "Epoch [3380/50000], Train Loss: 12454945.0000, Val Loss: 7659188.5000\n",
      "Epoch [3381/50000], Train Loss: 12452398.0000, Val Loss: 7657226.5000\n",
      "Epoch [3382/50000], Train Loss: 12449853.0000, Val Loss: 7655267.0000\n",
      "Epoch [3383/50000], Train Loss: 12447308.0000, Val Loss: 7653305.0000\n",
      "Epoch [3384/50000], Train Loss: 12444761.0000, Val Loss: 7651347.0000\n",
      "Epoch [3385/50000], Train Loss: 12442219.0000, Val Loss: 7649390.0000\n",
      "Epoch [3386/50000], Train Loss: 12439678.0000, Val Loss: 7647432.5000\n",
      "Epoch [3387/50000], Train Loss: 12437137.0000, Val Loss: 7645474.5000\n",
      "Epoch [3388/50000], Train Loss: 12434593.0000, Val Loss: 7643521.0000\n",
      "Epoch [3389/50000], Train Loss: 12432056.0000, Val Loss: 7641566.0000\n",
      "Epoch [3390/50000], Train Loss: 12429519.0000, Val Loss: 7639613.0000\n",
      "Epoch [3391/50000], Train Loss: 12426980.0000, Val Loss: 7637661.0000\n",
      "Epoch [3392/50000], Train Loss: 12424445.0000, Val Loss: 7635709.5000\n",
      "Epoch [3393/50000], Train Loss: 12421909.0000, Val Loss: 7633759.5000\n",
      "Epoch [3394/50000], Train Loss: 12419375.0000, Val Loss: 7631808.0000\n",
      "Epoch [3395/50000], Train Loss: 12416839.0000, Val Loss: 7629860.5000\n",
      "Epoch [3396/50000], Train Loss: 12414308.0000, Val Loss: 7627912.0000\n",
      "Epoch [3397/50000], Train Loss: 12411778.0000, Val Loss: 7625965.5000\n",
      "Epoch [3398/50000], Train Loss: 12409246.0000, Val Loss: 7624019.0000\n",
      "Epoch [3399/50000], Train Loss: 12406719.0000, Val Loss: 7622073.5000\n",
      "Epoch [3400/50000], Train Loss: 12404186.0000, Val Loss: 7620129.5000\n",
      "Epoch [3401/50000], Train Loss: 12401662.0000, Val Loss: 7618185.5000\n",
      "Epoch [3402/50000], Train Loss: 12399134.0000, Val Loss: 7616241.0000\n",
      "Epoch [3403/50000], Train Loss: 12396604.0000, Val Loss: 7614299.5000\n",
      "Epoch [3404/50000], Train Loss: 12394081.0000, Val Loss: 7612360.5000\n",
      "Epoch [3405/50000], Train Loss: 12391558.0000, Val Loss: 7610420.5000\n",
      "Epoch [3406/50000], Train Loss: 12389035.0000, Val Loss: 7608481.0000\n",
      "Epoch [3407/50000], Train Loss: 12386512.0000, Val Loss: 7606543.0000\n",
      "Epoch [3408/50000], Train Loss: 12383990.0000, Val Loss: 7604606.5000\n",
      "Epoch [3409/50000], Train Loss: 12381471.0000, Val Loss: 7602668.5000\n",
      "Epoch [3410/50000], Train Loss: 12378948.0000, Val Loss: 7600734.0000\n",
      "Epoch [3411/50000], Train Loss: 12376432.0000, Val Loss: 7598800.5000\n",
      "Epoch [3412/50000], Train Loss: 12373917.0000, Val Loss: 7596866.0000\n",
      "Epoch [3413/50000], Train Loss: 12371398.0000, Val Loss: 7594935.0000\n",
      "Epoch [3414/50000], Train Loss: 12368884.0000, Val Loss: 7593002.5000\n",
      "Epoch [3415/50000], Train Loss: 12366370.0000, Val Loss: 7591071.0000\n",
      "Epoch [3416/50000], Train Loss: 12363855.0000, Val Loss: 7589139.0000\n",
      "Epoch [3417/50000], Train Loss: 12361342.0000, Val Loss: 7587210.5000\n",
      "Epoch [3418/50000], Train Loss: 12358829.0000, Val Loss: 7585283.0000\n",
      "Epoch [3419/50000], Train Loss: 12356319.0000, Val Loss: 7583357.0000\n",
      "Epoch [3420/50000], Train Loss: 12353810.0000, Val Loss: 7581430.5000\n",
      "Epoch [3421/50000], Train Loss: 12351300.0000, Val Loss: 7579505.0000\n",
      "Epoch [3422/50000], Train Loss: 12348794.0000, Val Loss: 7577578.5000\n",
      "Epoch [3423/50000], Train Loss: 12346284.0000, Val Loss: 7575655.5000\n",
      "Epoch [3424/50000], Train Loss: 12343778.0000, Val Loss: 7573731.0000\n",
      "Epoch [3425/50000], Train Loss: 12341272.0000, Val Loss: 7571811.0000\n",
      "Epoch [3426/50000], Train Loss: 12338770.0000, Val Loss: 7569890.0000\n",
      "Epoch [3427/50000], Train Loss: 12336266.0000, Val Loss: 7567969.0000\n",
      "Epoch [3428/50000], Train Loss: 12333764.0000, Val Loss: 7566051.5000\n",
      "Epoch [3429/50000], Train Loss: 12331265.0000, Val Loss: 7564133.5000\n",
      "Epoch [3430/50000], Train Loss: 12328762.0000, Val Loss: 7562215.5000\n",
      "Epoch [3431/50000], Train Loss: 12326264.0000, Val Loss: 7560298.5000\n",
      "Epoch [3432/50000], Train Loss: 12323765.0000, Val Loss: 7558382.0000\n",
      "Epoch [3433/50000], Train Loss: 12321265.0000, Val Loss: 7556468.5000\n",
      "Epoch [3434/50000], Train Loss: 12318771.0000, Val Loss: 7554554.5000\n",
      "Epoch [3435/50000], Train Loss: 12316276.0000, Val Loss: 7552640.5000\n",
      "Epoch [3436/50000], Train Loss: 12313780.0000, Val Loss: 7550729.5000\n",
      "Epoch [3437/50000], Train Loss: 12311286.0000, Val Loss: 7548818.5000\n",
      "Epoch [3438/50000], Train Loss: 12308793.0000, Val Loss: 7546909.0000\n",
      "Epoch [3439/50000], Train Loss: 12306301.0000, Val Loss: 7544998.0000\n",
      "Epoch [3440/50000], Train Loss: 12303809.0000, Val Loss: 7543089.0000\n",
      "Epoch [3441/50000], Train Loss: 12301319.0000, Val Loss: 7541182.0000\n",
      "Epoch [3442/50000], Train Loss: 12298831.0000, Val Loss: 7539275.5000\n",
      "Epoch [3443/50000], Train Loss: 12296342.0000, Val Loss: 7537370.5000\n",
      "Epoch [3444/50000], Train Loss: 12293857.0000, Val Loss: 7535464.5000\n",
      "Epoch [3445/50000], Train Loss: 12291370.0000, Val Loss: 7533561.5000\n",
      "Epoch [3446/50000], Train Loss: 12288886.0000, Val Loss: 7531656.0000\n",
      "Epoch [3447/50000], Train Loss: 12286399.0000, Val Loss: 7529755.5000\n",
      "Epoch [3448/50000], Train Loss: 12283916.0000, Val Loss: 7527853.5000\n",
      "Epoch [3449/50000], Train Loss: 12281432.0000, Val Loss: 7525953.0000\n",
      "Epoch [3450/50000], Train Loss: 12278951.0000, Val Loss: 7524053.5000\n",
      "Epoch [3451/50000], Train Loss: 12276470.0000, Val Loss: 7522155.5000\n",
      "Epoch [3452/50000], Train Loss: 12273991.0000, Val Loss: 7520257.5000\n",
      "Epoch [3453/50000], Train Loss: 12271513.0000, Val Loss: 7518359.5000\n",
      "Epoch [3454/50000], Train Loss: 12269035.0000, Val Loss: 7516463.0000\n",
      "Epoch [3455/50000], Train Loss: 12266557.0000, Val Loss: 7514569.0000\n",
      "Epoch [3456/50000], Train Loss: 12264081.0000, Val Loss: 7512674.5000\n",
      "Epoch [3457/50000], Train Loss: 12261605.0000, Val Loss: 7510783.0000\n",
      "Epoch [3458/50000], Train Loss: 12259135.0000, Val Loss: 7508889.5000\n",
      "Epoch [3459/50000], Train Loss: 12256660.0000, Val Loss: 7506997.5000\n",
      "Epoch [3460/50000], Train Loss: 12254186.0000, Val Loss: 7505106.5000\n",
      "Epoch [3461/50000], Train Loss: 12251715.0000, Val Loss: 7503217.0000\n",
      "Epoch [3462/50000], Train Loss: 12249246.0000, Val Loss: 7501328.0000\n",
      "Epoch [3463/50000], Train Loss: 12246777.0000, Val Loss: 7499439.5000\n",
      "Epoch [3464/50000], Train Loss: 12244306.0000, Val Loss: 7497552.0000\n",
      "Epoch [3465/50000], Train Loss: 12241837.0000, Val Loss: 7495667.0000\n",
      "Epoch [3466/50000], Train Loss: 12239376.0000, Val Loss: 7493782.0000\n",
      "Epoch [3467/50000], Train Loss: 12236909.0000, Val Loss: 7491896.5000\n",
      "Epoch [3468/50000], Train Loss: 12234443.0000, Val Loss: 7490013.5000\n",
      "Epoch [3469/50000], Train Loss: 12231981.0000, Val Loss: 7488130.0000\n",
      "Epoch [3470/50000], Train Loss: 12229517.0000, Val Loss: 7486247.5000\n",
      "Epoch [3471/50000], Train Loss: 12227053.0000, Val Loss: 7484368.0000\n",
      "Epoch [3472/50000], Train Loss: 12224595.0000, Val Loss: 7482487.5000\n",
      "Epoch [3473/50000], Train Loss: 12222131.0000, Val Loss: 7480609.5000\n",
      "Epoch [3474/50000], Train Loss: 12219676.0000, Val Loss: 7478731.5000\n",
      "Epoch [3475/50000], Train Loss: 12217218.0000, Val Loss: 7476854.5000\n",
      "Epoch [3476/50000], Train Loss: 12214760.0000, Val Loss: 7474977.0000\n",
      "Epoch [3477/50000], Train Loss: 12212303.0000, Val Loss: 7473101.0000\n",
      "Epoch [3478/50000], Train Loss: 12209846.0000, Val Loss: 7471225.5000\n",
      "Epoch [3479/50000], Train Loss: 12207393.0000, Val Loss: 7469354.5000\n",
      "Epoch [3480/50000], Train Loss: 12204940.0000, Val Loss: 7467479.5000\n",
      "Epoch [3481/50000], Train Loss: 12202487.0000, Val Loss: 7465609.5000\n",
      "Epoch [3482/50000], Train Loss: 12200036.0000, Val Loss: 7463738.0000\n",
      "Epoch [3483/50000], Train Loss: 12197586.0000, Val Loss: 7461866.5000\n",
      "Epoch [3484/50000], Train Loss: 12195136.0000, Val Loss: 7459995.5000\n",
      "Epoch [3485/50000], Train Loss: 12192685.0000, Val Loss: 7458128.5000\n",
      "Epoch [3486/50000], Train Loss: 12190237.0000, Val Loss: 7456261.5000\n",
      "Epoch [3487/50000], Train Loss: 12187791.0000, Val Loss: 7454395.0000\n",
      "Epoch [3488/50000], Train Loss: 12185346.0000, Val Loss: 7452528.0000\n",
      "Epoch [3489/50000], Train Loss: 12182900.0000, Val Loss: 7450663.5000\n",
      "Epoch [3490/50000], Train Loss: 12180455.0000, Val Loss: 7448799.5000\n",
      "Epoch [3491/50000], Train Loss: 12178012.0000, Val Loss: 7446935.5000\n",
      "Epoch [3492/50000], Train Loss: 12175569.0000, Val Loss: 7445073.0000\n",
      "Epoch [3493/50000], Train Loss: 12173128.0000, Val Loss: 7443213.0000\n",
      "Epoch [3494/50000], Train Loss: 12170688.0000, Val Loss: 7441352.5000\n",
      "Epoch [3495/50000], Train Loss: 12168249.0000, Val Loss: 7439492.5000\n",
      "Epoch [3496/50000], Train Loss: 12165809.0000, Val Loss: 7437634.0000\n",
      "Epoch [3497/50000], Train Loss: 12163373.0000, Val Loss: 7435777.0000\n",
      "Epoch [3498/50000], Train Loss: 12160936.0000, Val Loss: 7433918.5000\n",
      "Epoch [3499/50000], Train Loss: 12158500.0000, Val Loss: 7432063.0000\n",
      "Epoch [3500/50000], Train Loss: 12156065.0000, Val Loss: 7430207.0000\n",
      "Epoch [3501/50000], Train Loss: 12153631.0000, Val Loss: 7428352.0000\n",
      "Epoch [3502/50000], Train Loss: 12151199.0000, Val Loss: 7426499.5000\n",
      "Epoch [3503/50000], Train Loss: 12148766.0000, Val Loss: 7424647.5000\n",
      "Epoch [3504/50000], Train Loss: 12146337.0000, Val Loss: 7422795.5000\n",
      "Epoch [3505/50000], Train Loss: 12143905.0000, Val Loss: 7420943.5000\n",
      "Epoch [3506/50000], Train Loss: 12141475.0000, Val Loss: 7419094.5000\n",
      "Epoch [3507/50000], Train Loss: 12139048.0000, Val Loss: 7417246.0000\n",
      "Epoch [3508/50000], Train Loss: 12136620.0000, Val Loss: 7415399.5000\n",
      "Epoch [3509/50000], Train Loss: 12134196.0000, Val Loss: 7413552.0000\n",
      "Epoch [3510/50000], Train Loss: 12131770.0000, Val Loss: 7411705.5000\n",
      "Epoch [3511/50000], Train Loss: 12129345.0000, Val Loss: 7409859.5000\n",
      "Epoch [3512/50000], Train Loss: 12126923.0000, Val Loss: 7408015.0000\n",
      "Epoch [3513/50000], Train Loss: 12124499.0000, Val Loss: 7406169.5000\n",
      "Epoch [3514/50000], Train Loss: 12122074.0000, Val Loss: 7404327.0000\n",
      "Epoch [3515/50000], Train Loss: 12119655.0000, Val Loss: 7402486.5000\n",
      "Epoch [3516/50000], Train Loss: 12117236.0000, Val Loss: 7400645.5000\n",
      "Epoch [3517/50000], Train Loss: 12114818.0000, Val Loss: 7398806.5000\n",
      "Epoch [3518/50000], Train Loss: 12112400.0000, Val Loss: 7396965.5000\n",
      "Epoch [3519/50000], Train Loss: 12109981.0000, Val Loss: 7395128.5000\n",
      "Epoch [3520/50000], Train Loss: 12107566.0000, Val Loss: 7393289.0000\n",
      "Epoch [3521/50000], Train Loss: 12105149.0000, Val Loss: 7391452.5000\n",
      "Epoch [3522/50000], Train Loss: 12102736.0000, Val Loss: 7389618.0000\n",
      "Epoch [3523/50000], Train Loss: 12100323.0000, Val Loss: 7387781.5000\n",
      "Epoch [3524/50000], Train Loss: 12097910.0000, Val Loss: 7385949.0000\n",
      "Epoch [3525/50000], Train Loss: 12095499.0000, Val Loss: 7384116.5000\n",
      "Epoch [3526/50000], Train Loss: 12093091.0000, Val Loss: 7382283.0000\n",
      "Epoch [3527/50000], Train Loss: 12090678.0000, Val Loss: 7380451.5000\n",
      "Epoch [3528/50000], Train Loss: 12088271.0000, Val Loss: 7378621.0000\n",
      "Epoch [3529/50000], Train Loss: 12085863.0000, Val Loss: 7376790.0000\n",
      "Epoch [3530/50000], Train Loss: 12083454.0000, Val Loss: 7374963.0000\n",
      "Epoch [3531/50000], Train Loss: 12081049.0000, Val Loss: 7373134.0000\n",
      "Epoch [3532/50000], Train Loss: 12078641.0000, Val Loss: 7371307.0000\n",
      "Epoch [3533/50000], Train Loss: 12076239.0000, Val Loss: 7369480.5000\n",
      "Epoch [3534/50000], Train Loss: 12073834.0000, Val Loss: 7367654.5000\n",
      "Epoch [3535/50000], Train Loss: 12071431.0000, Val Loss: 7365830.5000\n",
      "Epoch [3536/50000], Train Loss: 12069031.0000, Val Loss: 7364007.5000\n",
      "Epoch [3537/50000], Train Loss: 12066631.0000, Val Loss: 7362185.0000\n",
      "Epoch [3538/50000], Train Loss: 12064232.0000, Val Loss: 7360363.5000\n",
      "Epoch [3539/50000], Train Loss: 12061833.0000, Val Loss: 7358543.0000\n",
      "Epoch [3540/50000], Train Loss: 12059436.0000, Val Loss: 7356722.0000\n",
      "Epoch [3541/50000], Train Loss: 12057040.0000, Val Loss: 7354904.0000\n",
      "Epoch [3542/50000], Train Loss: 12054644.0000, Val Loss: 7353085.0000\n",
      "Epoch [3543/50000], Train Loss: 12052248.0000, Val Loss: 7351267.0000\n",
      "Epoch [3544/50000], Train Loss: 12049854.0000, Val Loss: 7349451.5000\n",
      "Epoch [3545/50000], Train Loss: 12047462.0000, Val Loss: 7347637.0000\n",
      "Epoch [3546/50000], Train Loss: 12045072.0000, Val Loss: 7345822.0000\n",
      "Epoch [3547/50000], Train Loss: 12042679.0000, Val Loss: 7344008.5000\n",
      "Epoch [3548/50000], Train Loss: 12040290.0000, Val Loss: 7342195.5000\n",
      "Epoch [3549/50000], Train Loss: 12037901.0000, Val Loss: 7340383.5000\n",
      "Epoch [3550/50000], Train Loss: 12035512.0000, Val Loss: 7338572.5000\n",
      "Epoch [3551/50000], Train Loss: 12033125.0000, Val Loss: 7336760.0000\n",
      "Epoch [3552/50000], Train Loss: 12030735.0000, Val Loss: 7334951.5000\n",
      "Epoch [3553/50000], Train Loss: 12028351.0000, Val Loss: 7333143.5000\n",
      "Epoch [3554/50000], Train Loss: 12025967.0000, Val Loss: 7331336.5000\n",
      "Epoch [3555/50000], Train Loss: 12023583.0000, Val Loss: 7329529.5000\n",
      "Epoch [3556/50000], Train Loss: 12021203.0000, Val Loss: 7327723.5000\n",
      "Epoch [3557/50000], Train Loss: 12018820.0000, Val Loss: 7325918.0000\n",
      "Epoch [3558/50000], Train Loss: 12016439.0000, Val Loss: 7324112.0000\n",
      "Epoch [3559/50000], Train Loss: 12014055.0000, Val Loss: 7322310.0000\n",
      "Epoch [3560/50000], Train Loss: 12011679.0000, Val Loss: 7320509.0000\n",
      "Epoch [3561/50000], Train Loss: 12009301.0000, Val Loss: 7318706.5000\n",
      "Epoch [3562/50000], Train Loss: 12006923.0000, Val Loss: 7316905.5000\n",
      "Epoch [3563/50000], Train Loss: 12004547.0000, Val Loss: 7315105.5000\n",
      "Epoch [3564/50000], Train Loss: 12002173.0000, Val Loss: 7313307.5000\n",
      "Epoch [3565/50000], Train Loss: 11999798.0000, Val Loss: 7311509.0000\n",
      "Epoch [3566/50000], Train Loss: 11997423.0000, Val Loss: 7309712.5000\n",
      "Epoch [3567/50000], Train Loss: 11995053.0000, Val Loss: 7307915.5000\n",
      "Epoch [3568/50000], Train Loss: 11992679.0000, Val Loss: 7306120.5000\n",
      "Epoch [3569/50000], Train Loss: 11990309.0000, Val Loss: 7304327.5000\n",
      "Epoch [3570/50000], Train Loss: 11987941.0000, Val Loss: 7302531.5000\n",
      "Epoch [3571/50000], Train Loss: 11985570.0000, Val Loss: 7300740.5000\n",
      "Epoch [3572/50000], Train Loss: 11983203.0000, Val Loss: 7298947.0000\n",
      "Epoch [3573/50000], Train Loss: 11980835.0000, Val Loss: 7297155.0000\n",
      "Epoch [3574/50000], Train Loss: 11978468.0000, Val Loss: 7295365.5000\n",
      "Epoch [3575/50000], Train Loss: 11976103.0000, Val Loss: 7293576.5000\n",
      "Epoch [3576/50000], Train Loss: 11973740.0000, Val Loss: 7291787.5000\n",
      "Epoch [3577/50000], Train Loss: 11971376.0000, Val Loss: 7290000.5000\n",
      "Epoch [3578/50000], Train Loss: 11969013.0000, Val Loss: 7288214.0000\n",
      "Epoch [3579/50000], Train Loss: 11966650.0000, Val Loss: 7286427.5000\n",
      "Epoch [3580/50000], Train Loss: 11964289.0000, Val Loss: 7284642.0000\n",
      "Epoch [3581/50000], Train Loss: 11961929.0000, Val Loss: 7282858.0000\n",
      "Epoch [3582/50000], Train Loss: 11959571.0000, Val Loss: 7281076.5000\n",
      "Epoch [3583/50000], Train Loss: 11957213.0000, Val Loss: 7279294.5000\n",
      "Epoch [3584/50000], Train Loss: 11954859.0000, Val Loss: 7277512.5000\n",
      "Epoch [3585/50000], Train Loss: 11952503.0000, Val Loss: 7275731.0000\n",
      "Epoch [3586/50000], Train Loss: 11950145.0000, Val Loss: 7273952.0000\n",
      "Epoch [3587/50000], Train Loss: 11947794.0000, Val Loss: 7272172.5000\n",
      "Epoch [3588/50000], Train Loss: 11945437.0000, Val Loss: 7270395.0000\n",
      "Epoch [3589/50000], Train Loss: 11943087.0000, Val Loss: 7268617.5000\n",
      "Epoch [3590/50000], Train Loss: 11940736.0000, Val Loss: 7266841.0000\n",
      "Epoch [3591/50000], Train Loss: 11938385.0000, Val Loss: 7265066.0000\n",
      "Epoch [3592/50000], Train Loss: 11936035.0000, Val Loss: 7263292.5000\n",
      "Epoch [3593/50000], Train Loss: 11933687.0000, Val Loss: 7261518.0000\n",
      "Epoch [3594/50000], Train Loss: 11931338.0000, Val Loss: 7259746.0000\n",
      "Epoch [3595/50000], Train Loss: 11928991.0000, Val Loss: 7257972.5000\n",
      "Epoch [3596/50000], Train Loss: 11926644.0000, Val Loss: 7256202.5000\n",
      "Epoch [3597/50000], Train Loss: 11924300.0000, Val Loss: 7254432.5000\n",
      "Epoch [3598/50000], Train Loss: 11921958.0000, Val Loss: 7252663.5000\n",
      "Epoch [3599/50000], Train Loss: 11919615.0000, Val Loss: 7250895.0000\n",
      "Epoch [3600/50000], Train Loss: 11917272.0000, Val Loss: 7249126.5000\n",
      "Epoch [3601/50000], Train Loss: 11914929.0000, Val Loss: 7247358.5000\n",
      "Epoch [3602/50000], Train Loss: 11912589.0000, Val Loss: 7245594.0000\n",
      "Epoch [3603/50000], Train Loss: 11910250.0000, Val Loss: 7243829.5000\n",
      "Epoch [3604/50000], Train Loss: 11907912.0000, Val Loss: 7242066.5000\n",
      "Epoch [3605/50000], Train Loss: 11905575.0000, Val Loss: 7240302.5000\n",
      "Epoch [3606/50000], Train Loss: 11903238.0000, Val Loss: 7238541.0000\n",
      "Epoch [3607/50000], Train Loss: 11900901.0000, Val Loss: 7236777.5000\n",
      "Epoch [3608/50000], Train Loss: 11898564.0000, Val Loss: 7235017.5000\n",
      "Epoch [3609/50000], Train Loss: 11896232.0000, Val Loss: 7233256.0000\n",
      "Epoch [3610/50000], Train Loss: 11893897.0000, Val Loss: 7231498.0000\n",
      "Epoch [3611/50000], Train Loss: 11891566.0000, Val Loss: 7229741.0000\n",
      "Epoch [3612/50000], Train Loss: 11889235.0000, Val Loss: 7227984.0000\n",
      "Epoch [3613/50000], Train Loss: 11886904.0000, Val Loss: 7226227.0000\n",
      "Epoch [3614/50000], Train Loss: 11884577.0000, Val Loss: 7224472.5000\n",
      "Epoch [3615/50000], Train Loss: 11882248.0000, Val Loss: 7222717.5000\n",
      "Epoch [3616/50000], Train Loss: 11879919.0000, Val Loss: 7220963.0000\n",
      "Epoch [3617/50000], Train Loss: 11877594.0000, Val Loss: 7219210.0000\n",
      "Epoch [3618/50000], Train Loss: 11875267.0000, Val Loss: 7217459.0000\n",
      "Epoch [3619/50000], Train Loss: 11872942.0000, Val Loss: 7215707.5000\n",
      "Epoch [3620/50000], Train Loss: 11870618.0000, Val Loss: 7213958.0000\n",
      "Epoch [3621/50000], Train Loss: 11868297.0000, Val Loss: 7212207.0000\n",
      "Epoch [3622/50000], Train Loss: 11865973.0000, Val Loss: 7210459.0000\n",
      "Epoch [3623/50000], Train Loss: 11863653.0000, Val Loss: 7208710.5000\n",
      "Epoch [3624/50000], Train Loss: 11861331.0000, Val Loss: 7206964.5000\n",
      "Epoch [3625/50000], Train Loss: 11859011.0000, Val Loss: 7205219.0000\n",
      "Epoch [3626/50000], Train Loss: 11856694.0000, Val Loss: 7203473.5000\n",
      "Epoch [3627/50000], Train Loss: 11854376.0000, Val Loss: 7201729.0000\n",
      "Epoch [3628/50000], Train Loss: 11852059.0000, Val Loss: 7199986.5000\n",
      "Epoch [3629/50000], Train Loss: 11849745.0000, Val Loss: 7198245.0000\n",
      "Epoch [3630/50000], Train Loss: 11847430.0000, Val Loss: 7196502.0000\n",
      "Epoch [3631/50000], Train Loss: 11845113.0000, Val Loss: 7194760.5000\n",
      "Epoch [3632/50000], Train Loss: 11842800.0000, Val Loss: 7193020.5000\n",
      "Epoch [3633/50000], Train Loss: 11840490.0000, Val Loss: 7191282.5000\n",
      "Epoch [3634/50000], Train Loss: 11838179.0000, Val Loss: 7189544.5000\n",
      "Epoch [3635/50000], Train Loss: 11835868.0000, Val Loss: 7187807.0000\n",
      "Epoch [3636/50000], Train Loss: 11833558.0000, Val Loss: 7186071.5000\n",
      "Epoch [3637/50000], Train Loss: 11831251.0000, Val Loss: 7184336.0000\n",
      "Epoch [3638/50000], Train Loss: 11828945.0000, Val Loss: 7182599.5000\n",
      "Epoch [3639/50000], Train Loss: 11826635.0000, Val Loss: 7180866.0000\n",
      "Epoch [3640/50000], Train Loss: 11824330.0000, Val Loss: 7179134.0000\n",
      "Epoch [3641/50000], Train Loss: 11822026.0000, Val Loss: 7177402.5000\n",
      "Epoch [3642/50000], Train Loss: 11819723.0000, Val Loss: 7175672.5000\n",
      "Epoch [3643/50000], Train Loss: 11817423.0000, Val Loss: 7173940.5000\n",
      "Epoch [3644/50000], Train Loss: 11815119.0000, Val Loss: 7172209.5000\n",
      "Epoch [3645/50000], Train Loss: 11812816.0000, Val Loss: 7170481.5000\n",
      "Epoch [3646/50000], Train Loss: 11810516.0000, Val Loss: 7168754.0000\n",
      "Epoch [3647/50000], Train Loss: 11808216.0000, Val Loss: 7167027.0000\n",
      "Epoch [3648/50000], Train Loss: 11805917.0000, Val Loss: 7165301.5000\n",
      "Epoch [3649/50000], Train Loss: 11803621.0000, Val Loss: 7163576.5000\n",
      "Epoch [3650/50000], Train Loss: 11801325.0000, Val Loss: 7161851.5000\n",
      "Epoch [3651/50000], Train Loss: 11799029.0000, Val Loss: 7160129.0000\n",
      "Epoch [3652/50000], Train Loss: 11796734.0000, Val Loss: 7158406.0000\n",
      "Epoch [3653/50000], Train Loss: 11794442.0000, Val Loss: 7156683.0000\n",
      "Epoch [3654/50000], Train Loss: 11792146.0000, Val Loss: 7154962.5000\n",
      "Epoch [3655/50000], Train Loss: 11789855.0000, Val Loss: 7153242.5000\n",
      "Epoch [3656/50000], Train Loss: 11787564.0000, Val Loss: 7151524.5000\n",
      "Epoch [3657/50000], Train Loss: 11785274.0000, Val Loss: 7149805.0000\n",
      "Epoch [3658/50000], Train Loss: 11782984.0000, Val Loss: 7148086.5000\n",
      "Epoch [3659/50000], Train Loss: 11780696.0000, Val Loss: 7146370.5000\n",
      "Epoch [3660/50000], Train Loss: 11778408.0000, Val Loss: 7144653.5000\n",
      "Epoch [3661/50000], Train Loss: 11776121.0000, Val Loss: 7142940.5000\n",
      "Epoch [3662/50000], Train Loss: 11773836.0000, Val Loss: 7141224.0000\n",
      "Epoch [3663/50000], Train Loss: 11771550.0000, Val Loss: 7139512.5000\n",
      "Epoch [3664/50000], Train Loss: 11769268.0000, Val Loss: 7137799.5000\n",
      "Epoch [3665/50000], Train Loss: 11766986.0000, Val Loss: 7136089.5000\n",
      "Epoch [3666/50000], Train Loss: 11764702.0000, Val Loss: 7134378.5000\n",
      "Epoch [3667/50000], Train Loss: 11762421.0000, Val Loss: 7132667.5000\n",
      "Epoch [3668/50000], Train Loss: 11760140.0000, Val Loss: 7130959.0000\n",
      "Epoch [3669/50000], Train Loss: 11757861.0000, Val Loss: 7129251.5000\n",
      "Epoch [3670/50000], Train Loss: 11755583.0000, Val Loss: 7127544.5000\n",
      "Epoch [3671/50000], Train Loss: 11753306.0000, Val Loss: 7125838.0000\n",
      "Epoch [3672/50000], Train Loss: 11751030.0000, Val Loss: 7124133.5000\n",
      "Epoch [3673/50000], Train Loss: 11748754.0000, Val Loss: 7122429.5000\n",
      "Epoch [3674/50000], Train Loss: 11746480.0000, Val Loss: 7120723.5000\n",
      "Epoch [3675/50000], Train Loss: 11744204.0000, Val Loss: 7119020.5000\n",
      "Epoch [3676/50000], Train Loss: 11741931.0000, Val Loss: 7117319.0000\n",
      "Epoch [3677/50000], Train Loss: 11739660.0000, Val Loss: 7115618.0000\n",
      "Epoch [3678/50000], Train Loss: 11737390.0000, Val Loss: 7113918.0000\n",
      "Epoch [3679/50000], Train Loss: 11735120.0000, Val Loss: 7112217.5000\n",
      "Epoch [3680/50000], Train Loss: 11732848.0000, Val Loss: 7110518.5000\n",
      "Epoch [3681/50000], Train Loss: 11730579.0000, Val Loss: 7108821.0000\n",
      "Epoch [3682/50000], Train Loss: 11728312.0000, Val Loss: 7107123.5000\n",
      "Epoch [3683/50000], Train Loss: 11726045.0000, Val Loss: 7105429.5000\n",
      "Epoch [3684/50000], Train Loss: 11723782.0000, Val Loss: 7103734.5000\n",
      "Epoch [3685/50000], Train Loss: 11721517.0000, Val Loss: 7102039.5000\n",
      "Epoch [3686/50000], Train Loss: 11719252.0000, Val Loss: 7100345.5000\n",
      "Epoch [3687/50000], Train Loss: 11716989.0000, Val Loss: 7098654.0000\n",
      "Epoch [3688/50000], Train Loss: 11714727.0000, Val Loss: 7096960.0000\n",
      "Epoch [3689/50000], Train Loss: 11712465.0000, Val Loss: 7095270.0000\n",
      "Epoch [3690/50000], Train Loss: 11710205.0000, Val Loss: 7093580.5000\n",
      "Epoch [3691/50000], Train Loss: 11707946.0000, Val Loss: 7091891.5000\n",
      "Epoch [3692/50000], Train Loss: 11705688.0000, Val Loss: 7090202.5000\n",
      "Epoch [3693/50000], Train Loss: 11703431.0000, Val Loss: 7088514.0000\n",
      "Epoch [3694/50000], Train Loss: 11701173.0000, Val Loss: 7086828.5000\n",
      "Epoch [3695/50000], Train Loss: 11698919.0000, Val Loss: 7085142.0000\n",
      "Epoch [3696/50000], Train Loss: 11696662.0000, Val Loss: 7083456.0000\n",
      "Epoch [3697/50000], Train Loss: 11694407.0000, Val Loss: 7081772.5000\n",
      "Epoch [3698/50000], Train Loss: 11692154.0000, Val Loss: 7080091.0000\n",
      "Epoch [3699/50000], Train Loss: 11689904.0000, Val Loss: 7078406.5000\n",
      "Epoch [3700/50000], Train Loss: 11687650.0000, Val Loss: 7076726.5000\n",
      "Epoch [3701/50000], Train Loss: 11685403.0000, Val Loss: 7075045.5000\n",
      "Epoch [3702/50000], Train Loss: 11683153.0000, Val Loss: 7073366.0000\n",
      "Epoch [3703/50000], Train Loss: 11680903.0000, Val Loss: 7071685.5000\n",
      "Epoch [3704/50000], Train Loss: 11678655.0000, Val Loss: 7070008.5000\n",
      "Epoch [3705/50000], Train Loss: 11676408.0000, Val Loss: 7068330.0000\n",
      "Epoch [3706/50000], Train Loss: 11674161.0000, Val Loss: 7066655.5000\n",
      "Epoch [3707/50000], Train Loss: 11671919.0000, Val Loss: 7064980.5000\n",
      "Epoch [3708/50000], Train Loss: 11669676.0000, Val Loss: 7063304.5000\n",
      "Epoch [3709/50000], Train Loss: 11667431.0000, Val Loss: 7061630.5000\n",
      "Epoch [3710/50000], Train Loss: 11665189.0000, Val Loss: 7059957.5000\n",
      "Epoch [3711/50000], Train Loss: 11662948.0000, Val Loss: 7058284.5000\n",
      "Epoch [3712/50000], Train Loss: 11660707.0000, Val Loss: 7056613.5000\n",
      "Epoch [3713/50000], Train Loss: 11658467.0000, Val Loss: 7054943.0000\n",
      "Epoch [3714/50000], Train Loss: 11656228.0000, Val Loss: 7053273.5000\n",
      "Epoch [3715/50000], Train Loss: 11653990.0000, Val Loss: 7051605.5000\n",
      "Epoch [3716/50000], Train Loss: 11651756.0000, Val Loss: 7049936.0000\n",
      "Epoch [3717/50000], Train Loss: 11649518.0000, Val Loss: 7048270.0000\n",
      "Epoch [3718/50000], Train Loss: 11647283.0000, Val Loss: 7046603.0000\n",
      "Epoch [3719/50000], Train Loss: 11645048.0000, Val Loss: 7044937.5000\n",
      "Epoch [3720/50000], Train Loss: 11642815.0000, Val Loss: 7043274.0000\n",
      "Epoch [3721/50000], Train Loss: 11640584.0000, Val Loss: 7041610.5000\n",
      "Epoch [3722/50000], Train Loss: 11638352.0000, Val Loss: 7039946.0000\n",
      "Epoch [3723/50000], Train Loss: 11636121.0000, Val Loss: 7038285.5000\n",
      "Epoch [3724/50000], Train Loss: 11633891.0000, Val Loss: 7036623.0000\n",
      "Epoch [3725/50000], Train Loss: 11631662.0000, Val Loss: 7034962.5000\n",
      "Epoch [3726/50000], Train Loss: 11629434.0000, Val Loss: 7033304.0000\n",
      "Epoch [3727/50000], Train Loss: 11627208.0000, Val Loss: 7031645.0000\n",
      "Epoch [3728/50000], Train Loss: 11624981.0000, Val Loss: 7029987.0000\n",
      "Epoch [3729/50000], Train Loss: 11622757.0000, Val Loss: 7028329.5000\n",
      "Epoch [3730/50000], Train Loss: 11620532.0000, Val Loss: 7026674.5000\n",
      "Epoch [3731/50000], Train Loss: 11618308.0000, Val Loss: 7025017.5000\n",
      "Epoch [3732/50000], Train Loss: 11616084.0000, Val Loss: 7023363.5000\n",
      "Epoch [3733/50000], Train Loss: 11613862.0000, Val Loss: 7021711.0000\n",
      "Epoch [3734/50000], Train Loss: 11611644.0000, Val Loss: 7020058.5000\n",
      "Epoch [3735/50000], Train Loss: 11609426.0000, Val Loss: 7018406.5000\n",
      "Epoch [3736/50000], Train Loss: 11607206.0000, Val Loss: 7016756.5000\n",
      "Epoch [3737/50000], Train Loss: 11604989.0000, Val Loss: 7015104.0000\n",
      "Epoch [3738/50000], Train Loss: 11602769.0000, Val Loss: 7013456.5000\n",
      "Epoch [3739/50000], Train Loss: 11600555.0000, Val Loss: 7011807.0000\n",
      "Epoch [3740/50000], Train Loss: 11598338.0000, Val Loss: 7010161.0000\n",
      "Epoch [3741/50000], Train Loss: 11596125.0000, Val Loss: 7008513.0000\n",
      "Epoch [3742/50000], Train Loss: 11593911.0000, Val Loss: 7006868.5000\n",
      "Epoch [3743/50000], Train Loss: 11591698.0000, Val Loss: 7005223.5000\n",
      "Epoch [3744/50000], Train Loss: 11589488.0000, Val Loss: 7003578.5000\n",
      "Epoch [3745/50000], Train Loss: 11587275.0000, Val Loss: 7001935.0000\n",
      "Epoch [3746/50000], Train Loss: 11585066.0000, Val Loss: 7000292.5000\n",
      "Epoch [3747/50000], Train Loss: 11582856.0000, Val Loss: 6998651.5000\n",
      "Epoch [3748/50000], Train Loss: 11580651.0000, Val Loss: 6997010.0000\n",
      "Epoch [3749/50000], Train Loss: 11578443.0000, Val Loss: 6995370.0000\n",
      "Epoch [3750/50000], Train Loss: 11576236.0000, Val Loss: 6993731.5000\n",
      "Epoch [3751/50000], Train Loss: 11574033.0000, Val Loss: 6992093.0000\n",
      "Epoch [3752/50000], Train Loss: 11571825.0000, Val Loss: 6990455.0000\n",
      "Epoch [3753/50000], Train Loss: 11569622.0000, Val Loss: 6988817.5000\n",
      "Epoch [3754/50000], Train Loss: 11567418.0000, Val Loss: 6987183.5000\n",
      "Epoch [3755/50000], Train Loss: 11565219.0000, Val Loss: 6985547.5000\n",
      "Epoch [3756/50000], Train Loss: 11563017.0000, Val Loss: 6983913.5000\n",
      "Epoch [3757/50000], Train Loss: 11560819.0000, Val Loss: 6982282.5000\n",
      "Epoch [3758/50000], Train Loss: 11558619.0000, Val Loss: 6980648.0000\n",
      "Epoch [3759/50000], Train Loss: 11556418.0000, Val Loss: 6979017.5000\n",
      "Epoch [3760/50000], Train Loss: 11554222.0000, Val Loss: 6977385.5000\n",
      "Epoch [3761/50000], Train Loss: 11552025.0000, Val Loss: 6975758.0000\n",
      "Epoch [3762/50000], Train Loss: 11549830.0000, Val Loss: 6974127.5000\n",
      "Epoch [3763/50000], Train Loss: 11547636.0000, Val Loss: 6972499.0000\n",
      "Epoch [3764/50000], Train Loss: 11545441.0000, Val Loss: 6970873.5000\n",
      "Epoch [3765/50000], Train Loss: 11543251.0000, Val Loss: 6969247.0000\n",
      "Epoch [3766/50000], Train Loss: 11541058.0000, Val Loss: 6967619.0000\n",
      "Epoch [3767/50000], Train Loss: 11538865.0000, Val Loss: 6965994.5000\n",
      "Epoch [3768/50000], Train Loss: 11536675.0000, Val Loss: 6964369.5000\n",
      "Epoch [3769/50000], Train Loss: 11534484.0000, Val Loss: 6962747.5000\n",
      "Epoch [3770/50000], Train Loss: 11532297.0000, Val Loss: 6961125.0000\n",
      "Epoch [3771/50000], Train Loss: 11530110.0000, Val Loss: 6959504.5000\n",
      "Epoch [3772/50000], Train Loss: 11527924.0000, Val Loss: 6957883.5000\n",
      "Epoch [3773/50000], Train Loss: 11525739.0000, Val Loss: 6956264.5000\n",
      "Epoch [3774/50000], Train Loss: 11523554.0000, Val Loss: 6954644.5000\n",
      "Epoch [3775/50000], Train Loss: 11521368.0000, Val Loss: 6953026.0000\n",
      "Epoch [3776/50000], Train Loss: 11519184.0000, Val Loss: 6951409.0000\n",
      "Epoch [3777/50000], Train Loss: 11517003.0000, Val Loss: 6949793.5000\n",
      "Epoch [3778/50000], Train Loss: 11514823.0000, Val Loss: 6948176.0000\n",
      "Epoch [3779/50000], Train Loss: 11512639.0000, Val Loss: 6946561.5000\n",
      "Epoch [3780/50000], Train Loss: 11510461.0000, Val Loss: 6944948.5000\n",
      "Epoch [3781/50000], Train Loss: 11508282.0000, Val Loss: 6943335.5000\n",
      "Epoch [3782/50000], Train Loss: 11506105.0000, Val Loss: 6941722.5000\n",
      "Epoch [3783/50000], Train Loss: 11503926.0000, Val Loss: 6940110.0000\n",
      "Epoch [3784/50000], Train Loss: 11501751.0000, Val Loss: 6938501.5000\n",
      "Epoch [3785/50000], Train Loss: 11499576.0000, Val Loss: 6936891.5000\n",
      "Epoch [3786/50000], Train Loss: 11497402.0000, Val Loss: 6935282.5000\n",
      "Epoch [3787/50000], Train Loss: 11495229.0000, Val Loss: 6933673.5000\n",
      "Epoch [3788/50000], Train Loss: 11493056.0000, Val Loss: 6932066.0000\n",
      "Epoch [3789/50000], Train Loss: 11490884.0000, Val Loss: 6930461.0000\n",
      "Epoch [3790/50000], Train Loss: 11488714.0000, Val Loss: 6928854.5000\n",
      "Epoch [3791/50000], Train Loss: 11486543.0000, Val Loss: 6927250.0000\n",
      "Epoch [3792/50000], Train Loss: 11484376.0000, Val Loss: 6925647.0000\n",
      "Epoch [3793/50000], Train Loss: 11482207.0000, Val Loss: 6924043.5000\n",
      "Epoch [3794/50000], Train Loss: 11480040.0000, Val Loss: 6922441.0000\n",
      "Epoch [3795/50000], Train Loss: 11477875.0000, Val Loss: 6920839.0000\n",
      "Epoch [3796/50000], Train Loss: 11475709.0000, Val Loss: 6919239.0000\n",
      "Epoch [3797/50000], Train Loss: 11473544.0000, Val Loss: 6917639.5000\n",
      "Epoch [3798/50000], Train Loss: 11471381.0000, Val Loss: 6916040.5000\n",
      "Epoch [3799/50000], Train Loss: 11469217.0000, Val Loss: 6914441.5000\n",
      "Epoch [3800/50000], Train Loss: 11467055.0000, Val Loss: 6912845.0000\n",
      "Epoch [3801/50000], Train Loss: 11464895.0000, Val Loss: 6911249.5000\n",
      "Epoch [3802/50000], Train Loss: 11462737.0000, Val Loss: 6909654.5000\n",
      "Epoch [3803/50000], Train Loss: 11460575.0000, Val Loss: 6908058.5000\n",
      "Epoch [3804/50000], Train Loss: 11458416.0000, Val Loss: 6906465.0000\n",
      "Epoch [3805/50000], Train Loss: 11456260.0000, Val Loss: 6904872.5000\n",
      "Epoch [3806/50000], Train Loss: 11454105.0000, Val Loss: 6903280.5000\n",
      "Epoch [3807/50000], Train Loss: 11451948.0000, Val Loss: 6901689.5000\n",
      "Epoch [3808/50000], Train Loss: 11449794.0000, Val Loss: 6900098.0000\n",
      "Epoch [3809/50000], Train Loss: 11447640.0000, Val Loss: 6898509.0000\n",
      "Epoch [3810/50000], Train Loss: 11445487.0000, Val Loss: 6896919.5000\n",
      "Epoch [3811/50000], Train Loss: 11443334.0000, Val Loss: 6895331.5000\n",
      "Epoch [3812/50000], Train Loss: 11441183.0000, Val Loss: 6893744.0000\n",
      "Epoch [3813/50000], Train Loss: 11439032.0000, Val Loss: 6892158.0000\n",
      "Epoch [3814/50000], Train Loss: 11436885.0000, Val Loss: 6890574.0000\n",
      "Epoch [3815/50000], Train Loss: 11434736.0000, Val Loss: 6888987.5000\n",
      "Epoch [3816/50000], Train Loss: 11432589.0000, Val Loss: 6887405.5000\n",
      "Epoch [3817/50000], Train Loss: 11430443.0000, Val Loss: 6885820.5000\n",
      "Epoch [3818/50000], Train Loss: 11428292.0000, Val Loss: 6884239.5000\n",
      "Epoch [3819/50000], Train Loss: 11426150.0000, Val Loss: 6882658.0000\n",
      "Epoch [3820/50000], Train Loss: 11424006.0000, Val Loss: 6881079.0000\n",
      "Epoch [3821/50000], Train Loss: 11421864.0000, Val Loss: 6879499.5000\n",
      "Epoch [3822/50000], Train Loss: 11419721.0000, Val Loss: 6877919.0000\n",
      "Epoch [3823/50000], Train Loss: 11417579.0000, Val Loss: 6876342.0000\n",
      "Epoch [3824/50000], Train Loss: 11415439.0000, Val Loss: 6874763.5000\n",
      "Epoch [3825/50000], Train Loss: 11413297.0000, Val Loss: 6873188.5000\n",
      "Epoch [3826/50000], Train Loss: 11411162.0000, Val Loss: 6871613.5000\n",
      "Epoch [3827/50000], Train Loss: 11409024.0000, Val Loss: 6870038.0000\n",
      "Epoch [3828/50000], Train Loss: 11406886.0000, Val Loss: 6868463.0000\n",
      "Epoch [3829/50000], Train Loss: 11404748.0000, Val Loss: 6866891.5000\n",
      "Epoch [3830/50000], Train Loss: 11402614.0000, Val Loss: 6865318.5000\n",
      "Epoch [3831/50000], Train Loss: 11400481.0000, Val Loss: 6863747.5000\n",
      "Epoch [3832/50000], Train Loss: 11398346.0000, Val Loss: 6862176.0000\n",
      "Epoch [3833/50000], Train Loss: 11396213.0000, Val Loss: 6860607.0000\n",
      "Epoch [3834/50000], Train Loss: 11394084.0000, Val Loss: 6859039.0000\n",
      "Epoch [3835/50000], Train Loss: 11391952.0000, Val Loss: 6857471.0000\n",
      "Epoch [3836/50000], Train Loss: 11389823.0000, Val Loss: 6855904.5000\n",
      "Epoch [3837/50000], Train Loss: 11387694.0000, Val Loss: 6854336.0000\n",
      "Epoch [3838/50000], Train Loss: 11385562.0000, Val Loss: 6852771.5000\n",
      "Epoch [3839/50000], Train Loss: 11383437.0000, Val Loss: 6851206.5000\n",
      "Epoch [3840/50000], Train Loss: 11381309.0000, Val Loss: 6849643.5000\n",
      "Epoch [3841/50000], Train Loss: 11379187.0000, Val Loss: 6848079.5000\n",
      "Epoch [3842/50000], Train Loss: 11377060.0000, Val Loss: 6846518.5000\n",
      "Epoch [3843/50000], Train Loss: 11374938.0000, Val Loss: 6844957.0000\n",
      "Epoch [3844/50000], Train Loss: 11372813.0000, Val Loss: 6843394.0000\n",
      "Epoch [3845/50000], Train Loss: 11370689.0000, Val Loss: 6841835.0000\n",
      "Epoch [3846/50000], Train Loss: 11368569.0000, Val Loss: 6840278.5000\n",
      "Epoch [3847/50000], Train Loss: 11366452.0000, Val Loss: 6838720.5000\n",
      "Epoch [3848/50000], Train Loss: 11364331.0000, Val Loss: 6837162.5000\n",
      "Epoch [3849/50000], Train Loss: 11362213.0000, Val Loss: 6835606.0000\n",
      "Epoch [3850/50000], Train Loss: 11360094.0000, Val Loss: 6834050.0000\n",
      "Epoch [3851/50000], Train Loss: 11357978.0000, Val Loss: 6832495.5000\n",
      "Epoch [3852/50000], Train Loss: 11355862.0000, Val Loss: 6830941.0000\n",
      "Epoch [3853/50000], Train Loss: 11353747.0000, Val Loss: 6829390.0000\n",
      "Epoch [3854/50000], Train Loss: 11351634.0000, Val Loss: 6827835.0000\n",
      "Epoch [3855/50000], Train Loss: 11349517.0000, Val Loss: 6826285.5000\n",
      "Epoch [3856/50000], Train Loss: 11347408.0000, Val Loss: 6824734.5000\n",
      "Epoch [3857/50000], Train Loss: 11345296.0000, Val Loss: 6823184.0000\n",
      "Epoch [3858/50000], Train Loss: 11343186.0000, Val Loss: 6821635.5000\n",
      "Epoch [3859/50000], Train Loss: 11341076.0000, Val Loss: 6820088.0000\n",
      "Epoch [3860/50000], Train Loss: 11338969.0000, Val Loss: 6818539.5000\n",
      "Epoch [3861/50000], Train Loss: 11336859.0000, Val Loss: 6816994.0000\n",
      "Epoch [3862/50000], Train Loss: 11334754.0000, Val Loss: 6815447.5000\n",
      "Epoch [3863/50000], Train Loss: 11332645.0000, Val Loss: 6813903.0000\n",
      "Epoch [3864/50000], Train Loss: 11330540.0000, Val Loss: 6812360.5000\n",
      "Epoch [3865/50000], Train Loss: 11328437.0000, Val Loss: 6810816.0000\n",
      "Epoch [3866/50000], Train Loss: 11326333.0000, Val Loss: 6809273.5000\n",
      "Epoch [3867/50000], Train Loss: 11324228.0000, Val Loss: 6807731.5000\n",
      "Epoch [3868/50000], Train Loss: 11322128.0000, Val Loss: 6806192.0000\n",
      "Epoch [3869/50000], Train Loss: 11320028.0000, Val Loss: 6804653.0000\n",
      "Epoch [3870/50000], Train Loss: 11317927.0000, Val Loss: 6803113.0000\n",
      "Epoch [3871/50000], Train Loss: 11315828.0000, Val Loss: 6801576.0000\n",
      "Epoch [3872/50000], Train Loss: 11313731.0000, Val Loss: 6800038.5000\n",
      "Epoch [3873/50000], Train Loss: 11311633.0000, Val Loss: 6798501.5000\n",
      "Epoch [3874/50000], Train Loss: 11309535.0000, Val Loss: 6796965.0000\n",
      "Epoch [3875/50000], Train Loss: 11307439.0000, Val Loss: 6795429.5000\n",
      "Epoch [3876/50000], Train Loss: 11305345.0000, Val Loss: 6793896.5000\n",
      "Epoch [3877/50000], Train Loss: 11303251.0000, Val Loss: 6792362.5000\n",
      "Epoch [3878/50000], Train Loss: 11301157.0000, Val Loss: 6790831.0000\n",
      "Epoch [3879/50000], Train Loss: 11299066.0000, Val Loss: 6789298.5000\n",
      "Epoch [3880/50000], Train Loss: 11296975.0000, Val Loss: 6787767.0000\n",
      "Epoch [3881/50000], Train Loss: 11294883.0000, Val Loss: 6786238.0000\n",
      "Epoch [3882/50000], Train Loss: 11292793.0000, Val Loss: 6784708.5000\n",
      "Epoch [3883/50000], Train Loss: 11290706.0000, Val Loss: 6783182.0000\n",
      "Epoch [3884/50000], Train Loss: 11288620.0000, Val Loss: 6781654.5000\n",
      "Epoch [3885/50000], Train Loss: 11286531.0000, Val Loss: 6780127.0000\n",
      "Epoch [3886/50000], Train Loss: 11284445.0000, Val Loss: 6778600.5000\n",
      "Epoch [3887/50000], Train Loss: 11282360.0000, Val Loss: 6777075.5000\n",
      "Epoch [3888/50000], Train Loss: 11280276.0000, Val Loss: 6775551.0000\n",
      "Epoch [3889/50000], Train Loss: 11278193.0000, Val Loss: 6774029.0000\n",
      "Epoch [3890/50000], Train Loss: 11276111.0000, Val Loss: 6772506.5000\n",
      "Epoch [3891/50000], Train Loss: 11274030.0000, Val Loss: 6770983.5000\n",
      "Epoch [3892/50000], Train Loss: 11271948.0000, Val Loss: 6769464.5000\n",
      "Epoch [3893/50000], Train Loss: 11269870.0000, Val Loss: 6767942.5000\n",
      "Epoch [3894/50000], Train Loss: 11267789.0000, Val Loss: 6766424.5000\n",
      "Epoch [3895/50000], Train Loss: 11265712.0000, Val Loss: 6764907.5000\n",
      "Epoch [3896/50000], Train Loss: 11263635.0000, Val Loss: 6763388.5000\n",
      "Epoch [3897/50000], Train Loss: 11261560.0000, Val Loss: 6761872.0000\n",
      "Epoch [3898/50000], Train Loss: 11259484.0000, Val Loss: 6760355.5000\n",
      "Epoch [3899/50000], Train Loss: 11257408.0000, Val Loss: 6758840.5000\n",
      "Epoch [3900/50000], Train Loss: 11255334.0000, Val Loss: 6757326.5000\n",
      "Epoch [3901/50000], Train Loss: 11253263.0000, Val Loss: 6755813.5000\n",
      "Epoch [3902/50000], Train Loss: 11251190.0000, Val Loss: 6754301.5000\n",
      "Epoch [3903/50000], Train Loss: 11249119.0000, Val Loss: 6752789.5000\n",
      "Epoch [3904/50000], Train Loss: 11247050.0000, Val Loss: 6751279.0000\n",
      "Epoch [3905/50000], Train Loss: 11244982.0000, Val Loss: 6749767.0000\n",
      "Epoch [3906/50000], Train Loss: 11242909.0000, Val Loss: 6748258.0000\n",
      "Epoch [3907/50000], Train Loss: 11240842.0000, Val Loss: 6746749.0000\n",
      "Epoch [3908/50000], Train Loss: 11238775.0000, Val Loss: 6745242.0000\n",
      "Epoch [3909/50000], Train Loss: 11236711.0000, Val Loss: 6743736.0000\n",
      "Epoch [3910/50000], Train Loss: 11234646.0000, Val Loss: 6742229.5000\n",
      "Epoch [3911/50000], Train Loss: 11232580.0000, Val Loss: 6740723.5000\n",
      "Epoch [3912/50000], Train Loss: 11230518.0000, Val Loss: 6739220.5000\n",
      "Epoch [3913/50000], Train Loss: 11228458.0000, Val Loss: 6737716.5000\n",
      "Epoch [3914/50000], Train Loss: 11226395.0000, Val Loss: 6736212.5000\n",
      "Epoch [3915/50000], Train Loss: 11224333.0000, Val Loss: 6734710.5000\n",
      "Epoch [3916/50000], Train Loss: 11222274.0000, Val Loss: 6733208.5000\n",
      "Epoch [3917/50000], Train Loss: 11220214.0000, Val Loss: 6731709.0000\n",
      "Epoch [3918/50000], Train Loss: 11218156.0000, Val Loss: 6730210.5000\n",
      "Epoch [3919/50000], Train Loss: 11216102.0000, Val Loss: 6728711.5000\n",
      "Epoch [3920/50000], Train Loss: 11214045.0000, Val Loss: 6727213.5000\n",
      "Epoch [3921/50000], Train Loss: 11211988.0000, Val Loss: 6725716.5000\n",
      "Epoch [3922/50000], Train Loss: 11209935.0000, Val Loss: 6724220.5000\n",
      "Epoch [3923/50000], Train Loss: 11207880.0000, Val Loss: 6722725.0000\n",
      "Epoch [3924/50000], Train Loss: 11205827.0000, Val Loss: 6721230.0000\n",
      "Epoch [3925/50000], Train Loss: 11203776.0000, Val Loss: 6719736.5000\n",
      "Epoch [3926/50000], Train Loss: 11201725.0000, Val Loss: 6718242.0000\n",
      "Epoch [3927/50000], Train Loss: 11199673.0000, Val Loss: 6716750.5000\n",
      "Epoch [3928/50000], Train Loss: 11197624.0000, Val Loss: 6715258.5000\n",
      "Epoch [3929/50000], Train Loss: 11195575.0000, Val Loss: 6713768.0000\n",
      "Epoch [3930/50000], Train Loss: 11193527.0000, Val Loss: 6712279.5000\n",
      "Epoch [3931/50000], Train Loss: 11191482.0000, Val Loss: 6710789.5000\n",
      "Epoch [3932/50000], Train Loss: 11189436.0000, Val Loss: 6709302.0000\n",
      "Epoch [3933/50000], Train Loss: 11187391.0000, Val Loss: 6707814.5000\n",
      "Epoch [3934/50000], Train Loss: 11185346.0000, Val Loss: 6706328.0000\n",
      "Epoch [3935/50000], Train Loss: 11183304.0000, Val Loss: 6704840.5000\n",
      "Epoch [3936/50000], Train Loss: 11181259.0000, Val Loss: 6703357.0000\n",
      "Epoch [3937/50000], Train Loss: 11179220.0000, Val Loss: 6701873.5000\n",
      "Epoch [3938/50000], Train Loss: 11177179.0000, Val Loss: 6700389.5000\n",
      "Epoch [3939/50000], Train Loss: 11175138.0000, Val Loss: 6698907.0000\n",
      "Epoch [3940/50000], Train Loss: 11173099.0000, Val Loss: 6697425.0000\n",
      "Epoch [3941/50000], Train Loss: 11171060.0000, Val Loss: 6695944.5000\n",
      "Epoch [3942/50000], Train Loss: 11169023.0000, Val Loss: 6694465.0000\n",
      "Epoch [3943/50000], Train Loss: 11166987.0000, Val Loss: 6692986.0000\n",
      "Epoch [3944/50000], Train Loss: 11164953.0000, Val Loss: 6691507.5000\n",
      "Epoch [3945/50000], Train Loss: 11162918.0000, Val Loss: 6690030.5000\n",
      "Epoch [3946/50000], Train Loss: 11160884.0000, Val Loss: 6688553.5000\n",
      "Epoch [3947/50000], Train Loss: 11158851.0000, Val Loss: 6687078.5000\n",
      "Epoch [3948/50000], Train Loss: 11156819.0000, Val Loss: 6685602.0000\n",
      "Epoch [3949/50000], Train Loss: 11154787.0000, Val Loss: 6684128.5000\n",
      "Epoch [3950/50000], Train Loss: 11152758.0000, Val Loss: 6682655.5000\n",
      "Epoch [3951/50000], Train Loss: 11150729.0000, Val Loss: 6681183.0000\n",
      "Epoch [3952/50000], Train Loss: 11148701.0000, Val Loss: 6679711.5000\n",
      "Epoch [3953/50000], Train Loss: 11146675.0000, Val Loss: 6678240.0000\n",
      "Epoch [3954/50000], Train Loss: 11144647.0000, Val Loss: 6676770.0000\n",
      "Epoch [3955/50000], Train Loss: 11142621.0000, Val Loss: 6675298.0000\n",
      "Epoch [3956/50000], Train Loss: 11140591.0000, Val Loss: 6673830.5000\n",
      "Epoch [3957/50000], Train Loss: 11138570.0000, Val Loss: 6672362.0000\n",
      "Epoch [3958/50000], Train Loss: 11136547.0000, Val Loss: 6670896.0000\n",
      "Epoch [3959/50000], Train Loss: 11134525.0000, Val Loss: 6669430.5000\n",
      "Epoch [3960/50000], Train Loss: 11132504.0000, Val Loss: 6667964.5000\n",
      "Epoch [3961/50000], Train Loss: 11130481.0000, Val Loss: 6666500.5000\n",
      "Epoch [3962/50000], Train Loss: 11128461.0000, Val Loss: 6665037.0000\n",
      "Epoch [3963/50000], Train Loss: 11126443.0000, Val Loss: 6663573.5000\n",
      "Epoch [3964/50000], Train Loss: 11124425.0000, Val Loss: 6662111.0000\n",
      "Epoch [3965/50000], Train Loss: 11122407.0000, Val Loss: 6660650.5000\n",
      "Epoch [3966/50000], Train Loss: 11120392.0000, Val Loss: 6659190.0000\n",
      "Epoch [3967/50000], Train Loss: 11118375.0000, Val Loss: 6657730.5000\n",
      "Epoch [3968/50000], Train Loss: 11116362.0000, Val Loss: 6656270.5000\n",
      "Epoch [3969/50000], Train Loss: 11114347.0000, Val Loss: 6654814.0000\n",
      "Epoch [3970/50000], Train Loss: 11112333.0000, Val Loss: 6653356.5000\n",
      "Epoch [3971/50000], Train Loss: 11110323.0000, Val Loss: 6651899.5000\n",
      "Epoch [3972/50000], Train Loss: 11108309.0000, Val Loss: 6650444.5000\n",
      "Epoch [3973/50000], Train Loss: 11106302.0000, Val Loss: 6648989.5000\n",
      "Epoch [3974/50000], Train Loss: 11104292.0000, Val Loss: 6647534.0000\n",
      "Epoch [3975/50000], Train Loss: 11102281.0000, Val Loss: 6646081.5000\n",
      "Epoch [3976/50000], Train Loss: 11100273.0000, Val Loss: 6644629.5000\n",
      "Epoch [3977/50000], Train Loss: 11098267.0000, Val Loss: 6643178.5000\n",
      "Epoch [3978/50000], Train Loss: 11096262.0000, Val Loss: 6641728.0000\n",
      "Epoch [3979/50000], Train Loss: 11094257.0000, Val Loss: 6640277.5000\n",
      "Epoch [3980/50000], Train Loss: 11092253.0000, Val Loss: 6638827.5000\n",
      "Epoch [3981/50000], Train Loss: 11090248.0000, Val Loss: 6637379.5000\n",
      "Epoch [3982/50000], Train Loss: 11088246.0000, Val Loss: 6635931.5000\n",
      "Epoch [3983/50000], Train Loss: 11086244.0000, Val Loss: 6634485.5000\n",
      "Epoch [3984/50000], Train Loss: 11084243.0000, Val Loss: 6633039.0000\n",
      "Epoch [3985/50000], Train Loss: 11082242.0000, Val Loss: 6631594.5000\n",
      "Epoch [3986/50000], Train Loss: 11080244.0000, Val Loss: 6630149.5000\n",
      "Epoch [3987/50000], Train Loss: 11078244.0000, Val Loss: 6628706.0000\n",
      "Epoch [3988/50000], Train Loss: 11076248.0000, Val Loss: 6627262.0000\n",
      "Epoch [3989/50000], Train Loss: 11074249.0000, Val Loss: 6625820.5000\n",
      "Epoch [3990/50000], Train Loss: 11072254.0000, Val Loss: 6624380.5000\n",
      "Epoch [3991/50000], Train Loss: 11070260.0000, Val Loss: 6622940.5000\n",
      "Epoch [3992/50000], Train Loss: 11068266.0000, Val Loss: 6621500.5000\n",
      "Epoch [3993/50000], Train Loss: 11066271.0000, Val Loss: 6620062.0000\n",
      "Epoch [3994/50000], Train Loss: 11064281.0000, Val Loss: 6618623.5000\n",
      "Epoch [3995/50000], Train Loss: 11062288.0000, Val Loss: 6617186.5000\n",
      "Epoch [3996/50000], Train Loss: 11060299.0000, Val Loss: 6615749.5000\n",
      "Epoch [3997/50000], Train Loss: 11058307.0000, Val Loss: 6614315.0000\n",
      "Epoch [3998/50000], Train Loss: 11056320.0000, Val Loss: 6612879.5000\n",
      "Epoch [3999/50000], Train Loss: 11054330.0000, Val Loss: 6611447.5000\n",
      "Epoch [4000/50000], Train Loss: 11052345.0000, Val Loss: 6610013.5000\n",
      "Epoch [4001/50000], Train Loss: 11050357.0000, Val Loss: 6608581.0000\n",
      "Epoch [4002/50000], Train Loss: 11048371.0000, Val Loss: 6607148.5000\n",
      "Epoch [4003/50000], Train Loss: 11046385.0000, Val Loss: 6605718.5000\n",
      "Epoch [4004/50000], Train Loss: 11044405.0000, Val Loss: 6604288.0000\n",
      "Epoch [4005/50000], Train Loss: 11042420.0000, Val Loss: 6602859.0000\n",
      "Epoch [4006/50000], Train Loss: 11040438.0000, Val Loss: 6601431.0000\n",
      "Epoch [4007/50000], Train Loss: 11038455.0000, Val Loss: 6600003.5000\n",
      "Epoch [4008/50000], Train Loss: 11036476.0000, Val Loss: 6598577.0000\n",
      "Epoch [4009/50000], Train Loss: 11034496.0000, Val Loss: 6597151.0000\n",
      "Epoch [4010/50000], Train Loss: 11032518.0000, Val Loss: 6595726.5000\n",
      "Epoch [4011/50000], Train Loss: 11030540.0000, Val Loss: 6594302.5000\n",
      "Epoch [4012/50000], Train Loss: 11028564.0000, Val Loss: 6592878.5000\n",
      "Epoch [4013/50000], Train Loss: 11026587.0000, Val Loss: 6591456.0000\n",
      "Epoch [4014/50000], Train Loss: 11024612.0000, Val Loss: 6590034.5000\n",
      "Epoch [4015/50000], Train Loss: 11022637.0000, Val Loss: 6588615.5000\n",
      "Epoch [4016/50000], Train Loss: 11020666.0000, Val Loss: 6587193.0000\n",
      "Epoch [4017/50000], Train Loss: 11018692.0000, Val Loss: 6585775.5000\n",
      "Epoch [4018/50000], Train Loss: 11016722.0000, Val Loss: 6584355.5000\n",
      "Epoch [4019/50000], Train Loss: 11014750.0000, Val Loss: 6582939.0000\n",
      "Epoch [4020/50000], Train Loss: 11012782.0000, Val Loss: 6581521.0000\n",
      "Epoch [4021/50000], Train Loss: 11010813.0000, Val Loss: 6580105.5000\n",
      "Epoch [4022/50000], Train Loss: 11008843.0000, Val Loss: 6578689.0000\n",
      "Epoch [4023/50000], Train Loss: 11006875.0000, Val Loss: 6577274.5000\n",
      "Epoch [4024/50000], Train Loss: 11004909.0000, Val Loss: 6575861.5000\n",
      "Epoch [4025/50000], Train Loss: 11002943.0000, Val Loss: 6574448.0000\n",
      "Epoch [4026/50000], Train Loss: 11000978.0000, Val Loss: 6573035.5000\n",
      "Epoch [4027/50000], Train Loss: 10999013.0000, Val Loss: 6571625.0000\n",
      "Epoch [4028/50000], Train Loss: 10997050.0000, Val Loss: 6570213.5000\n",
      "Epoch [4029/50000], Train Loss: 10995089.0000, Val Loss: 6568803.5000\n",
      "Epoch [4030/50000], Train Loss: 10993126.0000, Val Loss: 6567395.0000\n",
      "Epoch [4031/50000], Train Loss: 10991166.0000, Val Loss: 6565986.5000\n",
      "Epoch [4032/50000], Train Loss: 10989205.0000, Val Loss: 6564579.5000\n",
      "Epoch [4033/50000], Train Loss: 10987247.0000, Val Loss: 6563173.0000\n",
      "Epoch [4034/50000], Train Loss: 10985289.0000, Val Loss: 6561766.5000\n",
      "Epoch [4035/50000], Train Loss: 10983332.0000, Val Loss: 6560361.5000\n",
      "Epoch [4036/50000], Train Loss: 10981374.0000, Val Loss: 6558957.0000\n",
      "Epoch [4037/50000], Train Loss: 10979418.0000, Val Loss: 6557555.0000\n",
      "Epoch [4038/50000], Train Loss: 10977464.0000, Val Loss: 6556151.0000\n",
      "Epoch [4039/50000], Train Loss: 10975510.0000, Val Loss: 6554749.5000\n",
      "Epoch [4040/50000], Train Loss: 10973557.0000, Val Loss: 6553348.5000\n",
      "Epoch [4041/50000], Train Loss: 10971605.0000, Val Loss: 6551948.5000\n",
      "Epoch [4042/50000], Train Loss: 10969654.0000, Val Loss: 6550547.0000\n",
      "Epoch [4043/50000], Train Loss: 10967702.0000, Val Loss: 6549149.5000\n",
      "Epoch [4044/50000], Train Loss: 10965752.0000, Val Loss: 6547753.5000\n",
      "Epoch [4045/50000], Train Loss: 10963805.0000, Val Loss: 6546357.0000\n",
      "Epoch [4046/50000], Train Loss: 10961858.0000, Val Loss: 6544960.0000\n",
      "Epoch [4047/50000], Train Loss: 10959911.0000, Val Loss: 6543563.5000\n",
      "Epoch [4048/50000], Train Loss: 10957964.0000, Val Loss: 6542169.5000\n",
      "Epoch [4049/50000], Train Loss: 10956020.0000, Val Loss: 6540774.0000\n",
      "Epoch [4050/50000], Train Loss: 10954074.0000, Val Loss: 6539382.5000\n",
      "Epoch [4051/50000], Train Loss: 10952131.0000, Val Loss: 6537990.5000\n",
      "Epoch [4052/50000], Train Loss: 10950189.0000, Val Loss: 6536598.5000\n",
      "Epoch [4053/50000], Train Loss: 10948246.0000, Val Loss: 6535208.5000\n",
      "Epoch [4054/50000], Train Loss: 10946305.0000, Val Loss: 6533818.0000\n",
      "Epoch [4055/50000], Train Loss: 10944365.0000, Val Loss: 6532428.5000\n",
      "Epoch [4056/50000], Train Loss: 10942425.0000, Val Loss: 6531040.0000\n",
      "Epoch [4057/50000], Train Loss: 10940487.0000, Val Loss: 6529654.5000\n",
      "Epoch [4058/50000], Train Loss: 10938550.0000, Val Loss: 6528266.0000\n",
      "Epoch [4059/50000], Train Loss: 10936612.0000, Val Loss: 6526880.5000\n",
      "Epoch [4060/50000], Train Loss: 10934675.0000, Val Loss: 6525497.5000\n",
      "Epoch [4061/50000], Train Loss: 10932744.0000, Val Loss: 6524113.0000\n",
      "Epoch [4062/50000], Train Loss: 10930808.0000, Val Loss: 6522729.0000\n",
      "Epoch [4063/50000], Train Loss: 10928875.0000, Val Loss: 6521345.5000\n",
      "Epoch [4064/50000], Train Loss: 10926941.0000, Val Loss: 6519964.5000\n",
      "Epoch [4065/50000], Train Loss: 10925010.0000, Val Loss: 6518582.5000\n",
      "Epoch [4066/50000], Train Loss: 10923078.0000, Val Loss: 6517202.0000\n",
      "Epoch [4067/50000], Train Loss: 10921149.0000, Val Loss: 6515822.5000\n",
      "Epoch [4068/50000], Train Loss: 10919220.0000, Val Loss: 6514443.0000\n",
      "Epoch [4069/50000], Train Loss: 10917292.0000, Val Loss: 6513065.5000\n",
      "Epoch [4070/50000], Train Loss: 10915363.0000, Val Loss: 6511688.5000\n",
      "Epoch [4071/50000], Train Loss: 10913435.0000, Val Loss: 6510312.5000\n",
      "Epoch [4072/50000], Train Loss: 10911512.0000, Val Loss: 6508936.5000\n",
      "Epoch [4073/50000], Train Loss: 10909586.0000, Val Loss: 6507563.0000\n",
      "Epoch [4074/50000], Train Loss: 10907664.0000, Val Loss: 6506188.5000\n",
      "Epoch [4075/50000], Train Loss: 10905739.0000, Val Loss: 6504815.0000\n",
      "Epoch [4076/50000], Train Loss: 10903816.0000, Val Loss: 6503442.5000\n",
      "Epoch [4077/50000], Train Loss: 10901895.0000, Val Loss: 6502071.0000\n",
      "Epoch [4078/50000], Train Loss: 10899975.0000, Val Loss: 6500700.5000\n",
      "Epoch [4079/50000], Train Loss: 10898054.0000, Val Loss: 6499330.0000\n",
      "Epoch [4080/50000], Train Loss: 10896137.0000, Val Loss: 6497961.5000\n",
      "Epoch [4081/50000], Train Loss: 10894217.0000, Val Loss: 6496592.0000\n",
      "Epoch [4082/50000], Train Loss: 10892298.0000, Val Loss: 6495224.0000\n",
      "Epoch [4083/50000], Train Loss: 10890381.0000, Val Loss: 6493857.0000\n",
      "Epoch [4084/50000], Train Loss: 10888466.0000, Val Loss: 6492491.5000\n",
      "Epoch [4085/50000], Train Loss: 10886553.0000, Val Loss: 6491126.5000\n",
      "Epoch [4086/50000], Train Loss: 10884638.0000, Val Loss: 6489762.0000\n",
      "Epoch [4087/50000], Train Loss: 10882725.0000, Val Loss: 6488397.5000\n",
      "Epoch [4088/50000], Train Loss: 10880813.0000, Val Loss: 6487034.5000\n",
      "Epoch [4089/50000], Train Loss: 10878901.0000, Val Loss: 6485672.5000\n",
      "Epoch [4090/50000], Train Loss: 10876989.0000, Val Loss: 6484312.5000\n",
      "Epoch [4091/50000], Train Loss: 10875082.0000, Val Loss: 6482951.5000\n",
      "Epoch [4092/50000], Train Loss: 10873171.0000, Val Loss: 6481591.5000\n",
      "Epoch [4093/50000], Train Loss: 10871265.0000, Val Loss: 6480233.5000\n",
      "Epoch [4094/50000], Train Loss: 10869358.0000, Val Loss: 6478875.5000\n",
      "Epoch [4095/50000], Train Loss: 10867452.0000, Val Loss: 6477517.5000\n",
      "Epoch [4096/50000], Train Loss: 10865546.0000, Val Loss: 6476160.0000\n",
      "Epoch [4097/50000], Train Loss: 10863640.0000, Val Loss: 6474804.5000\n",
      "Epoch [4098/50000], Train Loss: 10861736.0000, Val Loss: 6473450.0000\n",
      "Epoch [4099/50000], Train Loss: 10859833.0000, Val Loss: 6472095.0000\n",
      "Epoch [4100/50000], Train Loss: 10857931.0000, Val Loss: 6470742.5000\n",
      "Epoch [4101/50000], Train Loss: 10856030.0000, Val Loss: 6469389.0000\n",
      "Epoch [4102/50000], Train Loss: 10854129.0000, Val Loss: 6468037.0000\n",
      "Epoch [4103/50000], Train Loss: 10852230.0000, Val Loss: 6466686.5000\n",
      "Epoch [4104/50000], Train Loss: 10850330.0000, Val Loss: 6465336.5000\n",
      "Epoch [4105/50000], Train Loss: 10848435.0000, Val Loss: 6463987.0000\n",
      "Epoch [4106/50000], Train Loss: 10846536.0000, Val Loss: 6462639.0000\n",
      "Epoch [4107/50000], Train Loss: 10844641.0000, Val Loss: 6461290.0000\n",
      "Epoch [4108/50000], Train Loss: 10842743.0000, Val Loss: 6459942.5000\n",
      "Epoch [4109/50000], Train Loss: 10840849.0000, Val Loss: 6458595.5000\n",
      "Epoch [4110/50000], Train Loss: 10838955.0000, Val Loss: 6457251.5000\n",
      "Epoch [4111/50000], Train Loss: 10837064.0000, Val Loss: 6455907.0000\n",
      "Epoch [4112/50000], Train Loss: 10835172.0000, Val Loss: 6454562.0000\n",
      "Epoch [4113/50000], Train Loss: 10833281.0000, Val Loss: 6453219.0000\n",
      "Epoch [4114/50000], Train Loss: 10831390.0000, Val Loss: 6451875.5000\n",
      "Epoch [4115/50000], Train Loss: 10829501.0000, Val Loss: 6450535.5000\n",
      "Epoch [4116/50000], Train Loss: 10827612.0000, Val Loss: 6449193.5000\n",
      "Epoch [4117/50000], Train Loss: 10825724.0000, Val Loss: 6447853.5000\n",
      "Epoch [4118/50000], Train Loss: 10823836.0000, Val Loss: 6446515.5000\n",
      "Epoch [4119/50000], Train Loss: 10821952.0000, Val Loss: 6445176.5000\n",
      "Epoch [4120/50000], Train Loss: 10820066.0000, Val Loss: 6443838.5000\n",
      "Epoch [4121/50000], Train Loss: 10818179.0000, Val Loss: 6442501.0000\n",
      "Epoch [4122/50000], Train Loss: 10816295.0000, Val Loss: 6441164.5000\n",
      "Epoch [4123/50000], Train Loss: 10814412.0000, Val Loss: 6439829.5000\n",
      "Epoch [4124/50000], Train Loss: 10812532.0000, Val Loss: 6438495.0000\n",
      "Epoch [4125/50000], Train Loss: 10810649.0000, Val Loss: 6437162.5000\n",
      "Epoch [4126/50000], Train Loss: 10808770.0000, Val Loss: 6435829.0000\n",
      "Epoch [4127/50000], Train Loss: 10806892.0000, Val Loss: 6434497.0000\n",
      "Epoch [4128/50000], Train Loss: 10805014.0000, Val Loss: 6433165.0000\n",
      "Epoch [4129/50000], Train Loss: 10803134.0000, Val Loss: 6431833.5000\n",
      "Epoch [4130/50000], Train Loss: 10801256.0000, Val Loss: 6430503.5000\n",
      "Epoch [4131/50000], Train Loss: 10799380.0000, Val Loss: 6429176.0000\n",
      "Epoch [4132/50000], Train Loss: 10797507.0000, Val Loss: 6427847.0000\n",
      "Epoch [4133/50000], Train Loss: 10795632.0000, Val Loss: 6426520.5000\n",
      "Epoch [4134/50000], Train Loss: 10793759.0000, Val Loss: 6425192.5000\n",
      "Epoch [4135/50000], Train Loss: 10791884.0000, Val Loss: 6423865.5000\n",
      "Epoch [4136/50000], Train Loss: 10790012.0000, Val Loss: 6422541.5000\n",
      "Epoch [4137/50000], Train Loss: 10788142.0000, Val Loss: 6421217.0000\n",
      "Epoch [4138/50000], Train Loss: 10786271.0000, Val Loss: 6419895.0000\n",
      "Epoch [4139/50000], Train Loss: 10784403.0000, Val Loss: 6418571.0000\n",
      "Epoch [4140/50000], Train Loss: 10782534.0000, Val Loss: 6417248.5000\n",
      "Epoch [4141/50000], Train Loss: 10780665.0000, Val Loss: 6415928.0000\n",
      "Epoch [4142/50000], Train Loss: 10778799.0000, Val Loss: 6414607.0000\n",
      "Epoch [4143/50000], Train Loss: 10776932.0000, Val Loss: 6413287.0000\n",
      "Epoch [4144/50000], Train Loss: 10775068.0000, Val Loss: 6411968.0000\n",
      "Epoch [4145/50000], Train Loss: 10773204.0000, Val Loss: 6410649.5000\n",
      "Epoch [4146/50000], Train Loss: 10771340.0000, Val Loss: 6409332.5000\n",
      "Epoch [4147/50000], Train Loss: 10769478.0000, Val Loss: 6408015.5000\n",
      "Epoch [4148/50000], Train Loss: 10767615.0000, Val Loss: 6406699.0000\n",
      "Epoch [4149/50000], Train Loss: 10765753.0000, Val Loss: 6405385.5000\n",
      "Epoch [4150/50000], Train Loss: 10763895.0000, Val Loss: 6404071.5000\n",
      "Epoch [4151/50000], Train Loss: 10762036.0000, Val Loss: 6402758.0000\n",
      "Epoch [4152/50000], Train Loss: 10760176.0000, Val Loss: 6401445.5000\n",
      "Epoch [4153/50000], Train Loss: 10758318.0000, Val Loss: 6400133.5000\n",
      "Epoch [4154/50000], Train Loss: 10756461.0000, Val Loss: 6398821.0000\n",
      "Epoch [4155/50000], Train Loss: 10754604.0000, Val Loss: 6397511.5000\n",
      "Epoch [4156/50000], Train Loss: 10752750.0000, Val Loss: 6396202.0000\n",
      "Epoch [4157/50000], Train Loss: 10750897.0000, Val Loss: 6394893.0000\n",
      "Epoch [4158/50000], Train Loss: 10749043.0000, Val Loss: 6393585.5000\n",
      "Epoch [4159/50000], Train Loss: 10747190.0000, Val Loss: 6392278.0000\n",
      "Epoch [4160/50000], Train Loss: 10745338.0000, Val Loss: 6390972.5000\n",
      "Epoch [4161/50000], Train Loss: 10743489.0000, Val Loss: 6389665.5000\n",
      "Epoch [4162/50000], Train Loss: 10741636.0000, Val Loss: 6388360.5000\n",
      "Epoch [4163/50000], Train Loss: 10739787.0000, Val Loss: 6387056.0000\n",
      "Epoch [4164/50000], Train Loss: 10737939.0000, Val Loss: 6385753.5000\n",
      "Epoch [4165/50000], Train Loss: 10736091.0000, Val Loss: 6384452.5000\n",
      "Epoch [4166/50000], Train Loss: 10734244.0000, Val Loss: 6383149.5000\n",
      "Epoch [4167/50000], Train Loss: 10732398.0000, Val Loss: 6381848.5000\n",
      "Epoch [4168/50000], Train Loss: 10730553.0000, Val Loss: 6380547.0000\n",
      "Epoch [4169/50000], Train Loss: 10728708.0000, Val Loss: 6379247.5000\n",
      "Epoch [4170/50000], Train Loss: 10726864.0000, Val Loss: 6377949.5000\n",
      "Epoch [4171/50000], Train Loss: 10725022.0000, Val Loss: 6376653.0000\n",
      "Epoch [4172/50000], Train Loss: 10723182.0000, Val Loss: 6375355.0000\n",
      "Epoch [4173/50000], Train Loss: 10721340.0000, Val Loss: 6374057.0000\n",
      "Epoch [4174/50000], Train Loss: 10719498.0000, Val Loss: 6372762.5000\n",
      "Epoch [4175/50000], Train Loss: 10717659.0000, Val Loss: 6371470.0000\n",
      "Epoch [4176/50000], Train Loss: 10715823.0000, Val Loss: 6370174.5000\n",
      "Epoch [4177/50000], Train Loss: 10713985.0000, Val Loss: 6368881.5000\n",
      "Epoch [4178/50000], Train Loss: 10712148.0000, Val Loss: 6367589.0000\n",
      "Epoch [4179/50000], Train Loss: 10710314.0000, Val Loss: 6366295.5000\n",
      "Epoch [4180/50000], Train Loss: 10708474.0000, Val Loss: 6365006.0000\n",
      "Epoch [4181/50000], Train Loss: 10706642.0000, Val Loss: 6363715.0000\n",
      "Epoch [4182/50000], Train Loss: 10704807.0000, Val Loss: 6362425.5000\n",
      "Epoch [4183/50000], Train Loss: 10702975.0000, Val Loss: 6361137.0000\n",
      "Epoch [4184/50000], Train Loss: 10701144.0000, Val Loss: 6359849.0000\n",
      "Epoch [4185/50000], Train Loss: 10699310.0000, Val Loss: 6358563.0000\n",
      "Epoch [4186/50000], Train Loss: 10697482.0000, Val Loss: 6357276.5000\n",
      "Epoch [4187/50000], Train Loss: 10695652.0000, Val Loss: 6355990.5000\n",
      "Epoch [4188/50000], Train Loss: 10693823.0000, Val Loss: 6354705.5000\n",
      "Epoch [4189/50000], Train Loss: 10691995.0000, Val Loss: 6353422.0000\n",
      "Epoch [4190/50000], Train Loss: 10690168.0000, Val Loss: 6352139.0000\n",
      "Epoch [4191/50000], Train Loss: 10688345.0000, Val Loss: 6350856.5000\n",
      "Epoch [4192/50000], Train Loss: 10686518.0000, Val Loss: 6349574.5000\n",
      "Epoch [4193/50000], Train Loss: 10684692.0000, Val Loss: 6348294.0000\n",
      "Epoch [4194/50000], Train Loss: 10682869.0000, Val Loss: 6347014.0000\n",
      "Epoch [4195/50000], Train Loss: 10681046.0000, Val Loss: 6345734.5000\n",
      "Epoch [4196/50000], Train Loss: 10679224.0000, Val Loss: 6344455.5000\n",
      "Epoch [4197/50000], Train Loss: 10677404.0000, Val Loss: 6343177.5000\n",
      "Epoch [4198/50000], Train Loss: 10675583.0000, Val Loss: 6341901.5000\n",
      "Epoch [4199/50000], Train Loss: 10673765.0000, Val Loss: 6340625.0000\n",
      "Epoch [4200/50000], Train Loss: 10671944.0000, Val Loss: 6339348.5000\n",
      "Epoch [4201/50000], Train Loss: 10670127.0000, Val Loss: 6338074.0000\n",
      "Epoch [4202/50000], Train Loss: 10668310.0000, Val Loss: 6336801.5000\n",
      "Epoch [4203/50000], Train Loss: 10666496.0000, Val Loss: 6335527.5000\n",
      "Epoch [4204/50000], Train Loss: 10664678.0000, Val Loss: 6334256.0000\n",
      "Epoch [4205/50000], Train Loss: 10662865.0000, Val Loss: 6332983.5000\n",
      "Epoch [4206/50000], Train Loss: 10661049.0000, Val Loss: 6331713.0000\n",
      "Epoch [4207/50000], Train Loss: 10659238.0000, Val Loss: 6330442.5000\n",
      "Epoch [4208/50000], Train Loss: 10657426.0000, Val Loss: 6329173.5000\n",
      "Epoch [4209/50000], Train Loss: 10655615.0000, Val Loss: 6327905.5000\n",
      "Epoch [4210/50000], Train Loss: 10653805.0000, Val Loss: 6326638.0000\n",
      "Epoch [4211/50000], Train Loss: 10651996.0000, Val Loss: 6325371.0000\n",
      "Epoch [4212/50000], Train Loss: 10650187.0000, Val Loss: 6324104.0000\n",
      "Epoch [4213/50000], Train Loss: 10648378.0000, Val Loss: 6322838.5000\n",
      "Epoch [4214/50000], Train Loss: 10646571.0000, Val Loss: 6321574.0000\n",
      "Epoch [4215/50000], Train Loss: 10644764.0000, Val Loss: 6320311.0000\n",
      "Epoch [4216/50000], Train Loss: 10642961.0000, Val Loss: 6319047.5000\n",
      "Epoch [4217/50000], Train Loss: 10641156.0000, Val Loss: 6317784.5000\n",
      "Epoch [4218/50000], Train Loss: 10639351.0000, Val Loss: 6316523.0000\n",
      "Epoch [4219/50000], Train Loss: 10637548.0000, Val Loss: 6315263.0000\n",
      "Epoch [4220/50000], Train Loss: 10635747.0000, Val Loss: 6314002.5000\n",
      "Epoch [4221/50000], Train Loss: 10633946.0000, Val Loss: 6312743.0000\n",
      "Epoch [4222/50000], Train Loss: 10632145.0000, Val Loss: 6311485.0000\n",
      "Epoch [4223/50000], Train Loss: 10630346.0000, Val Loss: 6310227.0000\n",
      "Epoch [4224/50000], Train Loss: 10628547.0000, Val Loss: 6308969.5000\n",
      "Epoch [4225/50000], Train Loss: 10626748.0000, Val Loss: 6307712.5000\n",
      "Epoch [4226/50000], Train Loss: 10624950.0000, Val Loss: 6306457.5000\n",
      "Epoch [4227/50000], Train Loss: 10623153.0000, Val Loss: 6305205.0000\n",
      "Epoch [4228/50000], Train Loss: 10621360.0000, Val Loss: 6303949.5000\n",
      "Epoch [4229/50000], Train Loss: 10619565.0000, Val Loss: 6302697.0000\n",
      "Epoch [4230/50000], Train Loss: 10617773.0000, Val Loss: 6301444.5000\n",
      "Epoch [4231/50000], Train Loss: 10615979.0000, Val Loss: 6300191.5000\n",
      "Epoch [4232/50000], Train Loss: 10614184.0000, Val Loss: 6298940.5000\n",
      "Epoch [4233/50000], Train Loss: 10612394.0000, Val Loss: 6297691.5000\n",
      "Epoch [4234/50000], Train Loss: 10610604.0000, Val Loss: 6296442.5000\n",
      "Epoch [4235/50000], Train Loss: 10608815.0000, Val Loss: 6295193.5000\n",
      "Epoch [4236/50000], Train Loss: 10607027.0000, Val Loss: 6293945.5000\n",
      "Epoch [4237/50000], Train Loss: 10605237.0000, Val Loss: 6292698.5000\n",
      "Epoch [4238/50000], Train Loss: 10603449.0000, Val Loss: 6291451.5000\n",
      "Epoch [4239/50000], Train Loss: 10601662.0000, Val Loss: 6290207.5000\n",
      "Epoch [4240/50000], Train Loss: 10599878.0000, Val Loss: 6288963.0000\n",
      "Epoch [4241/50000], Train Loss: 10598093.0000, Val Loss: 6287719.5000\n",
      "Epoch [4242/50000], Train Loss: 10596310.0000, Val Loss: 6286475.5000\n",
      "Epoch [4243/50000], Train Loss: 10594526.0000, Val Loss: 6285232.5000\n",
      "Epoch [4244/50000], Train Loss: 10592743.0000, Val Loss: 6283991.0000\n",
      "Epoch [4245/50000], Train Loss: 10590963.0000, Val Loss: 6282750.0000\n",
      "Epoch [4246/50000], Train Loss: 10589182.0000, Val Loss: 6281510.0000\n",
      "Epoch [4247/50000], Train Loss: 10587402.0000, Val Loss: 6280271.0000\n",
      "Epoch [4248/50000], Train Loss: 10585622.0000, Val Loss: 6279032.5000\n",
      "Epoch [4249/50000], Train Loss: 10583844.0000, Val Loss: 6277794.0000\n",
      "Epoch [4250/50000], Train Loss: 10582067.0000, Val Loss: 6276557.5000\n",
      "Epoch [4251/50000], Train Loss: 10580290.0000, Val Loss: 6275321.5000\n",
      "Epoch [4252/50000], Train Loss: 10578514.0000, Val Loss: 6274086.5000\n",
      "Epoch [4253/50000], Train Loss: 10576740.0000, Val Loss: 6272850.5000\n",
      "Epoch [4254/50000], Train Loss: 10574966.0000, Val Loss: 6271618.0000\n",
      "Epoch [4255/50000], Train Loss: 10573194.0000, Val Loss: 6270383.5000\n",
      "Epoch [4256/50000], Train Loss: 10571420.0000, Val Loss: 6269150.5000\n",
      "Epoch [4257/50000], Train Loss: 10569648.0000, Val Loss: 6267919.5000\n",
      "Epoch [4258/50000], Train Loss: 10567878.0000, Val Loss: 6266689.0000\n",
      "Epoch [4259/50000], Train Loss: 10566108.0000, Val Loss: 6265458.0000\n",
      "Epoch [4260/50000], Train Loss: 10564337.0000, Val Loss: 6264230.0000\n",
      "Epoch [4261/50000], Train Loss: 10562571.0000, Val Loss: 6263000.0000\n",
      "Epoch [4262/50000], Train Loss: 10560802.0000, Val Loss: 6261771.5000\n",
      "Epoch [4263/50000], Train Loss: 10559036.0000, Val Loss: 6260545.5000\n",
      "Epoch [4264/50000], Train Loss: 10557269.0000, Val Loss: 6259319.5000\n",
      "Epoch [4265/50000], Train Loss: 10555505.0000, Val Loss: 6258093.5000\n",
      "Epoch [4266/50000], Train Loss: 10553739.0000, Val Loss: 6256867.0000\n",
      "Epoch [4267/50000], Train Loss: 10551975.0000, Val Loss: 6255644.5000\n",
      "Epoch [4268/50000], Train Loss: 10550216.0000, Val Loss: 6254419.5000\n",
      "Epoch [4269/50000], Train Loss: 10548451.0000, Val Loss: 6253198.0000\n",
      "Epoch [4270/50000], Train Loss: 10546690.0000, Val Loss: 6251976.5000\n",
      "Epoch [4271/50000], Train Loss: 10544930.0000, Val Loss: 6250754.5000\n",
      "Epoch [4272/50000], Train Loss: 10543170.0000, Val Loss: 6249534.0000\n",
      "Epoch [4273/50000], Train Loss: 10541411.0000, Val Loss: 6248315.0000\n",
      "Epoch [4274/50000], Train Loss: 10539655.0000, Val Loss: 6247096.5000\n",
      "Epoch [4275/50000], Train Loss: 10537899.0000, Val Loss: 6245878.0000\n",
      "Epoch [4276/50000], Train Loss: 10536141.0000, Val Loss: 6244661.5000\n",
      "Epoch [4277/50000], Train Loss: 10534387.0000, Val Loss: 6243444.5000\n",
      "Epoch [4278/50000], Train Loss: 10532632.0000, Val Loss: 6242229.0000\n",
      "Epoch [4279/50000], Train Loss: 10530879.0000, Val Loss: 6241015.0000\n",
      "Epoch [4280/50000], Train Loss: 10529126.0000, Val Loss: 6239800.0000\n",
      "Epoch [4281/50000], Train Loss: 10527373.0000, Val Loss: 6238586.5000\n",
      "Epoch [4282/50000], Train Loss: 10525622.0000, Val Loss: 6237374.0000\n",
      "Epoch [4283/50000], Train Loss: 10523873.0000, Val Loss: 6236161.5000\n",
      "Epoch [4284/50000], Train Loss: 10522122.0000, Val Loss: 6234949.5000\n",
      "Epoch [4285/50000], Train Loss: 10520373.0000, Val Loss: 6233740.5000\n",
      "Epoch [4286/50000], Train Loss: 10518626.0000, Val Loss: 6232530.5000\n",
      "Epoch [4287/50000], Train Loss: 10516879.0000, Val Loss: 6231320.0000\n",
      "Epoch [4288/50000], Train Loss: 10515129.0000, Val Loss: 6230113.0000\n",
      "Epoch [4289/50000], Train Loss: 10513386.0000, Val Loss: 6228905.5000\n",
      "Epoch [4290/50000], Train Loss: 10511641.0000, Val Loss: 6227698.5000\n",
      "Epoch [4291/50000], Train Loss: 10509897.0000, Val Loss: 6226493.5000\n",
      "Epoch [4292/50000], Train Loss: 10508154.0000, Val Loss: 6225287.5000\n",
      "Epoch [4293/50000], Train Loss: 10506411.0000, Val Loss: 6224084.5000\n",
      "Epoch [4294/50000], Train Loss: 10504671.0000, Val Loss: 6222879.5000\n",
      "Epoch [4295/50000], Train Loss: 10502930.0000, Val Loss: 6221676.5000\n",
      "Epoch [4296/50000], Train Loss: 10501190.0000, Val Loss: 6220474.5000\n",
      "Epoch [4297/50000], Train Loss: 10499453.0000, Val Loss: 6219273.5000\n",
      "Epoch [4298/50000], Train Loss: 10497713.0000, Val Loss: 6218072.5000\n",
      "Epoch [4299/50000], Train Loss: 10495974.0000, Val Loss: 6216873.5000\n",
      "Epoch [4300/50000], Train Loss: 10494241.0000, Val Loss: 6215674.0000\n",
      "Epoch [4301/50000], Train Loss: 10492504.0000, Val Loss: 6214474.5000\n",
      "Epoch [4302/50000], Train Loss: 10490767.0000, Val Loss: 6213277.0000\n",
      "Epoch [4303/50000], Train Loss: 10489034.0000, Val Loss: 6212080.0000\n",
      "Epoch [4304/50000], Train Loss: 10487299.0000, Val Loss: 6210885.5000\n",
      "Epoch [4305/50000], Train Loss: 10485568.0000, Val Loss: 6209689.5000\n",
      "Epoch [4306/50000], Train Loss: 10483836.0000, Val Loss: 6208494.5000\n",
      "Epoch [4307/50000], Train Loss: 10482104.0000, Val Loss: 6207301.5000\n",
      "Epoch [4308/50000], Train Loss: 10480376.0000, Val Loss: 6206106.5000\n",
      "Epoch [4309/50000], Train Loss: 10478643.0000, Val Loss: 6204914.5000\n",
      "Epoch [4310/50000], Train Loss: 10476915.0000, Val Loss: 6203723.5000\n",
      "Epoch [4311/50000], Train Loss: 10475190.0000, Val Loss: 6202533.5000\n",
      "Epoch [4312/50000], Train Loss: 10473463.0000, Val Loss: 6201342.0000\n",
      "Epoch [4313/50000], Train Loss: 10471735.0000, Val Loss: 6200153.0000\n",
      "Epoch [4314/50000], Train Loss: 10470012.0000, Val Loss: 6198964.5000\n",
      "Epoch [4315/50000], Train Loss: 10468286.0000, Val Loss: 6197777.0000\n",
      "Epoch [4316/50000], Train Loss: 10466562.0000, Val Loss: 6196590.5000\n",
      "Epoch [4317/50000], Train Loss: 10464841.0000, Val Loss: 6195403.0000\n",
      "Epoch [4318/50000], Train Loss: 10463117.0000, Val Loss: 6194217.5000\n",
      "Epoch [4319/50000], Train Loss: 10461397.0000, Val Loss: 6193033.0000\n",
      "Epoch [4320/50000], Train Loss: 10459675.0000, Val Loss: 6191849.5000\n",
      "Epoch [4321/50000], Train Loss: 10457957.0000, Val Loss: 6190666.0000\n",
      "Epoch [4322/50000], Train Loss: 10456239.0000, Val Loss: 6189483.0000\n",
      "Epoch [4323/50000], Train Loss: 10454520.0000, Val Loss: 6188302.0000\n",
      "Epoch [4324/50000], Train Loss: 10452803.0000, Val Loss: 6187121.0000\n",
      "Epoch [4325/50000], Train Loss: 10451088.0000, Val Loss: 6185939.5000\n",
      "Epoch [4326/50000], Train Loss: 10449372.0000, Val Loss: 6184760.0000\n",
      "Epoch [4327/50000], Train Loss: 10447656.0000, Val Loss: 6183581.5000\n",
      "Epoch [4328/50000], Train Loss: 10445942.0000, Val Loss: 6182404.5000\n",
      "Epoch [4329/50000], Train Loss: 10444230.0000, Val Loss: 6181226.5000\n",
      "Epoch [4330/50000], Train Loss: 10442517.0000, Val Loss: 6180050.5000\n",
      "Epoch [4331/50000], Train Loss: 10440806.0000, Val Loss: 6178875.0000\n",
      "Epoch [4332/50000], Train Loss: 10439098.0000, Val Loss: 6177699.0000\n",
      "Epoch [4333/50000], Train Loss: 10437386.0000, Val Loss: 6176526.0000\n",
      "Epoch [4334/50000], Train Loss: 10435677.0000, Val Loss: 6175352.0000\n",
      "Epoch [4335/50000], Train Loss: 10433969.0000, Val Loss: 6174178.5000\n",
      "Epoch [4336/50000], Train Loss: 10432259.0000, Val Loss: 6173007.0000\n",
      "Epoch [4337/50000], Train Loss: 10430555.0000, Val Loss: 6171835.5000\n",
      "Epoch [4338/50000], Train Loss: 10428848.0000, Val Loss: 6170664.5000\n",
      "Epoch [4339/50000], Train Loss: 10427143.0000, Val Loss: 6169494.5000\n",
      "Epoch [4340/50000], Train Loss: 10425438.0000, Val Loss: 6168326.5000\n",
      "Epoch [4341/50000], Train Loss: 10423737.0000, Val Loss: 6167157.5000\n",
      "Epoch [4342/50000], Train Loss: 10422033.0000, Val Loss: 6165990.5000\n",
      "Epoch [4343/50000], Train Loss: 10420331.0000, Val Loss: 6164823.5000\n",
      "Epoch [4344/50000], Train Loss: 10418631.0000, Val Loss: 6163657.0000\n",
      "Epoch [4345/50000], Train Loss: 10416930.0000, Val Loss: 6162492.5000\n",
      "Epoch [4346/50000], Train Loss: 10415231.0000, Val Loss: 6161328.0000\n",
      "Epoch [4347/50000], Train Loss: 10413532.0000, Val Loss: 6160164.5000\n",
      "Epoch [4348/50000], Train Loss: 10411836.0000, Val Loss: 6159001.0000\n",
      "Epoch [4349/50000], Train Loss: 10410139.0000, Val Loss: 6157838.0000\n",
      "Epoch [4350/50000], Train Loss: 10408441.0000, Val Loss: 6156677.0000\n",
      "Epoch [4351/50000], Train Loss: 10406746.0000, Val Loss: 6155516.5000\n",
      "Epoch [4352/50000], Train Loss: 10405053.0000, Val Loss: 6154357.0000\n",
      "Epoch [4353/50000], Train Loss: 10403359.0000, Val Loss: 6153197.0000\n",
      "Epoch [4354/50000], Train Loss: 10401665.0000, Val Loss: 6152038.5000\n",
      "Epoch [4355/50000], Train Loss: 10399973.0000, Val Loss: 6150881.0000\n",
      "Epoch [4356/50000], Train Loss: 10398282.0000, Val Loss: 6149723.0000\n",
      "Epoch [4357/50000], Train Loss: 10396591.0000, Val Loss: 6148567.0000\n",
      "Epoch [4358/50000], Train Loss: 10394900.0000, Val Loss: 6147411.0000\n",
      "Epoch [4359/50000], Train Loss: 10393214.0000, Val Loss: 6146258.0000\n",
      "Epoch [4360/50000], Train Loss: 10391526.0000, Val Loss: 6145103.0000\n",
      "Epoch [4361/50000], Train Loss: 10389837.0000, Val Loss: 6143950.0000\n",
      "Epoch [4362/50000], Train Loss: 10388152.0000, Val Loss: 6142796.5000\n",
      "Epoch [4363/50000], Train Loss: 10386465.0000, Val Loss: 6141644.5000\n",
      "Epoch [4364/50000], Train Loss: 10384779.0000, Val Loss: 6140493.0000\n",
      "Epoch [4365/50000], Train Loss: 10383096.0000, Val Loss: 6139343.0000\n",
      "Epoch [4366/50000], Train Loss: 10381412.0000, Val Loss: 6138193.0000\n",
      "Epoch [4367/50000], Train Loss: 10379730.0000, Val Loss: 6137045.0000\n",
      "Epoch [4368/50000], Train Loss: 10378048.0000, Val Loss: 6135896.5000\n",
      "Epoch [4369/50000], Train Loss: 10376368.0000, Val Loss: 6134749.0000\n",
      "Epoch [4370/50000], Train Loss: 10374687.0000, Val Loss: 6133601.0000\n",
      "Epoch [4371/50000], Train Loss: 10373007.0000, Val Loss: 6132456.5000\n",
      "Epoch [4372/50000], Train Loss: 10371330.0000, Val Loss: 6131311.0000\n",
      "Epoch [4373/50000], Train Loss: 10369652.0000, Val Loss: 6130167.5000\n",
      "Epoch [4374/50000], Train Loss: 10367976.0000, Val Loss: 6129022.5000\n",
      "Epoch [4375/50000], Train Loss: 10366299.0000, Val Loss: 6127879.0000\n",
      "Epoch [4376/50000], Train Loss: 10364623.0000, Val Loss: 6126737.0000\n",
      "Epoch [4377/50000], Train Loss: 10362948.0000, Val Loss: 6125597.0000\n",
      "Epoch [4378/50000], Train Loss: 10361276.0000, Val Loss: 6124455.5000\n",
      "Epoch [4379/50000], Train Loss: 10359603.0000, Val Loss: 6123315.0000\n",
      "Epoch [4380/50000], Train Loss: 10357931.0000, Val Loss: 6122175.5000\n",
      "Epoch [4381/50000], Train Loss: 10356258.0000, Val Loss: 6121037.0000\n",
      "Epoch [4382/50000], Train Loss: 10354586.0000, Val Loss: 6119898.5000\n",
      "Epoch [4383/50000], Train Loss: 10352917.0000, Val Loss: 6118761.5000\n",
      "Epoch [4384/50000], Train Loss: 10351249.0000, Val Loss: 6117626.0000\n",
      "Epoch [4385/50000], Train Loss: 10349581.0000, Val Loss: 6116490.5000\n",
      "Epoch [4386/50000], Train Loss: 10347914.0000, Val Loss: 6115355.5000\n",
      "Epoch [4387/50000], Train Loss: 10346250.0000, Val Loss: 6114221.5000\n",
      "Epoch [4388/50000], Train Loss: 10344582.0000, Val Loss: 6113088.5000\n",
      "Epoch [4389/50000], Train Loss: 10342918.0000, Val Loss: 6111955.0000\n",
      "Epoch [4390/50000], Train Loss: 10341254.0000, Val Loss: 6110824.5000\n",
      "Epoch [4391/50000], Train Loss: 10339591.0000, Val Loss: 6109693.5000\n",
      "Epoch [4392/50000], Train Loss: 10337929.0000, Val Loss: 6108564.5000\n",
      "Epoch [4393/50000], Train Loss: 10336268.0000, Val Loss: 6107432.5000\n",
      "Epoch [4394/50000], Train Loss: 10334605.0000, Val Loss: 6106304.0000\n",
      "Epoch [4395/50000], Train Loss: 10332946.0000, Val Loss: 6105175.5000\n",
      "Epoch [4396/50000], Train Loss: 10331285.0000, Val Loss: 6104049.0000\n",
      "Epoch [4397/50000], Train Loss: 10329628.0000, Val Loss: 6102922.5000\n",
      "Epoch [4398/50000], Train Loss: 10327971.0000, Val Loss: 6101797.0000\n",
      "Epoch [4399/50000], Train Loss: 10326314.0000, Val Loss: 6100671.5000\n",
      "Epoch [4400/50000], Train Loss: 10324659.0000, Val Loss: 6099545.5000\n",
      "Epoch [4401/50000], Train Loss: 10323000.0000, Val Loss: 6098423.5000\n",
      "Epoch [4402/50000], Train Loss: 10321348.0000, Val Loss: 6097299.0000\n",
      "Epoch [4403/50000], Train Loss: 10319692.0000, Val Loss: 6096178.5000\n",
      "Epoch [4404/50000], Train Loss: 10318042.0000, Val Loss: 6095056.0000\n",
      "Epoch [4405/50000], Train Loss: 10316388.0000, Val Loss: 6093936.0000\n",
      "Epoch [4406/50000], Train Loss: 10314739.0000, Val Loss: 6092815.0000\n",
      "Epoch [4407/50000], Train Loss: 10313086.0000, Val Loss: 6091697.0000\n",
      "Epoch [4408/50000], Train Loss: 10311437.0000, Val Loss: 6090577.5000\n",
      "Epoch [4409/50000], Train Loss: 10309788.0000, Val Loss: 6089460.5000\n",
      "Epoch [4410/50000], Train Loss: 10308142.0000, Val Loss: 6088343.0000\n",
      "Epoch [4411/50000], Train Loss: 10306494.0000, Val Loss: 6087226.5000\n",
      "Epoch [4412/50000], Train Loss: 10304848.0000, Val Loss: 6086111.0000\n",
      "Epoch [4413/50000], Train Loss: 10303201.0000, Val Loss: 6084996.5000\n",
      "Epoch [4414/50000], Train Loss: 10301556.0000, Val Loss: 6083881.0000\n",
      "Epoch [4415/50000], Train Loss: 10299911.0000, Val Loss: 6082768.0000\n",
      "Epoch [4416/50000], Train Loss: 10298269.0000, Val Loss: 6081656.0000\n",
      "Epoch [4417/50000], Train Loss: 10296625.0000, Val Loss: 6080544.0000\n",
      "Epoch [4418/50000], Train Loss: 10294984.0000, Val Loss: 6079432.0000\n",
      "Epoch [4419/50000], Train Loss: 10293342.0000, Val Loss: 6078322.0000\n",
      "Epoch [4420/50000], Train Loss: 10291702.0000, Val Loss: 6077213.0000\n",
      "Epoch [4421/50000], Train Loss: 10290064.0000, Val Loss: 6076103.5000\n",
      "Epoch [4422/50000], Train Loss: 10288424.0000, Val Loss: 6074995.0000\n",
      "Epoch [4423/50000], Train Loss: 10286786.0000, Val Loss: 6073888.0000\n",
      "Epoch [4424/50000], Train Loss: 10285150.0000, Val Loss: 6072781.0000\n",
      "Epoch [4425/50000], Train Loss: 10283514.0000, Val Loss: 6071674.5000\n",
      "Epoch [4426/50000], Train Loss: 10281877.0000, Val Loss: 6070569.5000\n",
      "Epoch [4427/50000], Train Loss: 10280245.0000, Val Loss: 6069464.5000\n",
      "Epoch [4428/50000], Train Loss: 10278610.0000, Val Loss: 6068361.0000\n",
      "Epoch [4429/50000], Train Loss: 10276977.0000, Val Loss: 6067258.0000\n",
      "Epoch [4430/50000], Train Loss: 10275345.0000, Val Loss: 6066155.5000\n",
      "Epoch [4431/50000], Train Loss: 10273712.0000, Val Loss: 6065053.0000\n",
      "Epoch [4432/50000], Train Loss: 10272081.0000, Val Loss: 6063952.5000\n",
      "Epoch [4433/50000], Train Loss: 10270452.0000, Val Loss: 6062852.5000\n",
      "Epoch [4434/50000], Train Loss: 10268821.0000, Val Loss: 6061752.5000\n",
      "Epoch [4435/50000], Train Loss: 10267196.0000, Val Loss: 6060653.5000\n",
      "Epoch [4436/50000], Train Loss: 10265567.0000, Val Loss: 6059556.5000\n",
      "Epoch [4437/50000], Train Loss: 10263940.0000, Val Loss: 6058458.0000\n",
      "Epoch [4438/50000], Train Loss: 10262314.0000, Val Loss: 6057361.5000\n",
      "Epoch [4439/50000], Train Loss: 10260689.0000, Val Loss: 6056265.5000\n",
      "Epoch [4440/50000], Train Loss: 10259063.0000, Val Loss: 6055171.0000\n",
      "Epoch [4441/50000], Train Loss: 10257442.0000, Val Loss: 6054078.0000\n",
      "Epoch [4442/50000], Train Loss: 10255819.0000, Val Loss: 6052983.5000\n",
      "Epoch [4443/50000], Train Loss: 10254197.0000, Val Loss: 6051890.0000\n",
      "Epoch [4444/50000], Train Loss: 10252575.0000, Val Loss: 6050798.5000\n",
      "Epoch [4445/50000], Train Loss: 10250954.0000, Val Loss: 6049706.5000\n",
      "Epoch [4446/50000], Train Loss: 10249335.0000, Val Loss: 6048616.0000\n",
      "Epoch [4447/50000], Train Loss: 10247716.0000, Val Loss: 6047526.0000\n",
      "Epoch [4448/50000], Train Loss: 10246098.0000, Val Loss: 6046436.5000\n",
      "Epoch [4449/50000], Train Loss: 10244480.0000, Val Loss: 6045347.5000\n",
      "Epoch [4450/50000], Train Loss: 10242864.0000, Val Loss: 6044260.5000\n",
      "Epoch [4451/50000], Train Loss: 10241249.0000, Val Loss: 6043174.0000\n",
      "Epoch [4452/50000], Train Loss: 10239635.0000, Val Loss: 6042088.0000\n",
      "Epoch [4453/50000], Train Loss: 10238020.0000, Val Loss: 6041001.0000\n",
      "Epoch [4454/50000], Train Loss: 10236405.0000, Val Loss: 6039917.0000\n",
      "Epoch [4455/50000], Train Loss: 10234795.0000, Val Loss: 6038832.0000\n",
      "Epoch [4456/50000], Train Loss: 10233181.0000, Val Loss: 6037749.5000\n",
      "Epoch [4457/50000], Train Loss: 10231571.0000, Val Loss: 6036665.5000\n",
      "Epoch [4458/50000], Train Loss: 10229962.0000, Val Loss: 6035585.0000\n",
      "Epoch [4459/50000], Train Loss: 10228351.0000, Val Loss: 6034504.0000\n",
      "Epoch [4460/50000], Train Loss: 10226744.0000, Val Loss: 6033422.5000\n",
      "Epoch [4461/50000], Train Loss: 10225135.0000, Val Loss: 6032343.5000\n",
      "Epoch [4462/50000], Train Loss: 10223527.0000, Val Loss: 6031264.0000\n",
      "Epoch [4463/50000], Train Loss: 10221922.0000, Val Loss: 6030185.5000\n",
      "Epoch [4464/50000], Train Loss: 10220315.0000, Val Loss: 6029107.0000\n",
      "Epoch [4465/50000], Train Loss: 10218713.0000, Val Loss: 6028031.0000\n",
      "Epoch [4466/50000], Train Loss: 10217109.0000, Val Loss: 6026954.0000\n",
      "Epoch [4467/50000], Train Loss: 10215505.0000, Val Loss: 6025879.5000\n",
      "Epoch [4468/50000], Train Loss: 10213904.0000, Val Loss: 6024805.0000\n",
      "Epoch [4469/50000], Train Loss: 10212301.0000, Val Loss: 6023731.0000\n",
      "Epoch [4470/50000], Train Loss: 10210703.0000, Val Loss: 6022657.5000\n",
      "Epoch [4471/50000], Train Loss: 10209101.0000, Val Loss: 6021585.0000\n",
      "Epoch [4472/50000], Train Loss: 10207503.0000, Val Loss: 6020512.5000\n",
      "Epoch [4473/50000], Train Loss: 10205903.0000, Val Loss: 6019442.5000\n",
      "Epoch [4474/50000], Train Loss: 10204306.0000, Val Loss: 6018372.5000\n",
      "Epoch [4475/50000], Train Loss: 10202708.0000, Val Loss: 6017303.0000\n",
      "Epoch [4476/50000], Train Loss: 10201114.0000, Val Loss: 6016233.5000\n",
      "Epoch [4477/50000], Train Loss: 10199517.0000, Val Loss: 6015165.5000\n",
      "Epoch [4478/50000], Train Loss: 10197924.0000, Val Loss: 6014098.0000\n",
      "Epoch [4479/50000], Train Loss: 10196331.0000, Val Loss: 6013031.5000\n",
      "Epoch [4480/50000], Train Loss: 10194737.0000, Val Loss: 6011966.0000\n",
      "Epoch [4481/50000], Train Loss: 10193145.0000, Val Loss: 6010899.5000\n",
      "Epoch [4482/50000], Train Loss: 10191554.0000, Val Loss: 6009835.0000\n",
      "Epoch [4483/50000], Train Loss: 10189964.0000, Val Loss: 6008771.0000\n",
      "Epoch [4484/50000], Train Loss: 10188373.0000, Val Loss: 6007709.0000\n",
      "Epoch [4485/50000], Train Loss: 10186783.0000, Val Loss: 6006645.5000\n",
      "Epoch [4486/50000], Train Loss: 10185196.0000, Val Loss: 6005584.0000\n",
      "Epoch [4487/50000], Train Loss: 10183609.0000, Val Loss: 6004524.5000\n",
      "Epoch [4488/50000], Train Loss: 10182024.0000, Val Loss: 6003463.5000\n",
      "Epoch [4489/50000], Train Loss: 10180437.0000, Val Loss: 6002404.5000\n",
      "Epoch [4490/50000], Train Loss: 10178852.0000, Val Loss: 6001345.0000\n",
      "Epoch [4491/50000], Train Loss: 10177268.0000, Val Loss: 6000287.0000\n",
      "Epoch [4492/50000], Train Loss: 10175682.0000, Val Loss: 5999230.0000\n",
      "Epoch [4493/50000], Train Loss: 10174102.0000, Val Loss: 5998172.5000\n",
      "Epoch [4494/50000], Train Loss: 10172519.0000, Val Loss: 5997117.5000\n",
      "Epoch [4495/50000], Train Loss: 10170938.0000, Val Loss: 5996063.0000\n",
      "Epoch [4496/50000], Train Loss: 10169360.0000, Val Loss: 5995008.0000\n",
      "Epoch [4497/50000], Train Loss: 10167778.0000, Val Loss: 5993954.0000\n",
      "Epoch [4498/50000], Train Loss: 10166200.0000, Val Loss: 5992900.5000\n",
      "Epoch [4499/50000], Train Loss: 10164620.0000, Val Loss: 5991849.5000\n",
      "Epoch [4500/50000], Train Loss: 10163046.0000, Val Loss: 5990798.0000\n",
      "Epoch [4501/50000], Train Loss: 10161470.0000, Val Loss: 5989746.0000\n",
      "Epoch [4502/50000], Train Loss: 10159892.0000, Val Loss: 5988696.5000\n",
      "Epoch [4503/50000], Train Loss: 10158317.0000, Val Loss: 5987647.5000\n",
      "Epoch [4504/50000], Train Loss: 10156744.0000, Val Loss: 5986598.0000\n",
      "Epoch [4505/50000], Train Loss: 10155170.0000, Val Loss: 5985550.5000\n",
      "Epoch [4506/50000], Train Loss: 10153598.0000, Val Loss: 5984503.5000\n",
      "Epoch [4507/50000], Train Loss: 10152027.0000, Val Loss: 5983457.0000\n",
      "Epoch [4508/50000], Train Loss: 10150456.0000, Val Loss: 5982410.5000\n",
      "Epoch [4509/50000], Train Loss: 10148885.0000, Val Loss: 5981365.5000\n",
      "Epoch [4510/50000], Train Loss: 10147315.0000, Val Loss: 5980322.0000\n",
      "Epoch [4511/50000], Train Loss: 10145748.0000, Val Loss: 5979279.0000\n",
      "Epoch [4512/50000], Train Loss: 10144180.0000, Val Loss: 5978235.5000\n",
      "Epoch [4513/50000], Train Loss: 10142612.0000, Val Loss: 5977193.5000\n",
      "Epoch [4514/50000], Train Loss: 10141047.0000, Val Loss: 5976152.0000\n",
      "Epoch [4515/50000], Train Loss: 10139481.0000, Val Loss: 5975112.0000\n",
      "Epoch [4516/50000], Train Loss: 10137918.0000, Val Loss: 5974071.5000\n",
      "Epoch [4517/50000], Train Loss: 10136354.0000, Val Loss: 5973032.5000\n",
      "Epoch [4518/50000], Train Loss: 10134790.0000, Val Loss: 5971994.0000\n",
      "Epoch [4519/50000], Train Loss: 10133229.0000, Val Loss: 5970956.5000\n",
      "Epoch [4520/50000], Train Loss: 10131666.0000, Val Loss: 5969919.0000\n",
      "Epoch [4521/50000], Train Loss: 10130105.0000, Val Loss: 5968882.0000\n",
      "Epoch [4522/50000], Train Loss: 10128545.0000, Val Loss: 5967848.5000\n",
      "Epoch [4523/50000], Train Loss: 10126988.0000, Val Loss: 5966811.5000\n",
      "Epoch [4524/50000], Train Loss: 10125429.0000, Val Loss: 5965777.5000\n",
      "Epoch [4525/50000], Train Loss: 10123870.0000, Val Loss: 5964744.5000\n",
      "Epoch [4526/50000], Train Loss: 10122315.0000, Val Loss: 5963711.0000\n",
      "Epoch [4527/50000], Train Loss: 10120758.0000, Val Loss: 5962680.0000\n",
      "Epoch [4528/50000], Train Loss: 10119203.0000, Val Loss: 5961648.0000\n",
      "Epoch [4529/50000], Train Loss: 10117649.0000, Val Loss: 5960617.5000\n",
      "Epoch [4530/50000], Train Loss: 10116095.0000, Val Loss: 5959587.5000\n",
      "Epoch [4531/50000], Train Loss: 10114543.0000, Val Loss: 5958559.0000\n",
      "Epoch [4532/50000], Train Loss: 10112990.0000, Val Loss: 5957529.5000\n",
      "Epoch [4533/50000], Train Loss: 10111437.0000, Val Loss: 5956502.5000\n",
      "Epoch [4534/50000], Train Loss: 10109888.0000, Val Loss: 5955474.5000\n",
      "Epoch [4535/50000], Train Loss: 10108337.0000, Val Loss: 5954448.5000\n",
      "Epoch [4536/50000], Train Loss: 10106789.0000, Val Loss: 5953423.0000\n",
      "Epoch [4537/50000], Train Loss: 10105241.0000, Val Loss: 5952397.5000\n",
      "Epoch [4538/50000], Train Loss: 10103693.0000, Val Loss: 5951374.0000\n",
      "Epoch [4539/50000], Train Loss: 10102145.0000, Val Loss: 5950351.0000\n",
      "Epoch [4540/50000], Train Loss: 10100600.0000, Val Loss: 5949326.5000\n",
      "Epoch [4541/50000], Train Loss: 10099054.0000, Val Loss: 5948305.0000\n",
      "Epoch [4542/50000], Train Loss: 10097511.0000, Val Loss: 5947283.0000\n",
      "Epoch [4543/50000], Train Loss: 10095966.0000, Val Loss: 5946263.5000\n",
      "Epoch [4544/50000], Train Loss: 10094425.0000, Val Loss: 5945242.5000\n",
      "Epoch [4545/50000], Train Loss: 10092882.0000, Val Loss: 5944224.0000\n",
      "Epoch [4546/50000], Train Loss: 10091340.0000, Val Loss: 5943206.0000\n",
      "Epoch [4547/50000], Train Loss: 10089801.0000, Val Loss: 5942186.5000\n",
      "Epoch [4548/50000], Train Loss: 10088259.0000, Val Loss: 5941170.5000\n",
      "Epoch [4549/50000], Train Loss: 10086722.0000, Val Loss: 5940154.0000\n",
      "Epoch [4550/50000], Train Loss: 10085183.0000, Val Loss: 5939138.0000\n",
      "Epoch [4551/50000], Train Loss: 10083645.0000, Val Loss: 5938123.5000\n",
      "Epoch [4552/50000], Train Loss: 10082110.0000, Val Loss: 5937109.5000\n",
      "Epoch [4553/50000], Train Loss: 10080574.0000, Val Loss: 5936095.0000\n",
      "Epoch [4554/50000], Train Loss: 10079036.0000, Val Loss: 5935082.5000\n",
      "Epoch [4555/50000], Train Loss: 10077502.0000, Val Loss: 5934071.5000\n",
      "Epoch [4556/50000], Train Loss: 10075970.0000, Val Loss: 5933059.0000\n",
      "Epoch [4557/50000], Train Loss: 10074436.0000, Val Loss: 5932049.0000\n",
      "Epoch [4558/50000], Train Loss: 10072905.0000, Val Loss: 5931040.0000\n",
      "Epoch [4559/50000], Train Loss: 10071375.0000, Val Loss: 5930029.0000\n",
      "Epoch [4560/50000], Train Loss: 10069842.0000, Val Loss: 5929020.5000\n",
      "Epoch [4561/50000], Train Loss: 10068313.0000, Val Loss: 5928013.0000\n",
      "Epoch [4562/50000], Train Loss: 10066785.0000, Val Loss: 5927006.5000\n",
      "Epoch [4563/50000], Train Loss: 10065256.0000, Val Loss: 5925999.5000\n",
      "Epoch [4564/50000], Train Loss: 10063728.0000, Val Loss: 5924994.0000\n",
      "Epoch [4565/50000], Train Loss: 10062202.0000, Val Loss: 5923989.5000\n",
      "Epoch [4566/50000], Train Loss: 10060675.0000, Val Loss: 5922984.0000\n",
      "Epoch [4567/50000], Train Loss: 10059152.0000, Val Loss: 5921981.0000\n",
      "Epoch [4568/50000], Train Loss: 10057626.0000, Val Loss: 5920978.0000\n",
      "Epoch [4569/50000], Train Loss: 10056102.0000, Val Loss: 5919976.5000\n",
      "Epoch [4570/50000], Train Loss: 10054582.0000, Val Loss: 5918975.0000\n",
      "Epoch [4571/50000], Train Loss: 10053057.0000, Val Loss: 5917974.5000\n",
      "Epoch [4572/50000], Train Loss: 10051537.0000, Val Loss: 5916974.5000\n",
      "Epoch [4573/50000], Train Loss: 10050018.0000, Val Loss: 5915974.5000\n",
      "Epoch [4574/50000], Train Loss: 10048496.0000, Val Loss: 5914976.0000\n",
      "Epoch [4575/50000], Train Loss: 10046977.0000, Val Loss: 5913977.5000\n",
      "Epoch [4576/50000], Train Loss: 10045460.0000, Val Loss: 5912982.0000\n",
      "Epoch [4577/50000], Train Loss: 10043944.0000, Val Loss: 5911984.0000\n",
      "Epoch [4578/50000], Train Loss: 10042426.0000, Val Loss: 5910988.5000\n",
      "Epoch [4579/50000], Train Loss: 10040909.0000, Val Loss: 5909993.0000\n",
      "Epoch [4580/50000], Train Loss: 10039394.0000, Val Loss: 5908998.0000\n",
      "Epoch [4581/50000], Train Loss: 10037880.0000, Val Loss: 5908005.5000\n",
      "Epoch [4582/50000], Train Loss: 10036368.0000, Val Loss: 5907012.5000\n",
      "Epoch [4583/50000], Train Loss: 10034854.0000, Val Loss: 5906020.5000\n",
      "Epoch [4584/50000], Train Loss: 10033343.0000, Val Loss: 5905028.5000\n",
      "Epoch [4585/50000], Train Loss: 10031830.0000, Val Loss: 5904037.5000\n",
      "Epoch [4586/50000], Train Loss: 10030320.0000, Val Loss: 5903047.5000\n",
      "Epoch [4587/50000], Train Loss: 10028810.0000, Val Loss: 5902057.0000\n",
      "Epoch [4588/50000], Train Loss: 10027301.0000, Val Loss: 5901069.0000\n",
      "Epoch [4589/50000], Train Loss: 10025793.0000, Val Loss: 5900081.0000\n",
      "Epoch [4590/50000], Train Loss: 10024286.0000, Val Loss: 5899094.0000\n",
      "Epoch [4591/50000], Train Loss: 10022780.0000, Val Loss: 5898106.5000\n",
      "Epoch [4592/50000], Train Loss: 10021273.0000, Val Loss: 5897121.0000\n",
      "Epoch [4593/50000], Train Loss: 10019767.0000, Val Loss: 5896136.0000\n",
      "Epoch [4594/50000], Train Loss: 10018264.0000, Val Loss: 5895152.5000\n",
      "Epoch [4595/50000], Train Loss: 10016761.0000, Val Loss: 5894167.5000\n",
      "Epoch [4596/50000], Train Loss: 10015258.0000, Val Loss: 5893185.0000\n",
      "Epoch [4597/50000], Train Loss: 10013754.0000, Val Loss: 5892202.5000\n",
      "Epoch [4598/50000], Train Loss: 10012254.0000, Val Loss: 5891221.5000\n",
      "Epoch [4599/50000], Train Loss: 10010755.0000, Val Loss: 5890240.0000\n",
      "Epoch [4600/50000], Train Loss: 10009254.0000, Val Loss: 5889259.5000\n",
      "Epoch [4601/50000], Train Loss: 10007756.0000, Val Loss: 5888279.5000\n",
      "Epoch [4602/50000], Train Loss: 10006256.0000, Val Loss: 5887302.5000\n",
      "Epoch [4603/50000], Train Loss: 10004761.0000, Val Loss: 5886322.5000\n",
      "Epoch [4604/50000], Train Loss: 10003263.0000, Val Loss: 5885345.0000\n",
      "Epoch [4605/50000], Train Loss: 10001767.0000, Val Loss: 5884368.5000\n",
      "Epoch [4606/50000], Train Loss: 10000272.0000, Val Loss: 5883393.0000\n",
      "Epoch [4607/50000], Train Loss: 9998778.0000, Val Loss: 5882416.5000\n",
      "Epoch [4608/50000], Train Loss: 9997283.0000, Val Loss: 5881442.0000\n",
      "Epoch [4609/50000], Train Loss: 9995789.0000, Val Loss: 5880468.5000\n",
      "Epoch [4610/50000], Train Loss: 9994300.0000, Val Loss: 5879496.0000\n",
      "Epoch [4611/50000], Train Loss: 9992808.0000, Val Loss: 5878522.0000\n",
      "Epoch [4612/50000], Train Loss: 9991317.0000, Val Loss: 5877550.0000\n",
      "Epoch [4613/50000], Train Loss: 9989826.0000, Val Loss: 5876578.5000\n",
      "Epoch [4614/50000], Train Loss: 9988337.0000, Val Loss: 5875608.5000\n",
      "Epoch [4615/50000], Train Loss: 9986849.0000, Val Loss: 5874639.5000\n",
      "Epoch [4616/50000], Train Loss: 9985362.0000, Val Loss: 5873670.5000\n",
      "Epoch [4617/50000], Train Loss: 9983876.0000, Val Loss: 5872703.0000\n",
      "Epoch [4618/50000], Train Loss: 9982390.0000, Val Loss: 5871734.5000\n",
      "Epoch [4619/50000], Train Loss: 9980904.0000, Val Loss: 5870767.0000\n",
      "Epoch [4620/50000], Train Loss: 9979420.0000, Val Loss: 5869800.5000\n",
      "Epoch [4621/50000], Train Loss: 9977937.0000, Val Loss: 5868836.5000\n",
      "Epoch [4622/50000], Train Loss: 9976455.0000, Val Loss: 5867871.0000\n",
      "Epoch [4623/50000], Train Loss: 9974973.0000, Val Loss: 5866907.0000\n",
      "Epoch [4624/50000], Train Loss: 9973491.0000, Val Loss: 5865943.5000\n",
      "Epoch [4625/50000], Train Loss: 9972008.0000, Val Loss: 5864981.0000\n",
      "Epoch [4626/50000], Train Loss: 9970529.0000, Val Loss: 5864018.0000\n",
      "Epoch [4627/50000], Train Loss: 9969049.0000, Val Loss: 5863057.5000\n",
      "Epoch [4628/50000], Train Loss: 9967572.0000, Val Loss: 5862096.0000\n",
      "Epoch [4629/50000], Train Loss: 9966096.0000, Val Loss: 5861137.0000\n",
      "Epoch [4630/50000], Train Loss: 9964617.0000, Val Loss: 5860176.5000\n",
      "Epoch [4631/50000], Train Loss: 9963140.0000, Val Loss: 5859218.5000\n",
      "Epoch [4632/50000], Train Loss: 9961666.0000, Val Loss: 5858261.5000\n",
      "Epoch [4633/50000], Train Loss: 9960193.0000, Val Loss: 5857304.5000\n",
      "Epoch [4634/50000], Train Loss: 9958719.0000, Val Loss: 5856347.5000\n",
      "Epoch [4635/50000], Train Loss: 9957246.0000, Val Loss: 5855392.0000\n",
      "Epoch [4636/50000], Train Loss: 9955774.0000, Val Loss: 5854437.0000\n",
      "Epoch [4637/50000], Train Loss: 9954303.0000, Val Loss: 5853482.0000\n",
      "Epoch [4638/50000], Train Loss: 9952831.0000, Val Loss: 5852529.5000\n",
      "Epoch [4639/50000], Train Loss: 9951362.0000, Val Loss: 5851576.0000\n",
      "Epoch [4640/50000], Train Loss: 9949894.0000, Val Loss: 5850623.0000\n",
      "Epoch [4641/50000], Train Loss: 9948423.0000, Val Loss: 5849672.0000\n",
      "Epoch [4642/50000], Train Loss: 9946955.0000, Val Loss: 5848720.5000\n",
      "Epoch [4643/50000], Train Loss: 9945488.0000, Val Loss: 5847771.0000\n",
      "Epoch [4644/50000], Train Loss: 9944022.0000, Val Loss: 5846822.0000\n",
      "Epoch [4645/50000], Train Loss: 9942557.0000, Val Loss: 5845872.0000\n",
      "Epoch [4646/50000], Train Loss: 9941092.0000, Val Loss: 5844924.5000\n",
      "Epoch [4647/50000], Train Loss: 9939629.0000, Val Loss: 5843977.0000\n",
      "Epoch [4648/50000], Train Loss: 9938166.0000, Val Loss: 5843030.0000\n",
      "Epoch [4649/50000], Train Loss: 9936704.0000, Val Loss: 5842084.5000\n",
      "Epoch [4650/50000], Train Loss: 9935242.0000, Val Loss: 5841138.0000\n",
      "Epoch [4651/50000], Train Loss: 9933780.0000, Val Loss: 5840193.0000\n",
      "Epoch [4652/50000], Train Loss: 9932321.0000, Val Loss: 5839250.5000\n",
      "Epoch [4653/50000], Train Loss: 9930861.0000, Val Loss: 5838306.0000\n",
      "Epoch [4654/50000], Train Loss: 9929401.0000, Val Loss: 5837364.5000\n",
      "Epoch [4655/50000], Train Loss: 9927944.0000, Val Loss: 5836423.0000\n",
      "Epoch [4656/50000], Train Loss: 9926488.0000, Val Loss: 5835481.5000\n",
      "Epoch [4657/50000], Train Loss: 9925030.0000, Val Loss: 5834541.0000\n",
      "Epoch [4658/50000], Train Loss: 9923576.0000, Val Loss: 5833601.0000\n",
      "Epoch [4659/50000], Train Loss: 9922121.0000, Val Loss: 5832662.0000\n",
      "Epoch [4660/50000], Train Loss: 9920667.0000, Val Loss: 5831723.5000\n",
      "Epoch [4661/50000], Train Loss: 9919214.0000, Val Loss: 5830786.0000\n",
      "Epoch [4662/50000], Train Loss: 9917761.0000, Val Loss: 5829848.5000\n",
      "Epoch [4663/50000], Train Loss: 9916308.0000, Val Loss: 5828912.5000\n",
      "Epoch [4664/50000], Train Loss: 9914859.0000, Val Loss: 5827976.5000\n",
      "Epoch [4665/50000], Train Loss: 9913407.0000, Val Loss: 5827041.5000\n",
      "Epoch [4666/50000], Train Loss: 9911957.0000, Val Loss: 5826107.5000\n",
      "Epoch [4667/50000], Train Loss: 9910509.0000, Val Loss: 5825174.5000\n",
      "Epoch [4668/50000], Train Loss: 9909062.0000, Val Loss: 5824240.5000\n",
      "Epoch [4669/50000], Train Loss: 9907613.0000, Val Loss: 5823309.0000\n",
      "Epoch [4670/50000], Train Loss: 9906167.0000, Val Loss: 5822377.5000\n",
      "Epoch [4671/50000], Train Loss: 9904722.0000, Val Loss: 5821446.5000\n",
      "Epoch [4672/50000], Train Loss: 9903276.0000, Val Loss: 5820517.5000\n",
      "Epoch [4673/50000], Train Loss: 9901833.0000, Val Loss: 5819587.5000\n",
      "Epoch [4674/50000], Train Loss: 9900390.0000, Val Loss: 5818658.0000\n",
      "Epoch [4675/50000], Train Loss: 9898946.0000, Val Loss: 5817730.0000\n",
      "Epoch [4676/50000], Train Loss: 9897503.0000, Val Loss: 5816802.0000\n",
      "Epoch [4677/50000], Train Loss: 9896061.0000, Val Loss: 5815875.5000\n",
      "Epoch [4678/50000], Train Loss: 9894622.0000, Val Loss: 5814951.5000\n",
      "Epoch [4679/50000], Train Loss: 9893184.0000, Val Loss: 5814025.0000\n",
      "Epoch [4680/50000], Train Loss: 9891743.0000, Val Loss: 5813101.0000\n",
      "Epoch [4681/50000], Train Loss: 9890306.0000, Val Loss: 5812175.5000\n",
      "Epoch [4682/50000], Train Loss: 9888868.0000, Val Loss: 5811254.0000\n",
      "Epoch [4683/50000], Train Loss: 9887432.0000, Val Loss: 5810331.5000\n",
      "Epoch [4684/50000], Train Loss: 9885996.0000, Val Loss: 5809410.0000\n",
      "Epoch [4685/50000], Train Loss: 9884561.0000, Val Loss: 5808488.5000\n",
      "Epoch [4686/50000], Train Loss: 9883127.0000, Val Loss: 5807568.0000\n",
      "Epoch [4687/50000], Train Loss: 9881693.0000, Val Loss: 5806648.0000\n",
      "Epoch [4688/50000], Train Loss: 9880259.0000, Val Loss: 5805729.0000\n",
      "Epoch [4689/50000], Train Loss: 9878828.0000, Val Loss: 5804811.0000\n",
      "Epoch [4690/50000], Train Loss: 9877396.0000, Val Loss: 5803893.0000\n",
      "Epoch [4691/50000], Train Loss: 9875966.0000, Val Loss: 5802976.5000\n",
      "Epoch [4692/50000], Train Loss: 9874535.0000, Val Loss: 5802059.5000\n",
      "Epoch [4693/50000], Train Loss: 9873106.0000, Val Loss: 5801144.0000\n",
      "Epoch [4694/50000], Train Loss: 9871678.0000, Val Loss: 5800230.0000\n",
      "Epoch [4695/50000], Train Loss: 9870251.0000, Val Loss: 5799315.0000\n",
      "Epoch [4696/50000], Train Loss: 9868824.0000, Val Loss: 5798402.0000\n",
      "Epoch [4697/50000], Train Loss: 9867398.0000, Val Loss: 5797488.0000\n",
      "Epoch [4698/50000], Train Loss: 9865972.0000, Val Loss: 5796576.0000\n",
      "Epoch [4699/50000], Train Loss: 9864546.0000, Val Loss: 5795665.0000\n",
      "Epoch [4700/50000], Train Loss: 9863123.0000, Val Loss: 5794754.0000\n",
      "Epoch [4701/50000], Train Loss: 9861700.0000, Val Loss: 5793844.5000\n",
      "Epoch [4702/50000], Train Loss: 9860279.0000, Val Loss: 5792935.5000\n",
      "Epoch [4703/50000], Train Loss: 9858856.0000, Val Loss: 5792025.5000\n",
      "Epoch [4704/50000], Train Loss: 9857435.0000, Val Loss: 5791118.0000\n",
      "Epoch [4705/50000], Train Loss: 9856016.0000, Val Loss: 5790210.5000\n",
      "Epoch [4706/50000], Train Loss: 9854598.0000, Val Loss: 5789305.0000\n",
      "Epoch [4707/50000], Train Loss: 9853180.0000, Val Loss: 5788398.0000\n",
      "Epoch [4708/50000], Train Loss: 9851760.0000, Val Loss: 5787492.5000\n",
      "Epoch [4709/50000], Train Loss: 9850341.0000, Val Loss: 5786587.0000\n",
      "Epoch [4710/50000], Train Loss: 9848925.0000, Val Loss: 5785683.0000\n",
      "Epoch [4711/50000], Train Loss: 9847509.0000, Val Loss: 5784781.0000\n",
      "Epoch [4712/50000], Train Loss: 9846096.0000, Val Loss: 5783878.0000\n",
      "Epoch [4713/50000], Train Loss: 9844681.0000, Val Loss: 5782976.0000\n",
      "Epoch [4714/50000], Train Loss: 9843268.0000, Val Loss: 5782075.0000\n",
      "Epoch [4715/50000], Train Loss: 9841856.0000, Val Loss: 5781174.5000\n",
      "Epoch [4716/50000], Train Loss: 9840442.0000, Val Loss: 5780274.5000\n",
      "Epoch [4717/50000], Train Loss: 9839032.0000, Val Loss: 5779375.0000\n",
      "Epoch [4718/50000], Train Loss: 9837622.0000, Val Loss: 5778476.5000\n",
      "Epoch [4719/50000], Train Loss: 9836212.0000, Val Loss: 5777579.5000\n",
      "Epoch [4720/50000], Train Loss: 9834804.0000, Val Loss: 5776682.0000\n",
      "Epoch [4721/50000], Train Loss: 9833395.0000, Val Loss: 5775785.0000\n",
      "Epoch [4722/50000], Train Loss: 9831988.0000, Val Loss: 5774889.5000\n",
      "Epoch [4723/50000], Train Loss: 9830581.0000, Val Loss: 5773994.5000\n",
      "Epoch [4724/50000], Train Loss: 9829175.0000, Val Loss: 5773099.5000\n",
      "Epoch [4725/50000], Train Loss: 9827769.0000, Val Loss: 5772206.0000\n",
      "Epoch [4726/50000], Train Loss: 9826365.0000, Val Loss: 5771313.0000\n",
      "Epoch [4727/50000], Train Loss: 9824961.0000, Val Loss: 5770421.5000\n",
      "Epoch [4728/50000], Train Loss: 9823558.0000, Val Loss: 5769529.5000\n",
      "Epoch [4729/50000], Train Loss: 9822156.0000, Val Loss: 5768638.0000\n",
      "Epoch [4730/50000], Train Loss: 9820756.0000, Val Loss: 5767747.5000\n",
      "Epoch [4731/50000], Train Loss: 9819353.0000, Val Loss: 5766857.5000\n",
      "Epoch [4732/50000], Train Loss: 9817953.0000, Val Loss: 5765968.5000\n",
      "Epoch [4733/50000], Train Loss: 9816552.0000, Val Loss: 5765080.5000\n",
      "Epoch [4734/50000], Train Loss: 9815156.0000, Val Loss: 5764193.0000\n",
      "Epoch [4735/50000], Train Loss: 9813758.0000, Val Loss: 5763306.0000\n",
      "Epoch [4736/50000], Train Loss: 9812361.0000, Val Loss: 5762419.0000\n",
      "Epoch [4737/50000], Train Loss: 9810964.0000, Val Loss: 5761533.5000\n",
      "Epoch [4738/50000], Train Loss: 9809569.0000, Val Loss: 5760649.0000\n",
      "Epoch [4739/50000], Train Loss: 9808173.0000, Val Loss: 5759765.5000\n",
      "Epoch [4740/50000], Train Loss: 9806782.0000, Val Loss: 5758881.5000\n",
      "Epoch [4741/50000], Train Loss: 9805387.0000, Val Loss: 5757999.5000\n",
      "Epoch [4742/50000], Train Loss: 9803995.0000, Val Loss: 5757117.0000\n",
      "Epoch [4743/50000], Train Loss: 9802604.0000, Val Loss: 5756234.5000\n",
      "Epoch [4744/50000], Train Loss: 9801212.0000, Val Loss: 5755354.0000\n",
      "Epoch [4745/50000], Train Loss: 9799823.0000, Val Loss: 5754474.5000\n",
      "Epoch [4746/50000], Train Loss: 9798433.0000, Val Loss: 5753594.5000\n",
      "Epoch [4747/50000], Train Loss: 9797043.0000, Val Loss: 5752714.5000\n",
      "Epoch [4748/50000], Train Loss: 9795654.0000, Val Loss: 5751838.0000\n",
      "Epoch [4749/50000], Train Loss: 9794267.0000, Val Loss: 5750960.5000\n",
      "Epoch [4750/50000], Train Loss: 9792880.0000, Val Loss: 5750083.5000\n",
      "Epoch [4751/50000], Train Loss: 9791496.0000, Val Loss: 5749208.0000\n",
      "Epoch [4752/50000], Train Loss: 9790110.0000, Val Loss: 5748331.0000\n",
      "Epoch [4753/50000], Train Loss: 9788725.0000, Val Loss: 5747456.5000\n",
      "Epoch [4754/50000], Train Loss: 9787341.0000, Val Loss: 5746582.5000\n",
      "Epoch [4755/50000], Train Loss: 9785958.0000, Val Loss: 5745710.0000\n",
      "Epoch [4756/50000], Train Loss: 9784577.0000, Val Loss: 5744837.0000\n",
      "Epoch [4757/50000], Train Loss: 9783195.0000, Val Loss: 5743965.5000\n",
      "Epoch [4758/50000], Train Loss: 9781815.0000, Val Loss: 5743093.0000\n",
      "Epoch [4759/50000], Train Loss: 9780434.0000, Val Loss: 5742223.0000\n",
      "Epoch [4760/50000], Train Loss: 9779054.0000, Val Loss: 5741352.5000\n",
      "Epoch [4761/50000], Train Loss: 9777677.0000, Val Loss: 5740483.0000\n",
      "Epoch [4762/50000], Train Loss: 9776300.0000, Val Loss: 5739615.0000\n",
      "Epoch [4763/50000], Train Loss: 9774923.0000, Val Loss: 5738747.5000\n",
      "Epoch [4764/50000], Train Loss: 9773548.0000, Val Loss: 5737879.0000\n",
      "Epoch [4765/50000], Train Loss: 9772170.0000, Val Loss: 5737013.5000\n",
      "Epoch [4766/50000], Train Loss: 9770797.0000, Val Loss: 5736146.5000\n",
      "Epoch [4767/50000], Train Loss: 9769421.0000, Val Loss: 5735282.0000\n",
      "Epoch [4768/50000], Train Loss: 9768048.0000, Val Loss: 5734417.0000\n",
      "Epoch [4769/50000], Train Loss: 9766676.0000, Val Loss: 5733553.0000\n",
      "Epoch [4770/50000], Train Loss: 9765305.0000, Val Loss: 5732689.0000\n",
      "Epoch [4771/50000], Train Loss: 9763933.0000, Val Loss: 5731827.0000\n",
      "Epoch [4772/50000], Train Loss: 9762563.0000, Val Loss: 5730965.5000\n",
      "Epoch [4773/50000], Train Loss: 9761194.0000, Val Loss: 5730104.5000\n",
      "Epoch [4774/50000], Train Loss: 9759825.0000, Val Loss: 5729243.5000\n",
      "Epoch [4775/50000], Train Loss: 9758457.0000, Val Loss: 5728384.0000\n",
      "Epoch [4776/50000], Train Loss: 9757090.0000, Val Loss: 5727525.0000\n",
      "Epoch [4777/50000], Train Loss: 9755722.0000, Val Loss: 5726665.5000\n",
      "Epoch [4778/50000], Train Loss: 9754357.0000, Val Loss: 5725808.0000\n",
      "Epoch [4779/50000], Train Loss: 9752991.0000, Val Loss: 5724951.0000\n",
      "Epoch [4780/50000], Train Loss: 9751627.0000, Val Loss: 5724094.0000\n",
      "Epoch [4781/50000], Train Loss: 9750263.0000, Val Loss: 5723237.5000\n",
      "Epoch [4782/50000], Train Loss: 9748900.0000, Val Loss: 5722382.5000\n",
      "Epoch [4783/50000], Train Loss: 9747538.0000, Val Loss: 5721528.5000\n",
      "Epoch [4784/50000], Train Loss: 9746178.0000, Val Loss: 5720674.0000\n",
      "Epoch [4785/50000], Train Loss: 9744815.0000, Val Loss: 5719821.0000\n",
      "Epoch [4786/50000], Train Loss: 9743457.0000, Val Loss: 5718968.5000\n",
      "Epoch [4787/50000], Train Loss: 9742096.0000, Val Loss: 5718117.0000\n",
      "Epoch [4788/50000], Train Loss: 9740737.0000, Val Loss: 5717266.0000\n",
      "Epoch [4789/50000], Train Loss: 9739381.0000, Val Loss: 5716415.0000\n",
      "Epoch [4790/50000], Train Loss: 9738023.0000, Val Loss: 5715566.0000\n",
      "Epoch [4791/50000], Train Loss: 9736668.0000, Val Loss: 5714715.5000\n",
      "Epoch [4792/50000], Train Loss: 9735309.0000, Val Loss: 5713867.0000\n",
      "Epoch [4793/50000], Train Loss: 9733955.0000, Val Loss: 5713019.0000\n",
      "Epoch [4794/50000], Train Loss: 9732602.0000, Val Loss: 5712172.5000\n",
      "Epoch [4795/50000], Train Loss: 9731248.0000, Val Loss: 5711326.0000\n",
      "Epoch [4796/50000], Train Loss: 9729898.0000, Val Loss: 5710480.0000\n",
      "Epoch [4797/50000], Train Loss: 9728545.0000, Val Loss: 5709634.0000\n",
      "Epoch [4798/50000], Train Loss: 9727193.0000, Val Loss: 5708790.0000\n",
      "Epoch [4799/50000], Train Loss: 9725843.0000, Val Loss: 5707946.5000\n",
      "Epoch [4800/50000], Train Loss: 9724493.0000, Val Loss: 5707104.0000\n",
      "Epoch [4801/50000], Train Loss: 9723145.0000, Val Loss: 5706261.0000\n",
      "Epoch [4802/50000], Train Loss: 9721795.0000, Val Loss: 5705419.5000\n",
      "Epoch [4803/50000], Train Loss: 9720450.0000, Val Loss: 5704578.0000\n",
      "Epoch [4804/50000], Train Loss: 9719102.0000, Val Loss: 5703737.5000\n",
      "Epoch [4805/50000], Train Loss: 9717757.0000, Val Loss: 5702897.5000\n",
      "Epoch [4806/50000], Train Loss: 9716412.0000, Val Loss: 5702057.5000\n",
      "Epoch [4807/50000], Train Loss: 9715065.0000, Val Loss: 5701220.5000\n",
      "Epoch [4808/50000], Train Loss: 9713722.0000, Val Loss: 5700382.0000\n",
      "Epoch [4809/50000], Train Loss: 9712378.0000, Val Loss: 5699545.5000\n",
      "Epoch [4810/50000], Train Loss: 9711037.0000, Val Loss: 5698707.0000\n",
      "Epoch [4811/50000], Train Loss: 9709693.0000, Val Loss: 5697873.0000\n",
      "Epoch [4812/50000], Train Loss: 9708354.0000, Val Loss: 5697037.5000\n",
      "Epoch [4813/50000], Train Loss: 9707015.0000, Val Loss: 5696202.5000\n",
      "Epoch [4814/50000], Train Loss: 9705674.0000, Val Loss: 5695369.5000\n",
      "Epoch [4815/50000], Train Loss: 9704336.0000, Val Loss: 5694536.0000\n",
      "Epoch [4816/50000], Train Loss: 9703000.0000, Val Loss: 5693703.5000\n",
      "Epoch [4817/50000], Train Loss: 9701661.0000, Val Loss: 5692870.5000\n",
      "Epoch [4818/50000], Train Loss: 9700323.0000, Val Loss: 5692040.5000\n",
      "Epoch [4819/50000], Train Loss: 9698987.0000, Val Loss: 5691209.5000\n",
      "Epoch [4820/50000], Train Loss: 9697653.0000, Val Loss: 5690379.5000\n",
      "Epoch [4821/50000], Train Loss: 9696318.0000, Val Loss: 5689550.5000\n",
      "Epoch [4822/50000], Train Loss: 9694984.0000, Val Loss: 5688722.5000\n",
      "Epoch [4823/50000], Train Loss: 9693652.0000, Val Loss: 5687894.0000\n",
      "Epoch [4824/50000], Train Loss: 9692319.0000, Val Loss: 5687066.5000\n",
      "Epoch [4825/50000], Train Loss: 9690988.0000, Val Loss: 5686240.5000\n",
      "Epoch [4826/50000], Train Loss: 9689658.0000, Val Loss: 5685415.0000\n",
      "Epoch [4827/50000], Train Loss: 9688329.0000, Val Loss: 5684589.0000\n",
      "Epoch [4828/50000], Train Loss: 9686998.0000, Val Loss: 5683766.0000\n",
      "Epoch [4829/50000], Train Loss: 9685671.0000, Val Loss: 5682941.5000\n",
      "Epoch [4830/50000], Train Loss: 9684343.0000, Val Loss: 5682118.0000\n",
      "Epoch [4831/50000], Train Loss: 9683016.0000, Val Loss: 5681295.5000\n",
      "Epoch [4832/50000], Train Loss: 9681690.0000, Val Loss: 5680473.5000\n",
      "Epoch [4833/50000], Train Loss: 9680364.0000, Val Loss: 5679652.5000\n",
      "Epoch [4834/50000], Train Loss: 9679040.0000, Val Loss: 5678831.5000\n",
      "Epoch [4835/50000], Train Loss: 9677715.0000, Val Loss: 5678011.0000\n",
      "Epoch [4836/50000], Train Loss: 9676390.0000, Val Loss: 5677191.5000\n",
      "Epoch [4837/50000], Train Loss: 9675069.0000, Val Loss: 5676373.5000\n",
      "Epoch [4838/50000], Train Loss: 9673746.0000, Val Loss: 5675555.5000\n",
      "Epoch [4839/50000], Train Loss: 9672426.0000, Val Loss: 5674738.0000\n",
      "Epoch [4840/50000], Train Loss: 9671104.0000, Val Loss: 5673922.0000\n",
      "Epoch [4841/50000], Train Loss: 9669786.0000, Val Loss: 5673106.0000\n",
      "Epoch [4842/50000], Train Loss: 9668468.0000, Val Loss: 5672291.0000\n",
      "Epoch [4843/50000], Train Loss: 9667148.0000, Val Loss: 5671476.5000\n",
      "Epoch [4844/50000], Train Loss: 9665831.0000, Val Loss: 5670662.5000\n",
      "Epoch [4845/50000], Train Loss: 9664515.0000, Val Loss: 5669848.5000\n",
      "Epoch [4846/50000], Train Loss: 9663197.0000, Val Loss: 5669035.5000\n",
      "Epoch [4847/50000], Train Loss: 9661881.0000, Val Loss: 5668224.0000\n",
      "Epoch [4848/50000], Train Loss: 9660568.0000, Val Loss: 5667412.5000\n",
      "Epoch [4849/50000], Train Loss: 9659253.0000, Val Loss: 5666601.5000\n",
      "Epoch [4850/50000], Train Loss: 9657940.0000, Val Loss: 5665792.0000\n",
      "Epoch [4851/50000], Train Loss: 9656629.0000, Val Loss: 5664982.5000\n",
      "Epoch [4852/50000], Train Loss: 9655316.0000, Val Loss: 5664175.0000\n",
      "Epoch [4853/50000], Train Loss: 9654006.0000, Val Loss: 5663366.5000\n",
      "Epoch [4854/50000], Train Loss: 9652696.0000, Val Loss: 5662559.0000\n",
      "Epoch [4855/50000], Train Loss: 9651385.0000, Val Loss: 5661751.5000\n",
      "Epoch [4856/50000], Train Loss: 9650077.0000, Val Loss: 5660945.5000\n",
      "Epoch [4857/50000], Train Loss: 9648768.0000, Val Loss: 5660140.5000\n",
      "Epoch [4858/50000], Train Loss: 9647462.0000, Val Loss: 5659336.5000\n",
      "Epoch [4859/50000], Train Loss: 9646156.0000, Val Loss: 5658531.0000\n",
      "Epoch [4860/50000], Train Loss: 9644848.0000, Val Loss: 5657728.5000\n",
      "Epoch [4861/50000], Train Loss: 9643544.0000, Val Loss: 5656925.5000\n",
      "Epoch [4862/50000], Train Loss: 9642239.0000, Val Loss: 5656122.5000\n",
      "Epoch [4863/50000], Train Loss: 9640935.0000, Val Loss: 5655322.0000\n",
      "Epoch [4864/50000], Train Loss: 9639634.0000, Val Loss: 5654520.5000\n",
      "Epoch [4865/50000], Train Loss: 9638330.0000, Val Loss: 5653721.0000\n",
      "Epoch [4866/50000], Train Loss: 9637029.0000, Val Loss: 5652921.0000\n",
      "Epoch [4867/50000], Train Loss: 9635727.0000, Val Loss: 5652121.5000\n",
      "Epoch [4868/50000], Train Loss: 9634426.0000, Val Loss: 5651323.5000\n",
      "Epoch [4869/50000], Train Loss: 9633128.0000, Val Loss: 5650527.0000\n",
      "Epoch [4870/50000], Train Loss: 9631830.0000, Val Loss: 5649730.0000\n",
      "Epoch [4871/50000], Train Loss: 9630532.0000, Val Loss: 5648934.5000\n",
      "Epoch [4872/50000], Train Loss: 9629235.0000, Val Loss: 5648137.5000\n",
      "Epoch [4873/50000], Train Loss: 9627937.0000, Val Loss: 5647343.0000\n",
      "Epoch [4874/50000], Train Loss: 9626641.0000, Val Loss: 5646548.5000\n",
      "Epoch [4875/50000], Train Loss: 9625347.0000, Val Loss: 5645755.5000\n",
      "Epoch [4876/50000], Train Loss: 9624054.0000, Val Loss: 5644962.0000\n",
      "Epoch [4877/50000], Train Loss: 9622758.0000, Val Loss: 5644169.5000\n",
      "Epoch [4878/50000], Train Loss: 9621467.0000, Val Loss: 5643377.5000\n",
      "Epoch [4879/50000], Train Loss: 9620172.0000, Val Loss: 5642586.5000\n",
      "Epoch [4880/50000], Train Loss: 9618881.0000, Val Loss: 5641796.5000\n",
      "Epoch [4881/50000], Train Loss: 9617590.0000, Val Loss: 5641007.0000\n",
      "Epoch [4882/50000], Train Loss: 9616301.0000, Val Loss: 5640218.0000\n",
      "Epoch [4883/50000], Train Loss: 9615013.0000, Val Loss: 5639430.0000\n",
      "Epoch [4884/50000], Train Loss: 9613723.0000, Val Loss: 5638642.0000\n",
      "Epoch [4885/50000], Train Loss: 9612435.0000, Val Loss: 5637854.5000\n",
      "Epoch [4886/50000], Train Loss: 9611149.0000, Val Loss: 5637067.5000\n",
      "Epoch [4887/50000], Train Loss: 9609863.0000, Val Loss: 5636281.0000\n",
      "Epoch [4888/50000], Train Loss: 9608575.0000, Val Loss: 5635497.0000\n",
      "Epoch [4889/50000], Train Loss: 9607290.0000, Val Loss: 5634713.5000\n",
      "Epoch [4890/50000], Train Loss: 9606008.0000, Val Loss: 5633929.0000\n",
      "Epoch [4891/50000], Train Loss: 9604723.0000, Val Loss: 5633145.5000\n",
      "Epoch [4892/50000], Train Loss: 9603440.0000, Val Loss: 5632362.5000\n",
      "Epoch [4893/50000], Train Loss: 9602158.0000, Val Loss: 5631581.0000\n",
      "Epoch [4894/50000], Train Loss: 9600876.0000, Val Loss: 5630798.5000\n",
      "Epoch [4895/50000], Train Loss: 9599595.0000, Val Loss: 5630019.0000\n",
      "Epoch [4896/50000], Train Loss: 9598317.0000, Val Loss: 5629239.5000\n",
      "Epoch [4897/50000], Train Loss: 9597037.0000, Val Loss: 5628459.0000\n",
      "Epoch [4898/50000], Train Loss: 9595758.0000, Val Loss: 5627681.0000\n",
      "Epoch [4899/50000], Train Loss: 9594480.0000, Val Loss: 5626902.5000\n",
      "Epoch [4900/50000], Train Loss: 9593203.0000, Val Loss: 5626126.0000\n",
      "Epoch [4901/50000], Train Loss: 9591927.0000, Val Loss: 5625349.5000\n",
      "Epoch [4902/50000], Train Loss: 9590650.0000, Val Loss: 5624573.0000\n",
      "Epoch [4903/50000], Train Loss: 9589376.0000, Val Loss: 5623797.5000\n",
      "Epoch [4904/50000], Train Loss: 9588100.0000, Val Loss: 5623021.5000\n",
      "Epoch [4905/50000], Train Loss: 9586827.0000, Val Loss: 5622248.5000\n",
      "Epoch [4906/50000], Train Loss: 9585555.0000, Val Loss: 5621475.5000\n",
      "Epoch [4907/50000], Train Loss: 9584285.0000, Val Loss: 5620702.0000\n",
      "Epoch [4908/50000], Train Loss: 9583012.0000, Val Loss: 5619929.5000\n",
      "Epoch [4909/50000], Train Loss: 9581740.0000, Val Loss: 5619158.5000\n",
      "Epoch [4910/50000], Train Loss: 9580471.0000, Val Loss: 5618387.0000\n",
      "Epoch [4911/50000], Train Loss: 9579201.0000, Val Loss: 5617618.0000\n",
      "Epoch [4912/50000], Train Loss: 9577933.0000, Val Loss: 5616848.5000\n",
      "Epoch [4913/50000], Train Loss: 9576667.0000, Val Loss: 5616079.0000\n",
      "Epoch [4914/50000], Train Loss: 9575399.0000, Val Loss: 5615311.0000\n",
      "Epoch [4915/50000], Train Loss: 9574132.0000, Val Loss: 5614543.0000\n",
      "Epoch [4916/50000], Train Loss: 9572866.0000, Val Loss: 5613776.5000\n",
      "Epoch [4917/50000], Train Loss: 9571602.0000, Val Loss: 5613010.5000\n",
      "Epoch [4918/50000], Train Loss: 9570339.0000, Val Loss: 5612244.5000\n",
      "Epoch [4919/50000], Train Loss: 9569075.0000, Val Loss: 5611479.0000\n",
      "Epoch [4920/50000], Train Loss: 9567810.0000, Val Loss: 5610714.0000\n",
      "Epoch [4921/50000], Train Loss: 9566548.0000, Val Loss: 5609951.5000\n",
      "Epoch [4922/50000], Train Loss: 9565287.0000, Val Loss: 5609187.5000\n",
      "Epoch [4923/50000], Train Loss: 9564026.0000, Val Loss: 5608424.5000\n",
      "Epoch [4924/50000], Train Loss: 9562767.0000, Val Loss: 5607663.5000\n",
      "Epoch [4925/50000], Train Loss: 9561508.0000, Val Loss: 5606902.0000\n",
      "Epoch [4926/50000], Train Loss: 9560248.0000, Val Loss: 5606141.5000\n",
      "Epoch [4927/50000], Train Loss: 9558992.0000, Val Loss: 5605382.5000\n",
      "Epoch [4928/50000], Train Loss: 9557735.0000, Val Loss: 5604622.0000\n",
      "Epoch [4929/50000], Train Loss: 9556478.0000, Val Loss: 5603863.5000\n",
      "Epoch [4930/50000], Train Loss: 9555222.0000, Val Loss: 5603105.0000\n",
      "Epoch [4931/50000], Train Loss: 9553966.0000, Val Loss: 5602347.5000\n",
      "Epoch [4932/50000], Train Loss: 9552712.0000, Val Loss: 5601592.0000\n",
      "Epoch [4933/50000], Train Loss: 9551460.0000, Val Loss: 5600835.5000\n",
      "Epoch [4934/50000], Train Loss: 9550207.0000, Val Loss: 5600079.5000\n",
      "Epoch [4935/50000], Train Loss: 9548954.0000, Val Loss: 5599325.5000\n",
      "Epoch [4936/50000], Train Loss: 9547703.0000, Val Loss: 5598571.0000\n",
      "Epoch [4937/50000], Train Loss: 9546453.0000, Val Loss: 5597817.5000\n",
      "Epoch [4938/50000], Train Loss: 9545203.0000, Val Loss: 5597064.5000\n",
      "Epoch [4939/50000], Train Loss: 9543953.0000, Val Loss: 5596313.0000\n",
      "Epoch [4940/50000], Train Loss: 9542705.0000, Val Loss: 5595560.5000\n",
      "Epoch [4941/50000], Train Loss: 9541456.0000, Val Loss: 5594809.5000\n",
      "Epoch [4942/50000], Train Loss: 9540209.0000, Val Loss: 5594059.0000\n",
      "Epoch [4943/50000], Train Loss: 9538963.0000, Val Loss: 5593311.0000\n",
      "Epoch [4944/50000], Train Loss: 9537719.0000, Val Loss: 5592561.0000\n",
      "Epoch [4945/50000], Train Loss: 9536473.0000, Val Loss: 5591813.5000\n",
      "Epoch [4946/50000], Train Loss: 9535229.0000, Val Loss: 5591064.5000\n",
      "Epoch [4947/50000], Train Loss: 9533982.0000, Val Loss: 5590318.0000\n",
      "Epoch [4948/50000], Train Loss: 9532742.0000, Val Loss: 5589571.0000\n",
      "Epoch [4949/50000], Train Loss: 9531500.0000, Val Loss: 5588825.0000\n",
      "Epoch [4950/50000], Train Loss: 9530257.0000, Val Loss: 5588080.5000\n",
      "Epoch [4951/50000], Train Loss: 9529017.0000, Val Loss: 5587335.5000\n",
      "Epoch [4952/50000], Train Loss: 9527777.0000, Val Loss: 5586592.0000\n",
      "Epoch [4953/50000], Train Loss: 9526537.0000, Val Loss: 5585848.0000\n",
      "Epoch [4954/50000], Train Loss: 9525299.0000, Val Loss: 5585105.0000\n",
      "Epoch [4955/50000], Train Loss: 9524060.0000, Val Loss: 5584364.5000\n",
      "Epoch [4956/50000], Train Loss: 9522824.0000, Val Loss: 5583622.5000\n",
      "Epoch [4957/50000], Train Loss: 9521585.0000, Val Loss: 5582881.5000\n",
      "Epoch [4958/50000], Train Loss: 9520351.0000, Val Loss: 5582142.0000\n",
      "Epoch [4959/50000], Train Loss: 9519117.0000, Val Loss: 5581402.5000\n",
      "Epoch [4960/50000], Train Loss: 9517882.0000, Val Loss: 5580663.5000\n",
      "Epoch [4961/50000], Train Loss: 9516648.0000, Val Loss: 5579924.5000\n",
      "Epoch [4962/50000], Train Loss: 9515414.0000, Val Loss: 5579188.5000\n",
      "Epoch [4963/50000], Train Loss: 9514183.0000, Val Loss: 5578452.5000\n",
      "Epoch [4964/50000], Train Loss: 9512952.0000, Val Loss: 5577715.0000\n",
      "Epoch [4965/50000], Train Loss: 9511720.0000, Val Loss: 5576978.5000\n",
      "Epoch [4966/50000], Train Loss: 9510491.0000, Val Loss: 5576243.5000\n",
      "Epoch [4967/50000], Train Loss: 9509260.0000, Val Loss: 5575510.5000\n",
      "Epoch [4968/50000], Train Loss: 9508033.0000, Val Loss: 5574776.0000\n",
      "Epoch [4969/50000], Train Loss: 9506804.0000, Val Loss: 5574043.5000\n",
      "Epoch [4970/50000], Train Loss: 9505578.0000, Val Loss: 5573310.5000\n",
      "Epoch [4971/50000], Train Loss: 9504350.0000, Val Loss: 5572579.0000\n",
      "Epoch [4972/50000], Train Loss: 9503125.0000, Val Loss: 5571847.5000\n",
      "Epoch [4973/50000], Train Loss: 9501899.0000, Val Loss: 5571118.0000\n",
      "Epoch [4974/50000], Train Loss: 9500675.0000, Val Loss: 5570387.5000\n",
      "Epoch [4975/50000], Train Loss: 9499452.0000, Val Loss: 5569658.0000\n",
      "Epoch [4976/50000], Train Loss: 9498228.0000, Val Loss: 5568930.0000\n",
      "Epoch [4977/50000], Train Loss: 9497006.0000, Val Loss: 5568202.0000\n",
      "Epoch [4978/50000], Train Loss: 9495786.0000, Val Loss: 5567474.0000\n",
      "Epoch [4979/50000], Train Loss: 9494563.0000, Val Loss: 5566747.5000\n",
      "Epoch [4980/50000], Train Loss: 9493342.0000, Val Loss: 5566021.5000\n",
      "Epoch [4981/50000], Train Loss: 9492123.0000, Val Loss: 5565296.5000\n",
      "Epoch [4982/50000], Train Loss: 9490905.0000, Val Loss: 5564570.5000\n",
      "Epoch [4983/50000], Train Loss: 9489686.0000, Val Loss: 5563847.0000\n",
      "Epoch [4984/50000], Train Loss: 9488468.0000, Val Loss: 5563124.5000\n",
      "Epoch [4985/50000], Train Loss: 9487253.0000, Val Loss: 5562400.5000\n",
      "Epoch [4986/50000], Train Loss: 9486035.0000, Val Loss: 5561678.0000\n",
      "Epoch [4987/50000], Train Loss: 9484819.0000, Val Loss: 5560957.0000\n",
      "Epoch [4988/50000], Train Loss: 9483605.0000, Val Loss: 5560235.5000\n",
      "Epoch [4989/50000], Train Loss: 9482392.0000, Val Loss: 5559515.0000\n",
      "Epoch [4990/50000], Train Loss: 9481179.0000, Val Loss: 5558795.5000\n",
      "Epoch [4991/50000], Train Loss: 9479966.0000, Val Loss: 5558075.5000\n",
      "Epoch [4992/50000], Train Loss: 9478754.0000, Val Loss: 5557358.0000\n",
      "Epoch [4993/50000], Train Loss: 9477542.0000, Val Loss: 5556640.0000\n",
      "Epoch [4994/50000], Train Loss: 9476332.0000, Val Loss: 5555922.0000\n",
      "Epoch [4995/50000], Train Loss: 9475124.0000, Val Loss: 5555206.0000\n",
      "Epoch [4996/50000], Train Loss: 9473913.0000, Val Loss: 5554489.5000\n",
      "Epoch [4997/50000], Train Loss: 9472705.0000, Val Loss: 5553774.5000\n",
      "Epoch [4998/50000], Train Loss: 9471497.0000, Val Loss: 5553060.5000\n",
      "Epoch [4999/50000], Train Loss: 9470291.0000, Val Loss: 5552345.5000\n",
      "Epoch [5000/50000], Train Loss: 9469085.0000, Val Loss: 5551632.0000\n",
      "Epoch [5001/50000], Train Loss: 9467880.0000, Val Loss: 5550919.0000\n",
      "Epoch [5002/50000], Train Loss: 9466673.0000, Val Loss: 5550206.5000\n",
      "Epoch [5003/50000], Train Loss: 9465469.0000, Val Loss: 5549495.5000\n",
      "Epoch [5004/50000], Train Loss: 9464266.0000, Val Loss: 5548785.5000\n",
      "Epoch [5005/50000], Train Loss: 9463065.0000, Val Loss: 5548074.5000\n",
      "Epoch [5006/50000], Train Loss: 9461862.0000, Val Loss: 5547365.5000\n",
      "Epoch [5007/50000], Train Loss: 9460661.0000, Val Loss: 5546655.5000\n",
      "Epoch [5008/50000], Train Loss: 9459459.0000, Val Loss: 5545947.0000\n",
      "Epoch [5009/50000], Train Loss: 9458260.0000, Val Loss: 5545240.5000\n",
      "Epoch [5010/50000], Train Loss: 9457062.0000, Val Loss: 5544532.5000\n",
      "Epoch [5011/50000], Train Loss: 9455863.0000, Val Loss: 5543826.5000\n",
      "Epoch [5012/50000], Train Loss: 9454666.0000, Val Loss: 5543120.0000\n",
      "Epoch [5013/50000], Train Loss: 9453468.0000, Val Loss: 5542414.5000\n",
      "Epoch [5014/50000], Train Loss: 9452271.0000, Val Loss: 5541710.5000\n",
      "Epoch [5015/50000], Train Loss: 9451075.0000, Val Loss: 5541006.0000\n",
      "Epoch [5016/50000], Train Loss: 9449881.0000, Val Loss: 5540302.5000\n",
      "Epoch [5017/50000], Train Loss: 9448686.0000, Val Loss: 5539600.0000\n",
      "Epoch [5018/50000], Train Loss: 9447494.0000, Val Loss: 5538898.0000\n",
      "Epoch [5019/50000], Train Loss: 9446301.0000, Val Loss: 5538196.5000\n",
      "Epoch [5020/50000], Train Loss: 9445108.0000, Val Loss: 5537495.5000\n",
      "Epoch [5021/50000], Train Loss: 9443916.0000, Val Loss: 5536795.0000\n",
      "Epoch [5022/50000], Train Loss: 9442726.0000, Val Loss: 5536096.0000\n",
      "Epoch [5023/50000], Train Loss: 9441535.0000, Val Loss: 5535397.0000\n",
      "Epoch [5024/50000], Train Loss: 9440346.0000, Val Loss: 5534697.5000\n",
      "Epoch [5025/50000], Train Loss: 9439157.0000, Val Loss: 5534001.0000\n",
      "Epoch [5026/50000], Train Loss: 9437970.0000, Val Loss: 5533303.0000\n",
      "Epoch [5027/50000], Train Loss: 9436782.0000, Val Loss: 5532606.5000\n",
      "Epoch [5028/50000], Train Loss: 9435594.0000, Val Loss: 5531911.5000\n",
      "Epoch [5029/50000], Train Loss: 9434410.0000, Val Loss: 5531215.5000\n",
      "Epoch [5030/50000], Train Loss: 9433224.0000, Val Loss: 5530520.5000\n",
      "Epoch [5031/50000], Train Loss: 9432038.0000, Val Loss: 5529827.0000\n",
      "Epoch [5032/50000], Train Loss: 9430856.0000, Val Loss: 5529133.5000\n",
      "Epoch [5033/50000], Train Loss: 9429672.0000, Val Loss: 5528441.0000\n",
      "Epoch [5034/50000], Train Loss: 9428489.0000, Val Loss: 5527749.5000\n",
      "Epoch [5035/50000], Train Loss: 9427309.0000, Val Loss: 5527057.0000\n",
      "Epoch [5036/50000], Train Loss: 9426127.0000, Val Loss: 5526367.0000\n",
      "Epoch [5037/50000], Train Loss: 9424948.0000, Val Loss: 5525676.5000\n",
      "Epoch [5038/50000], Train Loss: 9423766.0000, Val Loss: 5524986.5000\n",
      "Epoch [5039/50000], Train Loss: 9422589.0000, Val Loss: 5524297.5000\n",
      "Epoch [5040/50000], Train Loss: 9421410.0000, Val Loss: 5523608.5000\n",
      "Epoch [5041/50000], Train Loss: 9420232.0000, Val Loss: 5522921.5000\n",
      "Epoch [5042/50000], Train Loss: 9419056.0000, Val Loss: 5522234.5000\n",
      "Epoch [5043/50000], Train Loss: 9417878.0000, Val Loss: 5521547.5000\n",
      "Epoch [5044/50000], Train Loss: 9416703.0000, Val Loss: 5520863.0000\n",
      "Epoch [5045/50000], Train Loss: 9415530.0000, Val Loss: 5520176.5000\n",
      "Epoch [5046/50000], Train Loss: 9414356.0000, Val Loss: 5519492.5000\n",
      "Epoch [5047/50000], Train Loss: 9413183.0000, Val Loss: 5518807.5000\n",
      "Epoch [5048/50000], Train Loss: 9412008.0000, Val Loss: 5518125.0000\n",
      "Epoch [5049/50000], Train Loss: 9410837.0000, Val Loss: 5517442.0000\n",
      "Epoch [5050/50000], Train Loss: 9409666.0000, Val Loss: 5516759.5000\n",
      "Epoch [5051/50000], Train Loss: 9408494.0000, Val Loss: 5516078.0000\n",
      "Epoch [5052/50000], Train Loss: 9407325.0000, Val Loss: 5515397.0000\n",
      "Epoch [5053/50000], Train Loss: 9406155.0000, Val Loss: 5514716.5000\n",
      "Epoch [5054/50000], Train Loss: 9404986.0000, Val Loss: 5514037.0000\n",
      "Epoch [5055/50000], Train Loss: 9403819.0000, Val Loss: 5513358.0000\n",
      "Epoch [5056/50000], Train Loss: 9402653.0000, Val Loss: 5512679.5000\n",
      "Epoch [5057/50000], Train Loss: 9401485.0000, Val Loss: 5512001.0000\n",
      "Epoch [5058/50000], Train Loss: 9400319.0000, Val Loss: 5511323.5000\n",
      "Epoch [5059/50000], Train Loss: 9399155.0000, Val Loss: 5510647.5000\n",
      "Epoch [5060/50000], Train Loss: 9397990.0000, Val Loss: 5509971.0000\n",
      "Epoch [5061/50000], Train Loss: 9396825.0000, Val Loss: 5509296.0000\n",
      "Epoch [5062/50000], Train Loss: 9395664.0000, Val Loss: 5508621.5000\n",
      "Epoch [5063/50000], Train Loss: 9394500.0000, Val Loss: 5507947.0000\n",
      "Epoch [5064/50000], Train Loss: 9393340.0000, Val Loss: 5507273.5000\n",
      "Epoch [5065/50000], Train Loss: 9392179.0000, Val Loss: 5506600.5000\n",
      "Epoch [5066/50000], Train Loss: 9391018.0000, Val Loss: 5505928.5000\n",
      "Epoch [5067/50000], Train Loss: 9389858.0000, Val Loss: 5505257.0000\n",
      "Epoch [5068/50000], Train Loss: 9388699.0000, Val Loss: 5504586.5000\n",
      "Epoch [5069/50000], Train Loss: 9387542.0000, Val Loss: 5503915.5000\n",
      "Epoch [5070/50000], Train Loss: 9386384.0000, Val Loss: 5503246.0000\n",
      "Epoch [5071/50000], Train Loss: 9385227.0000, Val Loss: 5502576.0000\n",
      "Epoch [5072/50000], Train Loss: 9384070.0000, Val Loss: 5501907.5000\n",
      "Epoch [5073/50000], Train Loss: 9382915.0000, Val Loss: 5501240.0000\n",
      "Epoch [5074/50000], Train Loss: 9381761.0000, Val Loss: 5500571.5000\n",
      "Epoch [5075/50000], Train Loss: 9380606.0000, Val Loss: 5499906.0000\n",
      "Epoch [5076/50000], Train Loss: 9379453.0000, Val Loss: 5499239.5000\n",
      "Epoch [5077/50000], Train Loss: 9378300.0000, Val Loss: 5498574.0000\n",
      "Epoch [5078/50000], Train Loss: 9377148.0000, Val Loss: 5497909.0000\n",
      "Epoch [5079/50000], Train Loss: 9375997.0000, Val Loss: 5497245.5000\n",
      "Epoch [5080/50000], Train Loss: 9374847.0000, Val Loss: 5496581.0000\n",
      "Epoch [5081/50000], Train Loss: 9373695.0000, Val Loss: 5495919.0000\n",
      "Epoch [5082/50000], Train Loss: 9372548.0000, Val Loss: 5495255.5000\n",
      "Epoch [5083/50000], Train Loss: 9371398.0000, Val Loss: 5494593.0000\n",
      "Epoch [5084/50000], Train Loss: 9370249.0000, Val Loss: 5493932.5000\n",
      "Epoch [5085/50000], Train Loss: 9369103.0000, Val Loss: 5493271.5000\n",
      "Epoch [5086/50000], Train Loss: 9367956.0000, Val Loss: 5492611.0000\n",
      "Epoch [5087/50000], Train Loss: 9366810.0000, Val Loss: 5491952.0000\n",
      "Epoch [5088/50000], Train Loss: 9365665.0000, Val Loss: 5491294.5000\n",
      "Epoch [5089/50000], Train Loss: 9364522.0000, Val Loss: 5490635.5000\n",
      "Epoch [5090/50000], Train Loss: 9363376.0000, Val Loss: 5489978.0000\n",
      "Epoch [5091/50000], Train Loss: 9362234.0000, Val Loss: 5489321.0000\n",
      "Epoch [5092/50000], Train Loss: 9361091.0000, Val Loss: 5488664.5000\n",
      "Epoch [5093/50000], Train Loss: 9359949.0000, Val Loss: 5488008.5000\n",
      "Epoch [5094/50000], Train Loss: 9358808.0000, Val Loss: 5487353.5000\n",
      "Epoch [5095/50000], Train Loss: 9357668.0000, Val Loss: 5486699.0000\n",
      "Epoch [5096/50000], Train Loss: 9356529.0000, Val Loss: 5486045.0000\n",
      "Epoch [5097/50000], Train Loss: 9355389.0000, Val Loss: 5485392.0000\n",
      "Epoch [5098/50000], Train Loss: 9354251.0000, Val Loss: 5484739.5000\n",
      "Epoch [5099/50000], Train Loss: 9353114.0000, Val Loss: 5484087.5000\n",
      "Epoch [5100/50000], Train Loss: 9351976.0000, Val Loss: 5483435.5000\n",
      "Epoch [5101/50000], Train Loss: 9350841.0000, Val Loss: 5482785.0000\n",
      "Epoch [5102/50000], Train Loss: 9349704.0000, Val Loss: 5482133.5000\n",
      "Epoch [5103/50000], Train Loss: 9348568.0000, Val Loss: 5481485.0000\n",
      "Epoch [5104/50000], Train Loss: 9347435.0000, Val Loss: 5480835.0000\n",
      "Epoch [5105/50000], Train Loss: 9346300.0000, Val Loss: 5480187.0000\n",
      "Epoch [5106/50000], Train Loss: 9345168.0000, Val Loss: 5479539.0000\n",
      "Epoch [5107/50000], Train Loss: 9344035.0000, Val Loss: 5478891.5000\n",
      "Epoch [5108/50000], Train Loss: 9342904.0000, Val Loss: 5478245.5000\n",
      "Epoch [5109/50000], Train Loss: 9341772.0000, Val Loss: 5477599.5000\n",
      "Epoch [5110/50000], Train Loss: 9340642.0000, Val Loss: 5476953.5000\n",
      "Epoch [5111/50000], Train Loss: 9339513.0000, Val Loss: 5476309.5000\n",
      "Epoch [5112/50000], Train Loss: 9338385.0000, Val Loss: 5475664.5000\n",
      "Epoch [5113/50000], Train Loss: 9337255.0000, Val Loss: 5475021.5000\n",
      "Epoch [5114/50000], Train Loss: 9336129.0000, Val Loss: 5474378.5000\n",
      "Epoch [5115/50000], Train Loss: 9335001.0000, Val Loss: 5473736.5000\n",
      "Epoch [5116/50000], Train Loss: 9333877.0000, Val Loss: 5473094.0000\n",
      "Epoch [5117/50000], Train Loss: 9332749.0000, Val Loss: 5472453.0000\n",
      "Epoch [5118/50000], Train Loss: 9331624.0000, Val Loss: 5471812.5000\n",
      "Epoch [5119/50000], Train Loss: 9330500.0000, Val Loss: 5471172.5000\n",
      "Epoch [5120/50000], Train Loss: 9329377.0000, Val Loss: 5470534.0000\n",
      "Epoch [5121/50000], Train Loss: 9328255.0000, Val Loss: 5469894.5000\n",
      "Epoch [5122/50000], Train Loss: 9327134.0000, Val Loss: 5469256.5000\n",
      "Epoch [5123/50000], Train Loss: 9326012.0000, Val Loss: 5468618.5000\n",
      "Epoch [5124/50000], Train Loss: 9324891.0000, Val Loss: 5467982.0000\n",
      "Epoch [5125/50000], Train Loss: 9323771.0000, Val Loss: 5467345.5000\n",
      "Epoch [5126/50000], Train Loss: 9322652.0000, Val Loss: 5466710.5000\n",
      "Epoch [5127/50000], Train Loss: 9321533.0000, Val Loss: 5466074.5000\n",
      "Epoch [5128/50000], Train Loss: 9320414.0000, Val Loss: 5465440.5000\n",
      "Epoch [5129/50000], Train Loss: 9319298.0000, Val Loss: 5464806.0000\n",
      "Epoch [5130/50000], Train Loss: 9318180.0000, Val Loss: 5464174.0000\n",
      "Epoch [5131/50000], Train Loss: 9317066.0000, Val Loss: 5463540.5000\n",
      "Epoch [5132/50000], Train Loss: 9315949.0000, Val Loss: 5462908.5000\n",
      "Epoch [5133/50000], Train Loss: 9314836.0000, Val Loss: 5462277.5000\n",
      "Epoch [5134/50000], Train Loss: 9313722.0000, Val Loss: 5461646.5000\n",
      "Epoch [5135/50000], Train Loss: 9312609.0000, Val Loss: 5461015.5000\n",
      "Epoch [5136/50000], Train Loss: 9311496.0000, Val Loss: 5460386.5000\n",
      "Epoch [5137/50000], Train Loss: 9310384.0000, Val Loss: 5459757.0000\n",
      "Epoch [5138/50000], Train Loss: 9309273.0000, Val Loss: 5459128.5000\n",
      "Epoch [5139/50000], Train Loss: 9308162.0000, Val Loss: 5458501.5000\n",
      "Epoch [5140/50000], Train Loss: 9307053.0000, Val Loss: 5457874.0000\n",
      "Epoch [5141/50000], Train Loss: 9305943.0000, Val Loss: 5457247.0000\n",
      "Epoch [5142/50000], Train Loss: 9304834.0000, Val Loss: 5456621.5000\n",
      "Epoch [5143/50000], Train Loss: 9303727.0000, Val Loss: 5455995.5000\n",
      "Epoch [5144/50000], Train Loss: 9302620.0000, Val Loss: 5455370.5000\n",
      "Epoch [5145/50000], Train Loss: 9301513.0000, Val Loss: 5454747.5000\n",
      "Epoch [5146/50000], Train Loss: 9300409.0000, Val Loss: 5454122.5000\n",
      "Epoch [5147/50000], Train Loss: 9299302.0000, Val Loss: 5453500.5000\n",
      "Epoch [5148/50000], Train Loss: 9298199.0000, Val Loss: 5452877.5000\n",
      "Epoch [5149/50000], Train Loss: 9297095.0000, Val Loss: 5452256.0000\n",
      "Epoch [5150/50000], Train Loss: 9295992.0000, Val Loss: 5451635.5000\n",
      "Epoch [5151/50000], Train Loss: 9294891.0000, Val Loss: 5451013.0000\n",
      "Epoch [5152/50000], Train Loss: 9293786.0000, Val Loss: 5450393.5000\n",
      "Epoch [5153/50000], Train Loss: 9292687.0000, Val Loss: 5449774.0000\n",
      "Epoch [5154/50000], Train Loss: 9291584.0000, Val Loss: 5449155.0000\n",
      "Epoch [5155/50000], Train Loss: 9290486.0000, Val Loss: 5448537.0000\n",
      "Epoch [5156/50000], Train Loss: 9289387.0000, Val Loss: 5447919.0000\n",
      "Epoch [5157/50000], Train Loss: 9288288.0000, Val Loss: 5447302.5000\n",
      "Epoch [5158/50000], Train Loss: 9287190.0000, Val Loss: 5446686.0000\n",
      "Epoch [5159/50000], Train Loss: 9286093.0000, Val Loss: 5446070.0000\n",
      "Epoch [5160/50000], Train Loss: 9284997.0000, Val Loss: 5445454.0000\n",
      "Epoch [5161/50000], Train Loss: 9283901.0000, Val Loss: 5444840.5000\n",
      "Epoch [5162/50000], Train Loss: 9282806.0000, Val Loss: 5444226.0000\n",
      "Epoch [5163/50000], Train Loss: 9281713.0000, Val Loss: 5443611.5000\n",
      "Epoch [5164/50000], Train Loss: 9280619.0000, Val Loss: 5442999.5000\n",
      "Epoch [5165/50000], Train Loss: 9279526.0000, Val Loss: 5442387.0000\n",
      "Epoch [5166/50000], Train Loss: 9278433.0000, Val Loss: 5441776.0000\n",
      "Epoch [5167/50000], Train Loss: 9277342.0000, Val Loss: 5441165.0000\n",
      "Epoch [5168/50000], Train Loss: 9276252.0000, Val Loss: 5440554.5000\n",
      "Epoch [5169/50000], Train Loss: 9275161.0000, Val Loss: 5439944.5000\n",
      "Epoch [5170/50000], Train Loss: 9274070.0000, Val Loss: 5439335.0000\n",
      "Epoch [5171/50000], Train Loss: 9272981.0000, Val Loss: 5438725.5000\n",
      "Epoch [5172/50000], Train Loss: 9271894.0000, Val Loss: 5438118.5000\n",
      "Epoch [5173/50000], Train Loss: 9270806.0000, Val Loss: 5437511.0000\n",
      "Epoch [5174/50000], Train Loss: 9269720.0000, Val Loss: 5436904.5000\n",
      "Epoch [5175/50000], Train Loss: 9268634.0000, Val Loss: 5436296.5000\n",
      "Epoch [5176/50000], Train Loss: 9267546.0000, Val Loss: 5435691.0000\n",
      "Epoch [5177/50000], Train Loss: 9266462.0000, Val Loss: 5435087.5000\n",
      "Epoch [5178/50000], Train Loss: 9265379.0000, Val Loss: 5434481.5000\n",
      "Epoch [5179/50000], Train Loss: 9264294.0000, Val Loss: 5433878.0000\n",
      "Epoch [5180/50000], Train Loss: 9263210.0000, Val Loss: 5433274.5000\n",
      "Epoch [5181/50000], Train Loss: 9262128.0000, Val Loss: 5432671.5000\n",
      "Epoch [5182/50000], Train Loss: 9261046.0000, Val Loss: 5432069.5000\n",
      "Epoch [5183/50000], Train Loss: 9259966.0000, Val Loss: 5431467.5000\n",
      "Epoch [5184/50000], Train Loss: 9258885.0000, Val Loss: 5430866.5000\n",
      "Epoch [5185/50000], Train Loss: 9257806.0000, Val Loss: 5430267.5000\n",
      "Epoch [5186/50000], Train Loss: 9256728.0000, Val Loss: 5429667.0000\n",
      "Epoch [5187/50000], Train Loss: 9255649.0000, Val Loss: 5429068.5000\n",
      "Epoch [5188/50000], Train Loss: 9254572.0000, Val Loss: 5428469.5000\n",
      "Epoch [5189/50000], Train Loss: 9253495.0000, Val Loss: 5427871.5000\n",
      "Epoch [5190/50000], Train Loss: 9252419.0000, Val Loss: 5427273.5000\n",
      "Epoch [5191/50000], Train Loss: 9251343.0000, Val Loss: 5426677.0000\n",
      "Epoch [5192/50000], Train Loss: 9250269.0000, Val Loss: 5426081.5000\n",
      "Epoch [5193/50000], Train Loss: 9249196.0000, Val Loss: 5425485.0000\n",
      "Epoch [5194/50000], Train Loss: 9248120.0000, Val Loss: 5424890.0000\n",
      "Epoch [5195/50000], Train Loss: 9247047.0000, Val Loss: 5424296.0000\n",
      "Epoch [5196/50000], Train Loss: 9245975.0000, Val Loss: 5423702.0000\n",
      "Epoch [5197/50000], Train Loss: 9244903.0000, Val Loss: 5423108.5000\n",
      "Epoch [5198/50000], Train Loss: 9243832.0000, Val Loss: 5422515.5000\n",
      "Epoch [5199/50000], Train Loss: 9242762.0000, Val Loss: 5421923.0000\n",
      "Epoch [5200/50000], Train Loss: 9241693.0000, Val Loss: 5421332.5000\n",
      "Epoch [5201/50000], Train Loss: 9240624.0000, Val Loss: 5420741.5000\n",
      "Epoch [5202/50000], Train Loss: 9239555.0000, Val Loss: 5420150.5000\n",
      "Epoch [5203/50000], Train Loss: 9238487.0000, Val Loss: 5419561.0000\n",
      "Epoch [5204/50000], Train Loss: 9237421.0000, Val Loss: 5418973.0000\n",
      "Epoch [5205/50000], Train Loss: 9236355.0000, Val Loss: 5418383.0000\n",
      "Epoch [5206/50000], Train Loss: 9235288.0000, Val Loss: 5417795.5000\n",
      "Epoch [5207/50000], Train Loss: 9234225.0000, Val Loss: 5417208.5000\n",
      "Epoch [5208/50000], Train Loss: 9233161.0000, Val Loss: 5416622.0000\n",
      "Epoch [5209/50000], Train Loss: 9232097.0000, Val Loss: 5416035.0000\n",
      "Epoch [5210/50000], Train Loss: 9231034.0000, Val Loss: 5415449.5000\n",
      "Epoch [5211/50000], Train Loss: 9229971.0000, Val Loss: 5414865.0000\n",
      "Epoch [5212/50000], Train Loss: 9228910.0000, Val Loss: 5414280.0000\n",
      "Epoch [5213/50000], Train Loss: 9227849.0000, Val Loss: 5413696.0000\n",
      "Epoch [5214/50000], Train Loss: 9226788.0000, Val Loss: 5413112.5000\n",
      "Epoch [5215/50000], Train Loss: 9225728.0000, Val Loss: 5412531.0000\n",
      "Epoch [5216/50000], Train Loss: 9224670.0000, Val Loss: 5411949.0000\n",
      "Epoch [5217/50000], Train Loss: 9223612.0000, Val Loss: 5411366.5000\n",
      "Epoch [5218/50000], Train Loss: 9222553.0000, Val Loss: 5410785.5000\n",
      "Epoch [5219/50000], Train Loss: 9221496.0000, Val Loss: 5410205.5000\n",
      "Epoch [5220/50000], Train Loss: 9220440.0000, Val Loss: 5409625.0000\n",
      "Epoch [5221/50000], Train Loss: 9219384.0000, Val Loss: 5409046.5000\n",
      "Epoch [5222/50000], Train Loss: 9218330.0000, Val Loss: 5408467.5000\n",
      "Epoch [5223/50000], Train Loss: 9217276.0000, Val Loss: 5407890.0000\n",
      "Epoch [5224/50000], Train Loss: 9216222.0000, Val Loss: 5407313.0000\n",
      "Epoch [5225/50000], Train Loss: 9215170.0000, Val Loss: 5406736.0000\n",
      "Epoch [5226/50000], Train Loss: 9214118.0000, Val Loss: 5406159.0000\n",
      "Epoch [5227/50000], Train Loss: 9213065.0000, Val Loss: 5405584.0000\n",
      "Epoch [5228/50000], Train Loss: 9212014.0000, Val Loss: 5405009.0000\n",
      "Epoch [5229/50000], Train Loss: 9210965.0000, Val Loss: 5404434.0000\n",
      "Epoch [5230/50000], Train Loss: 9209913.0000, Val Loss: 5403860.5000\n",
      "Epoch [5231/50000], Train Loss: 9208865.0000, Val Loss: 5403286.5000\n",
      "Epoch [5232/50000], Train Loss: 9207817.0000, Val Loss: 5402714.5000\n",
      "Epoch [5233/50000], Train Loss: 9206769.0000, Val Loss: 5402142.5000\n",
      "Epoch [5234/50000], Train Loss: 9205721.0000, Val Loss: 5401570.0000\n",
      "Epoch [5235/50000], Train Loss: 9204675.0000, Val Loss: 5400999.5000\n",
      "Epoch [5236/50000], Train Loss: 9203629.0000, Val Loss: 5400429.0000\n",
      "Epoch [5237/50000], Train Loss: 9202584.0000, Val Loss: 5399859.0000\n",
      "Epoch [5238/50000], Train Loss: 9201539.0000, Val Loss: 5399290.0000\n",
      "Epoch [5239/50000], Train Loss: 9200497.0000, Val Loss: 5398721.0000\n",
      "Epoch [5240/50000], Train Loss: 9199453.0000, Val Loss: 5398153.0000\n",
      "Epoch [5241/50000], Train Loss: 9198411.0000, Val Loss: 5397586.0000\n",
      "Epoch [5242/50000], Train Loss: 9197369.0000, Val Loss: 5397018.5000\n",
      "Epoch [5243/50000], Train Loss: 9196328.0000, Val Loss: 5396452.5000\n",
      "Epoch [5244/50000], Train Loss: 9195287.0000, Val Loss: 5395887.0000\n",
      "Epoch [5245/50000], Train Loss: 9194247.0000, Val Loss: 5395321.5000\n",
      "Epoch [5246/50000], Train Loss: 9193208.0000, Val Loss: 5394756.5000\n",
      "Epoch [5247/50000], Train Loss: 9192170.0000, Val Loss: 5394192.5000\n",
      "Epoch [5248/50000], Train Loss: 9191132.0000, Val Loss: 5393629.5000\n",
      "Epoch [5249/50000], Train Loss: 9190095.0000, Val Loss: 5393066.5000\n",
      "Epoch [5250/50000], Train Loss: 9189058.0000, Val Loss: 5392503.5000\n",
      "Epoch [5251/50000], Train Loss: 9188021.0000, Val Loss: 5391942.5000\n",
      "Epoch [5252/50000], Train Loss: 9186987.0000, Val Loss: 5391381.5000\n",
      "Epoch [5253/50000], Train Loss: 9185951.0000, Val Loss: 5390820.5000\n",
      "Epoch [5254/50000], Train Loss: 9184918.0000, Val Loss: 5390261.0000\n",
      "Epoch [5255/50000], Train Loss: 9183884.0000, Val Loss: 5389701.0000\n",
      "Epoch [5256/50000], Train Loss: 9182852.0000, Val Loss: 5389142.5000\n",
      "Epoch [5257/50000], Train Loss: 9181821.0000, Val Loss: 5388584.5000\n",
      "Epoch [5258/50000], Train Loss: 9180789.0000, Val Loss: 5388026.5000\n",
      "Epoch [5259/50000], Train Loss: 9179759.0000, Val Loss: 5387470.0000\n",
      "Epoch [5260/50000], Train Loss: 9178728.0000, Val Loss: 5386912.5000\n",
      "Epoch [5261/50000], Train Loss: 9177699.0000, Val Loss: 5386358.0000\n",
      "Epoch [5262/50000], Train Loss: 9176670.0000, Val Loss: 5385802.0000\n",
      "Epoch [5263/50000], Train Loss: 9175643.0000, Val Loss: 5385247.5000\n",
      "Epoch [5264/50000], Train Loss: 9174616.0000, Val Loss: 5384693.5000\n",
      "Epoch [5265/50000], Train Loss: 9173589.0000, Val Loss: 5384139.0000\n",
      "Epoch [5266/50000], Train Loss: 9172563.0000, Val Loss: 5383586.5000\n",
      "Epoch [5267/50000], Train Loss: 9171537.0000, Val Loss: 5383033.5000\n",
      "Epoch [5268/50000], Train Loss: 9170511.0000, Val Loss: 5382482.0000\n",
      "Epoch [5269/50000], Train Loss: 9169488.0000, Val Loss: 5381930.0000\n",
      "Epoch [5270/50000], Train Loss: 9168463.0000, Val Loss: 5381379.0000\n",
      "Epoch [5271/50000], Train Loss: 9167442.0000, Val Loss: 5380829.5000\n",
      "Epoch [5272/50000], Train Loss: 9166419.0000, Val Loss: 5380280.0000\n",
      "Epoch [5273/50000], Train Loss: 9165398.0000, Val Loss: 5379731.5000\n",
      "Epoch [5274/50000], Train Loss: 9164378.0000, Val Loss: 5379182.0000\n",
      "Epoch [5275/50000], Train Loss: 9163357.0000, Val Loss: 5378634.5000\n",
      "Epoch [5276/50000], Train Loss: 9162337.0000, Val Loss: 5378087.5000\n",
      "Epoch [5277/50000], Train Loss: 9161318.0000, Val Loss: 5377540.5000\n",
      "Epoch [5278/50000], Train Loss: 9160301.0000, Val Loss: 5376994.5000\n",
      "Epoch [5279/50000], Train Loss: 9159283.0000, Val Loss: 5376448.0000\n",
      "Epoch [5280/50000], Train Loss: 9158265.0000, Val Loss: 5375903.5000\n",
      "Epoch [5281/50000], Train Loss: 9157249.0000, Val Loss: 5375359.5000\n",
      "Epoch [5282/50000], Train Loss: 9156235.0000, Val Loss: 5374815.5000\n",
      "Epoch [5283/50000], Train Loss: 9155219.0000, Val Loss: 5374272.0000\n",
      "Epoch [5284/50000], Train Loss: 9154204.0000, Val Loss: 5373728.5000\n",
      "Epoch [5285/50000], Train Loss: 9153190.0000, Val Loss: 5373187.5000\n",
      "Epoch [5286/50000], Train Loss: 9152179.0000, Val Loss: 5372646.0000\n",
      "Epoch [5287/50000], Train Loss: 9151165.0000, Val Loss: 5372105.0000\n",
      "Epoch [5288/50000], Train Loss: 9150154.0000, Val Loss: 5371564.5000\n",
      "Epoch [5289/50000], Train Loss: 9149142.0000, Val Loss: 5371024.0000\n",
      "Epoch [5290/50000], Train Loss: 9148132.0000, Val Loss: 5370486.0000\n",
      "Epoch [5291/50000], Train Loss: 9147123.0000, Val Loss: 5369946.5000\n",
      "Epoch [5292/50000], Train Loss: 9146114.0000, Val Loss: 5369408.5000\n",
      "Epoch [5293/50000], Train Loss: 9145104.0000, Val Loss: 5368870.5000\n",
      "Epoch [5294/50000], Train Loss: 9144097.0000, Val Loss: 5368334.0000\n",
      "Epoch [5295/50000], Train Loss: 9143090.0000, Val Loss: 5367797.5000\n",
      "Epoch [5296/50000], Train Loss: 9142083.0000, Val Loss: 5367262.0000\n",
      "Epoch [5297/50000], Train Loss: 9141077.0000, Val Loss: 5366726.5000\n",
      "Epoch [5298/50000], Train Loss: 9140072.0000, Val Loss: 5366191.0000\n",
      "Epoch [5299/50000], Train Loss: 9139068.0000, Val Loss: 5365657.5000\n",
      "Epoch [5300/50000], Train Loss: 9138063.0000, Val Loss: 5365123.0000\n",
      "Epoch [5301/50000], Train Loss: 9137060.0000, Val Loss: 5364591.5000\n",
      "Epoch [5302/50000], Train Loss: 9136059.0000, Val Loss: 5364058.5000\n",
      "Epoch [5303/50000], Train Loss: 9135056.0000, Val Loss: 5363526.0000\n",
      "Epoch [5304/50000], Train Loss: 9134055.0000, Val Loss: 5362994.5000\n",
      "Epoch [5305/50000], Train Loss: 9133053.0000, Val Loss: 5362464.0000\n",
      "Epoch [5306/50000], Train Loss: 9132053.0000, Val Loss: 5361934.0000\n",
      "Epoch [5307/50000], Train Loss: 9131053.0000, Val Loss: 5361404.5000\n",
      "Epoch [5308/50000], Train Loss: 9130056.0000, Val Loss: 5360875.0000\n",
      "Epoch [5309/50000], Train Loss: 9129057.0000, Val Loss: 5360346.5000\n",
      "Epoch [5310/50000], Train Loss: 9128059.0000, Val Loss: 5359818.5000\n",
      "Epoch [5311/50000], Train Loss: 9127062.0000, Val Loss: 5359291.5000\n",
      "Epoch [5312/50000], Train Loss: 9126066.0000, Val Loss: 5358765.0000\n",
      "Epoch [5313/50000], Train Loss: 9125072.0000, Val Loss: 5358240.0000\n",
      "Epoch [5314/50000], Train Loss: 9124078.0000, Val Loss: 5357713.5000\n",
      "Epoch [5315/50000], Train Loss: 9123082.0000, Val Loss: 5357187.5000\n",
      "Epoch [5316/50000], Train Loss: 9122087.0000, Val Loss: 5356663.5000\n",
      "Epoch [5317/50000], Train Loss: 9121095.0000, Val Loss: 5356140.5000\n",
      "Epoch [5318/50000], Train Loss: 9120104.0000, Val Loss: 5355616.5000\n",
      "Epoch [5319/50000], Train Loss: 9119111.0000, Val Loss: 5355093.5000\n",
      "Epoch [5320/50000], Train Loss: 9118121.0000, Val Loss: 5354571.5000\n",
      "Epoch [5321/50000], Train Loss: 9117131.0000, Val Loss: 5354050.0000\n",
      "Epoch [5322/50000], Train Loss: 9116140.0000, Val Loss: 5353529.0000\n",
      "Epoch [5323/50000], Train Loss: 9115151.0000, Val Loss: 5353008.5000\n",
      "Epoch [5324/50000], Train Loss: 9114163.0000, Val Loss: 5352488.0000\n",
      "Epoch [5325/50000], Train Loss: 9113175.0000, Val Loss: 5351969.0000\n",
      "Epoch [5326/50000], Train Loss: 9112189.0000, Val Loss: 5351450.0000\n",
      "Epoch [5327/50000], Train Loss: 9111202.0000, Val Loss: 5350931.5000\n",
      "Epoch [5328/50000], Train Loss: 9110215.0000, Val Loss: 5350413.5000\n",
      "Epoch [5329/50000], Train Loss: 9109229.0000, Val Loss: 5349896.5000\n",
      "Epoch [5330/50000], Train Loss: 9108246.0000, Val Loss: 5349379.5000\n",
      "Epoch [5331/50000], Train Loss: 9107262.0000, Val Loss: 5348864.5000\n",
      "Epoch [5332/50000], Train Loss: 9106280.0000, Val Loss: 5348348.5000\n",
      "Epoch [5333/50000], Train Loss: 9105295.0000, Val Loss: 5347834.0000\n",
      "Epoch [5334/50000], Train Loss: 9104314.0000, Val Loss: 5347319.0000\n",
      "Epoch [5335/50000], Train Loss: 9103332.0000, Val Loss: 5346805.5000\n",
      "Epoch [5336/50000], Train Loss: 9102352.0000, Val Loss: 5346292.5000\n",
      "Epoch [5337/50000], Train Loss: 9101372.0000, Val Loss: 5345780.5000\n",
      "Epoch [5338/50000], Train Loss: 9100392.0000, Val Loss: 5345266.5000\n",
      "Epoch [5339/50000], Train Loss: 9099412.0000, Val Loss: 5344756.5000\n",
      "Epoch [5340/50000], Train Loss: 9098434.0000, Val Loss: 5344245.5000\n",
      "Epoch [5341/50000], Train Loss: 9097457.0000, Val Loss: 5343734.5000\n",
      "Epoch [5342/50000], Train Loss: 9096481.0000, Val Loss: 5343224.5000\n",
      "Epoch [5343/50000], Train Loss: 9095504.0000, Val Loss: 5342715.5000\n",
      "Epoch [5344/50000], Train Loss: 9094529.0000, Val Loss: 5342206.5000\n",
      "Epoch [5345/50000], Train Loss: 9093553.0000, Val Loss: 5341698.5000\n",
      "Epoch [5346/50000], Train Loss: 9092580.0000, Val Loss: 5341190.5000\n",
      "Epoch [5347/50000], Train Loss: 9091605.0000, Val Loss: 5340683.5000\n",
      "Epoch [5348/50000], Train Loss: 9090631.0000, Val Loss: 5340177.5000\n",
      "Epoch [5349/50000], Train Loss: 9089660.0000, Val Loss: 5339672.0000\n",
      "Epoch [5350/50000], Train Loss: 9088689.0000, Val Loss: 5339166.0000\n",
      "Epoch [5351/50000], Train Loss: 9087717.0000, Val Loss: 5338661.0000\n",
      "Epoch [5352/50000], Train Loss: 9086745.0000, Val Loss: 5338157.0000\n",
      "Epoch [5353/50000], Train Loss: 9085776.0000, Val Loss: 5337653.5000\n",
      "Epoch [5354/50000], Train Loss: 9084808.0000, Val Loss: 5337150.0000\n",
      "Epoch [5355/50000], Train Loss: 9083838.0000, Val Loss: 5336647.0000\n",
      "Epoch [5356/50000], Train Loss: 9082871.0000, Val Loss: 5336145.0000\n",
      "Epoch [5357/50000], Train Loss: 9081903.0000, Val Loss: 5335643.5000\n",
      "Epoch [5358/50000], Train Loss: 9080938.0000, Val Loss: 5335142.5000\n",
      "Epoch [5359/50000], Train Loss: 9079971.0000, Val Loss: 5334642.5000\n",
      "Epoch [5360/50000], Train Loss: 9079006.0000, Val Loss: 5334142.0000\n",
      "Epoch [5361/50000], Train Loss: 9078040.0000, Val Loss: 5333643.0000\n",
      "Epoch [5362/50000], Train Loss: 9077076.0000, Val Loss: 5333145.0000\n",
      "Epoch [5363/50000], Train Loss: 9076113.0000, Val Loss: 5332646.5000\n",
      "Epoch [5364/50000], Train Loss: 9075151.0000, Val Loss: 5332148.5000\n",
      "Epoch [5365/50000], Train Loss: 9074187.0000, Val Loss: 5331652.5000\n",
      "Epoch [5366/50000], Train Loss: 9073227.0000, Val Loss: 5331155.0000\n",
      "Epoch [5367/50000], Train Loss: 9072263.0000, Val Loss: 5330659.0000\n",
      "Epoch [5368/50000], Train Loss: 9071305.0000, Val Loss: 5330163.5000\n",
      "Epoch [5369/50000], Train Loss: 9070345.0000, Val Loss: 5329669.5000\n",
      "Epoch [5370/50000], Train Loss: 9069385.0000, Val Loss: 5329175.0000\n",
      "Epoch [5371/50000], Train Loss: 9068427.0000, Val Loss: 5328680.5000\n",
      "Epoch [5372/50000], Train Loss: 9067469.0000, Val Loss: 5328187.5000\n",
      "Epoch [5373/50000], Train Loss: 9066512.0000, Val Loss: 5327696.0000\n",
      "Epoch [5374/50000], Train Loss: 9065556.0000, Val Loss: 5327203.0000\n",
      "Epoch [5375/50000], Train Loss: 9064601.0000, Val Loss: 5326712.5000\n",
      "Epoch [5376/50000], Train Loss: 9063645.0000, Val Loss: 5326221.0000\n",
      "Epoch [5377/50000], Train Loss: 9062689.0000, Val Loss: 5325731.0000\n",
      "Epoch [5378/50000], Train Loss: 9061736.0000, Val Loss: 5325240.5000\n",
      "Epoch [5379/50000], Train Loss: 9060783.0000, Val Loss: 5324751.0000\n",
      "Epoch [5380/50000], Train Loss: 9059830.0000, Val Loss: 5324262.5000\n",
      "Epoch [5381/50000], Train Loss: 9058878.0000, Val Loss: 5323775.0000\n",
      "Epoch [5382/50000], Train Loss: 9057926.0000, Val Loss: 5323286.5000\n",
      "Epoch [5383/50000], Train Loss: 9056975.0000, Val Loss: 5322799.5000\n",
      "Epoch [5384/50000], Train Loss: 9056024.0000, Val Loss: 5322313.0000\n",
      "Epoch [5385/50000], Train Loss: 9055076.0000, Val Loss: 5321827.0000\n",
      "Epoch [5386/50000], Train Loss: 9054127.0000, Val Loss: 5321341.5000\n",
      "Epoch [5387/50000], Train Loss: 9053178.0000, Val Loss: 5320857.0000\n",
      "Epoch [5388/50000], Train Loss: 9052231.0000, Val Loss: 5320373.0000\n",
      "Epoch [5389/50000], Train Loss: 9051284.0000, Val Loss: 5319888.0000\n",
      "Epoch [5390/50000], Train Loss: 9050337.0000, Val Loss: 5319405.0000\n",
      "Epoch [5391/50000], Train Loss: 9049391.0000, Val Loss: 5318922.5000\n",
      "Epoch [5392/50000], Train Loss: 9048446.0000, Val Loss: 5318440.5000\n",
      "Epoch [5393/50000], Train Loss: 9047502.0000, Val Loss: 5317959.0000\n",
      "Epoch [5394/50000], Train Loss: 9046557.0000, Val Loss: 5317478.0000\n",
      "Epoch [5395/50000], Train Loss: 9045614.0000, Val Loss: 5316997.5000\n",
      "Epoch [5396/50000], Train Loss: 9044672.0000, Val Loss: 5316517.5000\n",
      "Epoch [5397/50000], Train Loss: 9043730.0000, Val Loss: 5316038.5000\n",
      "Epoch [5398/50000], Train Loss: 9042789.0000, Val Loss: 5315560.0000\n",
      "Epoch [5399/50000], Train Loss: 9041848.0000, Val Loss: 5315081.5000\n",
      "Epoch [5400/50000], Train Loss: 9040908.0000, Val Loss: 5314604.5000\n",
      "Epoch [5401/50000], Train Loss: 9039969.0000, Val Loss: 5314126.5000\n",
      "Epoch [5402/50000], Train Loss: 9039030.0000, Val Loss: 5313650.0000\n",
      "Epoch [5403/50000], Train Loss: 9038091.0000, Val Loss: 5313173.5000\n",
      "Epoch [5404/50000], Train Loss: 9037154.0000, Val Loss: 5312698.5000\n",
      "Epoch [5405/50000], Train Loss: 9036218.0000, Val Loss: 5312223.0000\n",
      "Epoch [5406/50000], Train Loss: 9035281.0000, Val Loss: 5311749.0000\n",
      "Epoch [5407/50000], Train Loss: 9034344.0000, Val Loss: 5311275.0000\n",
      "Epoch [5408/50000], Train Loss: 9033410.0000, Val Loss: 5310801.0000\n",
      "Epoch [5409/50000], Train Loss: 9032474.0000, Val Loss: 5310329.0000\n",
      "Epoch [5410/50000], Train Loss: 9031542.0000, Val Loss: 5309856.0000\n",
      "Epoch [5411/50000], Train Loss: 9030609.0000, Val Loss: 5309384.5000\n",
      "Epoch [5412/50000], Train Loss: 9029675.0000, Val Loss: 5308913.0000\n",
      "Epoch [5413/50000], Train Loss: 9028744.0000, Val Loss: 5308442.5000\n",
      "Epoch [5414/50000], Train Loss: 9027812.0000, Val Loss: 5307972.5000\n",
      "Epoch [5415/50000], Train Loss: 9026882.0000, Val Loss: 5307503.0000\n",
      "Epoch [5416/50000], Train Loss: 9025952.0000, Val Loss: 5307033.5000\n",
      "Epoch [5417/50000], Train Loss: 9025022.0000, Val Loss: 5306565.5000\n",
      "Epoch [5418/50000], Train Loss: 9024093.0000, Val Loss: 5306098.0000\n",
      "Epoch [5419/50000], Train Loss: 9023166.0000, Val Loss: 5305630.0000\n",
      "Epoch [5420/50000], Train Loss: 9022237.0000, Val Loss: 5305163.5000\n",
      "Epoch [5421/50000], Train Loss: 9021311.0000, Val Loss: 5304697.5000\n",
      "Epoch [5422/50000], Train Loss: 9020385.0000, Val Loss: 5304231.5000\n",
      "Epoch [5423/50000], Train Loss: 9019459.0000, Val Loss: 5303766.5000\n",
      "Epoch [5424/50000], Train Loss: 9018533.0000, Val Loss: 5303301.5000\n",
      "Epoch [5425/50000], Train Loss: 9017610.0000, Val Loss: 5302837.5000\n",
      "Epoch [5426/50000], Train Loss: 9016685.0000, Val Loss: 5302373.5000\n",
      "Epoch [5427/50000], Train Loss: 9015762.0000, Val Loss: 5301910.5000\n",
      "Epoch [5428/50000], Train Loss: 9014838.0000, Val Loss: 5301448.5000\n",
      "Epoch [5429/50000], Train Loss: 9013918.0000, Val Loss: 5300986.0000\n",
      "Epoch [5430/50000], Train Loss: 9012996.0000, Val Loss: 5300525.5000\n",
      "Epoch [5431/50000], Train Loss: 9012075.0000, Val Loss: 5300064.0000\n",
      "Epoch [5432/50000], Train Loss: 9011155.0000, Val Loss: 5299603.5000\n",
      "Epoch [5433/50000], Train Loss: 9010236.0000, Val Loss: 5299143.5000\n",
      "Epoch [5434/50000], Train Loss: 9009318.0000, Val Loss: 5298686.0000\n",
      "Epoch [5435/50000], Train Loss: 9008400.0000, Val Loss: 5298226.5000\n",
      "Epoch [5436/50000], Train Loss: 9007482.0000, Val Loss: 5297768.0000\n",
      "Epoch [5437/50000], Train Loss: 9006565.0000, Val Loss: 5297311.0000\n",
      "Epoch [5438/50000], Train Loss: 9005649.0000, Val Loss: 5296853.5000\n",
      "Epoch [5439/50000], Train Loss: 9004734.0000, Val Loss: 5296397.5000\n",
      "Epoch [5440/50000], Train Loss: 9003818.0000, Val Loss: 5295941.5000\n",
      "Epoch [5441/50000], Train Loss: 9002903.0000, Val Loss: 5295485.5000\n",
      "Epoch [5442/50000], Train Loss: 9001988.0000, Val Loss: 5295030.5000\n",
      "Epoch [5443/50000], Train Loss: 9001076.0000, Val Loss: 5294576.0000\n",
      "Epoch [5444/50000], Train Loss: 9000163.0000, Val Loss: 5294122.0000\n",
      "Epoch [5445/50000], Train Loss: 8999251.0000, Val Loss: 5293669.0000\n",
      "Epoch [5446/50000], Train Loss: 8998340.0000, Val Loss: 5293216.0000\n",
      "Epoch [5447/50000], Train Loss: 8997429.0000, Val Loss: 5292764.5000\n",
      "Epoch [5448/50000], Train Loss: 8996519.0000, Val Loss: 5292312.5000\n",
      "Epoch [5449/50000], Train Loss: 8995609.0000, Val Loss: 5291861.5000\n",
      "Epoch [5450/50000], Train Loss: 8994700.0000, Val Loss: 5291411.0000\n",
      "Epoch [5451/50000], Train Loss: 8993794.0000, Val Loss: 5290960.5000\n",
      "Epoch [5452/50000], Train Loss: 8992885.0000, Val Loss: 5290511.0000\n",
      "Epoch [5453/50000], Train Loss: 8991978.0000, Val Loss: 5290063.0000\n",
      "Epoch [5454/50000], Train Loss: 8991071.0000, Val Loss: 5289613.0000\n",
      "Epoch [5455/50000], Train Loss: 8990165.0000, Val Loss: 5289166.0000\n",
      "Epoch [5456/50000], Train Loss: 8989261.0000, Val Loss: 5288718.5000\n",
      "Epoch [5457/50000], Train Loss: 8988356.0000, Val Loss: 5288271.0000\n",
      "Epoch [5458/50000], Train Loss: 8987452.0000, Val Loss: 5287825.0000\n",
      "Epoch [5459/50000], Train Loss: 8986548.0000, Val Loss: 5287378.0000\n",
      "Epoch [5460/50000], Train Loss: 8985646.0000, Val Loss: 5286933.5000\n",
      "Epoch [5461/50000], Train Loss: 8984743.0000, Val Loss: 5286488.5000\n",
      "Epoch [5462/50000], Train Loss: 8983841.0000, Val Loss: 5286045.0000\n",
      "Epoch [5463/50000], Train Loss: 8982942.0000, Val Loss: 5285601.5000\n",
      "Epoch [5464/50000], Train Loss: 8982042.0000, Val Loss: 5285158.5000\n",
      "Epoch [5465/50000], Train Loss: 8981142.0000, Val Loss: 5284715.5000\n",
      "Epoch [5466/50000], Train Loss: 8980243.0000, Val Loss: 5284274.0000\n",
      "Epoch [5467/50000], Train Loss: 8979345.0000, Val Loss: 5283831.5000\n",
      "Epoch [5468/50000], Train Loss: 8978446.0000, Val Loss: 5283391.0000\n",
      "Epoch [5469/50000], Train Loss: 8977549.0000, Val Loss: 5282950.5000\n",
      "Epoch [5470/50000], Train Loss: 8976653.0000, Val Loss: 5282510.5000\n",
      "Epoch [5471/50000], Train Loss: 8975757.0000, Val Loss: 5282070.5000\n",
      "Epoch [5472/50000], Train Loss: 8974861.0000, Val Loss: 5281632.5000\n",
      "Epoch [5473/50000], Train Loss: 8973967.0000, Val Loss: 5281194.0000\n",
      "Epoch [5474/50000], Train Loss: 8973072.0000, Val Loss: 5280757.0000\n",
      "Epoch [5475/50000], Train Loss: 8972181.0000, Val Loss: 5280319.0000\n",
      "Epoch [5476/50000], Train Loss: 8971286.0000, Val Loss: 5279882.0000\n",
      "Epoch [5477/50000], Train Loss: 8970394.0000, Val Loss: 5279446.5000\n",
      "Epoch [5478/50000], Train Loss: 8969503.0000, Val Loss: 5279010.5000\n",
      "Epoch [5479/50000], Train Loss: 8968611.0000, Val Loss: 5278575.0000\n",
      "Epoch [5480/50000], Train Loss: 8967721.0000, Val Loss: 5278141.0000\n",
      "Epoch [5481/50000], Train Loss: 8966832.0000, Val Loss: 5277706.0000\n",
      "Epoch [5482/50000], Train Loss: 8965942.0000, Val Loss: 5277273.5000\n",
      "Epoch [5483/50000], Train Loss: 8965055.0000, Val Loss: 5276840.5000\n",
      "Epoch [5484/50000], Train Loss: 8964166.0000, Val Loss: 5276407.5000\n",
      "Epoch [5485/50000], Train Loss: 8963278.0000, Val Loss: 5275975.5000\n",
      "Epoch [5486/50000], Train Loss: 8962393.0000, Val Loss: 5275544.0000\n",
      "Epoch [5487/50000], Train Loss: 8961506.0000, Val Loss: 5275113.5000\n",
      "Epoch [5488/50000], Train Loss: 8960620.0000, Val Loss: 5274683.0000\n",
      "Epoch [5489/50000], Train Loss: 8959735.0000, Val Loss: 5274253.5000\n",
      "Epoch [5490/50000], Train Loss: 8958851.0000, Val Loss: 5273824.0000\n",
      "Epoch [5491/50000], Train Loss: 8957968.0000, Val Loss: 5273394.5000\n",
      "Epoch [5492/50000], Train Loss: 8957085.0000, Val Loss: 5272967.0000\n",
      "Epoch [5493/50000], Train Loss: 8956202.0000, Val Loss: 5272538.5000\n",
      "Epoch [5494/50000], Train Loss: 8955321.0000, Val Loss: 5272112.0000\n",
      "Epoch [5495/50000], Train Loss: 8954439.0000, Val Loss: 5271685.0000\n",
      "Epoch [5496/50000], Train Loss: 8953558.0000, Val Loss: 5271259.5000\n",
      "Epoch [5497/50000], Train Loss: 8952679.0000, Val Loss: 5270833.0000\n",
      "Epoch [5498/50000], Train Loss: 8951798.0000, Val Loss: 5270407.5000\n",
      "Epoch [5499/50000], Train Loss: 8950920.0000, Val Loss: 5269983.0000\n",
      "Epoch [5500/50000], Train Loss: 8950042.0000, Val Loss: 5269560.5000\n",
      "Epoch [5501/50000], Train Loss: 8949167.0000, Val Loss: 5269136.0000\n",
      "Epoch [5502/50000], Train Loss: 8948289.0000, Val Loss: 5268712.5000\n",
      "Epoch [5503/50000], Train Loss: 8947411.0000, Val Loss: 5268290.0000\n",
      "Epoch [5504/50000], Train Loss: 8946535.0000, Val Loss: 5267868.5000\n",
      "Epoch [5505/50000], Train Loss: 8945660.0000, Val Loss: 5267447.0000\n",
      "Epoch [5506/50000], Train Loss: 8944785.0000, Val Loss: 5267026.0000\n",
      "Epoch [5507/50000], Train Loss: 8943912.0000, Val Loss: 5266606.0000\n",
      "Epoch [5508/50000], Train Loss: 8943039.0000, Val Loss: 5266186.5000\n",
      "Epoch [5509/50000], Train Loss: 8942167.0000, Val Loss: 5265766.5000\n",
      "Epoch [5510/50000], Train Loss: 8941293.0000, Val Loss: 5265347.0000\n",
      "Epoch [5511/50000], Train Loss: 8940422.0000, Val Loss: 5264929.5000\n",
      "Epoch [5512/50000], Train Loss: 8939551.0000, Val Loss: 5264512.0000\n",
      "Epoch [5513/50000], Train Loss: 8938681.0000, Val Loss: 5264095.0000\n",
      "Epoch [5514/50000], Train Loss: 8937813.0000, Val Loss: 5263678.0000\n",
      "Epoch [5515/50000], Train Loss: 8936943.0000, Val Loss: 5263262.0000\n",
      "Epoch [5516/50000], Train Loss: 8936075.0000, Val Loss: 5262846.0000\n",
      "Epoch [5517/50000], Train Loss: 8935207.0000, Val Loss: 5262431.5000\n",
      "Epoch [5518/50000], Train Loss: 8934340.0000, Val Loss: 5262016.5000\n",
      "Epoch [5519/50000], Train Loss: 8933474.0000, Val Loss: 5261601.5000\n",
      "Epoch [5520/50000], Train Loss: 8932607.0000, Val Loss: 5261188.5000\n",
      "Epoch [5521/50000], Train Loss: 8931741.0000, Val Loss: 5260775.5000\n",
      "Epoch [5522/50000], Train Loss: 8930876.0000, Val Loss: 5260363.0000\n",
      "Epoch [5523/50000], Train Loss: 8930014.0000, Val Loss: 5259950.5000\n",
      "Epoch [5524/50000], Train Loss: 8929149.0000, Val Loss: 5259539.0000\n",
      "Epoch [5525/50000], Train Loss: 8928286.0000, Val Loss: 5259128.5000\n",
      "Epoch [5526/50000], Train Loss: 8927423.0000, Val Loss: 5258718.0000\n",
      "Epoch [5527/50000], Train Loss: 8926561.0000, Val Loss: 5258307.5000\n",
      "Epoch [5528/50000], Train Loss: 8925700.0000, Val Loss: 5257898.5000\n",
      "Epoch [5529/50000], Train Loss: 8924840.0000, Val Loss: 5257489.5000\n",
      "Epoch [5530/50000], Train Loss: 8923980.0000, Val Loss: 5257081.0000\n",
      "Epoch [5531/50000], Train Loss: 8923121.0000, Val Loss: 5256673.5000\n",
      "Epoch [5532/50000], Train Loss: 8922262.0000, Val Loss: 5256266.0000\n",
      "Epoch [5533/50000], Train Loss: 8921403.0000, Val Loss: 5255859.5000\n",
      "Epoch [5534/50000], Train Loss: 8920548.0000, Val Loss: 5255453.0000\n",
      "Epoch [5535/50000], Train Loss: 8919690.0000, Val Loss: 5255047.0000\n",
      "Epoch [5536/50000], Train Loss: 8918832.0000, Val Loss: 5254642.0000\n",
      "Epoch [5537/50000], Train Loss: 8917978.0000, Val Loss: 5254237.5000\n",
      "Epoch [5538/50000], Train Loss: 8917123.0000, Val Loss: 5253833.0000\n",
      "Epoch [5539/50000], Train Loss: 8916268.0000, Val Loss: 5253429.5000\n",
      "Epoch [5540/50000], Train Loss: 8915413.0000, Val Loss: 5253026.0000\n",
      "Epoch [5541/50000], Train Loss: 8914561.0000, Val Loss: 5252623.5000\n",
      "Epoch [5542/50000], Train Loss: 8913709.0000, Val Loss: 5252222.0000\n",
      "Epoch [5543/50000], Train Loss: 8912857.0000, Val Loss: 5251818.5000\n",
      "Epoch [5544/50000], Train Loss: 8912004.0000, Val Loss: 5251418.5000\n",
      "Epoch [5545/50000], Train Loss: 8911153.0000, Val Loss: 5251017.5000\n",
      "Epoch [5546/50000], Train Loss: 8910303.0000, Val Loss: 5250617.5000\n",
      "Epoch [5547/50000], Train Loss: 8909453.0000, Val Loss: 5250218.5000\n",
      "Epoch [5548/50000], Train Loss: 8908605.0000, Val Loss: 5249819.0000\n",
      "Epoch [5549/50000], Train Loss: 8907756.0000, Val Loss: 5249421.0000\n",
      "Epoch [5550/50000], Train Loss: 8906908.0000, Val Loss: 5249022.5000\n",
      "Epoch [5551/50000], Train Loss: 8906060.0000, Val Loss: 5248624.5000\n",
      "Epoch [5552/50000], Train Loss: 8905213.0000, Val Loss: 5248227.5000\n",
      "Epoch [5553/50000], Train Loss: 8904368.0000, Val Loss: 5247832.0000\n",
      "Epoch [5554/50000], Train Loss: 8903523.0000, Val Loss: 5247435.5000\n",
      "Epoch [5555/50000], Train Loss: 8902678.0000, Val Loss: 5247039.5000\n",
      "Epoch [5556/50000], Train Loss: 8901832.0000, Val Loss: 5246645.5000\n",
      "Epoch [5557/50000], Train Loss: 8900991.0000, Val Loss: 5246250.5000\n",
      "Epoch [5558/50000], Train Loss: 8900147.0000, Val Loss: 5245857.0000\n",
      "Epoch [5559/50000], Train Loss: 8899304.0000, Val Loss: 5245463.5000\n",
      "Epoch [5560/50000], Train Loss: 8898463.0000, Val Loss: 5245071.0000\n",
      "Epoch [5561/50000], Train Loss: 8897622.0000, Val Loss: 5244678.5000\n",
      "Epoch [5562/50000], Train Loss: 8896780.0000, Val Loss: 5244286.0000\n",
      "Epoch [5563/50000], Train Loss: 8895939.0000, Val Loss: 5243895.5000\n",
      "Epoch [5564/50000], Train Loss: 8895101.0000, Val Loss: 5243504.0000\n",
      "Epoch [5565/50000], Train Loss: 8894262.0000, Val Loss: 5243113.5000\n",
      "Epoch [5566/50000], Train Loss: 8893423.0000, Val Loss: 5242724.5000\n",
      "Epoch [5567/50000], Train Loss: 8892586.0000, Val Loss: 5242334.5000\n",
      "Epoch [5568/50000], Train Loss: 8891750.0000, Val Loss: 5241945.5000\n",
      "Epoch [5569/50000], Train Loss: 8890912.0000, Val Loss: 5241557.5000\n",
      "Epoch [5570/50000], Train Loss: 8890077.0000, Val Loss: 5241170.0000\n",
      "Epoch [5571/50000], Train Loss: 8889241.0000, Val Loss: 5240783.0000\n",
      "Epoch [5572/50000], Train Loss: 8888407.0000, Val Loss: 5240395.5000\n",
      "Epoch [5573/50000], Train Loss: 8887572.0000, Val Loss: 5240009.5000\n",
      "Epoch [5574/50000], Train Loss: 8886739.0000, Val Loss: 5239623.5000\n",
      "Epoch [5575/50000], Train Loss: 8885907.0000, Val Loss: 5239238.5000\n",
      "Epoch [5576/50000], Train Loss: 8885074.0000, Val Loss: 5238854.0000\n",
      "Epoch [5577/50000], Train Loss: 8884240.0000, Val Loss: 5238470.5000\n",
      "Epoch [5578/50000], Train Loss: 8883411.0000, Val Loss: 5238086.5000\n",
      "Epoch [5579/50000], Train Loss: 8882581.0000, Val Loss: 5237702.5000\n",
      "Epoch [5580/50000], Train Loss: 8881750.0000, Val Loss: 5237320.0000\n",
      "Epoch [5581/50000], Train Loss: 8880921.0000, Val Loss: 5236938.0000\n",
      "Epoch [5582/50000], Train Loss: 8880093.0000, Val Loss: 5236556.5000\n",
      "Epoch [5583/50000], Train Loss: 8879263.0000, Val Loss: 5236175.0000\n",
      "Epoch [5584/50000], Train Loss: 8878436.0000, Val Loss: 5235794.0000\n",
      "Epoch [5585/50000], Train Loss: 8877610.0000, Val Loss: 5235414.0000\n",
      "Epoch [5586/50000], Train Loss: 8876783.0000, Val Loss: 5235034.0000\n",
      "Epoch [5587/50000], Train Loss: 8875957.0000, Val Loss: 5234655.5000\n",
      "Epoch [5588/50000], Train Loss: 8875132.0000, Val Loss: 5234277.0000\n",
      "Epoch [5589/50000], Train Loss: 8874308.0000, Val Loss: 5233898.5000\n",
      "Epoch [5590/50000], Train Loss: 8873484.0000, Val Loss: 5233521.0000\n",
      "Epoch [5591/50000], Train Loss: 8872660.0000, Val Loss: 5233143.5000\n",
      "Epoch [5592/50000], Train Loss: 8871837.0000, Val Loss: 5232767.5000\n",
      "Epoch [5593/50000], Train Loss: 8871015.0000, Val Loss: 5232391.0000\n",
      "Epoch [5594/50000], Train Loss: 8870193.0000, Val Loss: 5232015.0000\n",
      "Epoch [5595/50000], Train Loss: 8869370.0000, Val Loss: 5231640.5000\n",
      "Epoch [5596/50000], Train Loss: 8868552.0000, Val Loss: 5231265.5000\n",
      "Epoch [5597/50000], Train Loss: 8867731.0000, Val Loss: 5230891.5000\n",
      "Epoch [5598/50000], Train Loss: 8866911.0000, Val Loss: 5230518.0000\n",
      "Epoch [5599/50000], Train Loss: 8866094.0000, Val Loss: 5230145.0000\n",
      "Epoch [5600/50000], Train Loss: 8865276.0000, Val Loss: 5229773.0000\n",
      "Epoch [5601/50000], Train Loss: 8864459.0000, Val Loss: 5229401.0000\n",
      "Epoch [5602/50000], Train Loss: 8863642.0000, Val Loss: 5229029.0000\n",
      "Epoch [5603/50000], Train Loss: 8862826.0000, Val Loss: 5228658.0000\n",
      "Epoch [5604/50000], Train Loss: 8862009.0000, Val Loss: 5228288.0000\n",
      "Epoch [5605/50000], Train Loss: 8861194.0000, Val Loss: 5227917.5000\n",
      "Epoch [5606/50000], Train Loss: 8860379.0000, Val Loss: 5227547.5000\n",
      "Epoch [5607/50000], Train Loss: 8859565.0000, Val Loss: 5227178.5000\n",
      "Epoch [5608/50000], Train Loss: 8858751.0000, Val Loss: 5226810.0000\n",
      "Epoch [5609/50000], Train Loss: 8857939.0000, Val Loss: 5226442.5000\n",
      "Epoch [5610/50000], Train Loss: 8857127.0000, Val Loss: 5226074.5000\n",
      "Epoch [5611/50000], Train Loss: 8856315.0000, Val Loss: 5225707.0000\n",
      "Epoch [5612/50000], Train Loss: 8855504.0000, Val Loss: 5225341.0000\n",
      "Epoch [5613/50000], Train Loss: 8854694.0000, Val Loss: 5224975.0000\n",
      "Epoch [5614/50000], Train Loss: 8853885.0000, Val Loss: 5224609.0000\n",
      "Epoch [5615/50000], Train Loss: 8853074.0000, Val Loss: 5224244.5000\n",
      "Epoch [5616/50000], Train Loss: 8852266.0000, Val Loss: 5223879.5000\n",
      "Epoch [5617/50000], Train Loss: 8851457.0000, Val Loss: 5223516.5000\n",
      "Epoch [5618/50000], Train Loss: 8850652.0000, Val Loss: 5223152.5000\n",
      "Epoch [5619/50000], Train Loss: 8849844.0000, Val Loss: 5222788.5000\n",
      "Epoch [5620/50000], Train Loss: 8849037.0000, Val Loss: 5222426.0000\n",
      "Epoch [5621/50000], Train Loss: 8848231.0000, Val Loss: 5222064.0000\n",
      "Epoch [5622/50000], Train Loss: 8847426.0000, Val Loss: 5221702.5000\n",
      "Epoch [5623/50000], Train Loss: 8846621.0000, Val Loss: 5221342.0000\n",
      "Epoch [5624/50000], Train Loss: 8845818.0000, Val Loss: 5220981.5000\n",
      "Epoch [5625/50000], Train Loss: 8845013.0000, Val Loss: 5220621.5000\n",
      "Epoch [5626/50000], Train Loss: 8844212.0000, Val Loss: 5220261.5000\n",
      "Epoch [5627/50000], Train Loss: 8843409.0000, Val Loss: 5219902.0000\n",
      "Epoch [5628/50000], Train Loss: 8842606.0000, Val Loss: 5219544.0000\n",
      "Epoch [5629/50000], Train Loss: 8841807.0000, Val Loss: 5219185.5000\n",
      "Epoch [5630/50000], Train Loss: 8841005.0000, Val Loss: 5218828.5000\n",
      "Epoch [5631/50000], Train Loss: 8840205.0000, Val Loss: 5218471.5000\n",
      "Epoch [5632/50000], Train Loss: 8839406.0000, Val Loss: 5218115.0000\n",
      "Epoch [5633/50000], Train Loss: 8838609.0000, Val Loss: 5217759.0000\n",
      "Epoch [5634/50000], Train Loss: 8837812.0000, Val Loss: 5217402.5000\n",
      "Epoch [5635/50000], Train Loss: 8837012.0000, Val Loss: 5217047.5000\n",
      "Epoch [5636/50000], Train Loss: 8836215.0000, Val Loss: 5216693.5000\n",
      "Epoch [5637/50000], Train Loss: 8835418.0000, Val Loss: 5216339.0000\n",
      "Epoch [5638/50000], Train Loss: 8834624.0000, Val Loss: 5215985.5000\n",
      "Epoch [5639/50000], Train Loss: 8833828.0000, Val Loss: 5215632.0000\n",
      "Epoch [5640/50000], Train Loss: 8833033.0000, Val Loss: 5215280.0000\n",
      "Epoch [5641/50000], Train Loss: 8832239.0000, Val Loss: 5214928.0000\n",
      "Epoch [5642/50000], Train Loss: 8831445.0000, Val Loss: 5214576.5000\n",
      "Epoch [5643/50000], Train Loss: 8830653.0000, Val Loss: 5214224.5000\n",
      "Epoch [5644/50000], Train Loss: 8829860.0000, Val Loss: 5213874.5000\n",
      "Epoch [5645/50000], Train Loss: 8829070.0000, Val Loss: 5213525.0000\n",
      "Epoch [5646/50000], Train Loss: 8828280.0000, Val Loss: 5213175.0000\n",
      "Epoch [5647/50000], Train Loss: 8827487.0000, Val Loss: 5212825.5000\n",
      "Epoch [5648/50000], Train Loss: 8826698.0000, Val Loss: 5212477.0000\n",
      "Epoch [5649/50000], Train Loss: 8825908.0000, Val Loss: 5212129.0000\n",
      "Epoch [5650/50000], Train Loss: 8825119.0000, Val Loss: 5211781.5000\n",
      "Epoch [5651/50000], Train Loss: 8824331.0000, Val Loss: 5211434.5000\n",
      "Epoch [5652/50000], Train Loss: 8823544.0000, Val Loss: 5211087.5000\n",
      "Epoch [5653/50000], Train Loss: 8822757.0000, Val Loss: 5210741.5000\n",
      "Epoch [5654/50000], Train Loss: 8821971.0000, Val Loss: 5210395.5000\n",
      "Epoch [5655/50000], Train Loss: 8821185.0000, Val Loss: 5210050.5000\n",
      "Epoch [5656/50000], Train Loss: 8820400.0000, Val Loss: 5209705.5000\n",
      "Epoch [5657/50000], Train Loss: 8819614.0000, Val Loss: 5209361.0000\n",
      "Epoch [5658/50000], Train Loss: 8818829.0000, Val Loss: 5209017.5000\n",
      "Epoch [5659/50000], Train Loss: 8818046.0000, Val Loss: 5208674.0000\n",
      "Epoch [5660/50000], Train Loss: 8817264.0000, Val Loss: 5208330.5000\n",
      "Epoch [5661/50000], Train Loss: 8816481.0000, Val Loss: 5207989.0000\n",
      "Epoch [5662/50000], Train Loss: 8815698.0000, Val Loss: 5207647.0000\n",
      "Epoch [5663/50000], Train Loss: 8814918.0000, Val Loss: 5207305.5000\n",
      "Epoch [5664/50000], Train Loss: 8814137.0000, Val Loss: 5206964.5000\n",
      "Epoch [5665/50000], Train Loss: 8813356.0000, Val Loss: 5206624.5000\n",
      "Epoch [5666/50000], Train Loss: 8812575.0000, Val Loss: 5206285.0000\n",
      "Epoch [5667/50000], Train Loss: 8811798.0000, Val Loss: 5205945.0000\n",
      "Epoch [5668/50000], Train Loss: 8811019.0000, Val Loss: 5205607.0000\n",
      "Epoch [5669/50000], Train Loss: 8810243.0000, Val Loss: 5205268.5000\n",
      "Epoch [5670/50000], Train Loss: 8809465.0000, Val Loss: 5204930.5000\n",
      "Epoch [5671/50000], Train Loss: 8808688.0000, Val Loss: 5204592.5000\n",
      "Epoch [5672/50000], Train Loss: 8807911.0000, Val Loss: 5204256.0000\n",
      "Epoch [5673/50000], Train Loss: 8807135.0000, Val Loss: 5203919.5000\n",
      "Epoch [5674/50000], Train Loss: 8806361.0000, Val Loss: 5203583.5000\n",
      "Epoch [5675/50000], Train Loss: 8805587.0000, Val Loss: 5203247.5000\n",
      "Epoch [5676/50000], Train Loss: 8804812.0000, Val Loss: 5202912.5000\n",
      "Epoch [5677/50000], Train Loss: 8804038.0000, Val Loss: 5202578.5000\n",
      "Epoch [5678/50000], Train Loss: 8803267.0000, Val Loss: 5202245.0000\n",
      "Epoch [5679/50000], Train Loss: 8802494.0000, Val Loss: 5201911.5000\n",
      "Epoch [5680/50000], Train Loss: 8801724.0000, Val Loss: 5201578.0000\n",
      "Epoch [5681/50000], Train Loss: 8800952.0000, Val Loss: 5201246.0000\n",
      "Epoch [5682/50000], Train Loss: 8800182.0000, Val Loss: 5200913.0000\n",
      "Epoch [5683/50000], Train Loss: 8799412.0000, Val Loss: 5200581.5000\n",
      "Epoch [5684/50000], Train Loss: 8798643.0000, Val Loss: 5200250.5000\n",
      "Epoch [5685/50000], Train Loss: 8797875.0000, Val Loss: 5199920.0000\n",
      "Epoch [5686/50000], Train Loss: 8797107.0000, Val Loss: 5199590.5000\n",
      "Epoch [5687/50000], Train Loss: 8796339.0000, Val Loss: 5199260.5000\n",
      "Epoch [5688/50000], Train Loss: 8795571.0000, Val Loss: 5198931.0000\n",
      "Epoch [5689/50000], Train Loss: 8794805.0000, Val Loss: 5198602.0000\n",
      "Epoch [5690/50000], Train Loss: 8794040.0000, Val Loss: 5198274.0000\n",
      "Epoch [5691/50000], Train Loss: 8793274.0000, Val Loss: 5197946.5000\n",
      "Epoch [5692/50000], Train Loss: 8792510.0000, Val Loss: 5197619.0000\n",
      "Epoch [5693/50000], Train Loss: 8791746.0000, Val Loss: 5197293.0000\n",
      "Epoch [5694/50000], Train Loss: 8790982.0000, Val Loss: 5196966.5000\n",
      "Epoch [5695/50000], Train Loss: 8790220.0000, Val Loss: 5196640.0000\n",
      "Epoch [5696/50000], Train Loss: 8789457.0000, Val Loss: 5196315.0000\n",
      "Epoch [5697/50000], Train Loss: 8788696.0000, Val Loss: 5195991.5000\n",
      "Epoch [5698/50000], Train Loss: 8787934.0000, Val Loss: 5195666.0000\n",
      "Epoch [5699/50000], Train Loss: 8787173.0000, Val Loss: 5195342.0000\n",
      "Epoch [5700/50000], Train Loss: 8786413.0000, Val Loss: 5195018.5000\n",
      "Epoch [5701/50000], Train Loss: 8785653.0000, Val Loss: 5194696.5000\n",
      "Epoch [5702/50000], Train Loss: 8784895.0000, Val Loss: 5194373.5000\n",
      "Epoch [5703/50000], Train Loss: 8784136.0000, Val Loss: 5194051.0000\n",
      "Epoch [5704/50000], Train Loss: 8783380.0000, Val Loss: 5193730.0000\n",
      "Epoch [5705/50000], Train Loss: 8782621.0000, Val Loss: 5193409.5000\n",
      "Epoch [5706/50000], Train Loss: 8781866.0000, Val Loss: 5193088.5000\n",
      "Epoch [5707/50000], Train Loss: 8781109.0000, Val Loss: 5192769.0000\n",
      "Epoch [5708/50000], Train Loss: 8780354.0000, Val Loss: 5192449.0000\n",
      "Epoch [5709/50000], Train Loss: 8779598.0000, Val Loss: 5192130.0000\n",
      "Epoch [5710/50000], Train Loss: 8778845.0000, Val Loss: 5191811.5000\n",
      "Epoch [5711/50000], Train Loss: 8778091.0000, Val Loss: 5191494.0000\n",
      "Epoch [5712/50000], Train Loss: 8777338.0000, Val Loss: 5191176.0000\n",
      "Epoch [5713/50000], Train Loss: 8776585.0000, Val Loss: 5190858.0000\n",
      "Epoch [5714/50000], Train Loss: 8775832.0000, Val Loss: 5190542.0000\n",
      "Epoch [5715/50000], Train Loss: 8775082.0000, Val Loss: 5190226.0000\n",
      "Epoch [5716/50000], Train Loss: 8774329.0000, Val Loss: 5189910.0000\n",
      "Epoch [5717/50000], Train Loss: 8773580.0000, Val Loss: 5189594.5000\n",
      "Epoch [5718/50000], Train Loss: 8772830.0000, Val Loss: 5189279.0000\n",
      "Epoch [5719/50000], Train Loss: 8772080.0000, Val Loss: 5188966.0000\n",
      "Epoch [5720/50000], Train Loss: 8771332.0000, Val Loss: 5188651.5000\n",
      "Epoch [5721/50000], Train Loss: 8770583.0000, Val Loss: 5188338.5000\n",
      "Epoch [5722/50000], Train Loss: 8769836.0000, Val Loss: 5188025.5000\n",
      "Epoch [5723/50000], Train Loss: 8769089.0000, Val Loss: 5187713.0000\n",
      "Epoch [5724/50000], Train Loss: 8768344.0000, Val Loss: 5187401.5000\n",
      "Epoch [5725/50000], Train Loss: 8767597.0000, Val Loss: 5187089.5000\n",
      "Epoch [5726/50000], Train Loss: 8766851.0000, Val Loss: 5186779.0000\n",
      "Epoch [5727/50000], Train Loss: 8766108.0000, Val Loss: 5186468.5000\n",
      "Epoch [5728/50000], Train Loss: 8765363.0000, Val Loss: 5186158.5000\n",
      "Epoch [5729/50000], Train Loss: 8764619.0000, Val Loss: 5185849.0000\n",
      "Epoch [5730/50000], Train Loss: 8763877.0000, Val Loss: 5185539.5000\n",
      "Epoch [5731/50000], Train Loss: 8763134.0000, Val Loss: 5185231.5000\n",
      "Epoch [5732/50000], Train Loss: 8762392.0000, Val Loss: 5184922.5000\n",
      "Epoch [5733/50000], Train Loss: 8761650.0000, Val Loss: 5184615.5000\n",
      "Epoch [5734/50000], Train Loss: 8760911.0000, Val Loss: 5184308.5000\n",
      "Epoch [5735/50000], Train Loss: 8760171.0000, Val Loss: 5184001.0000\n",
      "Epoch [5736/50000], Train Loss: 8759429.0000, Val Loss: 5183695.5000\n",
      "Epoch [5737/50000], Train Loss: 8758692.0000, Val Loss: 5183389.5000\n",
      "Epoch [5738/50000], Train Loss: 8757953.0000, Val Loss: 5183085.0000\n",
      "Epoch [5739/50000], Train Loss: 8757214.0000, Val Loss: 5182779.5000\n",
      "Epoch [5740/50000], Train Loss: 8756478.0000, Val Loss: 5182474.5000\n",
      "Epoch [5741/50000], Train Loss: 8755738.0000, Val Loss: 5182170.5000\n",
      "Epoch [5742/50000], Train Loss: 8755004.0000, Val Loss: 5181867.5000\n",
      "Epoch [5743/50000], Train Loss: 8754268.0000, Val Loss: 5181565.0000\n",
      "Epoch [5744/50000], Train Loss: 8753533.0000, Val Loss: 5181262.0000\n",
      "Epoch [5745/50000], Train Loss: 8752798.0000, Val Loss: 5180960.0000\n",
      "Epoch [5746/50000], Train Loss: 8752063.0000, Val Loss: 5180659.0000\n",
      "Epoch [5747/50000], Train Loss: 8751331.0000, Val Loss: 5180358.0000\n",
      "Epoch [5748/50000], Train Loss: 8750599.0000, Val Loss: 5180057.0000\n",
      "Epoch [5749/50000], Train Loss: 8749866.0000, Val Loss: 5179757.0000\n",
      "Epoch [5750/50000], Train Loss: 8749133.0000, Val Loss: 5179457.5000\n",
      "Epoch [5751/50000], Train Loss: 8748404.0000, Val Loss: 5179158.5000\n",
      "Epoch [5752/50000], Train Loss: 8747672.0000, Val Loss: 5178859.5000\n",
      "Epoch [5753/50000], Train Loss: 8746943.0000, Val Loss: 5178561.5000\n",
      "Epoch [5754/50000], Train Loss: 8746212.0000, Val Loss: 5178264.0000\n",
      "Epoch [5755/50000], Train Loss: 8745484.0000, Val Loss: 5177967.0000\n",
      "Epoch [5756/50000], Train Loss: 8744757.0000, Val Loss: 5177670.0000\n",
      "Epoch [5757/50000], Train Loss: 8744029.0000, Val Loss: 5177374.0000\n",
      "Epoch [5758/50000], Train Loss: 8743301.0000, Val Loss: 5177077.5000\n",
      "Epoch [5759/50000], Train Loss: 8742574.0000, Val Loss: 5176782.0000\n",
      "Epoch [5760/50000], Train Loss: 8741848.0000, Val Loss: 5176487.5000\n",
      "Epoch [5761/50000], Train Loss: 8741123.0000, Val Loss: 5176193.0000\n",
      "Epoch [5762/50000], Train Loss: 8740399.0000, Val Loss: 5175898.0000\n",
      "Epoch [5763/50000], Train Loss: 8739673.0000, Val Loss: 5175605.0000\n",
      "Epoch [5764/50000], Train Loss: 8738948.0000, Val Loss: 5175312.0000\n",
      "Epoch [5765/50000], Train Loss: 8738227.0000, Val Loss: 5175019.0000\n",
      "Epoch [5766/50000], Train Loss: 8737503.0000, Val Loss: 5174727.5000\n",
      "Epoch [5767/50000], Train Loss: 8736780.0000, Val Loss: 5174435.0000\n",
      "Epoch [5768/50000], Train Loss: 8736060.0000, Val Loss: 5174144.0000\n",
      "Epoch [5769/50000], Train Loss: 8735338.0000, Val Loss: 5173854.0000\n",
      "Epoch [5770/50000], Train Loss: 8734618.0000, Val Loss: 5173563.5000\n",
      "Epoch [5771/50000], Train Loss: 8733897.0000, Val Loss: 5173273.0000\n",
      "Epoch [5772/50000], Train Loss: 8733177.0000, Val Loss: 5172984.0000\n",
      "Epoch [5773/50000], Train Loss: 8732460.0000, Val Loss: 5172695.0000\n",
      "Epoch [5774/50000], Train Loss: 8731741.0000, Val Loss: 5172407.0000\n",
      "Epoch [5775/50000], Train Loss: 8731023.0000, Val Loss: 5172118.5000\n",
      "Epoch [5776/50000], Train Loss: 8730306.0000, Val Loss: 5171831.0000\n",
      "Epoch [5777/50000], Train Loss: 8729589.0000, Val Loss: 5171544.0000\n",
      "Epoch [5778/50000], Train Loss: 8728874.0000, Val Loss: 5171257.0000\n",
      "Epoch [5779/50000], Train Loss: 8728157.0000, Val Loss: 5170971.0000\n",
      "Epoch [5780/50000], Train Loss: 8727442.0000, Val Loss: 5170686.0000\n",
      "Epoch [5781/50000], Train Loss: 8726728.0000, Val Loss: 5170400.0000\n",
      "Epoch [5782/50000], Train Loss: 8726013.0000, Val Loss: 5170115.0000\n",
      "Epoch [5783/50000], Train Loss: 8725300.0000, Val Loss: 5169831.5000\n",
      "Epoch [5784/50000], Train Loss: 8724588.0000, Val Loss: 5169548.5000\n",
      "Epoch [5785/50000], Train Loss: 8723876.0000, Val Loss: 5169264.0000\n",
      "Epoch [5786/50000], Train Loss: 8723164.0000, Val Loss: 5168981.5000\n",
      "Epoch [5787/50000], Train Loss: 8722453.0000, Val Loss: 5168698.5000\n",
      "Epoch [5788/50000], Train Loss: 8721743.0000, Val Loss: 5168416.5000\n",
      "Epoch [5789/50000], Train Loss: 8721031.0000, Val Loss: 5168134.5000\n",
      "Epoch [5790/50000], Train Loss: 8720322.0000, Val Loss: 5167853.0000\n",
      "Epoch [5791/50000], Train Loss: 8719613.0000, Val Loss: 5167573.5000\n",
      "Epoch [5792/50000], Train Loss: 8718905.0000, Val Loss: 5167293.0000\n",
      "Epoch [5793/50000], Train Loss: 8718197.0000, Val Loss: 5167013.5000\n",
      "Epoch [5794/50000], Train Loss: 8717490.0000, Val Loss: 5166734.0000\n",
      "Epoch [5795/50000], Train Loss: 8716783.0000, Val Loss: 5166455.5000\n",
      "Epoch [5796/50000], Train Loss: 8716077.0000, Val Loss: 5166177.0000\n",
      "Epoch [5797/50000], Train Loss: 8715372.0000, Val Loss: 5165899.5000\n",
      "Epoch [5798/50000], Train Loss: 8714667.0000, Val Loss: 5165622.0000\n",
      "Epoch [5799/50000], Train Loss: 8713962.0000, Val Loss: 5165345.0000\n",
      "Epoch [5800/50000], Train Loss: 8713259.0000, Val Loss: 5165069.0000\n",
      "Epoch [5801/50000], Train Loss: 8712555.0000, Val Loss: 5164792.5000\n",
      "Epoch [5802/50000], Train Loss: 8711853.0000, Val Loss: 5164517.5000\n",
      "Epoch [5803/50000], Train Loss: 8711149.0000, Val Loss: 5164241.5000\n",
      "Epoch [5804/50000], Train Loss: 8710447.0000, Val Loss: 5163967.5000\n",
      "Epoch [5805/50000], Train Loss: 8709746.0000, Val Loss: 5163693.0000\n",
      "Epoch [5806/50000], Train Loss: 8709046.0000, Val Loss: 5163419.0000\n",
      "Epoch [5807/50000], Train Loss: 8708346.0000, Val Loss: 5163145.5000\n",
      "Epoch [5808/50000], Train Loss: 8707646.0000, Val Loss: 5162873.5000\n",
      "Epoch [5809/50000], Train Loss: 8706947.0000, Val Loss: 5162601.0000\n",
      "Epoch [5810/50000], Train Loss: 8706250.0000, Val Loss: 5162329.0000\n",
      "Epoch [5811/50000], Train Loss: 8705551.0000, Val Loss: 5162056.5000\n",
      "Epoch [5812/50000], Train Loss: 8704853.0000, Val Loss: 5161786.0000\n",
      "Epoch [5813/50000], Train Loss: 8704157.0000, Val Loss: 5161515.5000\n",
      "Epoch [5814/50000], Train Loss: 8703460.0000, Val Loss: 5161245.5000\n",
      "Epoch [5815/50000], Train Loss: 8702764.0000, Val Loss: 5160976.0000\n",
      "Epoch [5816/50000], Train Loss: 8702069.0000, Val Loss: 5160706.0000\n",
      "Epoch [5817/50000], Train Loss: 8701374.0000, Val Loss: 5160438.0000\n",
      "Epoch [5818/50000], Train Loss: 8700681.0000, Val Loss: 5160169.5000\n",
      "Epoch [5819/50000], Train Loss: 8699987.0000, Val Loss: 5159902.0000\n",
      "Epoch [5820/50000], Train Loss: 8699294.0000, Val Loss: 5159634.0000\n",
      "Epoch [5821/50000], Train Loss: 8698603.0000, Val Loss: 5159367.5000\n",
      "Epoch [5822/50000], Train Loss: 8697909.0000, Val Loss: 5159101.0000\n",
      "Epoch [5823/50000], Train Loss: 8697218.0000, Val Loss: 5158834.0000\n",
      "Epoch [5824/50000], Train Loss: 8696528.0000, Val Loss: 5158569.5000\n",
      "Epoch [5825/50000], Train Loss: 8695838.0000, Val Loss: 5158304.0000\n",
      "Epoch [5826/50000], Train Loss: 8695147.0000, Val Loss: 5158039.0000\n",
      "Epoch [5827/50000], Train Loss: 8694459.0000, Val Loss: 5157775.0000\n",
      "Epoch [5828/50000], Train Loss: 8693771.0000, Val Loss: 5157511.0000\n",
      "Epoch [5829/50000], Train Loss: 8693081.0000, Val Loss: 5157248.0000\n",
      "Epoch [5830/50000], Train Loss: 8692394.0000, Val Loss: 5156985.0000\n",
      "Epoch [5831/50000], Train Loss: 8691706.0000, Val Loss: 5156722.5000\n",
      "Epoch [5832/50000], Train Loss: 8691021.0000, Val Loss: 5156460.5000\n",
      "Epoch [5833/50000], Train Loss: 8690334.0000, Val Loss: 5156199.5000\n",
      "Epoch [5834/50000], Train Loss: 8689650.0000, Val Loss: 5155938.0000\n",
      "Epoch [5835/50000], Train Loss: 8688965.0000, Val Loss: 5155677.5000\n",
      "Epoch [5836/50000], Train Loss: 8688280.0000, Val Loss: 5155417.0000\n",
      "Epoch [5837/50000], Train Loss: 8687596.0000, Val Loss: 5155157.5000\n",
      "Epoch [5838/50000], Train Loss: 8686913.0000, Val Loss: 5154898.5000\n",
      "Epoch [5839/50000], Train Loss: 8686231.0000, Val Loss: 5154639.5000\n",
      "Epoch [5840/50000], Train Loss: 8685549.0000, Val Loss: 5154381.0000\n",
      "Epoch [5841/50000], Train Loss: 8684866.0000, Val Loss: 5154123.0000\n",
      "Epoch [5842/50000], Train Loss: 8684187.0000, Val Loss: 5153865.5000\n",
      "Epoch [5843/50000], Train Loss: 8683507.0000, Val Loss: 5153608.5000\n",
      "Epoch [5844/50000], Train Loss: 8682826.0000, Val Loss: 5153352.0000\n",
      "Epoch [5845/50000], Train Loss: 8682147.0000, Val Loss: 5153096.0000\n",
      "Epoch [5846/50000], Train Loss: 8681469.0000, Val Loss: 5152839.5000\n",
      "Epoch [5847/50000], Train Loss: 8680789.0000, Val Loss: 5152584.5000\n",
      "Epoch [5848/50000], Train Loss: 8680111.0000, Val Loss: 5152329.5000\n",
      "Epoch [5849/50000], Train Loss: 8679434.0000, Val Loss: 5152075.5000\n",
      "Epoch [5850/50000], Train Loss: 8678758.0000, Val Loss: 5151822.0000\n",
      "Epoch [5851/50000], Train Loss: 8678082.0000, Val Loss: 5151568.0000\n",
      "Epoch [5852/50000], Train Loss: 8677407.0000, Val Loss: 5151315.0000\n",
      "Epoch [5853/50000], Train Loss: 8676730.0000, Val Loss: 5151062.5000\n",
      "Epoch [5854/50000], Train Loss: 8676056.0000, Val Loss: 5150809.5000\n",
      "Epoch [5855/50000], Train Loss: 8675382.0000, Val Loss: 5150558.5000\n",
      "Epoch [5856/50000], Train Loss: 8674708.0000, Val Loss: 5150307.0000\n",
      "Epoch [5857/50000], Train Loss: 8674036.0000, Val Loss: 5150056.5000\n",
      "Epoch [5858/50000], Train Loss: 8673364.0000, Val Loss: 5149806.0000\n",
      "Epoch [5859/50000], Train Loss: 8672692.0000, Val Loss: 5149555.5000\n",
      "Epoch [5860/50000], Train Loss: 8672020.0000, Val Loss: 5149306.5000\n",
      "Epoch [5861/50000], Train Loss: 8671350.0000, Val Loss: 5149057.5000\n",
      "Epoch [5862/50000], Train Loss: 8670680.0000, Val Loss: 5148809.0000\n",
      "Epoch [5863/50000], Train Loss: 8670010.0000, Val Loss: 5148560.5000\n",
      "Epoch [5864/50000], Train Loss: 8669342.0000, Val Loss: 5148313.0000\n",
      "Epoch [5865/50000], Train Loss: 8668673.0000, Val Loss: 5148065.5000\n",
      "Epoch [5866/50000], Train Loss: 8668004.0000, Val Loss: 5147819.5000\n",
      "Epoch [5867/50000], Train Loss: 8667338.0000, Val Loss: 5147572.5000\n",
      "Epoch [5868/50000], Train Loss: 8666671.0000, Val Loss: 5147326.0000\n",
      "Epoch [5869/50000], Train Loss: 8666003.0000, Val Loss: 5147081.0000\n",
      "Epoch [5870/50000], Train Loss: 8665337.0000, Val Loss: 5146835.0000\n",
      "Epoch [5871/50000], Train Loss: 8664671.0000, Val Loss: 5146591.0000\n",
      "Epoch [5872/50000], Train Loss: 8664007.0000, Val Loss: 5146346.5000\n",
      "Epoch [5873/50000], Train Loss: 8663342.0000, Val Loss: 5146103.5000\n",
      "Epoch [5874/50000], Train Loss: 8662678.0000, Val Loss: 5145859.0000\n",
      "Epoch [5875/50000], Train Loss: 8662015.0000, Val Loss: 5145617.0000\n",
      "Epoch [5876/50000], Train Loss: 8661354.0000, Val Loss: 5145375.0000\n",
      "Epoch [5877/50000], Train Loss: 8660692.0000, Val Loss: 5145131.5000\n",
      "Epoch [5878/50000], Train Loss: 8660029.0000, Val Loss: 5144890.5000\n",
      "Epoch [5879/50000], Train Loss: 8659367.0000, Val Loss: 5144649.5000\n",
      "Epoch [5880/50000], Train Loss: 8658708.0000, Val Loss: 5144409.0000\n",
      "Epoch [5881/50000], Train Loss: 8658047.0000, Val Loss: 5144168.5000\n",
      "Epoch [5882/50000], Train Loss: 8657387.0000, Val Loss: 5143928.5000\n",
      "Epoch [5883/50000], Train Loss: 8656729.0000, Val Loss: 5143689.5000\n",
      "Epoch [5884/50000], Train Loss: 8656070.0000, Val Loss: 5143450.5000\n",
      "Epoch [5885/50000], Train Loss: 8655413.0000, Val Loss: 5143213.0000\n",
      "Epoch [5886/50000], Train Loss: 8654755.0000, Val Loss: 5142974.0000\n",
      "Epoch [5887/50000], Train Loss: 8654098.0000, Val Loss: 5142736.0000\n",
      "Epoch [5888/50000], Train Loss: 8653442.0000, Val Loss: 5142498.5000\n",
      "Epoch [5889/50000], Train Loss: 8652785.0000, Val Loss: 5142262.5000\n",
      "Epoch [5890/50000], Train Loss: 8652130.0000, Val Loss: 5142025.5000\n",
      "Epoch [5891/50000], Train Loss: 8651476.0000, Val Loss: 5141790.0000\n",
      "Epoch [5892/50000], Train Loss: 8650823.0000, Val Loss: 5141554.5000\n",
      "Epoch [5893/50000], Train Loss: 8650168.0000, Val Loss: 5141319.0000\n",
      "Epoch [5894/50000], Train Loss: 8649515.0000, Val Loss: 5141085.0000\n",
      "Epoch [5895/50000], Train Loss: 8648863.0000, Val Loss: 5140851.0000\n",
      "Epoch [5896/50000], Train Loss: 8648211.0000, Val Loss: 5140617.0000\n",
      "Epoch [5897/50000], Train Loss: 8647558.0000, Val Loss: 5140384.0000\n",
      "Epoch [5898/50000], Train Loss: 8646908.0000, Val Loss: 5140150.5000\n",
      "Epoch [5899/50000], Train Loss: 8646257.0000, Val Loss: 5139918.0000\n",
      "Epoch [5900/50000], Train Loss: 8645607.0000, Val Loss: 5139687.0000\n",
      "Epoch [5901/50000], Train Loss: 8644958.0000, Val Loss: 5139454.5000\n",
      "Epoch [5902/50000], Train Loss: 8644309.0000, Val Loss: 5139223.5000\n",
      "Epoch [5903/50000], Train Loss: 8643660.0000, Val Loss: 5138992.5000\n",
      "Epoch [5904/50000], Train Loss: 8643011.0000, Val Loss: 5138762.5000\n",
      "Epoch [5905/50000], Train Loss: 8642365.0000, Val Loss: 5138533.0000\n",
      "Epoch [5906/50000], Train Loss: 8641718.0000, Val Loss: 5138303.5000\n",
      "Epoch [5907/50000], Train Loss: 8641073.0000, Val Loss: 5138074.0000\n",
      "Epoch [5908/50000], Train Loss: 8640426.0000, Val Loss: 5137846.0000\n",
      "Epoch [5909/50000], Train Loss: 8639780.0000, Val Loss: 5137618.0000\n",
      "Epoch [5910/50000], Train Loss: 8639136.0000, Val Loss: 5137390.0000\n",
      "Epoch [5911/50000], Train Loss: 8638492.0000, Val Loss: 5137162.5000\n",
      "Epoch [5912/50000], Train Loss: 8637849.0000, Val Loss: 5136935.5000\n",
      "Epoch [5913/50000], Train Loss: 8637205.0000, Val Loss: 5136709.5000\n",
      "Epoch [5914/50000], Train Loss: 8636563.0000, Val Loss: 5136483.0000\n",
      "Epoch [5915/50000], Train Loss: 8635921.0000, Val Loss: 5136258.0000\n",
      "Epoch [5916/50000], Train Loss: 8635280.0000, Val Loss: 5136032.5000\n",
      "Epoch [5917/50000], Train Loss: 8634639.0000, Val Loss: 5135808.0000\n",
      "Epoch [5918/50000], Train Loss: 8633999.0000, Val Loss: 5135584.0000\n",
      "Epoch [5919/50000], Train Loss: 8633358.0000, Val Loss: 5135360.0000\n",
      "Epoch [5920/50000], Train Loss: 8632718.0000, Val Loss: 5135137.0000\n",
      "Epoch [5921/50000], Train Loss: 8632080.0000, Val Loss: 5134914.0000\n",
      "Epoch [5922/50000], Train Loss: 8631442.0000, Val Loss: 5134691.0000\n",
      "Epoch [5923/50000], Train Loss: 8630804.0000, Val Loss: 5134469.5000\n",
      "Epoch [5924/50000], Train Loss: 8630166.0000, Val Loss: 5134247.5000\n",
      "Epoch [5925/50000], Train Loss: 8629530.0000, Val Loss: 5134025.5000\n",
      "Epoch [5926/50000], Train Loss: 8628894.0000, Val Loss: 5133805.5000\n",
      "Epoch [5927/50000], Train Loss: 8628258.0000, Val Loss: 5133585.5000\n",
      "Epoch [5928/50000], Train Loss: 8627623.0000, Val Loss: 5133365.0000\n",
      "Epoch [5929/50000], Train Loss: 8626988.0000, Val Loss: 5133145.5000\n",
      "Epoch [5930/50000], Train Loss: 8626355.0000, Val Loss: 5132926.5000\n",
      "Epoch [5931/50000], Train Loss: 8625720.0000, Val Loss: 5132707.5000\n",
      "Epoch [5932/50000], Train Loss: 8625086.0000, Val Loss: 5132489.5000\n",
      "Epoch [5933/50000], Train Loss: 8624455.0000, Val Loss: 5132271.5000\n",
      "Epoch [5934/50000], Train Loss: 8623823.0000, Val Loss: 5132054.5000\n",
      "Epoch [5935/50000], Train Loss: 8623192.0000, Val Loss: 5131837.5000\n",
      "Epoch [5936/50000], Train Loss: 8622561.0000, Val Loss: 5131621.0000\n",
      "Epoch [5937/50000], Train Loss: 8621930.0000, Val Loss: 5131405.0000\n",
      "Epoch [5938/50000], Train Loss: 8621299.0000, Val Loss: 5131188.5000\n",
      "Epoch [5939/50000], Train Loss: 8620670.0000, Val Loss: 5130973.0000\n",
      "Epoch [5940/50000], Train Loss: 8620041.0000, Val Loss: 5130758.0000\n",
      "Epoch [5941/50000], Train Loss: 8619413.0000, Val Loss: 5130544.0000\n",
      "Epoch [5942/50000], Train Loss: 8618785.0000, Val Loss: 5130330.0000\n",
      "Epoch [5943/50000], Train Loss: 8618157.0000, Val Loss: 5130116.5000\n",
      "Epoch [5944/50000], Train Loss: 8617531.0000, Val Loss: 5129903.0000\n",
      "Epoch [5945/50000], Train Loss: 8616904.0000, Val Loss: 5129690.0000\n",
      "Epoch [5946/50000], Train Loss: 8616277.0000, Val Loss: 5129478.0000\n",
      "Epoch [5947/50000], Train Loss: 8615654.0000, Val Loss: 5129265.5000\n",
      "Epoch [5948/50000], Train Loss: 8615028.0000, Val Loss: 5129054.0000\n",
      "Epoch [5949/50000], Train Loss: 8614404.0000, Val Loss: 5128843.5000\n",
      "Epoch [5950/50000], Train Loss: 8613781.0000, Val Loss: 5128632.5000\n",
      "Epoch [5951/50000], Train Loss: 8613156.0000, Val Loss: 5128422.5000\n",
      "Epoch [5952/50000], Train Loss: 8612534.0000, Val Loss: 5128212.5000\n",
      "Epoch [5953/50000], Train Loss: 8611911.0000, Val Loss: 5128003.0000\n",
      "Epoch [5954/50000], Train Loss: 8611290.0000, Val Loss: 5127794.0000\n",
      "Epoch [5955/50000], Train Loss: 8610670.0000, Val Loss: 5127585.0000\n",
      "Epoch [5956/50000], Train Loss: 8610049.0000, Val Loss: 5127377.5000\n",
      "Epoch [5957/50000], Train Loss: 8609429.0000, Val Loss: 5127170.0000\n",
      "Epoch [5958/50000], Train Loss: 8608809.0000, Val Loss: 5126962.0000\n",
      "Epoch [5959/50000], Train Loss: 8608190.0000, Val Loss: 5126755.0000\n",
      "Epoch [5960/50000], Train Loss: 8607571.0000, Val Loss: 5126549.5000\n",
      "Epoch [5961/50000], Train Loss: 8606952.0000, Val Loss: 5126343.0000\n",
      "Epoch [5962/50000], Train Loss: 8606335.0000, Val Loss: 5126137.0000\n",
      "Epoch [5963/50000], Train Loss: 8605718.0000, Val Loss: 5125932.5000\n",
      "Epoch [5964/50000], Train Loss: 8605103.0000, Val Loss: 5125727.5000\n",
      "Epoch [5965/50000], Train Loss: 8604486.0000, Val Loss: 5125523.0000\n",
      "Epoch [5966/50000], Train Loss: 8603871.0000, Val Loss: 5125318.5000\n",
      "Epoch [5967/50000], Train Loss: 8603256.0000, Val Loss: 5125114.5000\n",
      "Epoch [5968/50000], Train Loss: 8602640.0000, Val Loss: 5124912.5000\n",
      "Epoch [5969/50000], Train Loss: 8602027.0000, Val Loss: 5124709.5000\n",
      "Epoch [5970/50000], Train Loss: 8601414.0000, Val Loss: 5124507.5000\n",
      "Epoch [5971/50000], Train Loss: 8600801.0000, Val Loss: 5124305.0000\n",
      "Epoch [5972/50000], Train Loss: 8600189.0000, Val Loss: 5124103.5000\n",
      "Epoch [5973/50000], Train Loss: 8599576.0000, Val Loss: 5123902.5000\n",
      "Epoch [5974/50000], Train Loss: 8598966.0000, Val Loss: 5123702.5000\n",
      "Epoch [5975/50000], Train Loss: 8598355.0000, Val Loss: 5123502.5000\n",
      "Epoch [5976/50000], Train Loss: 8597746.0000, Val Loss: 5123302.0000\n",
      "Epoch [5977/50000], Train Loss: 8597135.0000, Val Loss: 5123103.0000\n",
      "Epoch [5978/50000], Train Loss: 8596526.0000, Val Loss: 5122903.5000\n",
      "Epoch [5979/50000], Train Loss: 8595916.0000, Val Loss: 5122704.5000\n",
      "Epoch [5980/50000], Train Loss: 8595308.0000, Val Loss: 5122507.5000\n",
      "Epoch [5981/50000], Train Loss: 8594700.0000, Val Loss: 5122309.0000\n",
      "Epoch [5982/50000], Train Loss: 8594092.0000, Val Loss: 5122111.5000\n",
      "Epoch [5983/50000], Train Loss: 8593487.0000, Val Loss: 5121915.0000\n",
      "Epoch [5984/50000], Train Loss: 8592881.0000, Val Loss: 5121718.5000\n",
      "Epoch [5985/50000], Train Loss: 8592275.0000, Val Loss: 5121522.0000\n",
      "Epoch [5986/50000], Train Loss: 8591670.0000, Val Loss: 5121326.5000\n",
      "Epoch [5987/50000], Train Loss: 8591065.0000, Val Loss: 5121130.5000\n",
      "Epoch [5988/50000], Train Loss: 8590461.0000, Val Loss: 5120936.0000\n",
      "Epoch [5989/50000], Train Loss: 8589857.0000, Val Loss: 5120741.5000\n",
      "Epoch [5990/50000], Train Loss: 8589253.0000, Val Loss: 5120547.5000\n",
      "Epoch [5991/50000], Train Loss: 8588652.0000, Val Loss: 5120354.0000\n",
      "Epoch [5992/50000], Train Loss: 8588049.0000, Val Loss: 5120161.0000\n",
      "Epoch [5993/50000], Train Loss: 8587447.0000, Val Loss: 5119968.0000\n",
      "Epoch [5994/50000], Train Loss: 8586846.0000, Val Loss: 5119776.0000\n",
      "Epoch [5995/50000], Train Loss: 8586246.0000, Val Loss: 5119584.0000\n",
      "Epoch [5996/50000], Train Loss: 8585647.0000, Val Loss: 5119392.0000\n",
      "Epoch [5997/50000], Train Loss: 8585046.0000, Val Loss: 5119201.0000\n",
      "Epoch [5998/50000], Train Loss: 8584447.0000, Val Loss: 5119010.0000\n",
      "Epoch [5999/50000], Train Loss: 8583850.0000, Val Loss: 5118820.5000\n",
      "Epoch [6000/50000], Train Loss: 8583251.0000, Val Loss: 5118630.0000\n",
      "Epoch [6001/50000], Train Loss: 8582653.0000, Val Loss: 5118440.5000\n",
      "Epoch [6002/50000], Train Loss: 8582056.0000, Val Loss: 5118250.5000\n",
      "Epoch [6003/50000], Train Loss: 8581460.0000, Val Loss: 5118063.0000\n",
      "Epoch [6004/50000], Train Loss: 8580864.0000, Val Loss: 5117874.0000\n",
      "Epoch [6005/50000], Train Loss: 8580268.0000, Val Loss: 5117686.5000\n",
      "Epoch [6006/50000], Train Loss: 8579673.0000, Val Loss: 5117499.0000\n",
      "Epoch [6007/50000], Train Loss: 8579079.0000, Val Loss: 5117312.0000\n",
      "Epoch [6008/50000], Train Loss: 8578484.0000, Val Loss: 5117125.5000\n",
      "Epoch [6009/50000], Train Loss: 8577890.0000, Val Loss: 5116938.5000\n",
      "Epoch [6010/50000], Train Loss: 8577298.0000, Val Loss: 5116752.5000\n",
      "Epoch [6011/50000], Train Loss: 8576706.0000, Val Loss: 5116567.5000\n",
      "Epoch [6012/50000], Train Loss: 8576114.0000, Val Loss: 5116382.0000\n",
      "Epoch [6013/50000], Train Loss: 8575522.0000, Val Loss: 5116197.5000\n",
      "Epoch [6014/50000], Train Loss: 8574932.0000, Val Loss: 5116013.0000\n",
      "Epoch [6015/50000], Train Loss: 8574340.0000, Val Loss: 5115829.5000\n",
      "Epoch [6016/50000], Train Loss: 8573751.0000, Val Loss: 5115646.0000\n",
      "Epoch [6017/50000], Train Loss: 8573161.0000, Val Loss: 5115463.5000\n",
      "Epoch [6018/50000], Train Loss: 8572572.0000, Val Loss: 5115280.0000\n",
      "Epoch [6019/50000], Train Loss: 8571984.0000, Val Loss: 5115098.5000\n",
      "Epoch [6020/50000], Train Loss: 8571396.0000, Val Loss: 5114916.5000\n",
      "Epoch [6021/50000], Train Loss: 8570809.0000, Val Loss: 5114735.0000\n",
      "Epoch [6022/50000], Train Loss: 8570221.0000, Val Loss: 5114553.5000\n",
      "Epoch [6023/50000], Train Loss: 8569635.0000, Val Loss: 5114373.5000\n",
      "Epoch [6024/50000], Train Loss: 8569050.0000, Val Loss: 5114193.0000\n",
      "Epoch [6025/50000], Train Loss: 8568465.0000, Val Loss: 5114013.5000\n",
      "Epoch [6026/50000], Train Loss: 8567880.0000, Val Loss: 5113833.5000\n",
      "Epoch [6027/50000], Train Loss: 8567295.0000, Val Loss: 5113654.5000\n",
      "Epoch [6028/50000], Train Loss: 8566712.0000, Val Loss: 5113475.5000\n",
      "Epoch [6029/50000], Train Loss: 8566127.0000, Val Loss: 5113297.0000\n",
      "Epoch [6030/50000], Train Loss: 8565545.0000, Val Loss: 5113120.0000\n",
      "Epoch [6031/50000], Train Loss: 8564963.0000, Val Loss: 5112942.5000\n",
      "Epoch [6032/50000], Train Loss: 8564381.0000, Val Loss: 5112765.0000\n",
      "Epoch [6033/50000], Train Loss: 8563798.0000, Val Loss: 5112589.0000\n",
      "Epoch [6034/50000], Train Loss: 8563219.0000, Val Loss: 5112413.0000\n",
      "Epoch [6035/50000], Train Loss: 8562637.0000, Val Loss: 5112237.0000\n",
      "Epoch [6036/50000], Train Loss: 8562058.0000, Val Loss: 5112061.5000\n",
      "Epoch [6037/50000], Train Loss: 8561478.0000, Val Loss: 5111886.0000\n",
      "Epoch [6038/50000], Train Loss: 8560899.0000, Val Loss: 5111712.0000\n",
      "Epoch [6039/50000], Train Loss: 8560322.0000, Val Loss: 5111537.0000\n",
      "Epoch [6040/50000], Train Loss: 8559743.0000, Val Loss: 5111363.0000\n",
      "Epoch [6041/50000], Train Loss: 8559165.0000, Val Loss: 5111190.0000\n",
      "Epoch [6042/50000], Train Loss: 8558588.0000, Val Loss: 5111017.0000\n",
      "Epoch [6043/50000], Train Loss: 8558013.0000, Val Loss: 5110844.5000\n",
      "Epoch [6044/50000], Train Loss: 8557437.0000, Val Loss: 5110672.0000\n",
      "Epoch [6045/50000], Train Loss: 8556861.0000, Val Loss: 5110500.5000\n",
      "Epoch [6046/50000], Train Loss: 8556286.0000, Val Loss: 5110328.5000\n",
      "Epoch [6047/50000], Train Loss: 8555711.0000, Val Loss: 5110157.5000\n",
      "Epoch [6048/50000], Train Loss: 8555138.0000, Val Loss: 5109987.0000\n",
      "Epoch [6049/50000], Train Loss: 8554565.0000, Val Loss: 5109817.0000\n",
      "Epoch [6050/50000], Train Loss: 8553992.0000, Val Loss: 5109647.5000\n",
      "Epoch [6051/50000], Train Loss: 8553419.0000, Val Loss: 5109477.5000\n",
      "Epoch [6052/50000], Train Loss: 8552847.0000, Val Loss: 5109308.5000\n",
      "Epoch [6053/50000], Train Loss: 8552277.0000, Val Loss: 5109139.0000\n",
      "Epoch [6054/50000], Train Loss: 8551705.0000, Val Loss: 5108971.0000\n",
      "Epoch [6055/50000], Train Loss: 8551135.0000, Val Loss: 5108803.0000\n",
      "Epoch [6056/50000], Train Loss: 8550565.0000, Val Loss: 5108635.5000\n",
      "Epoch [6057/50000], Train Loss: 8549995.0000, Val Loss: 5108469.5000\n",
      "Epoch [6058/50000], Train Loss: 8549427.0000, Val Loss: 5108302.5000\n",
      "Epoch [6059/50000], Train Loss: 8548858.0000, Val Loss: 5108136.0000\n",
      "Epoch [6060/50000], Train Loss: 8548290.0000, Val Loss: 5107970.0000\n",
      "Epoch [6061/50000], Train Loss: 8547723.0000, Val Loss: 5107804.5000\n",
      "Epoch [6062/50000], Train Loss: 8547156.0000, Val Loss: 5107639.5000\n",
      "Epoch [6063/50000], Train Loss: 8546590.0000, Val Loss: 5107474.5000\n",
      "Epoch [6064/50000], Train Loss: 8546024.0000, Val Loss: 5107310.5000\n",
      "Epoch [6065/50000], Train Loss: 8545459.0000, Val Loss: 5107146.5000\n",
      "Epoch [6066/50000], Train Loss: 8544894.0000, Val Loss: 5106982.5000\n",
      "Epoch [6067/50000], Train Loss: 8544328.0000, Val Loss: 5106819.0000\n",
      "Epoch [6068/50000], Train Loss: 8543765.0000, Val Loss: 5106657.0000\n",
      "Epoch [6069/50000], Train Loss: 8543202.0000, Val Loss: 5106494.0000\n",
      "Epoch [6070/50000], Train Loss: 8542638.0000, Val Loss: 5106332.5000\n",
      "Epoch [6071/50000], Train Loss: 8542077.0000, Val Loss: 5106170.5000\n",
      "Epoch [6072/50000], Train Loss: 8541514.0000, Val Loss: 5106009.5000\n",
      "Epoch [6073/50000], Train Loss: 8540952.0000, Val Loss: 5105849.0000\n",
      "Epoch [6074/50000], Train Loss: 8540392.0000, Val Loss: 5105688.5000\n",
      "Epoch [6075/50000], Train Loss: 8539830.0000, Val Loss: 5105528.0000\n",
      "Epoch [6076/50000], Train Loss: 8539270.0000, Val Loss: 5105368.5000\n",
      "Epoch [6077/50000], Train Loss: 8538711.0000, Val Loss: 5105209.0000\n",
      "Epoch [6078/50000], Train Loss: 8538152.0000, Val Loss: 5105049.5000\n",
      "Epoch [6079/50000], Train Loss: 8537593.0000, Val Loss: 5104891.5000\n",
      "Epoch [6080/50000], Train Loss: 8537036.0000, Val Loss: 5104733.5000\n",
      "Epoch [6081/50000], Train Loss: 8536478.0000, Val Loss: 5104575.5000\n",
      "Epoch [6082/50000], Train Loss: 8535920.0000, Val Loss: 5104418.0000\n",
      "Epoch [6083/50000], Train Loss: 8535363.0000, Val Loss: 5104261.5000\n",
      "Epoch [6084/50000], Train Loss: 8534806.0000, Val Loss: 5104105.0000\n",
      "Epoch [6085/50000], Train Loss: 8534252.0000, Val Loss: 5103949.0000\n",
      "Epoch [6086/50000], Train Loss: 8533697.0000, Val Loss: 5103793.0000\n",
      "Epoch [6087/50000], Train Loss: 8533142.0000, Val Loss: 5103637.5000\n",
      "Epoch [6088/50000], Train Loss: 8532587.0000, Val Loss: 5103482.5000\n",
      "Epoch [6089/50000], Train Loss: 8532034.0000, Val Loss: 5103327.5000\n",
      "Epoch [6090/50000], Train Loss: 8531481.0000, Val Loss: 5103173.5000\n",
      "Epoch [6091/50000], Train Loss: 8530928.0000, Val Loss: 5103019.5000\n",
      "Epoch [6092/50000], Train Loss: 8530376.0000, Val Loss: 5102866.5000\n",
      "Epoch [6093/50000], Train Loss: 8529825.0000, Val Loss: 5102713.0000\n",
      "Epoch [6094/50000], Train Loss: 8529274.0000, Val Loss: 5102560.0000\n",
      "Epoch [6095/50000], Train Loss: 8528723.0000, Val Loss: 5102408.5000\n",
      "Epoch [6096/50000], Train Loss: 8528172.0000, Val Loss: 5102256.0000\n",
      "Epoch [6097/50000], Train Loss: 8527623.0000, Val Loss: 5102104.5000\n",
      "Epoch [6098/50000], Train Loss: 8527074.0000, Val Loss: 5101953.5000\n",
      "Epoch [6099/50000], Train Loss: 8526524.0000, Val Loss: 5101802.5000\n",
      "Epoch [6100/50000], Train Loss: 8525976.0000, Val Loss: 5101652.5000\n",
      "Epoch [6101/50000], Train Loss: 8525429.0000, Val Loss: 5101502.5000\n",
      "Epoch [6102/50000], Train Loss: 8524881.0000, Val Loss: 5101352.5000\n",
      "Epoch [6103/50000], Train Loss: 8524334.0000, Val Loss: 5101203.0000\n",
      "Epoch [6104/50000], Train Loss: 8523788.0000, Val Loss: 5101054.5000\n",
      "Epoch [6105/50000], Train Loss: 8523243.0000, Val Loss: 5100905.5000\n",
      "Epoch [6106/50000], Train Loss: 8522697.0000, Val Loss: 5100757.5000\n",
      "Epoch [6107/50000], Train Loss: 8522151.0000, Val Loss: 5100610.0000\n",
      "Epoch [6108/50000], Train Loss: 8521608.0000, Val Loss: 5100462.5000\n",
      "Epoch [6109/50000], Train Loss: 8521064.0000, Val Loss: 5100315.5000\n",
      "Epoch [6110/50000], Train Loss: 8520520.0000, Val Loss: 5100169.0000\n",
      "Epoch [6111/50000], Train Loss: 8519977.0000, Val Loss: 5100023.0000\n",
      "Epoch [6112/50000], Train Loss: 8519434.0000, Val Loss: 5099877.5000\n",
      "Epoch [6113/50000], Train Loss: 8518893.0000, Val Loss: 5099731.5000\n",
      "Epoch [6114/50000], Train Loss: 8518352.0000, Val Loss: 5099587.0000\n",
      "Epoch [6115/50000], Train Loss: 8517811.0000, Val Loss: 5099442.0000\n",
      "Epoch [6116/50000], Train Loss: 8517270.0000, Val Loss: 5099297.0000\n",
      "Epoch [6117/50000], Train Loss: 8516730.0000, Val Loss: 5099153.5000\n",
      "Epoch [6118/50000], Train Loss: 8516190.0000, Val Loss: 5099009.5000\n",
      "Epoch [6119/50000], Train Loss: 8515650.0000, Val Loss: 5098866.5000\n",
      "Epoch [6120/50000], Train Loss: 8515112.0000, Val Loss: 5098724.5000\n",
      "Epoch [6121/50000], Train Loss: 8514575.0000, Val Loss: 5098582.0000\n",
      "Epoch [6122/50000], Train Loss: 8514038.0000, Val Loss: 5098439.5000\n",
      "Epoch [6123/50000], Train Loss: 8513500.0000, Val Loss: 5098298.0000\n",
      "Epoch [6124/50000], Train Loss: 8512963.0000, Val Loss: 5098156.5000\n",
      "Epoch [6125/50000], Train Loss: 8512427.0000, Val Loss: 5098016.0000\n",
      "Epoch [6126/50000], Train Loss: 8511891.0000, Val Loss: 5097875.0000\n",
      "Epoch [6127/50000], Train Loss: 8511356.0000, Val Loss: 5097734.5000\n",
      "Epoch [6128/50000], Train Loss: 8510820.0000, Val Loss: 5097595.0000\n",
      "Epoch [6129/50000], Train Loss: 8510287.0000, Val Loss: 5097455.5000\n",
      "Epoch [6130/50000], Train Loss: 8509753.0000, Val Loss: 5097316.5000\n",
      "Epoch [6131/50000], Train Loss: 8509219.0000, Val Loss: 5097177.5000\n",
      "Epoch [6132/50000], Train Loss: 8508688.0000, Val Loss: 5097040.0000\n",
      "Epoch [6133/50000], Train Loss: 8508154.0000, Val Loss: 5096902.0000\n",
      "Epoch [6134/50000], Train Loss: 8507622.0000, Val Loss: 5096764.5000\n",
      "Epoch [6135/50000], Train Loss: 8507091.0000, Val Loss: 5096627.0000\n",
      "Epoch [6136/50000], Train Loss: 8506560.0000, Val Loss: 5096490.0000\n",
      "Epoch [6137/50000], Train Loss: 8506029.0000, Val Loss: 5096354.0000\n",
      "Epoch [6138/50000], Train Loss: 8505501.0000, Val Loss: 5096218.0000\n",
      "Epoch [6139/50000], Train Loss: 8504970.0000, Val Loss: 5096082.0000\n",
      "Epoch [6140/50000], Train Loss: 8504441.0000, Val Loss: 5095947.0000\n",
      "Epoch [6141/50000], Train Loss: 8503912.0000, Val Loss: 5095812.5000\n",
      "Epoch [6142/50000], Train Loss: 8503385.0000, Val Loss: 5095677.5000\n",
      "Epoch [6143/50000], Train Loss: 8502858.0000, Val Loss: 5095543.5000\n",
      "Epoch [6144/50000], Train Loss: 8502330.0000, Val Loss: 5095409.0000\n",
      "Epoch [6145/50000], Train Loss: 8501804.0000, Val Loss: 5095275.5000\n",
      "Epoch [6146/50000], Train Loss: 8501277.0000, Val Loss: 5095143.0000\n",
      "Epoch [6147/50000], Train Loss: 8500751.0000, Val Loss: 5095010.0000\n",
      "Epoch [6148/50000], Train Loss: 8500227.0000, Val Loss: 5094877.5000\n",
      "Epoch [6149/50000], Train Loss: 8499702.0000, Val Loss: 5094745.5000\n",
      "Epoch [6150/50000], Train Loss: 8499178.0000, Val Loss: 5094614.5000\n",
      "Epoch [6151/50000], Train Loss: 8498654.0000, Val Loss: 5094483.0000\n",
      "Epoch [6152/50000], Train Loss: 8498131.0000, Val Loss: 5094352.0000\n",
      "Epoch [6153/50000], Train Loss: 8497608.0000, Val Loss: 5094222.0000\n",
      "Epoch [6154/50000], Train Loss: 8497087.0000, Val Loss: 5094091.5000\n",
      "Epoch [6155/50000], Train Loss: 8496564.0000, Val Loss: 5093961.5000\n",
      "Epoch [6156/50000], Train Loss: 8496042.0000, Val Loss: 5093832.5000\n",
      "Epoch [6157/50000], Train Loss: 8495520.0000, Val Loss: 5093703.5000\n",
      "Epoch [6158/50000], Train Loss: 8495001.0000, Val Loss: 5093574.5000\n",
      "Epoch [6159/50000], Train Loss: 8494481.0000, Val Loss: 5093446.5000\n",
      "Epoch [6160/50000], Train Loss: 8493961.0000, Val Loss: 5093318.5000\n",
      "Epoch [6161/50000], Train Loss: 8493442.0000, Val Loss: 5093191.0000\n",
      "Epoch [6162/50000], Train Loss: 8492924.0000, Val Loss: 5093064.0000\n",
      "Epoch [6163/50000], Train Loss: 8492406.0000, Val Loss: 5092937.0000\n",
      "Epoch [6164/50000], Train Loss: 8491889.0000, Val Loss: 5092810.5000\n",
      "Epoch [6165/50000], Train Loss: 8491371.0000, Val Loss: 5092685.0000\n",
      "Epoch [6166/50000], Train Loss: 8490854.0000, Val Loss: 5092559.0000\n",
      "Epoch [6167/50000], Train Loss: 8490338.0000, Val Loss: 5092433.5000\n",
      "Epoch [6168/50000], Train Loss: 8489822.0000, Val Loss: 5092309.0000\n",
      "Epoch [6169/50000], Train Loss: 8489307.0000, Val Loss: 5092184.0000\n",
      "Epoch [6170/50000], Train Loss: 8488792.0000, Val Loss: 5092059.5000\n",
      "Epoch [6171/50000], Train Loss: 8488278.0000, Val Loss: 5091935.5000\n",
      "Epoch [6172/50000], Train Loss: 8487764.0000, Val Loss: 5091812.5000\n",
      "Epoch [6173/50000], Train Loss: 8487251.0000, Val Loss: 5091689.5000\n",
      "Epoch [6174/50000], Train Loss: 8486739.0000, Val Loss: 5091566.0000\n",
      "Epoch [6175/50000], Train Loss: 8486225.0000, Val Loss: 5091443.5000\n",
      "Epoch [6176/50000], Train Loss: 8485713.0000, Val Loss: 5091321.5000\n",
      "Epoch [6177/50000], Train Loss: 8485203.0000, Val Loss: 5091200.0000\n",
      "Epoch [6178/50000], Train Loss: 8484691.0000, Val Loss: 5091079.0000\n",
      "Epoch [6179/50000], Train Loss: 8484180.0000, Val Loss: 5090958.0000\n",
      "Epoch [6180/50000], Train Loss: 8483669.0000, Val Loss: 5090837.0000\n",
      "Epoch [6181/50000], Train Loss: 8483160.0000, Val Loss: 5090717.0000\n",
      "Epoch [6182/50000], Train Loss: 8482652.0000, Val Loss: 5090597.5000\n",
      "Epoch [6183/50000], Train Loss: 8482143.0000, Val Loss: 5090478.0000\n",
      "Epoch [6184/50000], Train Loss: 8481635.0000, Val Loss: 5090358.5000\n",
      "Epoch [6185/50000], Train Loss: 8481127.0000, Val Loss: 5090239.5000\n",
      "Epoch [6186/50000], Train Loss: 8480620.0000, Val Loss: 5090121.0000\n",
      "Epoch [6187/50000], Train Loss: 8480113.0000, Val Loss: 5090002.5000\n",
      "Epoch [6188/50000], Train Loss: 8479606.0000, Val Loss: 5089885.5000\n",
      "Epoch [6189/50000], Train Loss: 8479101.0000, Val Loss: 5089767.5000\n",
      "Epoch [6190/50000], Train Loss: 8478594.0000, Val Loss: 5089651.0000\n",
      "Epoch [6191/50000], Train Loss: 8478091.0000, Val Loss: 5089534.5000\n",
      "Epoch [6192/50000], Train Loss: 8477585.0000, Val Loss: 5089417.5000\n",
      "Epoch [6193/50000], Train Loss: 8477081.0000, Val Loss: 5089302.0000\n",
      "Epoch [6194/50000], Train Loss: 8476579.0000, Val Loss: 5089186.0000\n",
      "Epoch [6195/50000], Train Loss: 8476074.0000, Val Loss: 5089071.0000\n",
      "Epoch [6196/50000], Train Loss: 8475573.0000, Val Loss: 5088956.5000\n",
      "Epoch [6197/50000], Train Loss: 8475070.0000, Val Loss: 5088841.5000\n",
      "Epoch [6198/50000], Train Loss: 8474570.0000, Val Loss: 5088727.5000\n",
      "Epoch [6199/50000], Train Loss: 8474068.0000, Val Loss: 5088614.0000\n",
      "Epoch [6200/50000], Train Loss: 8473566.0000, Val Loss: 5088501.0000\n",
      "Epoch [6201/50000], Train Loss: 8473066.0000, Val Loss: 5088387.0000\n",
      "Epoch [6202/50000], Train Loss: 8472566.0000, Val Loss: 5088274.5000\n",
      "Epoch [6203/50000], Train Loss: 8472067.0000, Val Loss: 5088162.0000\n",
      "Epoch [6204/50000], Train Loss: 8471569.0000, Val Loss: 5088050.0000\n",
      "Epoch [6205/50000], Train Loss: 8471069.0000, Val Loss: 5087938.5000\n",
      "Epoch [6206/50000], Train Loss: 8470571.0000, Val Loss: 5087827.0000\n",
      "Epoch [6207/50000], Train Loss: 8470074.0000, Val Loss: 5087716.5000\n",
      "Epoch [6208/50000], Train Loss: 8469577.0000, Val Loss: 5087606.5000\n",
      "Epoch [6209/50000], Train Loss: 8469080.0000, Val Loss: 5087496.0000\n",
      "Epoch [6210/50000], Train Loss: 8468584.0000, Val Loss: 5087386.0000\n",
      "Epoch [6211/50000], Train Loss: 8468089.0000, Val Loss: 5087276.5000\n",
      "Epoch [6212/50000], Train Loss: 8467593.0000, Val Loss: 5087167.5000\n",
      "Epoch [6213/50000], Train Loss: 8467098.0000, Val Loss: 5087058.0000\n",
      "Epoch [6214/50000], Train Loss: 8466605.0000, Val Loss: 5086950.5000\n",
      "Epoch [6215/50000], Train Loss: 8466111.0000, Val Loss: 5086841.5000\n",
      "Epoch [6216/50000], Train Loss: 8465619.0000, Val Loss: 5086734.5000\n",
      "Epoch [6217/50000], Train Loss: 8465125.0000, Val Loss: 5086626.5000\n",
      "Epoch [6218/50000], Train Loss: 8464632.0000, Val Loss: 5086519.5000\n",
      "Epoch [6219/50000], Train Loss: 8464140.0000, Val Loss: 5086413.0000\n",
      "Epoch [6220/50000], Train Loss: 8463650.0000, Val Loss: 5086306.0000\n",
      "Epoch [6221/50000], Train Loss: 8463157.0000, Val Loss: 5086201.0000\n",
      "Epoch [6222/50000], Train Loss: 8462667.0000, Val Loss: 5086095.0000\n",
      "Epoch [6223/50000], Train Loss: 8462178.0000, Val Loss: 5085989.5000\n",
      "Epoch [6224/50000], Train Loss: 8461688.0000, Val Loss: 5085885.0000\n",
      "Epoch [6225/50000], Train Loss: 8461198.0000, Val Loss: 5085780.5000\n",
      "Epoch [6226/50000], Train Loss: 8460710.0000, Val Loss: 5085677.0000\n",
      "Epoch [6227/50000], Train Loss: 8460222.0000, Val Loss: 5085572.5000\n",
      "Epoch [6228/50000], Train Loss: 8459733.0000, Val Loss: 5085469.5000\n",
      "Epoch [6229/50000], Train Loss: 8459246.0000, Val Loss: 5085365.5000\n",
      "Epoch [6230/50000], Train Loss: 8458759.0000, Val Loss: 5085262.5000\n",
      "Epoch [6231/50000], Train Loss: 8458273.0000, Val Loss: 5085160.0000\n",
      "Epoch [6232/50000], Train Loss: 8457786.0000, Val Loss: 5085058.0000\n",
      "Epoch [6233/50000], Train Loss: 8457301.0000, Val Loss: 5084955.5000\n",
      "Epoch [6234/50000], Train Loss: 8456816.0000, Val Loss: 5084855.0000\n",
      "Epoch [6235/50000], Train Loss: 8456331.0000, Val Loss: 5084754.0000\n",
      "Epoch [6236/50000], Train Loss: 8455848.0000, Val Loss: 5084653.5000\n",
      "Epoch [6237/50000], Train Loss: 8455364.0000, Val Loss: 5084553.0000\n",
      "Epoch [6238/50000], Train Loss: 8454881.0000, Val Loss: 5084452.5000\n",
      "Epoch [6239/50000], Train Loss: 8454398.0000, Val Loss: 5084353.0000\n",
      "Epoch [6240/50000], Train Loss: 8453914.0000, Val Loss: 5084254.0000\n",
      "Epoch [6241/50000], Train Loss: 8453434.0000, Val Loss: 5084154.5000\n",
      "Epoch [6242/50000], Train Loss: 8452952.0000, Val Loss: 5084056.5000\n",
      "Epoch [6243/50000], Train Loss: 8452471.0000, Val Loss: 5083957.5000\n",
      "Epoch [6244/50000], Train Loss: 8451991.0000, Val Loss: 5083859.5000\n",
      "Epoch [6245/50000], Train Loss: 8451511.0000, Val Loss: 5083762.0000\n",
      "Epoch [6246/50000], Train Loss: 8451031.0000, Val Loss: 5083664.5000\n",
      "Epoch [6247/50000], Train Loss: 8450553.0000, Val Loss: 5083568.0000\n",
      "Epoch [6248/50000], Train Loss: 8450073.0000, Val Loss: 5083471.0000\n",
      "Epoch [6249/50000], Train Loss: 8449595.0000, Val Loss: 5083375.0000\n",
      "Epoch [6250/50000], Train Loss: 8449118.0000, Val Loss: 5083279.0000\n",
      "Epoch [6251/50000], Train Loss: 8448639.0000, Val Loss: 5083183.5000\n",
      "Epoch [6252/50000], Train Loss: 8448162.0000, Val Loss: 5083088.0000\n",
      "Epoch [6253/50000], Train Loss: 8447686.0000, Val Loss: 5082993.0000\n",
      "Epoch [6254/50000], Train Loss: 8447210.0000, Val Loss: 5082899.0000\n",
      "Epoch [6255/50000], Train Loss: 8446735.0000, Val Loss: 5082804.5000\n",
      "Epoch [6256/50000], Train Loss: 8446259.0000, Val Loss: 5082711.0000\n",
      "Epoch [6257/50000], Train Loss: 8445785.0000, Val Loss: 5082617.5000\n",
      "Epoch [6258/50000], Train Loss: 8445310.0000, Val Loss: 5082524.5000\n",
      "Epoch [6259/50000], Train Loss: 8444836.0000, Val Loss: 5082431.5000\n",
      "Epoch [6260/50000], Train Loss: 8444364.0000, Val Loss: 5082339.0000\n",
      "Epoch [6261/50000], Train Loss: 8443892.0000, Val Loss: 5082246.5000\n",
      "Epoch [6262/50000], Train Loss: 8443418.0000, Val Loss: 5082154.5000\n",
      "Epoch [6263/50000], Train Loss: 8442946.0000, Val Loss: 5082063.0000\n",
      "Epoch [6264/50000], Train Loss: 8442475.0000, Val Loss: 5081972.5000\n",
      "Epoch [6265/50000], Train Loss: 8442003.0000, Val Loss: 5081881.5000\n",
      "Epoch [6266/50000], Train Loss: 8441534.0000, Val Loss: 5081791.5000\n",
      "Epoch [6267/50000], Train Loss: 8441062.0000, Val Loss: 5081701.0000\n",
      "Epoch [6268/50000], Train Loss: 8440594.0000, Val Loss: 5081611.5000\n",
      "Epoch [6269/50000], Train Loss: 8440124.0000, Val Loss: 5081521.5000\n",
      "Epoch [6270/50000], Train Loss: 8439655.0000, Val Loss: 5081432.5000\n",
      "Epoch [6271/50000], Train Loss: 8439187.0000, Val Loss: 5081343.5000\n",
      "Epoch [6272/50000], Train Loss: 8438719.0000, Val Loss: 5081255.5000\n",
      "Epoch [6273/50000], Train Loss: 8438251.0000, Val Loss: 5081167.5000\n",
      "Epoch [6274/50000], Train Loss: 8437786.0000, Val Loss: 5081080.0000\n",
      "Epoch [6275/50000], Train Loss: 8437318.0000, Val Loss: 5080993.0000\n",
      "Epoch [6276/50000], Train Loss: 8436852.0000, Val Loss: 5080905.5000\n",
      "Epoch [6277/50000], Train Loss: 8436387.0000, Val Loss: 5080818.5000\n",
      "Epoch [6278/50000], Train Loss: 8435921.0000, Val Loss: 5080733.0000\n",
      "Epoch [6279/50000], Train Loss: 8435458.0000, Val Loss: 5080646.5000\n",
      "Epoch [6280/50000], Train Loss: 8434992.0000, Val Loss: 5080561.0000\n",
      "Epoch [6281/50000], Train Loss: 8434529.0000, Val Loss: 5080475.0000\n",
      "Epoch [6282/50000], Train Loss: 8434065.0000, Val Loss: 5080390.5000\n",
      "Epoch [6283/50000], Train Loss: 8433602.0000, Val Loss: 5080305.0000\n",
      "Epoch [6284/50000], Train Loss: 8433140.0000, Val Loss: 5080221.5000\n",
      "Epoch [6285/50000], Train Loss: 8432678.0000, Val Loss: 5080137.0000\n",
      "Epoch [6286/50000], Train Loss: 8432215.0000, Val Loss: 5080053.5000\n",
      "Epoch [6287/50000], Train Loss: 8431755.0000, Val Loss: 5079970.0000\n",
      "Epoch [6288/50000], Train Loss: 8431294.0000, Val Loss: 5079888.0000\n",
      "Epoch [6289/50000], Train Loss: 8430833.0000, Val Loss: 5079805.0000\n",
      "Epoch [6290/50000], Train Loss: 8430374.0000, Val Loss: 5079722.5000\n",
      "Epoch [6291/50000], Train Loss: 8429914.0000, Val Loss: 5079640.5000\n",
      "Epoch [6292/50000], Train Loss: 8429456.0000, Val Loss: 5079558.5000\n",
      "Epoch [6293/50000], Train Loss: 8428997.0000, Val Loss: 5079477.5000\n",
      "Epoch [6294/50000], Train Loss: 8428539.0000, Val Loss: 5079396.5000\n",
      "Epoch [6295/50000], Train Loss: 8428081.0000, Val Loss: 5079315.0000\n",
      "Epoch [6296/50000], Train Loss: 8427623.0000, Val Loss: 5079235.0000\n",
      "Epoch [6297/50000], Train Loss: 8427167.0000, Val Loss: 5079155.5000\n",
      "Epoch [6298/50000], Train Loss: 8426710.0000, Val Loss: 5079075.5000\n",
      "Epoch [6299/50000], Train Loss: 8426255.0000, Val Loss: 5078996.5000\n",
      "Epoch [6300/50000], Train Loss: 8425799.0000, Val Loss: 5078917.5000\n",
      "Epoch [6301/50000], Train Loss: 8425345.0000, Val Loss: 5078838.5000\n",
      "Epoch [6302/50000], Train Loss: 8424889.0000, Val Loss: 5078760.0000\n",
      "Epoch [6303/50000], Train Loss: 8424435.0000, Val Loss: 5078681.5000\n",
      "Epoch [6304/50000], Train Loss: 8423982.0000, Val Loss: 5078604.5000\n",
      "Epoch [6305/50000], Train Loss: 8423530.0000, Val Loss: 5078527.0000\n",
      "Epoch [6306/50000], Train Loss: 8423076.0000, Val Loss: 5078450.0000\n",
      "Epoch [6307/50000], Train Loss: 8422623.0000, Val Loss: 5078373.5000\n",
      "Epoch [6308/50000], Train Loss: 8422171.0000, Val Loss: 5078296.5000\n",
      "Epoch [6309/50000], Train Loss: 8421720.0000, Val Loss: 5078221.0000\n",
      "Epoch [6310/50000], Train Loss: 8421269.0000, Val Loss: 5078144.5000\n",
      "Epoch [6311/50000], Train Loss: 8420818.0000, Val Loss: 5078069.5000\n",
      "Epoch [6312/50000], Train Loss: 8420368.0000, Val Loss: 5077994.5000\n",
      "Epoch [6313/50000], Train Loss: 8419919.0000, Val Loss: 5077919.5000\n",
      "Epoch [6314/50000], Train Loss: 8419469.0000, Val Loss: 5077845.5000\n",
      "Epoch [6315/50000], Train Loss: 8419020.0000, Val Loss: 5077771.0000\n",
      "Epoch [6316/50000], Train Loss: 8418572.0000, Val Loss: 5077697.0000\n",
      "Epoch [6317/50000], Train Loss: 8418123.0000, Val Loss: 5077624.0000\n",
      "Epoch [6318/50000], Train Loss: 8417677.0000, Val Loss: 5077551.0000\n",
      "Epoch [6319/50000], Train Loss: 8417229.0000, Val Loss: 5077478.5000\n",
      "Epoch [6320/50000], Train Loss: 8416783.0000, Val Loss: 5077405.5000\n",
      "Epoch [6321/50000], Train Loss: 8416336.0000, Val Loss: 5077333.5000\n",
      "Epoch [6322/50000], Train Loss: 8415892.0000, Val Loss: 5077262.0000\n",
      "Epoch [6323/50000], Train Loss: 8415446.0000, Val Loss: 5077190.5000\n",
      "Epoch [6324/50000], Train Loss: 8415001.0000, Val Loss: 5077119.0000\n",
      "Epoch [6325/50000], Train Loss: 8414557.0000, Val Loss: 5077048.0000\n",
      "Epoch [6326/50000], Train Loss: 8414111.0000, Val Loss: 5076977.5000\n",
      "Epoch [6327/50000], Train Loss: 8413668.0000, Val Loss: 5076907.5000\n",
      "Epoch [6328/50000], Train Loss: 8413225.0000, Val Loss: 5076837.5000\n",
      "Epoch [6329/50000], Train Loss: 8412783.0000, Val Loss: 5076768.0000\n",
      "Epoch [6330/50000], Train Loss: 8412340.0000, Val Loss: 5076698.5000\n",
      "Epoch [6331/50000], Train Loss: 8411900.0000, Val Loss: 5076630.0000\n",
      "Epoch [6332/50000], Train Loss: 8411458.0000, Val Loss: 5076561.0000\n",
      "Epoch [6333/50000], Train Loss: 8411016.0000, Val Loss: 5076493.0000\n",
      "Epoch [6334/50000], Train Loss: 8410575.0000, Val Loss: 5076425.0000\n",
      "Epoch [6335/50000], Train Loss: 8410136.0000, Val Loss: 5076357.5000\n",
      "Epoch [6336/50000], Train Loss: 8409696.0000, Val Loss: 5076290.0000\n",
      "Epoch [6337/50000], Train Loss: 8409257.0000, Val Loss: 5076223.0000\n",
      "Epoch [6338/50000], Train Loss: 8408817.0000, Val Loss: 5076156.5000\n",
      "Epoch [6339/50000], Train Loss: 8408379.0000, Val Loss: 5076090.0000\n",
      "Epoch [6340/50000], Train Loss: 8407941.0000, Val Loss: 5076024.0000\n",
      "Epoch [6341/50000], Train Loss: 8407504.0000, Val Loss: 5075958.5000\n",
      "Epoch [6342/50000], Train Loss: 8407068.0000, Val Loss: 5075893.0000\n",
      "Epoch [6343/50000], Train Loss: 8406630.0000, Val Loss: 5075828.5000\n",
      "Epoch [6344/50000], Train Loss: 8406194.0000, Val Loss: 5075763.0000\n",
      "Epoch [6345/50000], Train Loss: 8405758.0000, Val Loss: 5075698.5000\n",
      "Epoch [6346/50000], Train Loss: 8405323.0000, Val Loss: 5075635.0000\n",
      "Epoch [6347/50000], Train Loss: 8404888.0000, Val Loss: 5075570.5000\n",
      "Epoch [6348/50000], Train Loss: 8404454.0000, Val Loss: 5075508.5000\n",
      "Epoch [6349/50000], Train Loss: 8404019.0000, Val Loss: 5075445.0000\n",
      "Epoch [6350/50000], Train Loss: 8403586.0000, Val Loss: 5075381.5000\n",
      "Epoch [6351/50000], Train Loss: 8403153.0000, Val Loss: 5075319.5000\n",
      "Epoch [6352/50000], Train Loss: 8402720.0000, Val Loss: 5075256.5000\n",
      "Epoch [6353/50000], Train Loss: 8402288.0000, Val Loss: 5075195.0000\n",
      "Epoch [6354/50000], Train Loss: 8401856.0000, Val Loss: 5075133.5000\n",
      "Epoch [6355/50000], Train Loss: 8401424.0000, Val Loss: 5075072.0000\n",
      "Epoch [6356/50000], Train Loss: 8400994.0000, Val Loss: 5075011.0000\n",
      "Epoch [6357/50000], Train Loss: 8400563.0000, Val Loss: 5074950.5000\n",
      "Epoch [6358/50000], Train Loss: 8400132.0000, Val Loss: 5074890.5000\n",
      "Epoch [6359/50000], Train Loss: 8399702.0000, Val Loss: 5074830.5000\n",
      "Epoch [6360/50000], Train Loss: 8399272.0000, Val Loss: 5074770.5000\n",
      "Epoch [6361/50000], Train Loss: 8398845.0000, Val Loss: 5074711.5000\n",
      "Epoch [6362/50000], Train Loss: 8398417.0000, Val Loss: 5074652.5000\n",
      "Epoch [6363/50000], Train Loss: 8397987.0000, Val Loss: 5074594.0000\n",
      "Epoch [6364/50000], Train Loss: 8397561.0000, Val Loss: 5074535.5000\n",
      "Epoch [6365/50000], Train Loss: 8397133.0000, Val Loss: 5074477.0000\n",
      "Epoch [6366/50000], Train Loss: 8396707.0000, Val Loss: 5074419.0000\n",
      "Epoch [6367/50000], Train Loss: 8396281.0000, Val Loss: 5074361.5000\n",
      "Epoch [6368/50000], Train Loss: 8395855.0000, Val Loss: 5074305.0000\n",
      "Epoch [6369/50000], Train Loss: 8395429.0000, Val Loss: 5074248.5000\n",
      "Epoch [6370/50000], Train Loss: 8395004.0000, Val Loss: 5074192.0000\n",
      "Epoch [6371/50000], Train Loss: 8394579.0000, Val Loss: 5074135.0000\n",
      "Epoch [6372/50000], Train Loss: 8394154.0000, Val Loss: 5074080.0000\n",
      "Epoch [6373/50000], Train Loss: 8393731.0000, Val Loss: 5074024.0000\n",
      "Epoch [6374/50000], Train Loss: 8393308.0000, Val Loss: 5073968.5000\n",
      "Epoch [6375/50000], Train Loss: 8392885.0000, Val Loss: 5073913.5000\n",
      "Epoch [6376/50000], Train Loss: 8392461.0000, Val Loss: 5073859.0000\n",
      "Epoch [6377/50000], Train Loss: 8392040.0000, Val Loss: 5073805.0000\n",
      "Epoch [6378/50000], Train Loss: 8391618.0000, Val Loss: 5073750.5000\n",
      "Epoch [6379/50000], Train Loss: 8391197.0000, Val Loss: 5073697.0000\n",
      "Epoch [6380/50000], Train Loss: 8390777.0000, Val Loss: 5073643.0000\n",
      "Epoch [6381/50000], Train Loss: 8390356.0000, Val Loss: 5073590.5000\n",
      "Epoch [6382/50000], Train Loss: 8389936.0000, Val Loss: 5073537.5000\n",
      "Epoch [6383/50000], Train Loss: 8389517.0000, Val Loss: 5073485.0000\n",
      "Epoch [6384/50000], Train Loss: 8389096.0000, Val Loss: 5073433.0000\n",
      "Epoch [6385/50000], Train Loss: 8388678.0000, Val Loss: 5073381.5000\n",
      "Epoch [6386/50000], Train Loss: 8388260.0000, Val Loss: 5073329.0000\n",
      "Epoch [6387/50000], Train Loss: 8387842.0000, Val Loss: 5073278.5000\n",
      "Epoch [6388/50000], Train Loss: 8387425.0000, Val Loss: 5073227.5000\n",
      "Epoch [6389/50000], Train Loss: 8387008.5000, Val Loss: 5073176.5000\n",
      "Epoch [6390/50000], Train Loss: 8386592.0000, Val Loss: 5073126.0000\n",
      "Epoch [6391/50000], Train Loss: 8386176.5000, Val Loss: 5073076.5000\n",
      "Epoch [6392/50000], Train Loss: 8385759.5000, Val Loss: 5073026.5000\n",
      "Epoch [6393/50000], Train Loss: 8385344.5000, Val Loss: 5072977.0000\n",
      "Epoch [6394/50000], Train Loss: 8384929.5000, Val Loss: 5072928.0000\n",
      "Epoch [6395/50000], Train Loss: 8384515.0000, Val Loss: 5072879.0000\n",
      "Epoch [6396/50000], Train Loss: 8384101.0000, Val Loss: 5072830.5000\n",
      "Epoch [6397/50000], Train Loss: 8383686.5000, Val Loss: 5072782.0000\n",
      "Epoch [6398/50000], Train Loss: 8383273.5000, Val Loss: 5072735.0000\n",
      "Epoch [6399/50000], Train Loss: 8382861.0000, Val Loss: 5072687.0000\n",
      "Epoch [6400/50000], Train Loss: 8382448.5000, Val Loss: 5072640.0000\n",
      "Epoch [6401/50000], Train Loss: 8382037.5000, Val Loss: 5072592.5000\n",
      "Epoch [6402/50000], Train Loss: 8381625.5000, Val Loss: 5072546.0000\n",
      "Epoch [6403/50000], Train Loss: 8381214.5000, Val Loss: 5072499.0000\n",
      "Epoch [6404/50000], Train Loss: 8380804.0000, Val Loss: 5072453.5000\n",
      "Epoch [6405/50000], Train Loss: 8380394.0000, Val Loss: 5072407.5000\n",
      "Epoch [6406/50000], Train Loss: 8379983.0000, Val Loss: 5072362.0000\n",
      "Epoch [6407/50000], Train Loss: 8379574.0000, Val Loss: 5072318.0000\n",
      "Epoch [6408/50000], Train Loss: 8379164.5000, Val Loss: 5072272.0000\n",
      "Epoch [6409/50000], Train Loss: 8378756.0000, Val Loss: 5072228.5000\n",
      "Epoch [6410/50000], Train Loss: 8378348.0000, Val Loss: 5072183.5000\n",
      "Epoch [6411/50000], Train Loss: 8377939.5000, Val Loss: 5072139.5000\n",
      "Epoch [6412/50000], Train Loss: 8377532.5000, Val Loss: 5072096.0000\n",
      "Epoch [6413/50000], Train Loss: 8377125.0000, Val Loss: 5072053.0000\n",
      "Epoch [6414/50000], Train Loss: 8376718.5000, Val Loss: 5072009.5000\n",
      "Epoch [6415/50000], Train Loss: 8376312.0000, Val Loss: 5071967.0000\n",
      "Epoch [6416/50000], Train Loss: 8375907.5000, Val Loss: 5071924.5000\n",
      "Epoch [6417/50000], Train Loss: 8375501.5000, Val Loss: 5071881.5000\n",
      "Epoch [6418/50000], Train Loss: 8375097.5000, Val Loss: 5071840.5000\n",
      "Epoch [6419/50000], Train Loss: 8374692.5000, Val Loss: 5071799.0000\n",
      "Epoch [6420/50000], Train Loss: 8374289.0000, Val Loss: 5071758.0000\n",
      "Epoch [6421/50000], Train Loss: 8373884.5000, Val Loss: 5071717.0000\n",
      "Epoch [6422/50000], Train Loss: 8373481.0000, Val Loss: 5071676.5000\n",
      "Epoch [6423/50000], Train Loss: 8373078.5000, Val Loss: 5071635.5000\n",
      "Epoch [6424/50000], Train Loss: 8372676.5000, Val Loss: 5071595.5000\n",
      "Epoch [6425/50000], Train Loss: 8372274.0000, Val Loss: 5071555.0000\n",
      "Epoch [6426/50000], Train Loss: 8371873.0000, Val Loss: 5071516.5000\n",
      "Epoch [6427/50000], Train Loss: 8371472.5000, Val Loss: 5071477.5000\n",
      "Epoch [6428/50000], Train Loss: 8371071.5000, Val Loss: 5071438.5000\n",
      "Epoch [6429/50000], Train Loss: 8370670.0000, Val Loss: 5071399.5000\n",
      "Epoch [6430/50000], Train Loss: 8370270.5000, Val Loss: 5071361.0000\n",
      "Epoch [6431/50000], Train Loss: 8369871.0000, Val Loss: 5071323.5000\n",
      "Epoch [6432/50000], Train Loss: 8369472.0000, Val Loss: 5071286.0000\n",
      "Epoch [6433/50000], Train Loss: 8369073.5000, Val Loss: 5071248.5000\n",
      "Epoch [6434/50000], Train Loss: 8368674.0000, Val Loss: 5071211.5000\n",
      "Epoch [6435/50000], Train Loss: 8368277.5000, Val Loss: 5071174.5000\n",
      "Epoch [6436/50000], Train Loss: 8367879.5000, Val Loss: 5071137.5000\n",
      "Epoch [6437/50000], Train Loss: 8367482.0000, Val Loss: 5071101.5000\n",
      "Epoch [6438/50000], Train Loss: 8367086.5000, Val Loss: 5071066.0000\n",
      "Epoch [6439/50000], Train Loss: 8366689.5000, Val Loss: 5071030.5000\n",
      "Epoch [6440/50000], Train Loss: 8366293.0000, Val Loss: 5070994.5000\n",
      "Epoch [6441/50000], Train Loss: 8365897.5000, Val Loss: 5070960.0000\n",
      "Epoch [6442/50000], Train Loss: 8365503.0000, Val Loss: 5070925.0000\n",
      "Epoch [6443/50000], Train Loss: 8365108.0000, Val Loss: 5070890.5000\n",
      "Epoch [6444/50000], Train Loss: 8364714.0000, Val Loss: 5070856.5000\n",
      "Epoch [6445/50000], Train Loss: 8364320.5000, Val Loss: 5070822.5000\n",
      "Epoch [6446/50000], Train Loss: 8363927.0000, Val Loss: 5070789.0000\n",
      "Epoch [6447/50000], Train Loss: 8363534.0000, Val Loss: 5070755.5000\n",
      "Epoch [6448/50000], Train Loss: 8363142.0000, Val Loss: 5070723.0000\n",
      "Epoch [6449/50000], Train Loss: 8362750.0000, Val Loss: 5070690.0000\n",
      "Epoch [6450/50000], Train Loss: 8362358.0000, Val Loss: 5070658.0000\n",
      "Epoch [6451/50000], Train Loss: 8361966.5000, Val Loss: 5070625.5000\n",
      "Epoch [6452/50000], Train Loss: 8361576.5000, Val Loss: 5070593.5000\n",
      "Epoch [6453/50000], Train Loss: 8361185.0000, Val Loss: 5070562.0000\n",
      "Epoch [6454/50000], Train Loss: 8360795.5000, Val Loss: 5070531.0000\n",
      "Epoch [6455/50000], Train Loss: 8360405.5000, Val Loss: 5070500.5000\n",
      "Epoch [6456/50000], Train Loss: 8360016.5000, Val Loss: 5070469.5000\n",
      "Epoch [6457/50000], Train Loss: 8359628.0000, Val Loss: 5070439.5000\n",
      "Epoch [6458/50000], Train Loss: 8359238.5000, Val Loss: 5070408.5000\n",
      "Epoch [6459/50000], Train Loss: 8358851.0000, Val Loss: 5070379.5000\n",
      "Epoch [6460/50000], Train Loss: 8358464.0000, Val Loss: 5070350.0000\n",
      "Epoch [6461/50000], Train Loss: 8358076.5000, Val Loss: 5070320.5000\n",
      "Epoch [6462/50000], Train Loss: 8357688.0000, Val Loss: 5070291.0000\n",
      "Epoch [6463/50000], Train Loss: 8357302.5000, Val Loss: 5070263.0000\n",
      "Epoch [6464/50000], Train Loss: 8356917.5000, Val Loss: 5070234.5000\n",
      "Epoch [6465/50000], Train Loss: 8356531.0000, Val Loss: 5070206.0000\n",
      "Epoch [6466/50000], Train Loss: 8356145.0000, Val Loss: 5070178.5000\n",
      "Epoch [6467/50000], Train Loss: 8355761.0000, Val Loss: 5070151.5000\n",
      "Epoch [6468/50000], Train Loss: 8355375.5000, Val Loss: 5070123.5000\n",
      "Epoch [6469/50000], Train Loss: 8354991.5000, Val Loss: 5070097.0000\n",
      "Epoch [6470/50000], Train Loss: 8354608.5000, Val Loss: 5070070.0000\n",
      "Epoch [6471/50000], Train Loss: 8354224.5000, Val Loss: 5070043.5000\n",
      "Epoch [6472/50000], Train Loss: 8353842.0000, Val Loss: 5070018.0000\n",
      "Epoch [6473/50000], Train Loss: 8353460.0000, Val Loss: 5069991.5000\n",
      "Epoch [6474/50000], Train Loss: 8353078.5000, Val Loss: 5069967.0000\n",
      "Epoch [6475/50000], Train Loss: 8352696.0000, Val Loss: 5069942.0000\n",
      "Epoch [6476/50000], Train Loss: 8352314.5000, Val Loss: 5069916.5000\n",
      "Epoch [6477/50000], Train Loss: 8351934.5000, Val Loss: 5069891.0000\n",
      "Epoch [6478/50000], Train Loss: 8351554.0000, Val Loss: 5069867.0000\n",
      "Epoch [6479/50000], Train Loss: 8351173.5000, Val Loss: 5069843.0000\n",
      "Epoch [6480/50000], Train Loss: 8350794.5000, Val Loss: 5069819.0000\n",
      "Epoch [6481/50000], Train Loss: 8350414.5000, Val Loss: 5069795.5000\n",
      "Epoch [6482/50000], Train Loss: 8350035.5000, Val Loss: 5069773.0000\n",
      "Epoch [6483/50000], Train Loss: 8349657.5000, Val Loss: 5069750.0000\n",
      "Epoch [6484/50000], Train Loss: 8349279.0000, Val Loss: 5069727.5000\n",
      "Epoch [6485/50000], Train Loss: 8348901.5000, Val Loss: 5069704.5000\n",
      "Epoch [6486/50000], Train Loss: 8348525.0000, Val Loss: 5069682.0000\n",
      "Epoch [6487/50000], Train Loss: 8348147.5000, Val Loss: 5069661.0000\n",
      "Epoch [6488/50000], Train Loss: 8347771.0000, Val Loss: 5069639.0000\n",
      "Epoch [6489/50000], Train Loss: 8347395.5000, Val Loss: 5069618.0000\n",
      "Epoch [6490/50000], Train Loss: 8347021.0000, Val Loss: 5069597.0000\n",
      "Epoch [6491/50000], Train Loss: 8346645.0000, Val Loss: 5069576.0000\n",
      "Epoch [6492/50000], Train Loss: 8346270.0000, Val Loss: 5069555.5000\n",
      "Epoch [6493/50000], Train Loss: 8345895.5000, Val Loss: 5069535.5000\n",
      "Epoch [6494/50000], Train Loss: 8345522.0000, Val Loss: 5069515.5000\n",
      "Epoch [6495/50000], Train Loss: 8345147.5000, Val Loss: 5069496.5000\n",
      "Epoch [6496/50000], Train Loss: 8344775.5000, Val Loss: 5069476.5000\n",
      "Epoch [6497/50000], Train Loss: 8344401.5000, Val Loss: 5069458.0000\n",
      "Epoch [6498/50000], Train Loss: 8344029.0000, Val Loss: 5069439.5000\n",
      "Epoch [6499/50000], Train Loss: 8343657.5000, Val Loss: 5069420.5000\n",
      "Epoch [6500/50000], Train Loss: 8343285.0000, Val Loss: 5069402.0000\n",
      "Epoch [6501/50000], Train Loss: 8342914.0000, Val Loss: 5069384.5000\n",
      "Epoch [6502/50000], Train Loss: 8342544.5000, Val Loss: 5069367.0000\n",
      "Epoch [6503/50000], Train Loss: 8342173.0000, Val Loss: 5069349.5000\n",
      "Epoch [6504/50000], Train Loss: 8341802.5000, Val Loss: 5069332.5000\n",
      "Epoch [6505/50000], Train Loss: 8341433.5000, Val Loss: 5069315.0000\n",
      "Epoch [6506/50000], Train Loss: 8341064.0000, Val Loss: 5069299.0000\n",
      "Epoch [6507/50000], Train Loss: 8340695.5000, Val Loss: 5069282.5000\n",
      "Epoch [6508/50000], Train Loss: 8340326.5000, Val Loss: 5069266.5000\n",
      "Epoch [6509/50000], Train Loss: 8339958.0000, Val Loss: 5069251.0000\n",
      "Epoch [6510/50000], Train Loss: 8339590.0000, Val Loss: 5069235.5000\n",
      "Epoch [6511/50000], Train Loss: 8339224.0000, Val Loss: 5069220.5000\n",
      "Epoch [6512/50000], Train Loss: 8338856.5000, Val Loss: 5069205.5000\n",
      "Epoch [6513/50000], Train Loss: 8338490.5000, Val Loss: 5069191.0000\n",
      "Epoch [6514/50000], Train Loss: 8338123.0000, Val Loss: 5069176.5000\n",
      "Epoch [6515/50000], Train Loss: 8337758.5000, Val Loss: 5069162.5000\n",
      "Epoch [6516/50000], Train Loss: 8337393.5000, Val Loss: 5069149.0000\n",
      "Epoch [6517/50000], Train Loss: 8337027.0000, Val Loss: 5069135.0000\n",
      "Epoch [6518/50000], Train Loss: 8336663.0000, Val Loss: 5069121.5000\n",
      "Epoch [6519/50000], Train Loss: 8336298.5000, Val Loss: 5069109.0000\n",
      "Epoch [6520/50000], Train Loss: 8335934.5000, Val Loss: 5069096.5000\n",
      "Epoch [6521/50000], Train Loss: 8335571.0000, Val Loss: 5069083.5000\n",
      "Epoch [6522/50000], Train Loss: 8335208.5000, Val Loss: 5069071.5000\n",
      "Epoch [6523/50000], Train Loss: 8334846.0000, Val Loss: 5069059.5000\n",
      "Epoch [6524/50000], Train Loss: 8334483.5000, Val Loss: 5069048.0000\n",
      "Epoch [6525/50000], Train Loss: 8334121.5000, Val Loss: 5069037.0000\n",
      "Epoch [6526/50000], Train Loss: 8333760.5000, Val Loss: 5069025.5000\n",
      "Epoch [6527/50000], Train Loss: 8333399.0000, Val Loss: 5069015.0000\n",
      "Epoch [6528/50000], Train Loss: 8333038.0000, Val Loss: 5069004.5000\n",
      "Epoch [6529/50000], Train Loss: 8332677.0000, Val Loss: 5068994.0000\n",
      "Epoch [6530/50000], Train Loss: 8332317.0000, Val Loss: 5068984.0000\n",
      "Epoch [6531/50000], Train Loss: 8331958.5000, Val Loss: 5068974.5000\n",
      "Epoch [6532/50000], Train Loss: 8331598.5000, Val Loss: 5068964.5000\n",
      "Epoch [6533/50000], Train Loss: 8331241.0000, Val Loss: 5068955.0000\n",
      "Epoch [6534/50000], Train Loss: 8330881.5000, Val Loss: 5068946.0000\n",
      "Epoch [6535/50000], Train Loss: 8330523.0000, Val Loss: 5068937.5000\n",
      "Epoch [6536/50000], Train Loss: 8330167.0000, Val Loss: 5068929.5000\n",
      "Epoch [6537/50000], Train Loss: 8329809.0000, Val Loss: 5068921.0000\n",
      "Epoch [6538/50000], Train Loss: 8329452.5000, Val Loss: 5068913.5000\n",
      "Epoch [6539/50000], Train Loss: 8329096.0000, Val Loss: 5068905.5000\n",
      "Epoch [6540/50000], Train Loss: 8328740.5000, Val Loss: 5068898.5000\n",
      "Epoch [6541/50000], Train Loss: 8328384.0000, Val Loss: 5068891.0000\n",
      "Epoch [6542/50000], Train Loss: 8328028.0000, Val Loss: 5068885.0000\n",
      "Epoch [6543/50000], Train Loss: 8327673.5000, Val Loss: 5068878.0000\n",
      "Epoch [6544/50000], Train Loss: 8327318.5000, Val Loss: 5068871.5000\n",
      "Epoch [6545/50000], Train Loss: 8326965.0000, Val Loss: 5068866.0000\n",
      "Epoch [6546/50000], Train Loss: 8326611.0000, Val Loss: 5068861.0000\n",
      "Epoch [6547/50000], Train Loss: 8326258.0000, Val Loss: 5068854.5000\n",
      "Epoch [6548/50000], Train Loss: 8325905.5000, Val Loss: 5068849.5000\n",
      "Epoch [6549/50000], Train Loss: 8325553.0000, Val Loss: 5068844.5000\n",
      "Epoch [6550/50000], Train Loss: 8325201.0000, Val Loss: 5068839.5000\n",
      "Epoch [6551/50000], Train Loss: 8324848.5000, Val Loss: 5068835.0000\n",
      "Epoch [6552/50000], Train Loss: 8324497.0000, Val Loss: 5068831.5000\n",
      "Epoch [6553/50000], Train Loss: 8324146.0000, Val Loss: 5068827.5000\n",
      "Epoch [6554/50000], Train Loss: 8323795.0000, Val Loss: 5068823.5000\n",
      "Epoch [6555/50000], Train Loss: 8323445.0000, Val Loss: 5068820.5000\n",
      "Epoch [6556/50000], Train Loss: 8323095.0000, Val Loss: 5068818.0000\n",
      "Epoch [6557/50000], Train Loss: 8322746.5000, Val Loss: 5068814.5000\n",
      "Epoch [6558/50000], Train Loss: 8322397.0000, Val Loss: 5068812.5000\n",
      "Epoch [6559/50000], Train Loss: 8322048.0000, Val Loss: 5068809.5000\n",
      "Epoch [6560/50000], Train Loss: 8321700.0000, Val Loss: 5068807.5000\n",
      "Epoch [6561/50000], Train Loss: 8321351.5000, Val Loss: 5068805.5000\n",
      "Epoch [6562/50000], Train Loss: 8321004.5000, Val Loss: 5068804.5000\n",
      "Epoch [6563/50000], Train Loss: 8320656.5000, Val Loss: 5068803.0000\n",
      "Epoch [6564/50000], Train Loss: 8320310.5000, Val Loss: 5068801.5000\n",
      "Epoch [6565/50000], Train Loss: 8319963.5000, Val Loss: 5068801.0000\n",
      "Epoch [6566/50000], Train Loss: 8319617.5000, Val Loss: 5068800.5000\n",
      "Epoch [6567/50000], Train Loss: 8319272.5000, Val Loss: 5068800.0000\n",
      "Epoch [6568/50000], Train Loss: 8318926.0000, Val Loss: 5068800.5000\n",
      "Epoch [6569/50000], Train Loss: 8318582.5000, Val Loss: 5068801.0000\n",
      "Epoch [6570/50000], Train Loss: 8318236.5000, Val Loss: 5068801.0000\n",
      "Epoch [6571/50000], Train Loss: 8317892.0000, Val Loss: 5068802.0000\n",
      "Epoch [6572/50000], Train Loss: 8317549.0000, Val Loss: 5068803.0000\n",
      "Epoch [6573/50000], Train Loss: 8317206.0000, Val Loss: 5068804.5000\n",
      "Epoch [6574/50000], Train Loss: 8316863.0000, Val Loss: 5068806.5000\n",
      "Epoch [6575/50000], Train Loss: 8316519.5000, Val Loss: 5068808.0000\n",
      "Epoch [6576/50000], Train Loss: 8316177.5000, Val Loss: 5068810.0000\n",
      "Epoch [6577/50000], Train Loss: 8315835.0000, Val Loss: 5068812.5000\n",
      "Epoch [6578/50000], Train Loss: 8315493.5000, Val Loss: 5068815.0000\n",
      "Epoch [6579/50000], Train Loss: 8315153.0000, Val Loss: 5068817.5000\n",
      "Epoch [6580/50000], Train Loss: 8314811.5000, Val Loss: 5068821.5000\n",
      "Epoch [6581/50000], Train Loss: 8314471.5000, Val Loss: 5068824.5000\n",
      "Epoch [6582/50000], Train Loss: 8314131.5000, Val Loss: 5068827.5000\n",
      "Epoch [6583/50000], Train Loss: 8313792.0000, Val Loss: 5068831.5000\n",
      "Epoch [6584/50000], Train Loss: 8313452.0000, Val Loss: 5068836.5000\n",
      "Epoch [6585/50000], Train Loss: 8313113.5000, Val Loss: 5068840.5000\n",
      "Epoch [6586/50000], Train Loss: 8312776.0000, Val Loss: 5068845.5000\n",
      "Epoch [6587/50000], Train Loss: 8312437.0000, Val Loss: 5068849.5000\n",
      "Epoch [6588/50000], Train Loss: 8312099.0000, Val Loss: 5068855.5000\n",
      "Epoch [6589/50000], Train Loss: 8311761.0000, Val Loss: 5068860.5000\n",
      "Epoch [6590/50000], Train Loss: 8311423.5000, Val Loss: 5068866.0000\n",
      "Epoch [6591/50000], Train Loss: 8311087.5000, Val Loss: 5068872.0000\n",
      "Epoch [6592/50000], Train Loss: 8310751.5000, Val Loss: 5068878.5000\n",
      "Epoch [6593/50000], Train Loss: 8310415.5000, Val Loss: 5068884.5000\n",
      "Epoch [6594/50000], Train Loss: 8310079.5000, Val Loss: 5068891.0000\n",
      "Epoch [6595/50000], Train Loss: 8309745.0000, Val Loss: 5068898.0000\n",
      "Epoch [6596/50000], Train Loss: 8309410.0000, Val Loss: 5068905.0000\n",
      "Epoch [6597/50000], Train Loss: 8309076.0000, Val Loss: 5068913.0000\n",
      "Epoch [6598/50000], Train Loss: 8308741.0000, Val Loss: 5068920.5000\n",
      "Epoch [6599/50000], Train Loss: 8308407.5000, Val Loss: 5068928.0000\n",
      "Epoch [6600/50000], Train Loss: 8308074.5000, Val Loss: 5068936.5000\n",
      "Epoch [6601/50000], Train Loss: 8307741.0000, Val Loss: 5068945.0000\n",
      "Epoch [6602/50000], Train Loss: 8307409.5000, Val Loss: 5068953.5000\n",
      "Epoch [6603/50000], Train Loss: 8307077.0000, Val Loss: 5068962.0000\n",
      "Epoch [6604/50000], Train Loss: 8306745.5000, Val Loss: 5068971.0000\n",
      "Epoch [6605/50000], Train Loss: 8306414.0000, Val Loss: 5068981.5000\n",
      "Epoch [6606/50000], Train Loss: 8306083.5000, Val Loss: 5068991.0000\n",
      "Epoch [6607/50000], Train Loss: 8305752.5000, Val Loss: 5069000.5000\n",
      "Epoch [6608/50000], Train Loss: 8305422.5000, Val Loss: 5069010.5000\n",
      "Epoch [6609/50000], Train Loss: 8305091.0000, Val Loss: 5069021.5000\n",
      "Epoch [6610/50000], Train Loss: 8304763.0000, Val Loss: 5069031.5000\n",
      "Epoch [6611/50000], Train Loss: 8304434.0000, Val Loss: 5069043.0000\n",
      "Epoch [6612/50000], Train Loss: 8304104.5000, Val Loss: 5069054.0000\n",
      "Epoch [6613/50000], Train Loss: 8303776.5000, Val Loss: 5069065.0000\n",
      "Epoch [6614/50000], Train Loss: 8303448.5000, Val Loss: 5069076.5000\n",
      "Epoch [6615/50000], Train Loss: 8303119.5000, Val Loss: 5069088.5000\n",
      "Epoch [6616/50000], Train Loss: 8302793.0000, Val Loss: 5069101.0000\n",
      "Epoch [6617/50000], Train Loss: 8302466.0000, Val Loss: 5069113.5000\n",
      "Epoch [6618/50000], Train Loss: 8302139.5000, Val Loss: 5069126.5000\n",
      "Epoch [6619/50000], Train Loss: 8301813.0000, Val Loss: 5069139.0000\n",
      "Epoch [6620/50000], Train Loss: 8301487.5000, Val Loss: 5069152.0000\n",
      "Epoch [6621/50000], Train Loss: 8301161.5000, Val Loss: 5069165.5000\n",
      "Epoch [6622/50000], Train Loss: 8300837.0000, Val Loss: 5069178.5000\n",
      "Epoch [6623/50000], Train Loss: 8300512.0000, Val Loss: 5069192.5000\n",
      "Epoch [6624/50000], Train Loss: 8300188.0000, Val Loss: 5069207.0000\n",
      "Epoch [6625/50000], Train Loss: 8299863.5000, Val Loss: 5069221.5000\n",
      "Epoch [6626/50000], Train Loss: 8299540.0000, Val Loss: 5069236.5000\n",
      "Epoch [6627/50000], Train Loss: 8299216.5000, Val Loss: 5069250.0000\n",
      "Epoch [6628/50000], Train Loss: 8298893.0000, Val Loss: 5069266.0000\n",
      "Epoch [6629/50000], Train Loss: 8298570.5000, Val Loss: 5069280.5000\n",
      "Epoch [6630/50000], Train Loss: 8298249.0000, Val Loss: 5069296.5000\n",
      "Epoch [6631/50000], Train Loss: 8297927.5000, Val Loss: 5069312.0000\n",
      "Epoch [6632/50000], Train Loss: 8297605.5000, Val Loss: 5069328.0000\n",
      "Epoch [6633/50000], Train Loss: 8297284.0000, Val Loss: 5069344.5000\n",
      "Epoch [6634/50000], Train Loss: 8296964.5000, Val Loss: 5069361.0000\n",
      "Epoch [6635/50000], Train Loss: 8296644.0000, Val Loss: 5069377.5000\n",
      "Epoch [6636/50000], Train Loss: 8296323.0000, Val Loss: 5069394.5000\n",
      "Epoch [6637/50000], Train Loss: 8296003.5000, Val Loss: 5069411.5000\n",
      "Epoch [6638/50000], Train Loss: 8295684.5000, Val Loss: 5069430.0000\n",
      "Epoch [6639/50000], Train Loss: 8295365.5000, Val Loss: 5069447.5000\n",
      "Epoch [6640/50000], Train Loss: 8295046.5000, Val Loss: 5069465.0000\n",
      "Epoch [6641/50000], Train Loss: 8294728.5000, Val Loss: 5069483.0000\n",
      "Epoch [6642/50000], Train Loss: 8294410.5000, Val Loss: 5069502.0000\n",
      "Epoch [6643/50000], Train Loss: 8294092.5000, Val Loss: 5069520.5000\n",
      "Epoch [6644/50000], Train Loss: 8293775.5000, Val Loss: 5069539.0000\n",
      "Epoch [6645/50000], Train Loss: 8293459.0000, Val Loss: 5069558.5000\n",
      "Epoch [6646/50000], Train Loss: 8293142.5000, Val Loss: 5069577.5000\n",
      "Epoch [6647/50000], Train Loss: 8292826.5000, Val Loss: 5069597.5000\n",
      "Epoch [6648/50000], Train Loss: 8292511.0000, Val Loss: 5069617.5000\n",
      "Epoch [6649/50000], Train Loss: 8292195.5000, Val Loss: 5069637.5000\n",
      "Epoch [6650/50000], Train Loss: 8291881.0000, Val Loss: 5069658.5000\n",
      "Epoch [6651/50000], Train Loss: 8291565.5000, Val Loss: 5069678.5000\n",
      "Epoch [6652/50000], Train Loss: 8291251.0000, Val Loss: 5069699.0000\n",
      "Epoch [6653/50000], Train Loss: 8290938.5000, Val Loss: 5069720.5000\n",
      "Epoch [6654/50000], Train Loss: 8290624.0000, Val Loss: 5069742.0000\n",
      "Epoch [6655/50000], Train Loss: 8290312.0000, Val Loss: 5069762.5000\n",
      "Epoch [6656/50000], Train Loss: 8289998.0000, Val Loss: 5069784.5000\n",
      "Epoch [6657/50000], Train Loss: 8289686.5000, Val Loss: 5069806.5000\n",
      "Epoch [6658/50000], Train Loss: 8289375.0000, Val Loss: 5069829.0000\n",
      "Epoch [6659/50000], Train Loss: 8289062.5000, Val Loss: 5069851.5000\n",
      "Epoch [6660/50000], Train Loss: 8288751.0000, Val Loss: 5069874.0000\n",
      "Epoch [6661/50000], Train Loss: 8288440.5000, Val Loss: 5069897.0000\n",
      "Epoch [6662/50000], Train Loss: 8288130.0000, Val Loss: 5069920.5000\n",
      "Epoch [6663/50000], Train Loss: 8287818.5000, Val Loss: 5069943.5000\n",
      "Epoch [6664/50000], Train Loss: 8287509.5000, Val Loss: 5069967.5000\n",
      "Epoch [6665/50000], Train Loss: 8287200.0000, Val Loss: 5069991.0000\n",
      "Epoch [6666/50000], Train Loss: 8286890.0000, Val Loss: 5070015.5000\n",
      "Epoch [6667/50000], Train Loss: 8286582.5000, Val Loss: 5070039.5000\n",
      "Epoch [6668/50000], Train Loss: 8286273.0000, Val Loss: 5070064.0000\n",
      "Epoch [6669/50000], Train Loss: 8285965.0000, Val Loss: 5070089.0000\n",
      "Epoch [6670/50000], Train Loss: 8285658.0000, Val Loss: 5070114.0000\n",
      "Epoch [6671/50000], Train Loss: 8285349.0000, Val Loss: 5070138.5000\n",
      "Epoch [6672/50000], Train Loss: 8285043.0000, Val Loss: 5070164.5000\n",
      "Epoch [6673/50000], Train Loss: 8284735.0000, Val Loss: 5070190.5000\n",
      "Epoch [6674/50000], Train Loss: 8284429.0000, Val Loss: 5070216.0000\n",
      "Epoch [6675/50000], Train Loss: 8284122.5000, Val Loss: 5070242.5000\n",
      "Epoch [6676/50000], Train Loss: 8283817.5000, Val Loss: 5070269.0000\n",
      "Epoch [6677/50000], Train Loss: 8283512.0000, Val Loss: 5070295.5000\n",
      "Epoch [6678/50000], Train Loss: 8283206.5000, Val Loss: 5070322.0000\n",
      "Epoch [6679/50000], Train Loss: 8282903.0000, Val Loss: 5070349.5000\n",
      "Epoch [6680/50000], Train Loss: 8282598.0000, Val Loss: 5070376.5000\n",
      "Epoch [6681/50000], Train Loss: 8282294.0000, Val Loss: 5070404.5000\n",
      "Epoch [6682/50000], Train Loss: 8281990.5000, Val Loss: 5070432.0000\n",
      "Epoch [6683/50000], Train Loss: 8281687.5000, Val Loss: 5070459.5000\n",
      "Epoch [6684/50000], Train Loss: 8281384.5000, Val Loss: 5070487.5000\n",
      "Epoch [6685/50000], Train Loss: 8281082.5000, Val Loss: 5070517.0000\n",
      "Epoch [6686/50000], Train Loss: 8280781.0000, Val Loss: 5070545.5000\n",
      "Epoch [6687/50000], Train Loss: 8280478.5000, Val Loss: 5070574.5000\n",
      "Epoch [6688/50000], Train Loss: 8280176.5000, Val Loss: 5070603.0000\n",
      "Epoch [6689/50000], Train Loss: 8279875.5000, Val Loss: 5070632.5000\n",
      "Epoch [6690/50000], Train Loss: 8279575.5000, Val Loss: 5070662.5000\n",
      "Epoch [6691/50000], Train Loss: 8279276.0000, Val Loss: 5070691.5000\n",
      "Epoch [6692/50000], Train Loss: 8278976.0000, Val Loss: 5070722.0000\n",
      "Epoch [6693/50000], Train Loss: 8278675.0000, Val Loss: 5070752.0000\n",
      "Epoch [6694/50000], Train Loss: 8278376.5000, Val Loss: 5070783.0000\n",
      "Epoch [6695/50000], Train Loss: 8278076.5000, Val Loss: 5070814.0000\n",
      "Epoch [6696/50000], Train Loss: 8277778.0000, Val Loss: 5070844.5000\n",
      "Epoch [6697/50000], Train Loss: 8277480.0000, Val Loss: 5070875.5000\n",
      "Epoch [6698/50000], Train Loss: 8277182.0000, Val Loss: 5070906.5000\n",
      "Epoch [6699/50000], Train Loss: 8276885.0000, Val Loss: 5070938.5000\n",
      "Epoch [6700/50000], Train Loss: 8276587.0000, Val Loss: 5070970.5000\n",
      "Epoch [6701/50000], Train Loss: 8276289.5000, Val Loss: 5071002.5000\n",
      "Epoch [6702/50000], Train Loss: 8275992.5000, Val Loss: 5071034.5000\n",
      "Epoch [6703/50000], Train Loss: 8275697.0000, Val Loss: 5071067.0000\n",
      "Epoch [6704/50000], Train Loss: 8275401.0000, Val Loss: 5071099.5000\n",
      "Epoch [6705/50000], Train Loss: 8275105.5000, Val Loss: 5071133.0000\n",
      "Epoch [6706/50000], Train Loss: 8274811.0000, Val Loss: 5071166.0000\n",
      "Epoch [6707/50000], Train Loss: 8274517.0000, Val Loss: 5071199.5000\n",
      "Epoch [6708/50000], Train Loss: 8274221.5000, Val Loss: 5071233.0000\n",
      "Epoch [6709/50000], Train Loss: 8273927.5000, Val Loss: 5071266.5000\n",
      "Epoch [6710/50000], Train Loss: 8273634.0000, Val Loss: 5071301.0000\n",
      "Epoch [6711/50000], Train Loss: 8273340.0000, Val Loss: 5071335.5000\n",
      "Epoch [6712/50000], Train Loss: 8273047.5000, Val Loss: 5071369.5000\n",
      "Epoch [6713/50000], Train Loss: 8272754.5000, Val Loss: 5071404.5000\n",
      "Epoch [6714/50000], Train Loss: 8272462.0000, Val Loss: 5071439.5000\n",
      "Epoch [6715/50000], Train Loss: 8272170.0000, Val Loss: 5071474.0000\n",
      "Epoch [6716/50000], Train Loss: 8271878.5000, Val Loss: 5071509.5000\n",
      "Epoch [6717/50000], Train Loss: 8271587.0000, Val Loss: 5071545.5000\n",
      "Epoch [6718/50000], Train Loss: 8271296.0000, Val Loss: 5071581.0000\n",
      "Epoch [6719/50000], Train Loss: 8271004.5000, Val Loss: 5071616.5000\n",
      "Epoch [6720/50000], Train Loss: 8270714.5000, Val Loss: 5071653.5000\n",
      "Epoch [6721/50000], Train Loss: 8270424.5000, Val Loss: 5071689.5000\n",
      "Epoch [6722/50000], Train Loss: 8270135.5000, Val Loss: 5071726.0000\n",
      "Epoch [6723/50000], Train Loss: 8269846.5000, Val Loss: 5071763.0000\n",
      "Epoch [6724/50000], Train Loss: 8269557.0000, Val Loss: 5071800.0000\n",
      "Epoch [6725/50000], Train Loss: 8269268.0000, Val Loss: 5071837.5000\n",
      "Epoch [6726/50000], Train Loss: 8268979.5000, Val Loss: 5071875.0000\n",
      "Epoch [6727/50000], Train Loss: 8268691.5000, Val Loss: 5071913.0000\n",
      "Epoch [6728/50000], Train Loss: 8268405.0000, Val Loss: 5071951.0000\n",
      "Epoch [6729/50000], Train Loss: 8268117.5000, Val Loss: 5071989.5000\n",
      "Epoch [6730/50000], Train Loss: 8267831.0000, Val Loss: 5072027.5000\n",
      "Epoch [6731/50000], Train Loss: 8267544.0000, Val Loss: 5072066.0000\n",
      "Epoch [6732/50000], Train Loss: 8267258.0000, Val Loss: 5072104.5000\n",
      "Epoch [6733/50000], Train Loss: 8266971.5000, Val Loss: 5072143.5000\n",
      "Epoch [6734/50000], Train Loss: 8266687.0000, Val Loss: 5072183.5000\n",
      "Epoch [6735/50000], Train Loss: 8266401.5000, Val Loss: 5072222.5000\n",
      "Epoch [6736/50000], Train Loss: 8266116.5000, Val Loss: 5072262.5000\n",
      "Epoch [6737/50000], Train Loss: 8265832.0000, Val Loss: 5072302.0000\n",
      "Epoch [6738/50000], Train Loss: 8265548.0000, Val Loss: 5072342.0000\n",
      "Epoch [6739/50000], Train Loss: 8265264.5000, Val Loss: 5072383.0000\n",
      "Epoch [6740/50000], Train Loss: 8264980.0000, Val Loss: 5072423.0000\n",
      "Epoch [6741/50000], Train Loss: 8264696.5000, Val Loss: 5072464.0000\n",
      "Epoch [6742/50000], Train Loss: 8264415.0000, Val Loss: 5072504.5000\n",
      "Epoch [6743/50000], Train Loss: 8264131.5000, Val Loss: 5072545.5000\n",
      "Epoch [6744/50000], Train Loss: 8263850.0000, Val Loss: 5072587.0000\n",
      "Epoch [6745/50000], Train Loss: 8263567.5000, Val Loss: 5072629.0000\n",
      "Epoch [6746/50000], Train Loss: 8263286.0000, Val Loss: 5072671.0000\n",
      "Epoch [6747/50000], Train Loss: 8263006.0000, Val Loss: 5072713.0000\n",
      "Epoch [6748/50000], Train Loss: 8262724.5000, Val Loss: 5072755.0000\n",
      "Epoch [6749/50000], Train Loss: 8262444.5000, Val Loss: 5072797.5000\n",
      "Epoch [6750/50000], Train Loss: 8262164.0000, Val Loss: 5072840.0000\n",
      "Epoch [6751/50000], Train Loss: 8261884.0000, Val Loss: 5072883.0000\n",
      "Epoch [6752/50000], Train Loss: 8261604.0000, Val Loss: 5072925.5000\n",
      "Epoch [6753/50000], Train Loss: 8261326.0000, Val Loss: 5072969.0000\n",
      "Epoch [6754/50000], Train Loss: 8261047.5000, Val Loss: 5073012.5000\n",
      "Epoch [6755/50000], Train Loss: 8260768.5000, Val Loss: 5073056.5000\n",
      "Epoch [6756/50000], Train Loss: 8260490.0000, Val Loss: 5073099.5000\n",
      "Epoch [6757/50000], Train Loss: 8260211.5000, Val Loss: 5073144.5000\n",
      "Epoch [6758/50000], Train Loss: 8259935.0000, Val Loss: 5073188.5000\n",
      "Epoch [6759/50000], Train Loss: 8259657.5000, Val Loss: 5073232.5000\n",
      "Epoch [6760/50000], Train Loss: 8259381.0000, Val Loss: 5073278.0000\n",
      "Epoch [6761/50000], Train Loss: 8259105.0000, Val Loss: 5073322.5000\n",
      "Epoch [6762/50000], Train Loss: 8258829.5000, Val Loss: 5073367.5000\n",
      "Epoch [6763/50000], Train Loss: 8258553.5000, Val Loss: 5073413.0000\n",
      "Epoch [6764/50000], Train Loss: 8258277.5000, Val Loss: 5073458.5000\n",
      "Epoch [6765/50000], Train Loss: 8258003.0000, Val Loss: 5073504.5000\n",
      "Epoch [6766/50000], Train Loss: 8257728.5000, Val Loss: 5073550.5000\n",
      "Epoch [6767/50000], Train Loss: 8257453.0000, Val Loss: 5073596.5000\n",
      "Epoch [6768/50000], Train Loss: 8257180.0000, Val Loss: 5073642.5000\n",
      "Epoch [6769/50000], Train Loss: 8256906.0000, Val Loss: 5073689.0000\n",
      "Epoch [6770/50000], Train Loss: 8256632.5000, Val Loss: 5073736.5000\n",
      "Epoch [6771/50000], Train Loss: 8256358.5000, Val Loss: 5073783.5000\n",
      "Epoch [6772/50000], Train Loss: 8256086.5000, Val Loss: 5073830.5000\n",
      "Epoch [6773/50000], Train Loss: 8255813.5000, Val Loss: 5073878.0000\n",
      "Epoch [6774/50000], Train Loss: 8255541.5000, Val Loss: 5073925.0000\n",
      "Epoch [6775/50000], Train Loss: 8255269.0000, Val Loss: 5073973.0000\n",
      "Epoch [6776/50000], Train Loss: 8254998.5000, Val Loss: 5074020.5000\n",
      "Epoch [6777/50000], Train Loss: 8254727.5000, Val Loss: 5074069.5000\n",
      "Epoch [6778/50000], Train Loss: 8254456.5000, Val Loss: 5074117.5000\n",
      "Epoch [6779/50000], Train Loss: 8254186.5000, Val Loss: 5074166.5000\n",
      "Epoch [6780/50000], Train Loss: 8253916.0000, Val Loss: 5074214.5000\n",
      "Epoch [6781/50000], Train Loss: 8253646.5000, Val Loss: 5074264.0000\n",
      "Epoch [6782/50000], Train Loss: 8253377.0000, Val Loss: 5074313.5000\n",
      "Epoch [6783/50000], Train Loss: 8253107.5000, Val Loss: 5074362.5000\n",
      "Epoch [6784/50000], Train Loss: 8252839.5000, Val Loss: 5074412.5000\n",
      "Epoch [6785/50000], Train Loss: 8252570.5000, Val Loss: 5074461.5000\n",
      "Epoch [6786/50000], Train Loss: 8252302.5000, Val Loss: 5074511.5000\n",
      "Epoch [6787/50000], Train Loss: 8252034.0000, Val Loss: 5074562.0000\n",
      "Epoch [6788/50000], Train Loss: 8251767.0000, Val Loss: 5074612.5000\n",
      "Epoch [6789/50000], Train Loss: 8251500.0000, Val Loss: 5074662.5000\n",
      "Epoch [6790/50000], Train Loss: 8251233.0000, Val Loss: 5074714.0000\n",
      "Epoch [6791/50000], Train Loss: 8250966.5000, Val Loss: 5074765.5000\n",
      "Epoch [6792/50000], Train Loss: 8250700.5000, Val Loss: 5074816.0000\n",
      "Epoch [6793/50000], Train Loss: 8250434.0000, Val Loss: 5074866.5000\n",
      "Epoch [6794/50000], Train Loss: 8250168.0000, Val Loss: 5074919.0000\n",
      "Epoch [6795/50000], Train Loss: 8249903.5000, Val Loss: 5074970.0000\n",
      "Epoch [6796/50000], Train Loss: 8249639.5000, Val Loss: 5075023.0000\n",
      "Epoch [6797/50000], Train Loss: 8249375.0000, Val Loss: 5075074.0000\n",
      "Epoch [6798/50000], Train Loss: 8249110.5000, Val Loss: 5075127.0000\n",
      "Epoch [6799/50000], Train Loss: 8248845.5000, Val Loss: 5075179.0000\n",
      "Epoch [6800/50000], Train Loss: 8248582.0000, Val Loss: 5075232.5000\n",
      "Epoch [6801/50000], Train Loss: 8248319.5000, Val Loss: 5075285.5000\n",
      "Epoch [6802/50000], Train Loss: 8248056.0000, Val Loss: 5075338.5000\n",
      "Epoch [6803/50000], Train Loss: 8247794.0000, Val Loss: 5075392.0000\n",
      "Epoch [6804/50000], Train Loss: 8247532.0000, Val Loss: 5075446.0000\n",
      "Epoch [6805/50000], Train Loss: 8247269.5000, Val Loss: 5075499.0000\n",
      "Epoch [6806/50000], Train Loss: 8247007.5000, Val Loss: 5075553.0000\n",
      "Epoch [6807/50000], Train Loss: 8246747.0000, Val Loss: 5075607.0000\n",
      "Epoch [6808/50000], Train Loss: 8246485.0000, Val Loss: 5075662.0000\n",
      "Epoch [6809/50000], Train Loss: 8246224.5000, Val Loss: 5075715.5000\n",
      "Epoch [6810/50000], Train Loss: 8245965.5000, Val Loss: 5075770.5000\n",
      "Epoch [6811/50000], Train Loss: 8245705.0000, Val Loss: 5075825.5000\n",
      "Epoch [6812/50000], Train Loss: 8245445.0000, Val Loss: 5075880.5000\n",
      "Epoch [6813/50000], Train Loss: 8245185.5000, Val Loss: 5075935.5000\n",
      "Epoch [6814/50000], Train Loss: 8244926.0000, Val Loss: 5075991.5000\n",
      "Epoch [6815/50000], Train Loss: 8244668.0000, Val Loss: 5076047.0000\n",
      "Epoch [6816/50000], Train Loss: 8244410.0000, Val Loss: 5076103.0000\n",
      "Epoch [6817/50000], Train Loss: 8244151.0000, Val Loss: 5076159.0000\n",
      "Epoch [6818/50000], Train Loss: 8243894.0000, Val Loss: 5076215.0000\n",
      "Epoch [6819/50000], Train Loss: 8243635.5000, Val Loss: 5076271.5000\n",
      "Epoch [6820/50000], Train Loss: 8243378.5000, Val Loss: 5076328.0000\n",
      "Epoch [6821/50000], Train Loss: 8243123.0000, Val Loss: 5076385.0000\n",
      "Epoch [6822/50000], Train Loss: 8242865.0000, Val Loss: 5076442.5000\n",
      "Epoch [6823/50000], Train Loss: 8242609.0000, Val Loss: 5076499.0000\n",
      "Epoch [6824/50000], Train Loss: 8242354.0000, Val Loss: 5076556.5000\n",
      "Epoch [6825/50000], Train Loss: 8242097.5000, Val Loss: 5076614.0000\n",
      "Epoch [6826/50000], Train Loss: 8241842.5000, Val Loss: 5076671.5000\n",
      "Epoch [6827/50000], Train Loss: 8241588.0000, Val Loss: 5076729.5000\n",
      "Epoch [6828/50000], Train Loss: 8241333.5000, Val Loss: 5076787.0000\n",
      "Epoch [6829/50000], Train Loss: 8241079.0000, Val Loss: 5076846.0000\n",
      "Epoch [6830/50000], Train Loss: 8240825.5000, Val Loss: 5076905.0000\n",
      "Epoch [6831/50000], Train Loss: 8240570.5000, Val Loss: 5076963.5000\n",
      "Epoch [6832/50000], Train Loss: 8240318.0000, Val Loss: 5077023.0000\n",
      "Epoch [6833/50000], Train Loss: 8240065.0000, Val Loss: 5077081.5000\n",
      "Epoch [6834/50000], Train Loss: 8239813.0000, Val Loss: 5077141.5000\n",
      "Epoch [6835/50000], Train Loss: 8239561.0000, Val Loss: 5077200.5000\n",
      "Epoch [6836/50000], Train Loss: 8239306.5000, Val Loss: 5077259.5000\n",
      "Epoch [6837/50000], Train Loss: 8239056.5000, Val Loss: 5077320.0000\n",
      "Epoch [6838/50000], Train Loss: 8238804.0000, Val Loss: 5077379.0000\n",
      "Epoch [6839/50000], Train Loss: 8238554.0000, Val Loss: 5077440.0000\n",
      "Epoch [6840/50000], Train Loss: 8238303.5000, Val Loss: 5077500.5000\n",
      "Epoch [6841/50000], Train Loss: 8238051.5000, Val Loss: 5077560.5000\n",
      "Epoch [6842/50000], Train Loss: 8237802.5000, Val Loss: 5077622.0000\n",
      "Epoch [6843/50000], Train Loss: 8237552.5000, Val Loss: 5077682.5000\n",
      "Epoch [6844/50000], Train Loss: 8237302.5000, Val Loss: 5077743.5000\n",
      "Epoch [6845/50000], Train Loss: 8237053.0000, Val Loss: 5077805.0000\n",
      "Epoch [6846/50000], Train Loss: 8236805.0000, Val Loss: 5077866.0000\n",
      "Epoch [6847/50000], Train Loss: 8236556.0000, Val Loss: 5077927.5000\n",
      "Epoch [6848/50000], Train Loss: 8236308.0000, Val Loss: 5077989.5000\n",
      "Epoch [6849/50000], Train Loss: 8236060.0000, Val Loss: 5078051.5000\n",
      "Epoch [6850/50000], Train Loss: 8235812.5000, Val Loss: 5078114.0000\n",
      "Epoch [6851/50000], Train Loss: 8235565.0000, Val Loss: 5078176.5000\n",
      "Epoch [6852/50000], Train Loss: 8235317.0000, Val Loss: 5078239.0000\n",
      "Epoch [6853/50000], Train Loss: 8235071.0000, Val Loss: 5078302.0000\n",
      "Epoch [6854/50000], Train Loss: 8234825.0000, Val Loss: 5078364.5000\n",
      "Epoch [6855/50000], Train Loss: 8234579.0000, Val Loss: 5078427.5000\n",
      "Epoch [6856/50000], Train Loss: 8234332.5000, Val Loss: 5078490.5000\n",
      "Epoch [6857/50000], Train Loss: 8234087.5000, Val Loss: 5078554.5000\n",
      "Epoch [6858/50000], Train Loss: 8233842.0000, Val Loss: 5078617.5000\n",
      "Epoch [6859/50000], Train Loss: 8233598.0000, Val Loss: 5078681.5000\n",
      "Epoch [6860/50000], Train Loss: 8233353.0000, Val Loss: 5078745.5000\n",
      "Epoch [6861/50000], Train Loss: 8233110.0000, Val Loss: 5078810.0000\n",
      "Epoch [6862/50000], Train Loss: 8232865.0000, Val Loss: 5078874.5000\n",
      "Epoch [6863/50000], Train Loss: 8232621.5000, Val Loss: 5078938.5000\n",
      "Epoch [6864/50000], Train Loss: 8232378.0000, Val Loss: 5079003.5000\n",
      "Epoch [6865/50000], Train Loss: 8232136.0000, Val Loss: 5079069.0000\n",
      "Epoch [6866/50000], Train Loss: 8231891.5000, Val Loss: 5079134.0000\n",
      "Epoch [6867/50000], Train Loss: 8231650.0000, Val Loss: 5079199.0000\n",
      "Epoch [6868/50000], Train Loss: 8231407.5000, Val Loss: 5079264.0000\n",
      "Epoch [6869/50000], Train Loss: 8231166.5000, Val Loss: 5079330.0000\n",
      "Epoch [6870/50000], Train Loss: 8230925.0000, Val Loss: 5079396.5000\n",
      "Epoch [6871/50000], Train Loss: 8230684.0000, Val Loss: 5079462.5000\n",
      "Epoch [6872/50000], Train Loss: 8230443.0000, Val Loss: 5079528.0000\n",
      "Epoch [6873/50000], Train Loss: 8230202.0000, Val Loss: 5079593.5000\n",
      "Epoch [6874/50000], Train Loss: 8229962.0000, Val Loss: 5079660.5000\n",
      "Epoch [6875/50000], Train Loss: 8229722.5000, Val Loss: 5079727.5000\n",
      "Epoch [6876/50000], Train Loss: 8229483.0000, Val Loss: 5079794.0000\n",
      "Epoch [6877/50000], Train Loss: 8229243.5000, Val Loss: 5079861.0000\n",
      "Epoch [6878/50000], Train Loss: 8229004.5000, Val Loss: 5079928.5000\n",
      "Epoch [6879/50000], Train Loss: 8228765.0000, Val Loss: 5079995.5000\n",
      "Epoch [6880/50000], Train Loss: 8228528.5000, Val Loss: 5080063.5000\n",
      "Epoch [6881/50000], Train Loss: 8228290.0000, Val Loss: 5080131.0000\n",
      "Epoch [6882/50000], Train Loss: 8228051.5000, Val Loss: 5080199.0000\n",
      "Epoch [6883/50000], Train Loss: 8227813.5000, Val Loss: 5080267.5000\n",
      "Epoch [6884/50000], Train Loss: 8227577.5000, Val Loss: 5080335.5000\n",
      "Epoch [6885/50000], Train Loss: 8227340.0000, Val Loss: 5080403.5000\n",
      "Epoch [6886/50000], Train Loss: 8227103.5000, Val Loss: 5080472.5000\n",
      "Epoch [6887/50000], Train Loss: 8226867.5000, Val Loss: 5080541.0000\n",
      "Epoch [6888/50000], Train Loss: 8226631.5000, Val Loss: 5080610.5000\n",
      "Epoch [6889/50000], Train Loss: 8226395.5000, Val Loss: 5080679.5000\n",
      "Epoch [6890/50000], Train Loss: 8226161.0000, Val Loss: 5080749.0000\n",
      "Epoch [6891/50000], Train Loss: 8225925.0000, Val Loss: 5080818.0000\n",
      "Epoch [6892/50000], Train Loss: 8225691.5000, Val Loss: 5080887.5000\n",
      "Epoch [6893/50000], Train Loss: 8225456.5000, Val Loss: 5080958.0000\n",
      "Epoch [6894/50000], Train Loss: 8225222.0000, Val Loss: 5081027.0000\n",
      "Epoch [6895/50000], Train Loss: 8224989.0000, Val Loss: 5081097.5000\n",
      "Epoch [6896/50000], Train Loss: 8224754.5000, Val Loss: 5081168.0000\n",
      "Epoch [6897/50000], Train Loss: 8224521.0000, Val Loss: 5081238.0000\n",
      "Epoch [6898/50000], Train Loss: 8224289.0000, Val Loss: 5081309.0000\n",
      "Epoch [6899/50000], Train Loss: 8224055.5000, Val Loss: 5081379.5000\n",
      "Epoch [6900/50000], Train Loss: 8223824.5000, Val Loss: 5081450.5000\n",
      "Epoch [6901/50000], Train Loss: 8223592.5000, Val Loss: 5081521.5000\n",
      "Epoch [6902/50000], Train Loss: 8223361.0000, Val Loss: 5081592.5000\n",
      "Epoch [6903/50000], Train Loss: 8223128.5000, Val Loss: 5081664.5000\n",
      "Epoch [6904/50000], Train Loss: 8222899.0000, Val Loss: 5081736.5000\n",
      "Epoch [6905/50000], Train Loss: 8222667.0000, Val Loss: 5081808.0000\n",
      "Epoch [6906/50000], Train Loss: 8222436.5000, Val Loss: 5081879.5000\n",
      "Epoch [6907/50000], Train Loss: 8222206.5000, Val Loss: 5081952.5000\n",
      "Epoch [6908/50000], Train Loss: 8221976.0000, Val Loss: 5082024.5000\n",
      "Epoch [6909/50000], Train Loss: 8221747.0000, Val Loss: 5082097.0000\n",
      "Epoch [6910/50000], Train Loss: 8221518.0000, Val Loss: 5082169.5000\n",
      "Epoch [6911/50000], Train Loss: 8221289.0000, Val Loss: 5082243.0000\n",
      "Epoch [6912/50000], Train Loss: 8221059.5000, Val Loss: 5082315.5000\n",
      "Epoch [6913/50000], Train Loss: 8220831.5000, Val Loss: 5082389.0000\n",
      "Epoch [6914/50000], Train Loss: 8220603.5000, Val Loss: 5082462.5000\n",
      "Epoch [6915/50000], Train Loss: 8220375.0000, Val Loss: 5082535.0000\n",
      "Epoch [6916/50000], Train Loss: 8220147.5000, Val Loss: 5082609.0000\n",
      "Epoch [6917/50000], Train Loss: 8219921.0000, Val Loss: 5082682.5000\n",
      "Epoch [6918/50000], Train Loss: 8219693.5000, Val Loss: 5082757.5000\n",
      "Epoch [6919/50000], Train Loss: 8219467.0000, Val Loss: 5082831.0000\n",
      "Epoch [6920/50000], Train Loss: 8219241.0000, Val Loss: 5082905.5000\n",
      "Epoch [6921/50000], Train Loss: 8219015.5000, Val Loss: 5082979.5000\n",
      "Epoch [6922/50000], Train Loss: 8218790.0000, Val Loss: 5083055.0000\n",
      "Epoch [6923/50000], Train Loss: 8218563.5000, Val Loss: 5083129.0000\n",
      "Epoch [6924/50000], Train Loss: 8218339.0000, Val Loss: 5083204.5000\n",
      "Epoch [6925/50000], Train Loss: 8218113.5000, Val Loss: 5083279.5000\n",
      "Epoch [6926/50000], Train Loss: 8217889.5000, Val Loss: 5083354.5000\n",
      "Epoch [6927/50000], Train Loss: 8217665.0000, Val Loss: 5083430.0000\n",
      "Epoch [6928/50000], Train Loss: 8217441.0000, Val Loss: 5083505.5000\n",
      "Epoch [6929/50000], Train Loss: 8217217.0000, Val Loss: 5083581.5000\n",
      "Epoch [6930/50000], Train Loss: 8216994.0000, Val Loss: 5083657.5000\n",
      "Epoch [6931/50000], Train Loss: 8216771.0000, Val Loss: 5083733.5000\n",
      "Epoch [6932/50000], Train Loss: 8216548.5000, Val Loss: 5083810.0000\n",
      "Epoch [6933/50000], Train Loss: 8216326.0000, Val Loss: 5083886.5000\n",
      "Epoch [6934/50000], Train Loss: 8216102.5000, Val Loss: 5083962.5000\n",
      "Epoch [6935/50000], Train Loss: 8215881.0000, Val Loss: 5084040.0000\n",
      "Epoch [6936/50000], Train Loss: 8215659.0000, Val Loss: 5084116.5000\n",
      "Epoch [6937/50000], Train Loss: 8215438.0000, Val Loss: 5084193.5000\n",
      "Epoch [6938/50000], Train Loss: 8215217.0000, Val Loss: 5084270.5000\n",
      "Epoch [6939/50000], Train Loss: 8214997.0000, Val Loss: 5084347.5000\n",
      "Epoch [6940/50000], Train Loss: 8214775.5000, Val Loss: 5084425.5000\n",
      "Epoch [6941/50000], Train Loss: 8214556.5000, Val Loss: 5084502.5000\n",
      "Epoch [6942/50000], Train Loss: 8214335.5000, Val Loss: 5084580.5000\n",
      "Epoch [6943/50000], Train Loss: 8214116.0000, Val Loss: 5084659.0000\n",
      "Epoch [6944/50000], Train Loss: 8213897.0000, Val Loss: 5084736.5000\n",
      "Epoch [6945/50000], Train Loss: 8213678.5000, Val Loss: 5084815.0000\n",
      "Epoch [6946/50000], Train Loss: 8213461.0000, Val Loss: 5084894.0000\n",
      "Epoch [6947/50000], Train Loss: 8213241.5000, Val Loss: 5084972.5000\n",
      "Epoch [6948/50000], Train Loss: 8213023.0000, Val Loss: 5085051.0000\n",
      "Epoch [6949/50000], Train Loss: 8212805.5000, Val Loss: 5085129.5000\n",
      "Epoch [6950/50000], Train Loss: 8212588.0000, Val Loss: 5085209.0000\n",
      "Epoch [6951/50000], Train Loss: 8212371.5000, Val Loss: 5085287.5000\n",
      "Epoch [6952/50000], Train Loss: 8212154.0000, Val Loss: 5085367.5000\n",
      "Epoch [6953/50000], Train Loss: 8211936.0000, Val Loss: 5085447.5000\n",
      "Epoch [6954/50000], Train Loss: 8211721.0000, Val Loss: 5085527.0000\n",
      "Epoch [6955/50000], Train Loss: 8211504.5000, Val Loss: 5085607.0000\n",
      "Epoch [6956/50000], Train Loss: 8211288.5000, Val Loss: 5085686.5000\n",
      "Epoch [6957/50000], Train Loss: 8211073.0000, Val Loss: 5085767.0000\n",
      "Epoch [6958/50000], Train Loss: 8210857.5000, Val Loss: 5085847.5000\n",
      "Epoch [6959/50000], Train Loss: 8210642.5000, Val Loss: 5085928.0000\n",
      "Epoch [6960/50000], Train Loss: 8210428.5000, Val Loss: 5086008.0000\n",
      "Epoch [6961/50000], Train Loss: 8210214.0000, Val Loss: 5086088.5000\n",
      "Epoch [6962/50000], Train Loss: 8210000.5000, Val Loss: 5086170.0000\n",
      "Epoch [6963/50000], Train Loss: 8209785.5000, Val Loss: 5086251.0000\n",
      "Epoch [6964/50000], Train Loss: 8209572.5000, Val Loss: 5086333.0000\n",
      "Epoch [6965/50000], Train Loss: 8209359.5000, Val Loss: 5086414.0000\n",
      "Epoch [6966/50000], Train Loss: 8209146.5000, Val Loss: 5086495.0000\n",
      "Epoch [6967/50000], Train Loss: 8208934.0000, Val Loss: 5086576.5000\n",
      "Epoch [6968/50000], Train Loss: 8208721.0000, Val Loss: 5086659.0000\n",
      "Epoch [6969/50000], Train Loss: 8208508.5000, Val Loss: 5086740.5000\n",
      "Epoch [6970/50000], Train Loss: 8208297.5000, Val Loss: 5086823.0000\n",
      "Epoch [6971/50000], Train Loss: 8208085.5000, Val Loss: 5086905.5000\n",
      "Epoch [6972/50000], Train Loss: 8207875.0000, Val Loss: 5086987.0000\n",
      "Epoch [6973/50000], Train Loss: 8207664.5000, Val Loss: 5087070.5000\n",
      "Epoch [6974/50000], Train Loss: 8207454.0000, Val Loss: 5087153.0000\n",
      "Epoch [6975/50000], Train Loss: 8207242.5000, Val Loss: 5087235.5000\n",
      "Epoch [6976/50000], Train Loss: 8207032.5000, Val Loss: 5087318.5000\n",
      "Epoch [6977/50000], Train Loss: 8206823.5000, Val Loss: 5087402.0000\n",
      "Epoch [6978/50000], Train Loss: 8206614.0000, Val Loss: 5087486.0000\n",
      "Epoch [6979/50000], Train Loss: 8206404.0000, Val Loss: 5087569.0000\n",
      "Epoch [6980/50000], Train Loss: 8206195.5000, Val Loss: 5087652.5000\n",
      "Epoch [6981/50000], Train Loss: 8205987.0000, Val Loss: 5087736.0000\n",
      "Epoch [6982/50000], Train Loss: 8205778.5000, Val Loss: 5087819.5000\n",
      "Epoch [6983/50000], Train Loss: 8205570.0000, Val Loss: 5087904.0000\n",
      "Epoch [6984/50000], Train Loss: 8205362.0000, Val Loss: 5087989.0000\n",
      "Epoch [6985/50000], Train Loss: 8205154.0000, Val Loss: 5088073.0000\n",
      "Epoch [6986/50000], Train Loss: 8204947.5000, Val Loss: 5088158.0000\n",
      "Epoch [6987/50000], Train Loss: 8204740.5000, Val Loss: 5088242.0000\n",
      "Epoch [6988/50000], Train Loss: 8204533.0000, Val Loss: 5088327.5000\n",
      "Epoch [6989/50000], Train Loss: 8204327.5000, Val Loss: 5088411.5000\n",
      "Epoch [6990/50000], Train Loss: 8204120.5000, Val Loss: 5088497.0000\n",
      "Epoch [6991/50000], Train Loss: 8203915.0000, Val Loss: 5088582.0000\n",
      "Epoch [6992/50000], Train Loss: 8203710.0000, Val Loss: 5088667.0000\n",
      "Epoch [6993/50000], Train Loss: 8203506.0000, Val Loss: 5088753.0000\n",
      "Epoch [6994/50000], Train Loss: 8203299.0000, Val Loss: 5088838.5000\n",
      "Epoch [6995/50000], Train Loss: 8203093.5000, Val Loss: 5088923.5000\n",
      "Epoch [6996/50000], Train Loss: 8202889.5000, Val Loss: 5089010.0000\n",
      "Epoch [6997/50000], Train Loss: 8202686.0000, Val Loss: 5089096.5000\n",
      "Epoch [6998/50000], Train Loss: 8202482.0000, Val Loss: 5089182.5000\n",
      "Epoch [6999/50000], Train Loss: 8202278.0000, Val Loss: 5089269.0000\n",
      "Epoch [7000/50000], Train Loss: 8202075.0000, Val Loss: 5089355.0000\n",
      "Epoch [7001/50000], Train Loss: 8201872.5000, Val Loss: 5089442.0000\n",
      "Epoch [7002/50000], Train Loss: 8201668.5000, Val Loss: 5089528.5000\n",
      "Epoch [7003/50000], Train Loss: 8201466.0000, Val Loss: 5089616.0000\n",
      "Epoch [7004/50000], Train Loss: 8201263.5000, Val Loss: 5089703.5000\n",
      "Epoch [7005/50000], Train Loss: 8201061.5000, Val Loss: 5089790.5000\n",
      "Epoch [7006/50000], Train Loss: 8200860.0000, Val Loss: 5089877.5000\n",
      "Epoch [7007/50000], Train Loss: 8200659.0000, Val Loss: 5089965.0000\n",
      "Epoch [7008/50000], Train Loss: 8200457.5000, Val Loss: 5090053.0000\n",
      "Epoch [7009/50000], Train Loss: 8200256.5000, Val Loss: 5090139.5000\n",
      "Epoch [7010/50000], Train Loss: 8200056.0000, Val Loss: 5090228.5000\n",
      "Epoch [7011/50000], Train Loss: 8199856.5000, Val Loss: 5090316.5000\n",
      "Epoch [7012/50000], Train Loss: 8199656.5000, Val Loss: 5090404.5000\n",
      "Epoch [7013/50000], Train Loss: 8199457.0000, Val Loss: 5090493.0000\n",
      "Epoch [7014/50000], Train Loss: 8199256.5000, Val Loss: 5090582.0000\n",
      "Epoch [7015/50000], Train Loss: 8199058.0000, Val Loss: 5090670.0000\n",
      "Epoch [7016/50000], Train Loss: 8198859.0000, Val Loss: 5090759.5000\n",
      "Epoch [7017/50000], Train Loss: 8198659.5000, Val Loss: 5090848.0000\n",
      "Epoch [7018/50000], Train Loss: 8198462.5000, Val Loss: 5090937.0000\n",
      "Epoch [7019/50000], Train Loss: 8198263.0000, Val Loss: 5091025.5000\n",
      "Epoch [7020/50000], Train Loss: 8198065.5000, Val Loss: 5091115.0000\n",
      "Epoch [7021/50000], Train Loss: 8197868.5000, Val Loss: 5091205.0000\n",
      "Epoch [7022/50000], Train Loss: 8197670.5000, Val Loss: 5091294.0000\n",
      "Epoch [7023/50000], Train Loss: 8197473.5000, Val Loss: 5091384.0000\n",
      "Epoch [7024/50000], Train Loss: 8197278.5000, Val Loss: 5091474.0000\n",
      "Epoch [7025/50000], Train Loss: 8197080.5000, Val Loss: 5091563.5000\n",
      "Epoch [7026/50000], Train Loss: 8196884.0000, Val Loss: 5091654.0000\n",
      "Epoch [7027/50000], Train Loss: 8196687.5000, Val Loss: 5091744.0000\n",
      "Epoch [7028/50000], Train Loss: 8196492.0000, Val Loss: 5091835.0000\n",
      "Epoch [7029/50000], Train Loss: 8196296.5000, Val Loss: 5091925.5000\n",
      "Epoch [7030/50000], Train Loss: 8196102.0000, Val Loss: 5092015.5000\n",
      "Epoch [7031/50000], Train Loss: 8195907.0000, Val Loss: 5092106.5000\n",
      "Epoch [7032/50000], Train Loss: 8195712.5000, Val Loss: 5092198.0000\n",
      "Epoch [7033/50000], Train Loss: 8195518.5000, Val Loss: 5092288.0000\n",
      "Epoch [7034/50000], Train Loss: 8195324.5000, Val Loss: 5092379.0000\n",
      "Epoch [7035/50000], Train Loss: 8195130.0000, Val Loss: 5092471.0000\n",
      "Epoch [7036/50000], Train Loss: 8194936.0000, Val Loss: 5092562.0000\n",
      "Epoch [7037/50000], Train Loss: 8194742.0000, Val Loss: 5092654.0000\n",
      "Epoch [7038/50000], Train Loss: 8194550.0000, Val Loss: 5092745.5000\n",
      "Epoch [7039/50000], Train Loss: 8194358.0000, Val Loss: 5092837.5000\n",
      "Epoch [7040/50000], Train Loss: 8194165.5000, Val Loss: 5092929.5000\n",
      "Epoch [7041/50000], Train Loss: 8193973.0000, Val Loss: 5093021.5000\n",
      "Epoch [7042/50000], Train Loss: 8193781.0000, Val Loss: 5093114.0000\n",
      "Epoch [7043/50000], Train Loss: 8193589.0000, Val Loss: 5093207.0000\n",
      "Epoch [7044/50000], Train Loss: 8193397.5000, Val Loss: 5093299.0000\n",
      "Epoch [7045/50000], Train Loss: 8193206.5000, Val Loss: 5093391.5000\n",
      "Epoch [7046/50000], Train Loss: 8193016.0000, Val Loss: 5093485.0000\n",
      "Epoch [7047/50000], Train Loss: 8192825.5000, Val Loss: 5093577.0000\n",
      "Epoch [7048/50000], Train Loss: 8192635.5000, Val Loss: 5093670.5000\n",
      "Epoch [7049/50000], Train Loss: 8192446.0000, Val Loss: 5093763.5000\n",
      "Epoch [7050/50000], Train Loss: 8192254.5000, Val Loss: 5093857.0000\n",
      "Epoch [7051/50000], Train Loss: 8192065.0000, Val Loss: 5093950.5000\n",
      "Epoch [7052/50000], Train Loss: 8191876.0000, Val Loss: 5094043.5000\n",
      "Epoch [7053/50000], Train Loss: 8191688.0000, Val Loss: 5094137.5000\n",
      "Epoch [7054/50000], Train Loss: 8191498.5000, Val Loss: 5094231.5000\n",
      "Epoch [7055/50000], Train Loss: 8191310.0000, Val Loss: 5094325.5000\n",
      "Epoch [7056/50000], Train Loss: 8191122.0000, Val Loss: 5094419.0000\n",
      "Epoch [7057/50000], Train Loss: 8190934.5000, Val Loss: 5094513.5000\n",
      "Epoch [7058/50000], Train Loss: 8190746.0000, Val Loss: 5094608.0000\n",
      "Epoch [7059/50000], Train Loss: 8190559.0000, Val Loss: 5094702.5000\n",
      "Epoch [7060/50000], Train Loss: 8190372.0000, Val Loss: 5094796.5000\n",
      "Epoch [7061/50000], Train Loss: 8190184.5000, Val Loss: 5094891.5000\n",
      "Epoch [7062/50000], Train Loss: 8189998.5000, Val Loss: 5094986.5000\n",
      "Epoch [7063/50000], Train Loss: 8189811.5000, Val Loss: 5095081.5000\n",
      "Epoch [7064/50000], Train Loss: 8189626.0000, Val Loss: 5095176.5000\n",
      "Epoch [7065/50000], Train Loss: 8189440.5000, Val Loss: 5095271.5000\n",
      "Epoch [7066/50000], Train Loss: 8189253.5000, Val Loss: 5095366.5000\n",
      "Epoch [7067/50000], Train Loss: 8189069.5000, Val Loss: 5095462.5000\n",
      "Epoch [7068/50000], Train Loss: 8188883.5000, Val Loss: 5095557.5000\n",
      "Epoch [7069/50000], Train Loss: 8188698.5000, Val Loss: 5095653.5000\n",
      "Epoch [7070/50000], Train Loss: 8188515.0000, Val Loss: 5095749.5000\n",
      "Epoch [7071/50000], Train Loss: 8188329.5000, Val Loss: 5095845.5000\n",
      "Epoch [7072/50000], Train Loss: 8188146.5000, Val Loss: 5095941.5000\n",
      "Epoch [7073/50000], Train Loss: 8187962.0000, Val Loss: 5096038.0000\n",
      "Epoch [7074/50000], Train Loss: 8187779.0000, Val Loss: 5096134.5000\n",
      "Epoch [7075/50000], Train Loss: 8187596.0000, Val Loss: 5096230.5000\n",
      "Epoch [7076/50000], Train Loss: 8187412.0000, Val Loss: 5096327.0000\n",
      "Epoch [7077/50000], Train Loss: 8187230.5000, Val Loss: 5096423.5000\n",
      "Epoch [7078/50000], Train Loss: 8187048.5000, Val Loss: 5096520.5000\n",
      "Epoch [7079/50000], Train Loss: 8186865.5000, Val Loss: 5096617.5000\n",
      "Epoch [7080/50000], Train Loss: 8186683.5000, Val Loss: 5096714.5000\n",
      "Epoch [7081/50000], Train Loss: 8186502.0000, Val Loss: 5096811.5000\n",
      "Epoch [7082/50000], Train Loss: 8186320.5000, Val Loss: 5096909.0000\n",
      "Epoch [7083/50000], Train Loss: 8186139.5000, Val Loss: 5097006.0000\n",
      "Epoch [7084/50000], Train Loss: 8185958.0000, Val Loss: 5097104.0000\n",
      "Epoch [7085/50000], Train Loss: 8185777.5000, Val Loss: 5097201.0000\n",
      "Epoch [7086/50000], Train Loss: 8185598.0000, Val Loss: 5097299.0000\n",
      "Epoch [7087/50000], Train Loss: 8185417.0000, Val Loss: 5097397.5000\n",
      "Epoch [7088/50000], Train Loss: 8185238.0000, Val Loss: 5097495.0000\n",
      "Epoch [7089/50000], Train Loss: 8185056.5000, Val Loss: 5097593.0000\n",
      "Epoch [7090/50000], Train Loss: 8184878.0000, Val Loss: 5097691.0000\n",
      "Epoch [7091/50000], Train Loss: 8184700.0000, Val Loss: 5097790.0000\n",
      "Epoch [7092/50000], Train Loss: 8184520.0000, Val Loss: 5097889.0000\n",
      "Epoch [7093/50000], Train Loss: 8184341.0000, Val Loss: 5097987.0000\n",
      "Epoch [7094/50000], Train Loss: 8184163.5000, Val Loss: 5098085.5000\n",
      "Epoch [7095/50000], Train Loss: 8183985.0000, Val Loss: 5098184.5000\n",
      "Epoch [7096/50000], Train Loss: 8183807.5000, Val Loss: 5098283.5000\n",
      "Epoch [7097/50000], Train Loss: 8183630.5000, Val Loss: 5098383.0000\n",
      "Epoch [7098/50000], Train Loss: 8183452.5000, Val Loss: 5098481.5000\n",
      "Epoch [7099/50000], Train Loss: 8183276.0000, Val Loss: 5098581.0000\n",
      "Epoch [7100/50000], Train Loss: 8183099.5000, Val Loss: 5098680.0000\n",
      "Epoch [7101/50000], Train Loss: 8182921.5000, Val Loss: 5098780.5000\n",
      "Epoch [7102/50000], Train Loss: 8182746.0000, Val Loss: 5098880.0000\n",
      "Epoch [7103/50000], Train Loss: 8182570.5000, Val Loss: 5098979.0000\n",
      "Epoch [7104/50000], Train Loss: 8182394.0000, Val Loss: 5099080.0000\n",
      "Epoch [7105/50000], Train Loss: 8182218.0000, Val Loss: 5099179.0000\n",
      "Epoch [7106/50000], Train Loss: 8182043.5000, Val Loss: 5099279.0000\n",
      "Epoch [7107/50000], Train Loss: 8181869.0000, Val Loss: 5099379.0000\n",
      "Epoch [7108/50000], Train Loss: 8181694.0000, Val Loss: 5099479.5000\n",
      "Epoch [7109/50000], Train Loss: 8181519.0000, Val Loss: 5099580.5000\n",
      "Epoch [7110/50000], Train Loss: 8181345.0000, Val Loss: 5099681.5000\n",
      "Epoch [7111/50000], Train Loss: 8181170.5000, Val Loss: 5099782.0000\n",
      "Epoch [7112/50000], Train Loss: 8180997.5000, Val Loss: 5099882.5000\n",
      "Epoch [7113/50000], Train Loss: 8180823.5000, Val Loss: 5099983.5000\n",
      "Epoch [7114/50000], Train Loss: 8180650.5000, Val Loss: 5100084.5000\n",
      "Epoch [7115/50000], Train Loss: 8180477.0000, Val Loss: 5100185.5000\n",
      "Epoch [7116/50000], Train Loss: 8180305.0000, Val Loss: 5100286.5000\n",
      "Epoch [7117/50000], Train Loss: 8180132.0000, Val Loss: 5100388.5000\n",
      "Epoch [7118/50000], Train Loss: 8179960.0000, Val Loss: 5100489.5000\n",
      "Epoch [7119/50000], Train Loss: 8179788.5000, Val Loss: 5100591.0000\n",
      "Epoch [7120/50000], Train Loss: 8179615.5000, Val Loss: 5100693.0000\n",
      "Epoch [7121/50000], Train Loss: 8179445.0000, Val Loss: 5100794.5000\n",
      "Epoch [7122/50000], Train Loss: 8179272.5000, Val Loss: 5100896.5000\n",
      "Epoch [7123/50000], Train Loss: 8179101.0000, Val Loss: 5100999.0000\n",
      "Epoch [7124/50000], Train Loss: 8178930.5000, Val Loss: 5101101.0000\n",
      "Epoch [7125/50000], Train Loss: 8178760.5000, Val Loss: 5101202.5000\n",
      "Epoch [7126/50000], Train Loss: 8178590.0000, Val Loss: 5101304.5000\n",
      "Epoch [7127/50000], Train Loss: 8178420.0000, Val Loss: 5101408.0000\n",
      "Epoch [7128/50000], Train Loss: 8178250.5000, Val Loss: 5101510.5000\n",
      "Epoch [7129/50000], Train Loss: 8178081.5000, Val Loss: 5101613.0000\n",
      "Epoch [7130/50000], Train Loss: 8177911.5000, Val Loss: 5101715.5000\n",
      "Epoch [7131/50000], Train Loss: 8177742.0000, Val Loss: 5101818.5000\n",
      "Epoch [7132/50000], Train Loss: 8177574.0000, Val Loss: 5101922.5000\n",
      "Epoch [7133/50000], Train Loss: 8177404.5000, Val Loss: 5102025.0000\n",
      "Epoch [7134/50000], Train Loss: 8177237.5000, Val Loss: 5102127.5000\n",
      "Epoch [7135/50000], Train Loss: 8177069.0000, Val Loss: 5102231.5000\n",
      "Epoch [7136/50000], Train Loss: 8176901.5000, Val Loss: 5102335.0000\n",
      "Epoch [7137/50000], Train Loss: 8176734.5000, Val Loss: 5102438.5000\n",
      "Epoch [7138/50000], Train Loss: 8176566.0000, Val Loss: 5102542.0000\n",
      "Epoch [7139/50000], Train Loss: 8176398.5000, Val Loss: 5102646.0000\n",
      "Epoch [7140/50000], Train Loss: 8176233.0000, Val Loss: 5102750.0000\n",
      "Epoch [7141/50000], Train Loss: 8176065.5000, Val Loss: 5102854.0000\n",
      "Epoch [7142/50000], Train Loss: 8175899.5000, Val Loss: 5102957.5000\n",
      "Epoch [7143/50000], Train Loss: 8175734.5000, Val Loss: 5103062.0000\n",
      "Epoch [7144/50000], Train Loss: 8175567.5000, Val Loss: 5103166.5000\n",
      "Epoch [7145/50000], Train Loss: 8175401.5000, Val Loss: 5103270.5000\n",
      "Epoch [7146/50000], Train Loss: 8175237.0000, Val Loss: 5103374.5000\n",
      "Epoch [7147/50000], Train Loss: 8175071.5000, Val Loss: 5103479.5000\n",
      "Epoch [7148/50000], Train Loss: 8174906.0000, Val Loss: 5103584.5000\n",
      "Epoch [7149/50000], Train Loss: 8174742.5000, Val Loss: 5103688.0000\n",
      "Epoch [7150/50000], Train Loss: 8174577.5000, Val Loss: 5103794.0000\n",
      "Epoch [7151/50000], Train Loss: 8174413.5000, Val Loss: 5103898.5000\n",
      "Epoch [7152/50000], Train Loss: 8174249.5000, Val Loss: 5104003.5000\n",
      "Epoch [7153/50000], Train Loss: 8174085.5000, Val Loss: 5104109.5000\n",
      "Epoch [7154/50000], Train Loss: 8173922.0000, Val Loss: 5104214.5000\n",
      "Epoch [7155/50000], Train Loss: 8173759.5000, Val Loss: 5104320.0000\n",
      "Epoch [7156/50000], Train Loss: 8173596.0000, Val Loss: 5104425.5000\n",
      "Epoch [7157/50000], Train Loss: 8173434.0000, Val Loss: 5104531.0000\n",
      "Epoch [7158/50000], Train Loss: 8173270.5000, Val Loss: 5104637.0000\n",
      "Epoch [7159/50000], Train Loss: 8173109.0000, Val Loss: 5104743.0000\n",
      "Epoch [7160/50000], Train Loss: 8172947.0000, Val Loss: 5104849.0000\n",
      "Epoch [7161/50000], Train Loss: 8172785.5000, Val Loss: 5104954.5000\n",
      "Epoch [7162/50000], Train Loss: 8172623.0000, Val Loss: 5105061.0000\n",
      "Epoch [7163/50000], Train Loss: 8172461.5000, Val Loss: 5105168.0000\n",
      "Epoch [7164/50000], Train Loss: 8172301.5000, Val Loss: 5105273.5000\n",
      "Epoch [7165/50000], Train Loss: 8172140.5000, Val Loss: 5105379.0000\n",
      "Epoch [7166/50000], Train Loss: 8171980.0000, Val Loss: 5105487.0000\n",
      "Epoch [7167/50000], Train Loss: 8171820.0000, Val Loss: 5105593.0000\n",
      "Epoch [7168/50000], Train Loss: 8171660.5000, Val Loss: 5105699.5000\n",
      "Epoch [7169/50000], Train Loss: 8171498.5000, Val Loss: 5105806.5000\n",
      "Epoch [7170/50000], Train Loss: 8171340.5000, Val Loss: 5105913.5000\n",
      "Epoch [7171/50000], Train Loss: 8171180.5000, Val Loss: 5106019.5000\n",
      "Epoch [7172/50000], Train Loss: 8171021.5000, Val Loss: 5106127.5000\n",
      "Epoch [7173/50000], Train Loss: 8170863.0000, Val Loss: 5106234.5000\n",
      "Epoch [7174/50000], Train Loss: 8170704.5000, Val Loss: 5106342.0000\n",
      "Epoch [7175/50000], Train Loss: 8170546.0000, Val Loss: 5106449.5000\n",
      "Epoch [7176/50000], Train Loss: 8170388.0000, Val Loss: 5106557.0000\n",
      "Epoch [7177/50000], Train Loss: 8170230.0000, Val Loss: 5106664.0000\n",
      "Epoch [7178/50000], Train Loss: 8170072.0000, Val Loss: 5106772.5000\n",
      "Epoch [7179/50000], Train Loss: 8169915.0000, Val Loss: 5106880.5000\n",
      "Epoch [7180/50000], Train Loss: 8169756.5000, Val Loss: 5106987.5000\n",
      "Epoch [7181/50000], Train Loss: 8169600.5000, Val Loss: 5107096.0000\n",
      "Epoch [7182/50000], Train Loss: 8169444.0000, Val Loss: 5107203.5000\n",
      "Epoch [7183/50000], Train Loss: 8169286.5000, Val Loss: 5107312.5000\n",
      "Epoch [7184/50000], Train Loss: 8169130.5000, Val Loss: 5107421.0000\n",
      "Epoch [7185/50000], Train Loss: 8168974.0000, Val Loss: 5107528.5000\n",
      "Epoch [7186/50000], Train Loss: 8168819.0000, Val Loss: 5107637.5000\n",
      "Epoch [7187/50000], Train Loss: 8168664.0000, Val Loss: 5107746.0000\n",
      "Epoch [7188/50000], Train Loss: 8168508.0000, Val Loss: 5107855.0000\n",
      "Epoch [7189/50000], Train Loss: 8168353.0000, Val Loss: 5107962.5000\n",
      "Epoch [7190/50000], Train Loss: 8168198.0000, Val Loss: 5108072.0000\n",
      "Epoch [7191/50000], Train Loss: 8168043.0000, Val Loss: 5108181.5000\n",
      "Epoch [7192/50000], Train Loss: 8167889.0000, Val Loss: 5108289.5000\n",
      "Epoch [7193/50000], Train Loss: 8167734.5000, Val Loss: 5108399.0000\n",
      "Epoch [7194/50000], Train Loss: 8167580.5000, Val Loss: 5108508.5000\n",
      "Epoch [7195/50000], Train Loss: 8167427.5000, Val Loss: 5108617.0000\n",
      "Epoch [7196/50000], Train Loss: 8167273.5000, Val Loss: 5108726.5000\n",
      "Epoch [7197/50000], Train Loss: 8167121.0000, Val Loss: 5108837.0000\n",
      "Epoch [7198/50000], Train Loss: 8166967.5000, Val Loss: 5108946.0000\n",
      "Epoch [7199/50000], Train Loss: 8166815.5000, Val Loss: 5109055.5000\n",
      "Epoch [7200/50000], Train Loss: 8166662.0000, Val Loss: 5109165.0000\n",
      "Epoch [7201/50000], Train Loss: 8166510.0000, Val Loss: 5109275.5000\n",
      "Epoch [7202/50000], Train Loss: 8166358.5000, Val Loss: 5109385.0000\n",
      "Epoch [7203/50000], Train Loss: 8166206.0000, Val Loss: 5109495.5000\n",
      "Epoch [7204/50000], Train Loss: 8166054.0000, Val Loss: 5109605.5000\n",
      "Epoch [7205/50000], Train Loss: 8165903.0000, Val Loss: 5109715.5000\n",
      "Epoch [7206/50000], Train Loss: 8165751.5000, Val Loss: 5109826.0000\n",
      "Epoch [7207/50000], Train Loss: 8165600.5000, Val Loss: 5109936.5000\n",
      "Epoch [7208/50000], Train Loss: 8165449.5000, Val Loss: 5110046.5000\n",
      "Epoch [7209/50000], Train Loss: 8165299.5000, Val Loss: 5110157.0000\n",
      "Epoch [7210/50000], Train Loss: 8165150.0000, Val Loss: 5110267.5000\n",
      "Epoch [7211/50000], Train Loss: 8164999.5000, Val Loss: 5110378.5000\n",
      "Epoch [7212/50000], Train Loss: 8164850.0000, Val Loss: 5110489.5000\n",
      "Epoch [7213/50000], Train Loss: 8164699.0000, Val Loss: 5110600.5000\n",
      "Epoch [7214/50000], Train Loss: 8164550.0000, Val Loss: 5110711.5000\n",
      "Epoch [7215/50000], Train Loss: 8164402.0000, Val Loss: 5110822.5000\n",
      "Epoch [7216/50000], Train Loss: 8164253.0000, Val Loss: 5110934.0000\n",
      "Epoch [7217/50000], Train Loss: 8164104.5000, Val Loss: 5111045.0000\n",
      "Epoch [7218/50000], Train Loss: 8163955.5000, Val Loss: 5111156.5000\n",
      "Epoch [7219/50000], Train Loss: 8163808.0000, Val Loss: 5111267.0000\n",
      "Epoch [7220/50000], Train Loss: 8163659.0000, Val Loss: 5111379.0000\n",
      "Epoch [7221/50000], Train Loss: 8163511.5000, Val Loss: 5111490.0000\n",
      "Epoch [7222/50000], Train Loss: 8163363.5000, Val Loss: 5111602.0000\n",
      "Epoch [7223/50000], Train Loss: 8163217.0000, Val Loss: 5111714.5000\n",
      "Epoch [7224/50000], Train Loss: 8163070.0000, Val Loss: 5111825.5000\n",
      "Epoch [7225/50000], Train Loss: 8162922.5000, Val Loss: 5111938.0000\n",
      "Epoch [7226/50000], Train Loss: 8162775.5000, Val Loss: 5112049.5000\n",
      "Epoch [7227/50000], Train Loss: 8162630.5000, Val Loss: 5112162.0000\n",
      "Epoch [7228/50000], Train Loss: 8162484.0000, Val Loss: 5112274.0000\n",
      "Epoch [7229/50000], Train Loss: 8162337.5000, Val Loss: 5112386.0000\n",
      "Epoch [7230/50000], Train Loss: 8162193.0000, Val Loss: 5112499.0000\n",
      "Epoch [7231/50000], Train Loss: 8162046.5000, Val Loss: 5112611.0000\n",
      "Epoch [7232/50000], Train Loss: 8161901.5000, Val Loss: 5112723.5000\n",
      "Epoch [7233/50000], Train Loss: 8161757.0000, Val Loss: 5112836.5000\n",
      "Epoch [7234/50000], Train Loss: 8161611.0000, Val Loss: 5112949.5000\n",
      "Epoch [7235/50000], Train Loss: 8161467.5000, Val Loss: 5113061.5000\n",
      "Epoch [7236/50000], Train Loss: 8161322.5000, Val Loss: 5113175.5000\n",
      "Epoch [7237/50000], Train Loss: 8161178.0000, Val Loss: 5113287.5000\n",
      "Epoch [7238/50000], Train Loss: 8161034.5000, Val Loss: 5113401.0000\n",
      "Epoch [7239/50000], Train Loss: 8160890.5000, Val Loss: 5113513.5000\n",
      "Epoch [7240/50000], Train Loss: 8160748.0000, Val Loss: 5113626.5000\n",
      "Epoch [7241/50000], Train Loss: 8160603.5000, Val Loss: 5113739.5000\n",
      "Epoch [7242/50000], Train Loss: 8160461.5000, Val Loss: 5113853.5000\n",
      "Epoch [7243/50000], Train Loss: 8160318.5000, Val Loss: 5113967.0000\n",
      "Epoch [7244/50000], Train Loss: 8160175.5000, Val Loss: 5114080.0000\n",
      "Epoch [7245/50000], Train Loss: 8160033.5000, Val Loss: 5114194.0000\n",
      "Epoch [7246/50000], Train Loss: 8159890.5000, Val Loss: 5114307.5000\n",
      "Epoch [7247/50000], Train Loss: 8159749.0000, Val Loss: 5114421.5000\n",
      "Epoch [7248/50000], Train Loss: 8159607.5000, Val Loss: 5114535.5000\n",
      "Epoch [7249/50000], Train Loss: 8159466.0000, Val Loss: 5114649.0000\n",
      "Epoch [7250/50000], Train Loss: 8159324.5000, Val Loss: 5114763.0000\n",
      "Epoch [7251/50000], Train Loss: 8159183.5000, Val Loss: 5114878.0000\n",
      "Epoch [7252/50000], Train Loss: 8159043.5000, Val Loss: 5114991.0000\n",
      "Epoch [7253/50000], Train Loss: 8158903.0000, Val Loss: 5115105.0000\n",
      "Epoch [7254/50000], Train Loss: 8158761.5000, Val Loss: 5115219.5000\n",
      "Epoch [7255/50000], Train Loss: 8158621.0000, Val Loss: 5115334.5000\n",
      "Epoch [7256/50000], Train Loss: 8158482.0000, Val Loss: 5115448.5000\n",
      "Epoch [7257/50000], Train Loss: 8158342.0000, Val Loss: 5115563.0000\n",
      "Epoch [7258/50000], Train Loss: 8158202.5000, Val Loss: 5115678.0000\n",
      "Epoch [7259/50000], Train Loss: 8158063.0000, Val Loss: 5115792.0000\n",
      "Epoch [7260/50000], Train Loss: 8157924.5000, Val Loss: 5115907.0000\n",
      "Epoch [7261/50000], Train Loss: 8157784.5000, Val Loss: 5116021.5000\n",
      "Epoch [7262/50000], Train Loss: 8157646.5000, Val Loss: 5116136.5000\n",
      "Epoch [7263/50000], Train Loss: 8157508.0000, Val Loss: 5116251.5000\n",
      "Epoch [7264/50000], Train Loss: 8157370.0000, Val Loss: 5116367.0000\n",
      "Epoch [7265/50000], Train Loss: 8157233.0000, Val Loss: 5116481.5000\n",
      "Epoch [7266/50000], Train Loss: 8157094.0000, Val Loss: 5116597.5000\n",
      "Epoch [7267/50000], Train Loss: 8156956.0000, Val Loss: 5116712.0000\n",
      "Epoch [7268/50000], Train Loss: 8156818.5000, Val Loss: 5116827.5000\n",
      "Epoch [7269/50000], Train Loss: 8156682.5000, Val Loss: 5116943.0000\n",
      "Epoch [7270/50000], Train Loss: 8156545.0000, Val Loss: 5117058.5000\n",
      "Epoch [7271/50000], Train Loss: 8156408.0000, Val Loss: 5117174.0000\n",
      "Epoch [7272/50000], Train Loss: 8156272.5000, Val Loss: 5117289.5000\n",
      "Epoch [7273/50000], Train Loss: 8156136.0000, Val Loss: 5117405.5000\n",
      "Epoch [7274/50000], Train Loss: 8155999.0000, Val Loss: 5117521.5000\n",
      "Epoch [7275/50000], Train Loss: 8155863.0000, Val Loss: 5117637.5000\n",
      "Epoch [7276/50000], Train Loss: 8155728.5000, Val Loss: 5117753.5000\n",
      "Epoch [7277/50000], Train Loss: 8155592.5000, Val Loss: 5117869.0000\n",
      "Epoch [7278/50000], Train Loss: 8155457.5000, Val Loss: 5117985.0000\n",
      "Epoch [7279/50000], Train Loss: 8155322.5000, Val Loss: 5118101.0000\n",
      "Epoch [7280/50000], Train Loss: 8155187.5000, Val Loss: 5118218.0000\n",
      "Epoch [7281/50000], Train Loss: 8155053.0000, Val Loss: 5118334.0000\n",
      "Epoch [7282/50000], Train Loss: 8154918.0000, Val Loss: 5118450.0000\n",
      "Epoch [7283/50000], Train Loss: 8154784.5000, Val Loss: 5118566.0000\n",
      "Epoch [7284/50000], Train Loss: 8154650.5000, Val Loss: 5118683.0000\n",
      "Epoch [7285/50000], Train Loss: 8154517.0000, Val Loss: 5118799.5000\n",
      "Epoch [7286/50000], Train Loss: 8154383.0000, Val Loss: 5118916.5000\n",
      "Epoch [7287/50000], Train Loss: 8154250.5000, Val Loss: 5119032.5000\n",
      "Epoch [7288/50000], Train Loss: 8154118.0000, Val Loss: 5119149.5000\n",
      "Epoch [7289/50000], Train Loss: 8153984.5000, Val Loss: 5119267.0000\n",
      "Epoch [7290/50000], Train Loss: 8153851.5000, Val Loss: 5119383.0000\n",
      "Epoch [7291/50000], Train Loss: 8153719.5000, Val Loss: 5119500.5000\n",
      "Epoch [7292/50000], Train Loss: 8153587.0000, Val Loss: 5119617.0000\n",
      "Epoch [7293/50000], Train Loss: 8153455.0000, Val Loss: 5119734.5000\n",
      "Epoch [7294/50000], Train Loss: 8153324.0000, Val Loss: 5119852.5000\n",
      "Epoch [7295/50000], Train Loss: 8153191.5000, Val Loss: 5119969.0000\n",
      "Epoch [7296/50000], Train Loss: 8153060.0000, Val Loss: 5120086.5000\n",
      "Epoch [7297/50000], Train Loss: 8152929.0000, Val Loss: 5120203.0000\n",
      "Epoch [7298/50000], Train Loss: 8152798.5000, Val Loss: 5120321.0000\n",
      "Epoch [7299/50000], Train Loss: 8152667.0000, Val Loss: 5120439.0000\n",
      "Epoch [7300/50000], Train Loss: 8152536.5000, Val Loss: 5120556.5000\n",
      "Epoch [7301/50000], Train Loss: 8152406.0000, Val Loss: 5120673.5000\n",
      "Epoch [7302/50000], Train Loss: 8152277.0000, Val Loss: 5120791.5000\n",
      "Epoch [7303/50000], Train Loss: 8152147.0000, Val Loss: 5120909.5000\n",
      "Epoch [7304/50000], Train Loss: 8152016.5000, Val Loss: 5121027.0000\n",
      "Epoch [7305/50000], Train Loss: 8151887.0000, Val Loss: 5121145.0000\n",
      "Epoch [7306/50000], Train Loss: 8151757.5000, Val Loss: 5121263.0000\n",
      "Epoch [7307/50000], Train Loss: 8151629.0000, Val Loss: 5121381.0000\n",
      "Epoch [7308/50000], Train Loss: 8151500.0000, Val Loss: 5121499.0000\n",
      "Epoch [7309/50000], Train Loss: 8151370.0000, Val Loss: 5121617.0000\n",
      "Epoch [7310/50000], Train Loss: 8151243.0000, Val Loss: 5121735.5000\n",
      "Epoch [7311/50000], Train Loss: 8151114.5000, Val Loss: 5121854.0000\n",
      "Epoch [7312/50000], Train Loss: 8150986.0000, Val Loss: 5121972.5000\n",
      "Epoch [7313/50000], Train Loss: 8150859.0000, Val Loss: 5122090.5000\n",
      "Epoch [7314/50000], Train Loss: 8150730.0000, Val Loss: 5122209.5000\n",
      "Epoch [7315/50000], Train Loss: 8150603.0000, Val Loss: 5122328.0000\n",
      "Epoch [7316/50000], Train Loss: 8150476.5000, Val Loss: 5122447.0000\n",
      "Epoch [7317/50000], Train Loss: 8150349.5000, Val Loss: 5122566.0000\n",
      "Epoch [7318/50000], Train Loss: 8150222.0000, Val Loss: 5122683.5000\n",
      "Epoch [7319/50000], Train Loss: 8150095.5000, Val Loss: 5122802.5000\n",
      "Epoch [7320/50000], Train Loss: 8149969.0000, Val Loss: 5122921.0000\n",
      "Epoch [7321/50000], Train Loss: 8149843.0000, Val Loss: 5123040.5000\n",
      "Epoch [7322/50000], Train Loss: 8149717.0000, Val Loss: 5123159.5000\n",
      "Epoch [7323/50000], Train Loss: 8149591.0000, Val Loss: 5123278.5000\n",
      "Epoch [7324/50000], Train Loss: 8149464.0000, Val Loss: 5123397.5000\n",
      "Epoch [7325/50000], Train Loss: 8149340.0000, Val Loss: 5123517.5000\n",
      "Epoch [7326/50000], Train Loss: 8149214.5000, Val Loss: 5123635.5000\n",
      "Epoch [7327/50000], Train Loss: 8149090.0000, Val Loss: 5123755.5000\n",
      "Epoch [7328/50000], Train Loss: 8148964.5000, Val Loss: 5123874.5000\n",
      "Epoch [7329/50000], Train Loss: 8148840.0000, Val Loss: 5123994.0000\n",
      "Epoch [7330/50000], Train Loss: 8148716.0000, Val Loss: 5124113.5000\n",
      "Epoch [7331/50000], Train Loss: 8148592.5000, Val Loss: 5124233.0000\n",
      "Epoch [7332/50000], Train Loss: 8148469.0000, Val Loss: 5124352.5000\n",
      "Epoch [7333/50000], Train Loss: 8148343.5000, Val Loss: 5124472.5000\n",
      "Epoch [7334/50000], Train Loss: 8148220.0000, Val Loss: 5124592.0000\n",
      "Epoch [7335/50000], Train Loss: 8148096.5000, Val Loss: 5124712.0000\n",
      "Epoch [7336/50000], Train Loss: 8147974.0000, Val Loss: 5124831.5000\n",
      "Epoch [7337/50000], Train Loss: 8147850.5000, Val Loss: 5124952.0000\n",
      "Epoch [7338/50000], Train Loss: 8147727.5000, Val Loss: 5125071.0000\n",
      "Epoch [7339/50000], Train Loss: 8147606.0000, Val Loss: 5125191.0000\n",
      "Epoch [7340/50000], Train Loss: 8147483.0000, Val Loss: 5125311.0000\n",
      "Epoch [7341/50000], Train Loss: 8147361.0000, Val Loss: 5125431.5000\n",
      "Epoch [7342/50000], Train Loss: 8147238.5000, Val Loss: 5125551.5000\n",
      "Epoch [7343/50000], Train Loss: 8147117.5000, Val Loss: 5125671.5000\n",
      "Epoch [7344/50000], Train Loss: 8146995.0000, Val Loss: 5125791.5000\n",
      "Epoch [7345/50000], Train Loss: 8146874.5000, Val Loss: 5125912.5000\n",
      "Epoch [7346/50000], Train Loss: 8146753.0000, Val Loss: 5126032.5000\n",
      "Epoch [7347/50000], Train Loss: 8146632.5000, Val Loss: 5126153.5000\n",
      "Epoch [7348/50000], Train Loss: 8146511.5000, Val Loss: 5126273.0000\n",
      "Epoch [7349/50000], Train Loss: 8146391.0000, Val Loss: 5126394.0000\n",
      "Epoch [7350/50000], Train Loss: 8146270.0000, Val Loss: 5126514.5000\n",
      "Epoch [7351/50000], Train Loss: 8146149.5000, Val Loss: 5126635.0000\n",
      "Epoch [7352/50000], Train Loss: 8146030.0000, Val Loss: 5126755.5000\n",
      "Epoch [7353/50000], Train Loss: 8145910.0000, Val Loss: 5126877.0000\n",
      "Epoch [7354/50000], Train Loss: 8145791.0000, Val Loss: 5126997.5000\n",
      "Epoch [7355/50000], Train Loss: 8145671.5000, Val Loss: 5127118.5000\n",
      "Epoch [7356/50000], Train Loss: 8145551.5000, Val Loss: 5127239.0000\n",
      "Epoch [7357/50000], Train Loss: 8145434.0000, Val Loss: 5127360.0000\n",
      "Epoch [7358/50000], Train Loss: 8145315.0000, Val Loss: 5127481.5000\n",
      "Epoch [7359/50000], Train Loss: 8145196.5000, Val Loss: 5127602.5000\n",
      "Epoch [7360/50000], Train Loss: 8145077.5000, Val Loss: 5127724.5000\n",
      "Epoch [7361/50000], Train Loss: 8144959.0000, Val Loss: 5127845.5000\n",
      "Epoch [7362/50000], Train Loss: 8144841.5000, Val Loss: 5127966.0000\n",
      "Epoch [7363/50000], Train Loss: 8144722.5000, Val Loss: 5128087.5000\n",
      "Epoch [7364/50000], Train Loss: 8144606.5000, Val Loss: 5128209.0000\n",
      "Epoch [7365/50000], Train Loss: 8144488.0000, Val Loss: 5128330.5000\n",
      "Epoch [7366/50000], Train Loss: 8144370.0000, Val Loss: 5128452.5000\n",
      "Epoch [7367/50000], Train Loss: 8144254.0000, Val Loss: 5128573.5000\n",
      "Epoch [7368/50000], Train Loss: 8144137.0000, Val Loss: 5128694.5000\n",
      "Epoch [7369/50000], Train Loss: 8144020.0000, Val Loss: 5128816.0000\n",
      "Epoch [7370/50000], Train Loss: 8143904.5000, Val Loss: 5128938.0000\n",
      "Epoch [7371/50000], Train Loss: 8143787.0000, Val Loss: 5129060.5000\n",
      "Epoch [7372/50000], Train Loss: 8143671.0000, Val Loss: 5129181.0000\n",
      "Epoch [7373/50000], Train Loss: 8143555.0000, Val Loss: 5129304.0000\n",
      "Epoch [7374/50000], Train Loss: 8143439.5000, Val Loss: 5129425.0000\n",
      "Epoch [7375/50000], Train Loss: 8143324.5000, Val Loss: 5129547.5000\n",
      "Epoch [7376/50000], Train Loss: 8143208.5000, Val Loss: 5129669.0000\n",
      "Epoch [7377/50000], Train Loss: 8143093.5000, Val Loss: 5129791.5000\n",
      "Epoch [7378/50000], Train Loss: 8142979.0000, Val Loss: 5129913.0000\n",
      "Epoch [7379/50000], Train Loss: 8142863.5000, Val Loss: 5130035.0000\n",
      "Epoch [7380/50000], Train Loss: 8142749.0000, Val Loss: 5130157.5000\n",
      "Epoch [7381/50000], Train Loss: 8142635.0000, Val Loss: 5130280.0000\n",
      "Epoch [7382/50000], Train Loss: 8142520.5000, Val Loss: 5130402.0000\n",
      "Epoch [7383/50000], Train Loss: 8142406.5000, Val Loss: 5130523.5000\n",
      "Epoch [7384/50000], Train Loss: 8142293.0000, Val Loss: 5130647.0000\n",
      "Epoch [7385/50000], Train Loss: 8142179.0000, Val Loss: 5130769.0000\n",
      "Epoch [7386/50000], Train Loss: 8142066.0000, Val Loss: 5130891.0000\n",
      "Epoch [7387/50000], Train Loss: 8141952.5000, Val Loss: 5131013.5000\n",
      "Epoch [7388/50000], Train Loss: 8141839.5000, Val Loss: 5131136.0000\n",
      "Epoch [7389/50000], Train Loss: 8141727.0000, Val Loss: 5131258.0000\n",
      "Epoch [7390/50000], Train Loss: 8141614.0000, Val Loss: 5131381.5000\n",
      "Epoch [7391/50000], Train Loss: 8141500.5000, Val Loss: 5131503.5000\n",
      "Epoch [7392/50000], Train Loss: 8141389.5000, Val Loss: 5131626.0000\n",
      "Epoch [7393/50000], Train Loss: 8141277.0000, Val Loss: 5131749.5000\n",
      "Epoch [7394/50000], Train Loss: 8141165.0000, Val Loss: 5131872.5000\n",
      "Epoch [7395/50000], Train Loss: 8141054.0000, Val Loss: 5131995.0000\n",
      "Epoch [7396/50000], Train Loss: 8140942.0000, Val Loss: 5132118.0000\n",
      "Epoch [7397/50000], Train Loss: 8140830.5000, Val Loss: 5132240.5000\n",
      "Epoch [7398/50000], Train Loss: 8140719.5000, Val Loss: 5132363.5000\n",
      "Epoch [7399/50000], Train Loss: 8140609.0000, Val Loss: 5132487.0000\n",
      "Epoch [7400/50000], Train Loss: 8140498.5000, Val Loss: 5132610.0000\n",
      "Epoch [7401/50000], Train Loss: 8140387.0000, Val Loss: 5132732.5000\n",
      "Epoch [7402/50000], Train Loss: 8140277.0000, Val Loss: 5132855.5000\n",
      "Epoch [7403/50000], Train Loss: 8140166.0000, Val Loss: 5132979.0000\n",
      "Epoch [7404/50000], Train Loss: 8140055.5000, Val Loss: 5133102.0000\n",
      "Epoch [7405/50000], Train Loss: 8139946.5000, Val Loss: 5133225.5000\n",
      "Epoch [7406/50000], Train Loss: 8139836.5000, Val Loss: 5133349.0000\n",
      "Epoch [7407/50000], Train Loss: 8139727.5000, Val Loss: 5133472.5000\n",
      "Epoch [7408/50000], Train Loss: 8139617.5000, Val Loss: 5133595.5000\n",
      "Epoch [7409/50000], Train Loss: 8139509.5000, Val Loss: 5133719.5000\n",
      "Epoch [7410/50000], Train Loss: 8139400.5000, Val Loss: 5133842.5000\n",
      "Epoch [7411/50000], Train Loss: 8139291.0000, Val Loss: 5133966.5000\n",
      "Epoch [7412/50000], Train Loss: 8139182.0000, Val Loss: 5134089.5000\n",
      "Epoch [7413/50000], Train Loss: 8139074.0000, Val Loss: 5134213.5000\n",
      "Epoch [7414/50000], Train Loss: 8138965.5000, Val Loss: 5134337.0000\n",
      "Epoch [7415/50000], Train Loss: 8138858.0000, Val Loss: 5134460.5000\n",
      "Epoch [7416/50000], Train Loss: 8138750.0000, Val Loss: 5134584.5000\n",
      "Epoch [7417/50000], Train Loss: 8138642.0000, Val Loss: 5134708.5000\n",
      "Epoch [7418/50000], Train Loss: 8138536.0000, Val Loss: 5134831.5000\n",
      "Epoch [7419/50000], Train Loss: 8138428.0000, Val Loss: 5134955.0000\n",
      "Epoch [7420/50000], Train Loss: 8138321.0000, Val Loss: 5135079.5000\n",
      "Epoch [7421/50000], Train Loss: 8138213.5000, Val Loss: 5135203.5000\n",
      "Epoch [7422/50000], Train Loss: 8138108.0000, Val Loss: 5135327.5000\n",
      "Epoch [7423/50000], Train Loss: 8138001.0000, Val Loss: 5135451.5000\n",
      "Epoch [7424/50000], Train Loss: 8137895.5000, Val Loss: 5135575.5000\n",
      "Epoch [7425/50000], Train Loss: 8137788.5000, Val Loss: 5135699.0000\n",
      "Epoch [7426/50000], Train Loss: 8137682.0000, Val Loss: 5135824.0000\n",
      "Epoch [7427/50000], Train Loss: 8137576.5000, Val Loss: 5135947.5000\n",
      "Epoch [7428/50000], Train Loss: 8137471.0000, Val Loss: 5136071.5000\n",
      "Epoch [7429/50000], Train Loss: 8137366.0000, Val Loss: 5136195.0000\n",
      "Epoch [7430/50000], Train Loss: 8137261.0000, Val Loss: 5136320.0000\n",
      "Epoch [7431/50000], Train Loss: 8137154.0000, Val Loss: 5136444.5000\n",
      "Epoch [7432/50000], Train Loss: 8137049.5000, Val Loss: 5136568.5000\n",
      "Epoch [7433/50000], Train Loss: 8136945.5000, Val Loss: 5136693.5000\n",
      "Epoch [7434/50000], Train Loss: 8136841.0000, Val Loss: 5136817.0000\n",
      "Epoch [7435/50000], Train Loss: 8136736.0000, Val Loss: 5136942.0000\n",
      "Epoch [7436/50000], Train Loss: 8136632.5000, Val Loss: 5137066.0000\n",
      "Epoch [7437/50000], Train Loss: 8136528.5000, Val Loss: 5137191.0000\n",
      "Epoch [7438/50000], Train Loss: 8136424.5000, Val Loss: 5137315.0000\n",
      "Epoch [7439/50000], Train Loss: 8136321.0000, Val Loss: 5137439.0000\n",
      "Epoch [7440/50000], Train Loss: 8136218.5000, Val Loss: 5137564.5000\n",
      "Epoch [7441/50000], Train Loss: 8136114.5000, Val Loss: 5137689.0000\n",
      "Epoch [7442/50000], Train Loss: 8136011.0000, Val Loss: 5137814.5000\n",
      "Epoch [7443/50000], Train Loss: 8135908.5000, Val Loss: 5137938.5000\n",
      "Epoch [7444/50000], Train Loss: 8135804.5000, Val Loss: 5138063.0000\n",
      "Epoch [7445/50000], Train Loss: 8135702.5000, Val Loss: 5138187.5000\n",
      "Epoch [7446/50000], Train Loss: 8135600.5000, Val Loss: 5138313.0000\n",
      "Epoch [7447/50000], Train Loss: 8135498.0000, Val Loss: 5138437.5000\n",
      "Epoch [7448/50000], Train Loss: 8135396.0000, Val Loss: 5138562.5000\n",
      "Epoch [7449/50000], Train Loss: 8135295.0000, Val Loss: 5138687.0000\n",
      "Epoch [7450/50000], Train Loss: 8135192.5000, Val Loss: 5138812.5000\n",
      "Epoch [7451/50000], Train Loss: 8135091.0000, Val Loss: 5138937.0000\n",
      "Epoch [7452/50000], Train Loss: 8134990.0000, Val Loss: 5139062.0000\n",
      "Epoch [7453/50000], Train Loss: 8134888.5000, Val Loss: 5139187.0000\n",
      "Epoch [7454/50000], Train Loss: 8134788.0000, Val Loss: 5139311.5000\n",
      "Epoch [7455/50000], Train Loss: 8134687.0000, Val Loss: 5139437.0000\n",
      "Epoch [7456/50000], Train Loss: 8134587.0000, Val Loss: 5139562.0000\n",
      "Epoch [7457/50000], Train Loss: 8134485.5000, Val Loss: 5139687.5000\n",
      "Epoch [7458/50000], Train Loss: 8134385.0000, Val Loss: 5139812.5000\n",
      "Epoch [7459/50000], Train Loss: 8134285.5000, Val Loss: 5139937.5000\n",
      "Epoch [7460/50000], Train Loss: 8134186.0000, Val Loss: 5140063.0000\n",
      "Epoch [7461/50000], Train Loss: 8134086.0000, Val Loss: 5140188.5000\n",
      "Epoch [7462/50000], Train Loss: 8133986.0000, Val Loss: 5140313.0000\n",
      "Epoch [7463/50000], Train Loss: 8133887.0000, Val Loss: 5140438.5000\n",
      "Epoch [7464/50000], Train Loss: 8133787.5000, Val Loss: 5140564.5000\n",
      "Epoch [7465/50000], Train Loss: 8133688.0000, Val Loss: 5140690.0000\n",
      "Epoch [7466/50000], Train Loss: 8133589.0000, Val Loss: 5140815.5000\n",
      "Epoch [7467/50000], Train Loss: 8133490.5000, Val Loss: 5140941.0000\n",
      "Epoch [7468/50000], Train Loss: 8133392.5000, Val Loss: 5141065.5000\n",
      "Epoch [7469/50000], Train Loss: 8133294.0000, Val Loss: 5141191.5000\n",
      "Epoch [7470/50000], Train Loss: 8133196.0000, Val Loss: 5141317.0000\n",
      "Epoch [7471/50000], Train Loss: 8133097.5000, Val Loss: 5141442.5000\n",
      "Epoch [7472/50000], Train Loss: 8133000.0000, Val Loss: 5141567.5000\n",
      "Epoch [7473/50000], Train Loss: 8132901.5000, Val Loss: 5141694.0000\n",
      "Epoch [7474/50000], Train Loss: 8132804.0000, Val Loss: 5141818.5000\n",
      "Epoch [7475/50000], Train Loss: 8132707.0000, Val Loss: 5141945.0000\n",
      "Epoch [7476/50000], Train Loss: 8132609.0000, Val Loss: 5142070.5000\n",
      "Epoch [7477/50000], Train Loss: 8132513.0000, Val Loss: 5142196.5000\n",
      "Epoch [7478/50000], Train Loss: 8132416.0000, Val Loss: 5142321.5000\n",
      "Epoch [7479/50000], Train Loss: 8132319.5000, Val Loss: 5142447.5000\n",
      "Epoch [7480/50000], Train Loss: 8132223.0000, Val Loss: 5142573.5000\n",
      "Epoch [7481/50000], Train Loss: 8132126.5000, Val Loss: 5142699.5000\n",
      "Epoch [7482/50000], Train Loss: 8132030.0000, Val Loss: 5142825.5000\n",
      "Epoch [7483/50000], Train Loss: 8131934.5000, Val Loss: 5142951.5000\n",
      "Epoch [7484/50000], Train Loss: 8131838.0000, Val Loss: 5143076.5000\n",
      "Epoch [7485/50000], Train Loss: 8131743.5000, Val Loss: 5143202.5000\n",
      "Epoch [7486/50000], Train Loss: 8131648.0000, Val Loss: 5143328.5000\n",
      "Epoch [7487/50000], Train Loss: 8131552.0000, Val Loss: 5143455.0000\n",
      "Epoch [7488/50000], Train Loss: 8131456.5000, Val Loss: 5143581.0000\n",
      "Epoch [7489/50000], Train Loss: 8131361.5000, Val Loss: 5143706.0000\n",
      "Epoch [7490/50000], Train Loss: 8131268.0000, Val Loss: 5143832.5000\n",
      "Epoch [7491/50000], Train Loss: 8131172.0000, Val Loss: 5143959.0000\n",
      "Epoch [7492/50000], Train Loss: 8131077.5000, Val Loss: 5144085.5000\n",
      "Epoch [7493/50000], Train Loss: 8130983.5000, Val Loss: 5144211.0000\n",
      "Epoch [7494/50000], Train Loss: 8130889.0000, Val Loss: 5144336.5000\n",
      "Epoch [7495/50000], Train Loss: 8130796.5000, Val Loss: 5144463.5000\n",
      "Epoch [7496/50000], Train Loss: 8130701.5000, Val Loss: 5144589.5000\n",
      "Epoch [7497/50000], Train Loss: 8130607.5000, Val Loss: 5144715.5000\n",
      "Epoch [7498/50000], Train Loss: 8130514.5000, Val Loss: 5144841.5000\n",
      "Epoch [7499/50000], Train Loss: 8130421.0000, Val Loss: 5144968.0000\n",
      "Epoch [7500/50000], Train Loss: 8130328.0000, Val Loss: 5145094.5000\n",
      "Epoch [7501/50000], Train Loss: 8130235.5000, Val Loss: 5145221.0000\n",
      "Epoch [7502/50000], Train Loss: 8130142.5000, Val Loss: 5145346.5000\n",
      "Epoch [7503/50000], Train Loss: 8130049.5000, Val Loss: 5145473.0000\n",
      "Epoch [7504/50000], Train Loss: 8129957.5000, Val Loss: 5145599.0000\n",
      "Epoch [7505/50000], Train Loss: 8129865.0000, Val Loss: 5145726.0000\n",
      "Epoch [7506/50000], Train Loss: 8129774.0000, Val Loss: 5145851.5000\n",
      "Epoch [7507/50000], Train Loss: 8129680.5000, Val Loss: 5145978.5000\n",
      "Epoch [7508/50000], Train Loss: 8129589.5000, Val Loss: 5146105.0000\n",
      "Epoch [7509/50000], Train Loss: 8129498.0000, Val Loss: 5146231.5000\n",
      "Epoch [7510/50000], Train Loss: 8129406.0000, Val Loss: 5146357.5000\n",
      "Epoch [7511/50000], Train Loss: 8129314.0000, Val Loss: 5146483.5000\n",
      "Epoch [7512/50000], Train Loss: 8129222.5000, Val Loss: 5146610.5000\n",
      "Epoch [7513/50000], Train Loss: 8129132.5000, Val Loss: 5146736.5000\n",
      "Epoch [7514/50000], Train Loss: 8129042.0000, Val Loss: 5146864.0000\n",
      "Epoch [7515/50000], Train Loss: 8128951.0000, Val Loss: 5146990.5000\n",
      "Epoch [7516/50000], Train Loss: 8128860.0000, Val Loss: 5147117.0000\n",
      "Epoch [7517/50000], Train Loss: 8128770.0000, Val Loss: 5147242.5000\n",
      "Epoch [7518/50000], Train Loss: 8128680.0000, Val Loss: 5147369.5000\n",
      "Epoch [7519/50000], Train Loss: 8128589.5000, Val Loss: 5147496.5000\n",
      "Epoch [7520/50000], Train Loss: 8128500.0000, Val Loss: 5147622.5000\n",
      "Epoch [7521/50000], Train Loss: 8128411.5000, Val Loss: 5147750.0000\n",
      "Epoch [7522/50000], Train Loss: 8128321.0000, Val Loss: 5147876.5000\n",
      "Epoch [7523/50000], Train Loss: 8128230.5000, Val Loss: 5148002.0000\n",
      "Epoch [7524/50000], Train Loss: 8128142.0000, Val Loss: 5148129.0000\n",
      "Epoch [7525/50000], Train Loss: 8128053.0000, Val Loss: 5148256.0000\n",
      "Epoch [7526/50000], Train Loss: 8127964.5000, Val Loss: 5148382.0000\n",
      "Epoch [7527/50000], Train Loss: 8127876.0000, Val Loss: 5148509.5000\n",
      "Epoch [7528/50000], Train Loss: 8127788.0000, Val Loss: 5148636.5000\n",
      "Epoch [7529/50000], Train Loss: 8127698.5000, Val Loss: 5148762.5000\n",
      "Epoch [7530/50000], Train Loss: 8127610.5000, Val Loss: 5148889.5000\n",
      "Epoch [7531/50000], Train Loss: 8127523.0000, Val Loss: 5149016.5000\n",
      "Epoch [7532/50000], Train Loss: 8127435.0000, Val Loss: 5149143.5000\n",
      "Epoch [7533/50000], Train Loss: 8127347.0000, Val Loss: 5149270.5000\n",
      "Epoch [7534/50000], Train Loss: 8127259.0000, Val Loss: 5149397.5000\n",
      "Epoch [7535/50000], Train Loss: 8127172.0000, Val Loss: 5149523.5000\n",
      "Epoch [7536/50000], Train Loss: 8127085.0000, Val Loss: 5149650.5000\n",
      "Epoch [7537/50000], Train Loss: 8126998.0000, Val Loss: 5149777.0000\n",
      "Epoch [7538/50000], Train Loss: 8126911.0000, Val Loss: 5149904.0000\n",
      "Epoch [7539/50000], Train Loss: 8126824.5000, Val Loss: 5150032.0000\n",
      "Epoch [7540/50000], Train Loss: 8126737.0000, Val Loss: 5150158.5000\n",
      "Epoch [7541/50000], Train Loss: 8126651.0000, Val Loss: 5150285.0000\n",
      "Epoch [7542/50000], Train Loss: 8126564.5000, Val Loss: 5150412.5000\n",
      "Epoch [7543/50000], Train Loss: 8126478.0000, Val Loss: 5150538.5000\n",
      "Epoch [7544/50000], Train Loss: 8126392.5000, Val Loss: 5150666.0000\n",
      "Epoch [7545/50000], Train Loss: 8126306.0000, Val Loss: 5150793.0000\n",
      "Epoch [7546/50000], Train Loss: 8126220.5000, Val Loss: 5150920.0000\n",
      "Epoch [7547/50000], Train Loss: 8126135.5000, Val Loss: 5151047.0000\n",
      "Epoch [7548/50000], Train Loss: 8126049.5000, Val Loss: 5151174.5000\n",
      "Epoch [7549/50000], Train Loss: 8125964.5000, Val Loss: 5151301.0000\n",
      "Epoch [7550/50000], Train Loss: 8125879.0000, Val Loss: 5151427.5000\n",
      "Epoch [7551/50000], Train Loss: 8125794.0000, Val Loss: 5151554.5000\n",
      "Epoch [7552/50000], Train Loss: 8125710.0000, Val Loss: 5151682.0000\n",
      "Epoch [7553/50000], Train Loss: 8125625.5000, Val Loss: 5151809.0000\n",
      "Epoch [7554/50000], Train Loss: 8125541.0000, Val Loss: 5151936.0000\n",
      "Epoch [7555/50000], Train Loss: 8125455.5000, Val Loss: 5152063.5000\n",
      "Epoch [7556/50000], Train Loss: 8125372.5000, Val Loss: 5152190.5000\n",
      "Epoch [7557/50000], Train Loss: 8125288.5000, Val Loss: 5152317.5000\n",
      "Epoch [7558/50000], Train Loss: 8125204.0000, Val Loss: 5152445.0000\n",
      "Epoch [7559/50000], Train Loss: 8125120.5000, Val Loss: 5152571.5000\n",
      "Epoch [7560/50000], Train Loss: 8125037.0000, Val Loss: 5152698.5000\n",
      "Epoch [7561/50000], Train Loss: 8124954.0000, Val Loss: 5152826.0000\n",
      "Epoch [7562/50000], Train Loss: 8124870.5000, Val Loss: 5152953.5000\n",
      "Epoch [7563/50000], Train Loss: 8124787.0000, Val Loss: 5153080.0000\n",
      "Epoch [7564/50000], Train Loss: 8124704.5000, Val Loss: 5153207.5000\n",
      "Epoch [7565/50000], Train Loss: 8124622.0000, Val Loss: 5153334.0000\n",
      "Epoch [7566/50000], Train Loss: 8124539.5000, Val Loss: 5153462.0000\n",
      "Epoch [7567/50000], Train Loss: 8124456.5000, Val Loss: 5153589.5000\n",
      "Epoch [7568/50000], Train Loss: 8124374.0000, Val Loss: 5153717.0000\n",
      "Epoch [7569/50000], Train Loss: 8124292.5000, Val Loss: 5153843.0000\n",
      "Epoch [7570/50000], Train Loss: 8124210.5000, Val Loss: 5153971.0000\n",
      "Epoch [7571/50000], Train Loss: 8124128.5000, Val Loss: 5154097.5000\n",
      "Epoch [7572/50000], Train Loss: 8124047.0000, Val Loss: 5154225.0000\n",
      "Epoch [7573/50000], Train Loss: 8123966.0000, Val Loss: 5154352.0000\n",
      "Epoch [7574/50000], Train Loss: 8123884.5000, Val Loss: 5154479.5000\n",
      "Epoch [7575/50000], Train Loss: 8123802.5000, Val Loss: 5154607.0000\n",
      "Epoch [7576/50000], Train Loss: 8123721.5000, Val Loss: 5154734.0000\n",
      "Epoch [7577/50000], Train Loss: 8123640.0000, Val Loss: 5154862.0000\n",
      "Epoch [7578/50000], Train Loss: 8123560.0000, Val Loss: 5154989.5000\n",
      "Epoch [7579/50000], Train Loss: 8123479.0000, Val Loss: 5155117.0000\n",
      "Epoch [7580/50000], Train Loss: 8123398.5000, Val Loss: 5155243.0000\n",
      "Epoch [7581/50000], Train Loss: 8123318.5000, Val Loss: 5155370.5000\n",
      "Epoch [7582/50000], Train Loss: 8123238.0000, Val Loss: 5155498.0000\n",
      "Epoch [7583/50000], Train Loss: 8123158.0000, Val Loss: 5155625.5000\n",
      "Epoch [7584/50000], Train Loss: 8123078.5000, Val Loss: 5155752.0000\n",
      "Epoch [7585/50000], Train Loss: 8122998.0000, Val Loss: 5155880.5000\n",
      "Epoch [7586/50000], Train Loss: 8122918.0000, Val Loss: 5156007.0000\n",
      "Epoch [7587/50000], Train Loss: 8122839.0000, Val Loss: 5156134.5000\n",
      "Epoch [7588/50000], Train Loss: 8122760.5000, Val Loss: 5156261.5000\n",
      "Epoch [7589/50000], Train Loss: 8122680.5000, Val Loss: 5156389.0000\n",
      "Epoch [7590/50000], Train Loss: 8122602.5000, Val Loss: 5156516.5000\n",
      "Epoch [7591/50000], Train Loss: 8122523.5000, Val Loss: 5156643.5000\n",
      "Epoch [7592/50000], Train Loss: 8122444.5000, Val Loss: 5156771.5000\n",
      "Epoch [7593/50000], Train Loss: 8122366.0000, Val Loss: 5156898.0000\n",
      "Epoch [7594/50000], Train Loss: 8122287.0000, Val Loss: 5157026.5000\n",
      "Epoch [7595/50000], Train Loss: 8122209.5000, Val Loss: 5157153.0000\n",
      "Epoch [7596/50000], Train Loss: 8122131.0000, Val Loss: 5157281.0000\n",
      "Epoch [7597/50000], Train Loss: 8122052.5000, Val Loss: 5157407.5000\n",
      "Epoch [7598/50000], Train Loss: 8121975.5000, Val Loss: 5157535.5000\n",
      "Epoch [7599/50000], Train Loss: 8121897.5000, Val Loss: 5157663.0000\n",
      "Epoch [7600/50000], Train Loss: 8121820.0000, Val Loss: 5157790.0000\n",
      "Epoch [7601/50000], Train Loss: 8121743.0000, Val Loss: 5157917.5000\n",
      "Epoch [7602/50000], Train Loss: 8121666.0000, Val Loss: 5158044.5000\n",
      "Epoch [7603/50000], Train Loss: 8121588.0000, Val Loss: 5158171.5000\n",
      "Epoch [7604/50000], Train Loss: 8121510.5000, Val Loss: 5158299.0000\n",
      "Epoch [7605/50000], Train Loss: 8121434.5000, Val Loss: 5158426.5000\n",
      "Epoch [7606/50000], Train Loss: 8121358.0000, Val Loss: 5158553.5000\n",
      "Epoch [7607/50000], Train Loss: 8121282.0000, Val Loss: 5158680.5000\n",
      "Epoch [7608/50000], Train Loss: 8121205.5000, Val Loss: 5158808.5000\n",
      "Epoch [7609/50000], Train Loss: 8121129.0000, Val Loss: 5158935.5000\n",
      "Epoch [7610/50000], Train Loss: 8121052.5000, Val Loss: 5159063.0000\n",
      "Epoch [7611/50000], Train Loss: 8120977.5000, Val Loss: 5159190.5000\n",
      "Epoch [7612/50000], Train Loss: 8120900.5000, Val Loss: 5159318.0000\n",
      "Epoch [7613/50000], Train Loss: 8120824.5000, Val Loss: 5159445.0000\n",
      "Epoch [7614/50000], Train Loss: 8120750.0000, Val Loss: 5159572.5000\n",
      "Epoch [7615/50000], Train Loss: 8120675.5000, Val Loss: 5159699.0000\n",
      "Epoch [7616/50000], Train Loss: 8120600.0000, Val Loss: 5159827.0000\n",
      "Epoch [7617/50000], Train Loss: 8120524.5000, Val Loss: 5159953.5000\n",
      "Epoch [7618/50000], Train Loss: 8120450.0000, Val Loss: 5160081.5000\n",
      "Epoch [7619/50000], Train Loss: 8120375.0000, Val Loss: 5160209.0000\n",
      "Epoch [7620/50000], Train Loss: 8120300.5000, Val Loss: 5160336.0000\n",
      "Epoch [7621/50000], Train Loss: 8120225.0000, Val Loss: 5160463.0000\n",
      "Epoch [7622/50000], Train Loss: 8120151.0000, Val Loss: 5160591.0000\n",
      "Epoch [7623/50000], Train Loss: 8120077.0000, Val Loss: 5160718.5000\n",
      "Epoch [7624/50000], Train Loss: 8120003.5000, Val Loss: 5160845.0000\n",
      "Epoch [7625/50000], Train Loss: 8119928.5000, Val Loss: 5160973.0000\n",
      "Epoch [7626/50000], Train Loss: 8119855.0000, Val Loss: 5161100.5000\n",
      "Epoch [7627/50000], Train Loss: 8119781.5000, Val Loss: 5161227.0000\n",
      "Epoch [7628/50000], Train Loss: 8119708.0000, Val Loss: 5161354.0000\n",
      "Epoch [7629/50000], Train Loss: 8119635.0000, Val Loss: 5161482.0000\n",
      "Epoch [7630/50000], Train Loss: 8119561.0000, Val Loss: 5161608.5000\n",
      "Epoch [7631/50000], Train Loss: 8119488.5000, Val Loss: 5161736.5000\n",
      "Epoch [7632/50000], Train Loss: 8119415.5000, Val Loss: 5161863.5000\n",
      "Epoch [7633/50000], Train Loss: 8119342.5000, Val Loss: 5161991.0000\n",
      "Epoch [7634/50000], Train Loss: 8119270.0000, Val Loss: 5162118.0000\n",
      "Epoch [7635/50000], Train Loss: 8119198.0000, Val Loss: 5162245.5000\n",
      "Epoch [7636/50000], Train Loss: 8119124.0000, Val Loss: 5162373.5000\n",
      "Epoch [7637/50000], Train Loss: 8119053.0000, Val Loss: 5162499.5000\n",
      "Epoch [7638/50000], Train Loss: 8118981.0000, Val Loss: 5162627.0000\n",
      "Epoch [7639/50000], Train Loss: 8118908.5000, Val Loss: 5162754.0000\n",
      "Epoch [7640/50000], Train Loss: 8118837.0000, Val Loss: 5162881.0000\n",
      "Epoch [7641/50000], Train Loss: 8118765.0000, Val Loss: 5163009.0000\n",
      "Epoch [7642/50000], Train Loss: 8118693.5000, Val Loss: 5163136.0000\n",
      "Epoch [7643/50000], Train Loss: 8118623.0000, Val Loss: 5163263.0000\n",
      "Epoch [7644/50000], Train Loss: 8118551.0000, Val Loss: 5163390.5000\n",
      "Epoch [7645/50000], Train Loss: 8118479.5000, Val Loss: 5163517.5000\n",
      "Epoch [7646/50000], Train Loss: 8118408.5000, Val Loss: 5163645.0000\n",
      "Epoch [7647/50000], Train Loss: 8118337.0000, Val Loss: 5163772.5000\n",
      "Epoch [7648/50000], Train Loss: 8118267.5000, Val Loss: 5163899.0000\n",
      "Epoch [7649/50000], Train Loss: 8118196.0000, Val Loss: 5164026.5000\n",
      "Epoch [7650/50000], Train Loss: 8118126.0000, Val Loss: 5164153.0000\n",
      "Epoch [7651/50000], Train Loss: 8118056.0000, Val Loss: 5164280.5000\n",
      "Epoch [7652/50000], Train Loss: 8117985.5000, Val Loss: 5164407.5000\n",
      "Epoch [7653/50000], Train Loss: 8117915.5000, Val Loss: 5164535.0000\n",
      "Epoch [7654/50000], Train Loss: 8117845.5000, Val Loss: 5164662.5000\n",
      "Epoch [7655/50000], Train Loss: 8117775.5000, Val Loss: 5164789.0000\n",
      "Epoch [7656/50000], Train Loss: 8117706.0000, Val Loss: 5164917.0000\n",
      "Epoch [7657/50000], Train Loss: 8117636.5000, Val Loss: 5165042.5000\n",
      "Epoch [7658/50000], Train Loss: 8117567.5000, Val Loss: 5165170.0000\n",
      "Epoch [7659/50000], Train Loss: 8117498.0000, Val Loss: 5165297.5000\n",
      "Epoch [7660/50000], Train Loss: 8117429.0000, Val Loss: 5165424.0000\n",
      "Epoch [7661/50000], Train Loss: 8117360.5000, Val Loss: 5165551.5000\n",
      "Epoch [7662/50000], Train Loss: 8117290.5000, Val Loss: 5165678.5000\n",
      "Epoch [7663/50000], Train Loss: 8117222.0000, Val Loss: 5165806.0000\n",
      "Epoch [7664/50000], Train Loss: 8117154.0000, Val Loss: 5165933.0000\n",
      "Epoch [7665/50000], Train Loss: 8117086.0000, Val Loss: 5166059.5000\n",
      "Epoch [7666/50000], Train Loss: 8117017.5000, Val Loss: 5166187.0000\n",
      "Epoch [7667/50000], Train Loss: 8116949.5000, Val Loss: 5166313.5000\n",
      "Epoch [7668/50000], Train Loss: 8116880.5000, Val Loss: 5166440.5000\n",
      "Epoch [7669/50000], Train Loss: 8116813.0000, Val Loss: 5166567.5000\n",
      "Epoch [7670/50000], Train Loss: 8116745.5000, Val Loss: 5166694.5000\n",
      "Epoch [7671/50000], Train Loss: 8116678.0000, Val Loss: 5166821.5000\n",
      "Epoch [7672/50000], Train Loss: 8116610.0000, Val Loss: 5166949.0000\n",
      "Epoch [7673/50000], Train Loss: 8116543.0000, Val Loss: 5167075.0000\n",
      "Epoch [7674/50000], Train Loss: 8116475.5000, Val Loss: 5167202.0000\n",
      "Epoch [7675/50000], Train Loss: 8116408.5000, Val Loss: 5167328.5000\n",
      "Epoch [7676/50000], Train Loss: 8116341.0000, Val Loss: 5167456.5000\n",
      "Epoch [7677/50000], Train Loss: 8116274.5000, Val Loss: 5167583.0000\n",
      "Epoch [7678/50000], Train Loss: 8116208.5000, Val Loss: 5167710.0000\n",
      "Epoch [7679/50000], Train Loss: 8116141.5000, Val Loss: 5167835.5000\n",
      "Epoch [7680/50000], Train Loss: 8116075.0000, Val Loss: 5167963.0000\n",
      "Epoch [7681/50000], Train Loss: 8116009.0000, Val Loss: 5168090.5000\n",
      "Epoch [7682/50000], Train Loss: 8115941.5000, Val Loss: 5168217.0000\n",
      "Epoch [7683/50000], Train Loss: 8115876.5000, Val Loss: 5168343.5000\n",
      "Epoch [7684/50000], Train Loss: 8115810.0000, Val Loss: 5168470.0000\n",
      "Epoch [7685/50000], Train Loss: 8115744.0000, Val Loss: 5168597.0000\n",
      "Epoch [7686/50000], Train Loss: 8115679.0000, Val Loss: 5168723.5000\n",
      "Epoch [7687/50000], Train Loss: 8115612.5000, Val Loss: 5168850.5000\n",
      "Epoch [7688/50000], Train Loss: 8115547.5000, Val Loss: 5168977.0000\n",
      "Epoch [7689/50000], Train Loss: 8115483.0000, Val Loss: 5169103.5000\n",
      "Epoch [7690/50000], Train Loss: 8115417.5000, Val Loss: 5169230.5000\n",
      "Epoch [7691/50000], Train Loss: 8115352.5000, Val Loss: 5169357.0000\n",
      "Epoch [7692/50000], Train Loss: 8115288.0000, Val Loss: 5169482.5000\n",
      "Epoch [7693/50000], Train Loss: 8115223.0000, Val Loss: 5169610.0000\n",
      "Epoch [7694/50000], Train Loss: 8115159.0000, Val Loss: 5169737.0000\n",
      "Epoch [7695/50000], Train Loss: 8115094.5000, Val Loss: 5169864.0000\n",
      "Epoch [7696/50000], Train Loss: 8115029.5000, Val Loss: 5169989.5000\n",
      "Epoch [7697/50000], Train Loss: 8114965.5000, Val Loss: 5170116.5000\n",
      "Epoch [7698/50000], Train Loss: 8114901.0000, Val Loss: 5170242.5000\n",
      "Epoch [7699/50000], Train Loss: 8114837.5000, Val Loss: 5170369.0000\n",
      "Epoch [7700/50000], Train Loss: 8114773.5000, Val Loss: 5170496.0000\n",
      "Epoch [7701/50000], Train Loss: 8114709.5000, Val Loss: 5170622.5000\n",
      "Epoch [7702/50000], Train Loss: 8114646.0000, Val Loss: 5170749.0000\n",
      "Epoch [7703/50000], Train Loss: 8114582.5000, Val Loss: 5170875.5000\n",
      "Epoch [7704/50000], Train Loss: 8114519.0000, Val Loss: 5171001.5000\n",
      "Epoch [7705/50000], Train Loss: 8114456.0000, Val Loss: 5171127.5000\n",
      "Epoch [7706/50000], Train Loss: 8114392.5000, Val Loss: 5171254.0000\n",
      "Epoch [7707/50000], Train Loss: 8114331.0000, Val Loss: 5171381.0000\n",
      "Epoch [7708/50000], Train Loss: 8114267.0000, Val Loss: 5171507.0000\n",
      "Epoch [7709/50000], Train Loss: 8114204.0000, Val Loss: 5171633.5000\n",
      "Epoch [7710/50000], Train Loss: 8114142.5000, Val Loss: 5171760.0000\n",
      "Epoch [7711/50000], Train Loss: 8114079.0000, Val Loss: 5171886.5000\n",
      "Epoch [7712/50000], Train Loss: 8114016.5000, Val Loss: 5172011.5000\n",
      "Epoch [7713/50000], Train Loss: 8113955.0000, Val Loss: 5172138.5000\n",
      "Epoch [7714/50000], Train Loss: 8113892.5000, Val Loss: 5172264.5000\n",
      "Epoch [7715/50000], Train Loss: 8113830.0000, Val Loss: 5172391.5000\n",
      "Epoch [7716/50000], Train Loss: 8113768.5000, Val Loss: 5172517.0000\n",
      "Epoch [7717/50000], Train Loss: 8113706.5000, Val Loss: 5172643.0000\n",
      "Epoch [7718/50000], Train Loss: 8113646.0000, Val Loss: 5172769.0000\n",
      "Epoch [7719/50000], Train Loss: 8113584.5000, Val Loss: 5172895.0000\n",
      "Epoch [7720/50000], Train Loss: 8113524.0000, Val Loss: 5173021.5000\n",
      "Epoch [7721/50000], Train Loss: 8113461.5000, Val Loss: 5173147.0000\n",
      "Epoch [7722/50000], Train Loss: 8113400.0000, Val Loss: 5173273.0000\n",
      "Epoch [7723/50000], Train Loss: 8113339.5000, Val Loss: 5173400.0000\n",
      "Epoch [7724/50000], Train Loss: 8113279.0000, Val Loss: 5173525.5000\n",
      "Epoch [7725/50000], Train Loss: 8113217.5000, Val Loss: 5173651.5000\n",
      "Epoch [7726/50000], Train Loss: 8113157.5000, Val Loss: 5173777.5000\n",
      "Epoch [7727/50000], Train Loss: 8113097.0000, Val Loss: 5173903.5000\n",
      "Epoch [7728/50000], Train Loss: 8113036.5000, Val Loss: 5174030.0000\n",
      "Epoch [7729/50000], Train Loss: 8112976.5000, Val Loss: 5174155.0000\n",
      "Epoch [7730/50000], Train Loss: 8112917.0000, Val Loss: 5174281.5000\n",
      "Epoch [7731/50000], Train Loss: 8112856.5000, Val Loss: 5174407.5000\n",
      "Epoch [7732/50000], Train Loss: 8112796.0000, Val Loss: 5174533.0000\n",
      "Epoch [7733/50000], Train Loss: 8112737.0000, Val Loss: 5174659.0000\n",
      "Epoch [7734/50000], Train Loss: 8112676.5000, Val Loss: 5174784.0000\n",
      "Epoch [7735/50000], Train Loss: 8112617.0000, Val Loss: 5174910.5000\n",
      "Epoch [7736/50000], Train Loss: 8112558.0000, Val Loss: 5175035.5000\n",
      "Epoch [7737/50000], Train Loss: 8112499.0000, Val Loss: 5175162.0000\n",
      "Epoch [7738/50000], Train Loss: 8112440.0000, Val Loss: 5175287.5000\n",
      "Epoch [7739/50000], Train Loss: 8112380.5000, Val Loss: 5175413.5000\n",
      "Epoch [7740/50000], Train Loss: 8112322.0000, Val Loss: 5175538.5000\n",
      "Epoch [7741/50000], Train Loss: 8112262.5000, Val Loss: 5175664.5000\n",
      "Epoch [7742/50000], Train Loss: 8112205.0000, Val Loss: 5175790.0000\n",
      "Epoch [7743/50000], Train Loss: 8112146.0000, Val Loss: 5175915.5000\n",
      "Epoch [7744/50000], Train Loss: 8112087.0000, Val Loss: 5176040.5000\n",
      "Epoch [7745/50000], Train Loss: 8112028.5000, Val Loss: 5176166.5000\n",
      "Epoch [7746/50000], Train Loss: 8111971.0000, Val Loss: 5176292.5000\n",
      "Epoch [7747/50000], Train Loss: 8111912.5000, Val Loss: 5176417.5000\n",
      "Epoch [7748/50000], Train Loss: 8111854.5000, Val Loss: 5176543.5000\n",
      "Epoch [7749/50000], Train Loss: 8111796.0000, Val Loss: 5176668.5000\n",
      "Epoch [7750/50000], Train Loss: 8111739.5000, Val Loss: 5176793.5000\n",
      "Epoch [7751/50000], Train Loss: 8111682.0000, Val Loss: 5176920.0000\n",
      "Epoch [7752/50000], Train Loss: 8111624.0000, Val Loss: 5177045.5000\n",
      "Epoch [7753/50000], Train Loss: 8111567.0000, Val Loss: 5177170.5000\n",
      "Epoch [7754/50000], Train Loss: 8111509.5000, Val Loss: 5177295.5000\n",
      "Epoch [7755/50000], Train Loss: 8111451.5000, Val Loss: 5177419.5000\n",
      "Epoch [7756/50000], Train Loss: 8111395.0000, Val Loss: 5177545.5000\n",
      "Epoch [7757/50000], Train Loss: 8111338.5000, Val Loss: 5177671.0000\n",
      "Epoch [7758/50000], Train Loss: 8111283.0000, Val Loss: 5177795.5000\n",
      "Epoch [7759/50000], Train Loss: 8111225.5000, Val Loss: 5177920.5000\n",
      "Epoch [7760/50000], Train Loss: 8111168.5000, Val Loss: 5178046.5000\n",
      "Epoch [7761/50000], Train Loss: 8111112.0000, Val Loss: 5178171.0000\n",
      "Epoch [7762/50000], Train Loss: 8111056.5000, Val Loss: 5178296.0000\n",
      "Epoch [7763/50000], Train Loss: 8111000.0000, Val Loss: 5178421.5000\n",
      "Epoch [7764/50000], Train Loss: 8110944.0000, Val Loss: 5178546.0000\n",
      "Epoch [7765/50000], Train Loss: 8110888.5000, Val Loss: 5178672.0000\n",
      "Epoch [7766/50000], Train Loss: 8110831.0000, Val Loss: 5178797.0000\n",
      "Epoch [7767/50000], Train Loss: 8110775.5000, Val Loss: 5178921.5000\n",
      "Epoch [7768/50000], Train Loss: 8110721.0000, Val Loss: 5179046.5000\n",
      "Epoch [7769/50000], Train Loss: 8110665.0000, Val Loss: 5179171.5000\n",
      "Epoch [7770/50000], Train Loss: 8110609.5000, Val Loss: 5179296.0000\n",
      "Epoch [7771/50000], Train Loss: 8110554.5000, Val Loss: 5179421.0000\n",
      "Epoch [7772/50000], Train Loss: 8110499.0000, Val Loss: 5179545.0000\n",
      "Epoch [7773/50000], Train Loss: 8110444.0000, Val Loss: 5179670.5000\n",
      "Epoch [7774/50000], Train Loss: 8110390.0000, Val Loss: 5179794.5000\n",
      "Epoch [7775/50000], Train Loss: 8110335.0000, Val Loss: 5179920.0000\n",
      "Epoch [7776/50000], Train Loss: 8110280.5000, Val Loss: 5180045.0000\n",
      "Epoch [7777/50000], Train Loss: 8110225.0000, Val Loss: 5180169.5000\n",
      "Epoch [7778/50000], Train Loss: 8110170.5000, Val Loss: 5180294.5000\n",
      "Epoch [7779/50000], Train Loss: 8110116.0000, Val Loss: 5180419.0000\n",
      "Epoch [7780/50000], Train Loss: 8110062.5000, Val Loss: 5180543.0000\n",
      "Epoch [7781/50000], Train Loss: 8110008.5000, Val Loss: 5180667.5000\n",
      "Epoch [7782/50000], Train Loss: 8109954.0000, Val Loss: 5180792.0000\n",
      "Epoch [7783/50000], Train Loss: 8109901.0000, Val Loss: 5180917.0000\n",
      "Epoch [7784/50000], Train Loss: 8109846.0000, Val Loss: 5181041.0000\n",
      "Epoch [7785/50000], Train Loss: 8109792.5000, Val Loss: 5181166.0000\n",
      "Epoch [7786/50000], Train Loss: 8109738.5000, Val Loss: 5181289.5000\n",
      "Epoch [7787/50000], Train Loss: 8109685.0000, Val Loss: 5181414.0000\n",
      "Epoch [7788/50000], Train Loss: 8109633.0000, Val Loss: 5181537.5000\n",
      "Epoch [7789/50000], Train Loss: 8109579.0000, Val Loss: 5181662.5000\n",
      "Epoch [7790/50000], Train Loss: 8109526.5000, Val Loss: 5181787.0000\n",
      "Epoch [7791/50000], Train Loss: 8109473.0000, Val Loss: 5181911.5000\n",
      "Epoch [7792/50000], Train Loss: 8109420.5000, Val Loss: 5182035.5000\n",
      "Epoch [7793/50000], Train Loss: 8109367.0000, Val Loss: 5182159.5000\n",
      "Epoch [7794/50000], Train Loss: 8109315.5000, Val Loss: 5182283.0000\n",
      "Epoch [7795/50000], Train Loss: 8109262.5000, Val Loss: 5182407.5000\n",
      "Epoch [7796/50000], Train Loss: 8109208.5000, Val Loss: 5182531.0000\n",
      "Epoch [7797/50000], Train Loss: 8109157.0000, Val Loss: 5182655.0000\n",
      "Epoch [7798/50000], Train Loss: 8109105.0000, Val Loss: 5182779.5000\n",
      "Epoch [7799/50000], Train Loss: 8109052.5000, Val Loss: 5182903.0000\n",
      "Epoch [7800/50000], Train Loss: 8109001.0000, Val Loss: 5183026.5000\n",
      "Epoch [7801/50000], Train Loss: 8108949.5000, Val Loss: 5183151.0000\n",
      "Epoch [7802/50000], Train Loss: 8108897.0000, Val Loss: 5183274.5000\n",
      "Epoch [7803/50000], Train Loss: 8108846.0000, Val Loss: 5183399.0000\n",
      "Epoch [7804/50000], Train Loss: 8108794.5000, Val Loss: 5183522.5000\n",
      "Epoch [7805/50000], Train Loss: 8108742.5000, Val Loss: 5183646.0000\n",
      "Epoch [7806/50000], Train Loss: 8108691.0000, Val Loss: 5183769.5000\n",
      "Epoch [7807/50000], Train Loss: 8108639.5000, Val Loss: 5183894.0000\n",
      "Epoch [7808/50000], Train Loss: 8108589.0000, Val Loss: 5184017.0000\n",
      "Epoch [7809/50000], Train Loss: 8108537.5000, Val Loss: 5184141.0000\n",
      "Epoch [7810/50000], Train Loss: 8108487.5000, Val Loss: 5184264.0000\n",
      "Epoch [7811/50000], Train Loss: 8108436.0000, Val Loss: 5184387.5000\n",
      "Epoch [7812/50000], Train Loss: 8108385.0000, Val Loss: 5184511.5000\n",
      "Epoch [7813/50000], Train Loss: 8108334.5000, Val Loss: 5184635.5000\n",
      "Epoch [7814/50000], Train Loss: 8108284.0000, Val Loss: 5184758.5000\n",
      "Epoch [7815/50000], Train Loss: 8108233.5000, Val Loss: 5184881.5000\n",
      "Epoch [7816/50000], Train Loss: 8108183.0000, Val Loss: 5185006.0000\n",
      "Epoch [7817/50000], Train Loss: 8108133.0000, Val Loss: 5185129.0000\n",
      "Epoch [7818/50000], Train Loss: 8108083.0000, Val Loss: 5185251.5000\n",
      "Epoch [7819/50000], Train Loss: 8108033.0000, Val Loss: 5185375.5000\n",
      "Epoch [7820/50000], Train Loss: 8107983.0000, Val Loss: 5185498.5000\n",
      "Epoch [7821/50000], Train Loss: 8107934.0000, Val Loss: 5185622.0000\n",
      "Epoch [7822/50000], Train Loss: 8107884.0000, Val Loss: 5185744.0000\n",
      "Epoch [7823/50000], Train Loss: 8107835.0000, Val Loss: 5185867.0000\n",
      "Epoch [7824/50000], Train Loss: 8107785.5000, Val Loss: 5185990.5000\n",
      "Epoch [7825/50000], Train Loss: 8107735.5000, Val Loss: 5186113.5000\n",
      "Epoch [7826/50000], Train Loss: 8107686.0000, Val Loss: 5186237.0000\n",
      "Epoch [7827/50000], Train Loss: 8107637.0000, Val Loss: 5186359.5000\n",
      "Epoch [7828/50000], Train Loss: 8107588.5000, Val Loss: 5186483.0000\n",
      "Epoch [7829/50000], Train Loss: 8107539.5000, Val Loss: 5186606.0000\n",
      "Epoch [7830/50000], Train Loss: 8107490.0000, Val Loss: 5186728.0000\n",
      "Epoch [7831/50000], Train Loss: 8107442.0000, Val Loss: 5186851.0000\n",
      "Epoch [7832/50000], Train Loss: 8107393.0000, Val Loss: 5186974.0000\n",
      "Epoch [7833/50000], Train Loss: 8107345.0000, Val Loss: 5187096.5000\n",
      "Epoch [7834/50000], Train Loss: 8107296.0000, Val Loss: 5187219.0000\n",
      "Epoch [7835/50000], Train Loss: 8107247.0000, Val Loss: 5187342.0000\n",
      "Epoch [7836/50000], Train Loss: 8107200.0000, Val Loss: 5187464.5000\n",
      "Epoch [7837/50000], Train Loss: 8107152.5000, Val Loss: 5187586.5000\n",
      "Epoch [7838/50000], Train Loss: 8107104.5000, Val Loss: 5187710.0000\n",
      "Epoch [7839/50000], Train Loss: 8107056.5000, Val Loss: 5187832.0000\n",
      "Epoch [7840/50000], Train Loss: 8107008.0000, Val Loss: 5187954.5000\n",
      "Epoch [7841/50000], Train Loss: 8106961.0000, Val Loss: 5188077.0000\n",
      "Epoch [7842/50000], Train Loss: 8106913.0000, Val Loss: 5188199.5000\n",
      "Epoch [7843/50000], Train Loss: 8106866.5000, Val Loss: 5188322.0000\n",
      "Epoch [7844/50000], Train Loss: 8106817.5000, Val Loss: 5188444.5000\n",
      "Epoch [7845/50000], Train Loss: 8106771.0000, Val Loss: 5188567.0000\n",
      "Epoch [7846/50000], Train Loss: 8106724.0000, Val Loss: 5188688.0000\n",
      "Epoch [7847/50000], Train Loss: 8106677.0000, Val Loss: 5188810.0000\n",
      "Epoch [7848/50000], Train Loss: 8106630.0000, Val Loss: 5188933.0000\n",
      "Epoch [7849/50000], Train Loss: 8106583.0000, Val Loss: 5189055.0000\n",
      "Epoch [7850/50000], Train Loss: 8106536.0000, Val Loss: 5189176.5000\n",
      "Epoch [7851/50000], Train Loss: 8106489.5000, Val Loss: 5189298.5000\n",
      "Epoch [7852/50000], Train Loss: 8106442.0000, Val Loss: 5189420.5000\n",
      "Epoch [7853/50000], Train Loss: 8106396.5000, Val Loss: 5189543.0000\n",
      "Epoch [7854/50000], Train Loss: 8106349.5000, Val Loss: 5189665.0000\n",
      "Epoch [7855/50000], Train Loss: 8106305.0000, Val Loss: 5189786.5000\n",
      "Epoch [7856/50000], Train Loss: 8106258.0000, Val Loss: 5189908.5000\n",
      "Epoch [7857/50000], Train Loss: 8106212.0000, Val Loss: 5190030.0000\n",
      "Epoch [7858/50000], Train Loss: 8106166.0000, Val Loss: 5190152.0000\n",
      "Epoch [7859/50000], Train Loss: 8106120.0000, Val Loss: 5190273.5000\n",
      "Epoch [7860/50000], Train Loss: 8106074.5000, Val Loss: 5190394.0000\n",
      "Epoch [7861/50000], Train Loss: 8106029.5000, Val Loss: 5190516.5000\n",
      "Epoch [7862/50000], Train Loss: 8105983.0000, Val Loss: 5190638.0000\n",
      "Epoch [7863/50000], Train Loss: 8105938.0000, Val Loss: 5190759.0000\n",
      "Epoch [7864/50000], Train Loss: 8105892.5000, Val Loss: 5190881.0000\n",
      "Epoch [7865/50000], Train Loss: 8105847.5000, Val Loss: 5191002.0000\n",
      "Epoch [7866/50000], Train Loss: 8105802.5000, Val Loss: 5191123.0000\n",
      "Epoch [7867/50000], Train Loss: 8105757.0000, Val Loss: 5191245.5000\n",
      "Epoch [7868/50000], Train Loss: 8105712.5000, Val Loss: 5191365.5000\n",
      "Epoch [7869/50000], Train Loss: 8105667.5000, Val Loss: 5191487.5000\n",
      "Epoch [7870/50000], Train Loss: 8105622.5000, Val Loss: 5191608.5000\n",
      "Epoch [7871/50000], Train Loss: 8105578.0000, Val Loss: 5191729.0000\n",
      "Epoch [7872/50000], Train Loss: 8105533.0000, Val Loss: 5191850.5000\n",
      "Epoch [7873/50000], Train Loss: 8105489.0000, Val Loss: 5191971.5000\n",
      "Epoch [7874/50000], Train Loss: 8105444.5000, Val Loss: 5192093.0000\n",
      "Epoch [7875/50000], Train Loss: 8105400.0000, Val Loss: 5192214.0000\n",
      "Epoch [7876/50000], Train Loss: 8105356.5000, Val Loss: 5192335.0000\n",
      "Epoch [7877/50000], Train Loss: 8105312.0000, Val Loss: 5192455.5000\n",
      "Epoch [7878/50000], Train Loss: 8105268.0000, Val Loss: 5192576.0000\n",
      "Epoch [7879/50000], Train Loss: 8105224.5000, Val Loss: 5192697.0000\n",
      "Epoch [7880/50000], Train Loss: 8105180.5000, Val Loss: 5192818.0000\n",
      "Epoch [7881/50000], Train Loss: 8105136.5000, Val Loss: 5192938.5000\n",
      "Epoch [7882/50000], Train Loss: 8105093.5000, Val Loss: 5193059.0000\n",
      "Epoch [7883/50000], Train Loss: 8105050.5000, Val Loss: 5193179.0000\n",
      "Epoch [7884/50000], Train Loss: 8105006.5000, Val Loss: 5193301.0000\n",
      "Epoch [7885/50000], Train Loss: 8104963.0000, Val Loss: 5193420.5000\n",
      "Epoch [7886/50000], Train Loss: 8104920.5000, Val Loss: 5193541.5000\n",
      "Epoch [7887/50000], Train Loss: 8104878.0000, Val Loss: 5193661.5000\n",
      "Epoch [7888/50000], Train Loss: 8104833.5000, Val Loss: 5193782.5000\n",
      "Epoch [7889/50000], Train Loss: 8104791.0000, Val Loss: 5193903.0000\n",
      "Epoch [7890/50000], Train Loss: 8104748.5000, Val Loss: 5194023.0000\n",
      "Epoch [7891/50000], Train Loss: 8104705.0000, Val Loss: 5194143.0000\n",
      "Epoch [7892/50000], Train Loss: 8104663.0000, Val Loss: 5194263.5000\n",
      "Epoch [7893/50000], Train Loss: 8104620.0000, Val Loss: 5194383.0000\n",
      "Epoch [7894/50000], Train Loss: 8104577.5000, Val Loss: 5194503.5000\n",
      "Epoch [7895/50000], Train Loss: 8104535.0000, Val Loss: 5194623.0000\n",
      "Epoch [7896/50000], Train Loss: 8104493.5000, Val Loss: 5194743.0000\n",
      "Epoch [7897/50000], Train Loss: 8104451.5000, Val Loss: 5194863.5000\n",
      "Epoch [7898/50000], Train Loss: 8104409.5000, Val Loss: 5194983.5000\n",
      "Epoch [7899/50000], Train Loss: 8104367.0000, Val Loss: 5195103.0000\n",
      "Epoch [7900/50000], Train Loss: 8104325.0000, Val Loss: 5195223.0000\n",
      "Epoch [7901/50000], Train Loss: 8104284.0000, Val Loss: 5195342.5000\n",
      "Epoch [7902/50000], Train Loss: 8104242.0000, Val Loss: 5195462.5000\n",
      "Epoch [7903/50000], Train Loss: 8104201.0000, Val Loss: 5195582.0000\n",
      "Epoch [7904/50000], Train Loss: 8104158.5000, Val Loss: 5195702.0000\n",
      "Epoch [7905/50000], Train Loss: 8104117.5000, Val Loss: 5195821.0000\n",
      "Epoch [7906/50000], Train Loss: 8104076.5000, Val Loss: 5195941.0000\n",
      "Epoch [7907/50000], Train Loss: 8104035.0000, Val Loss: 5196060.5000\n",
      "Epoch [7908/50000], Train Loss: 8103994.5000, Val Loss: 5196179.5000\n",
      "Epoch [7909/50000], Train Loss: 8103952.5000, Val Loss: 5196299.0000\n",
      "Epoch [7910/50000], Train Loss: 8103912.0000, Val Loss: 5196418.0000\n",
      "Epoch [7911/50000], Train Loss: 8103871.0000, Val Loss: 5196537.5000\n",
      "Epoch [7912/50000], Train Loss: 8103830.5000, Val Loss: 5196657.0000\n",
      "Epoch [7913/50000], Train Loss: 8103789.0000, Val Loss: 5196776.0000\n",
      "Epoch [7914/50000], Train Loss: 8103748.5000, Val Loss: 5196895.5000\n",
      "Epoch [7915/50000], Train Loss: 8103708.5000, Val Loss: 5197014.0000\n",
      "Epoch [7916/50000], Train Loss: 8103668.0000, Val Loss: 5197133.0000\n",
      "Epoch [7917/50000], Train Loss: 8103627.0000, Val Loss: 5197252.5000\n",
      "Epoch [7918/50000], Train Loss: 8103587.5000, Val Loss: 5197370.5000\n",
      "Epoch [7919/50000], Train Loss: 8103548.0000, Val Loss: 5197489.5000\n",
      "Epoch [7920/50000], Train Loss: 8103507.5000, Val Loss: 5197608.5000\n",
      "Epoch [7921/50000], Train Loss: 8103468.0000, Val Loss: 5197727.5000\n",
      "Epoch [7922/50000], Train Loss: 8103427.5000, Val Loss: 5197846.0000\n",
      "Epoch [7923/50000], Train Loss: 8103387.5000, Val Loss: 5197965.0000\n",
      "Epoch [7924/50000], Train Loss: 8103348.0000, Val Loss: 5198083.5000\n",
      "Epoch [7925/50000], Train Loss: 8103309.0000, Val Loss: 5198202.0000\n",
      "Epoch [7926/50000], Train Loss: 8103269.0000, Val Loss: 5198320.0000\n",
      "Epoch [7927/50000], Train Loss: 8103230.0000, Val Loss: 5198438.5000\n",
      "Epoch [7928/50000], Train Loss: 8103190.0000, Val Loss: 5198558.0000\n",
      "Epoch [7929/50000], Train Loss: 8103150.0000, Val Loss: 5198675.0000\n",
      "Epoch [7930/50000], Train Loss: 8103112.0000, Val Loss: 5198794.5000\n",
      "Epoch [7931/50000], Train Loss: 8103072.0000, Val Loss: 5198912.5000\n",
      "Epoch [7932/50000], Train Loss: 8103034.0000, Val Loss: 5199030.5000\n",
      "Epoch [7933/50000], Train Loss: 8102995.0000, Val Loss: 5199149.5000\n",
      "Epoch [7934/50000], Train Loss: 8102956.5000, Val Loss: 5199267.0000\n",
      "Epoch [7935/50000], Train Loss: 8102917.0000, Val Loss: 5199384.5000\n",
      "Epoch [7936/50000], Train Loss: 8102879.5000, Val Loss: 5199502.5000\n",
      "Epoch [7937/50000], Train Loss: 8102841.5000, Val Loss: 5199620.5000\n",
      "Epoch [7938/50000], Train Loss: 8102803.0000, Val Loss: 5199738.0000\n",
      "Epoch [7939/50000], Train Loss: 8102763.0000, Val Loss: 5199856.5000\n",
      "Epoch [7940/50000], Train Loss: 8102725.0000, Val Loss: 5199974.5000\n",
      "Epoch [7941/50000], Train Loss: 8102687.0000, Val Loss: 5200091.5000\n",
      "Epoch [7942/50000], Train Loss: 8102650.0000, Val Loss: 5200209.0000\n",
      "Epoch [7943/50000], Train Loss: 8102611.5000, Val Loss: 5200327.0000\n",
      "Epoch [7944/50000], Train Loss: 8102573.5000, Val Loss: 5200445.0000\n",
      "Epoch [7945/50000], Train Loss: 8102536.0000, Val Loss: 5200561.5000\n",
      "Epoch [7946/50000], Train Loss: 8102499.0000, Val Loss: 5200680.0000\n",
      "Epoch [7947/50000], Train Loss: 8102460.0000, Val Loss: 5200797.0000\n",
      "Epoch [7948/50000], Train Loss: 8102423.0000, Val Loss: 5200915.0000\n",
      "Epoch [7949/50000], Train Loss: 8102385.0000, Val Loss: 5201032.0000\n",
      "Epoch [7950/50000], Train Loss: 8102348.0000, Val Loss: 5201149.0000\n",
      "Epoch [7951/50000], Train Loss: 8102310.5000, Val Loss: 5201266.5000\n",
      "Epoch [7952/50000], Train Loss: 8102273.0000, Val Loss: 5201383.5000\n",
      "Epoch [7953/50000], Train Loss: 8102236.0000, Val Loss: 5201501.0000\n",
      "Epoch [7954/50000], Train Loss: 8102199.0000, Val Loss: 5201617.0000\n",
      "Epoch [7955/50000], Train Loss: 8102161.5000, Val Loss: 5201734.5000\n",
      "Epoch [7956/50000], Train Loss: 8102125.0000, Val Loss: 5201851.5000\n",
      "Epoch [7957/50000], Train Loss: 8102089.5000, Val Loss: 5201968.5000\n",
      "Epoch [7958/50000], Train Loss: 8102052.5000, Val Loss: 5202085.0000\n",
      "Epoch [7959/50000], Train Loss: 8102015.5000, Val Loss: 5202201.5000\n",
      "Epoch [7960/50000], Train Loss: 8101979.0000, Val Loss: 5202319.0000\n",
      "Epoch [7961/50000], Train Loss: 8101943.0000, Val Loss: 5202435.0000\n",
      "Epoch [7962/50000], Train Loss: 8101906.0000, Val Loss: 5202551.5000\n",
      "Epoch [7963/50000], Train Loss: 8101869.5000, Val Loss: 5202668.5000\n",
      "Epoch [7964/50000], Train Loss: 8101833.5000, Val Loss: 5202785.5000\n",
      "Epoch [7965/50000], Train Loss: 8101797.0000, Val Loss: 5202901.5000\n",
      "Epoch [7966/50000], Train Loss: 8101761.0000, Val Loss: 5203017.5000\n",
      "Epoch [7967/50000], Train Loss: 8101725.0000, Val Loss: 5203134.0000\n",
      "Epoch [7968/50000], Train Loss: 8101690.0000, Val Loss: 5203250.5000\n",
      "Epoch [7969/50000], Train Loss: 8101653.5000, Val Loss: 5203366.5000\n",
      "Epoch [7970/50000], Train Loss: 8101618.0000, Val Loss: 5203482.0000\n",
      "Epoch [7971/50000], Train Loss: 8101582.5000, Val Loss: 5203598.5000\n",
      "Epoch [7972/50000], Train Loss: 8101547.0000, Val Loss: 5203714.5000\n",
      "Epoch [7973/50000], Train Loss: 8101512.0000, Val Loss: 5203830.5000\n",
      "Epoch [7974/50000], Train Loss: 8101476.0000, Val Loss: 5203946.5000\n",
      "Epoch [7975/50000], Train Loss: 8101440.0000, Val Loss: 5204062.5000\n",
      "Epoch [7976/50000], Train Loss: 8101406.5000, Val Loss: 5204178.0000\n",
      "Epoch [7977/50000], Train Loss: 8101370.5000, Val Loss: 5204293.5000\n",
      "Epoch [7978/50000], Train Loss: 8101335.0000, Val Loss: 5204409.5000\n",
      "Epoch [7979/50000], Train Loss: 8101301.0000, Val Loss: 5204525.0000\n",
      "Epoch [7980/50000], Train Loss: 8101266.0000, Val Loss: 5204640.0000\n",
      "Epoch [7981/50000], Train Loss: 8101231.0000, Val Loss: 5204756.5000\n",
      "Epoch [7982/50000], Train Loss: 8101196.5000, Val Loss: 5204871.5000\n",
      "Epoch [7983/50000], Train Loss: 8101161.5000, Val Loss: 5204987.0000\n",
      "Epoch [7984/50000], Train Loss: 8101126.5000, Val Loss: 5205103.0000\n",
      "Epoch [7985/50000], Train Loss: 8101093.0000, Val Loss: 5205217.5000\n",
      "Epoch [7986/50000], Train Loss: 8101058.0000, Val Loss: 5205333.5000\n",
      "Epoch [7987/50000], Train Loss: 8101023.5000, Val Loss: 5205448.0000\n",
      "Epoch [7988/50000], Train Loss: 8100990.0000, Val Loss: 5205563.0000\n",
      "Epoch [7989/50000], Train Loss: 8100955.5000, Val Loss: 5205678.5000\n",
      "Epoch [7990/50000], Train Loss: 8100921.5000, Val Loss: 5205793.5000\n",
      "Epoch [7991/50000], Train Loss: 8100887.5000, Val Loss: 5205908.5000\n",
      "Epoch [7992/50000], Train Loss: 8100853.0000, Val Loss: 5206023.0000\n",
      "Epoch [7993/50000], Train Loss: 8100819.5000, Val Loss: 5206138.0000\n",
      "Epoch [7994/50000], Train Loss: 8100786.0000, Val Loss: 5206253.0000\n",
      "Epoch [7995/50000], Train Loss: 8100751.5000, Val Loss: 5206367.5000\n",
      "Epoch [7996/50000], Train Loss: 8100718.0000, Val Loss: 5206482.0000\n",
      "Epoch [7997/50000], Train Loss: 8100685.5000, Val Loss: 5206596.5000\n",
      "Epoch [7998/50000], Train Loss: 8100651.0000, Val Loss: 5206711.0000\n",
      "Epoch [7999/50000], Train Loss: 8100618.0000, Val Loss: 5206825.0000\n",
      "Epoch [8000/50000], Train Loss: 8100585.0000, Val Loss: 5206939.5000\n",
      "Epoch [8001/50000], Train Loss: 8100551.5000, Val Loss: 5207054.5000\n",
      "Epoch [8002/50000], Train Loss: 8100518.5000, Val Loss: 5207168.5000\n",
      "Epoch [8003/50000], Train Loss: 8100486.0000, Val Loss: 5207282.5000\n",
      "Epoch [8004/50000], Train Loss: 8100452.5000, Val Loss: 5207397.0000\n",
      "Epoch [8005/50000], Train Loss: 8100420.0000, Val Loss: 5207511.0000\n",
      "Epoch [8006/50000], Train Loss: 8100387.0000, Val Loss: 5207624.5000\n",
      "Epoch [8007/50000], Train Loss: 8100354.0000, Val Loss: 5207738.5000\n",
      "Epoch [8008/50000], Train Loss: 8100321.5000, Val Loss: 5207853.0000\n",
      "Epoch [8009/50000], Train Loss: 8100289.5000, Val Loss: 5207967.0000\n",
      "Epoch [8010/50000], Train Loss: 8100256.5000, Val Loss: 5208080.0000\n",
      "Epoch [8011/50000], Train Loss: 8100223.5000, Val Loss: 5208194.5000\n",
      "Epoch [8012/50000], Train Loss: 8100192.5000, Val Loss: 5208307.5000\n",
      "Epoch [8013/50000], Train Loss: 8100160.0000, Val Loss: 5208421.5000\n",
      "Epoch [8014/50000], Train Loss: 8100128.0000, Val Loss: 5208534.5000\n",
      "Epoch [8015/50000], Train Loss: 8100095.5000, Val Loss: 5208648.0000\n",
      "Epoch [8016/50000], Train Loss: 8100063.5000, Val Loss: 5208762.0000\n",
      "Epoch [8017/50000], Train Loss: 8100031.5000, Val Loss: 5208875.0000\n",
      "Epoch [8018/50000], Train Loss: 8100000.0000, Val Loss: 5208987.5000\n",
      "Epoch [8019/50000], Train Loss: 8099967.5000, Val Loss: 5209101.5000\n",
      "Epoch [8020/50000], Train Loss: 8099936.5000, Val Loss: 5209214.5000\n",
      "Epoch [8021/50000], Train Loss: 8099905.0000, Val Loss: 5209328.0000\n",
      "Epoch [8022/50000], Train Loss: 8099873.5000, Val Loss: 5209441.0000\n",
      "Epoch [8023/50000], Train Loss: 8099841.5000, Val Loss: 5209553.5000\n",
      "Epoch [8024/50000], Train Loss: 8099810.0000, Val Loss: 5209666.5000\n",
      "Epoch [8025/50000], Train Loss: 8099779.0000, Val Loss: 5209779.0000\n",
      "Epoch [8026/50000], Train Loss: 8099747.5000, Val Loss: 5209892.5000\n",
      "Epoch [8027/50000], Train Loss: 8099716.0000, Val Loss: 5210005.5000\n",
      "Epoch [8028/50000], Train Loss: 8099684.5000, Val Loss: 5210117.5000\n",
      "Epoch [8029/50000], Train Loss: 8099654.0000, Val Loss: 5210230.5000\n",
      "Epoch [8030/50000], Train Loss: 8099623.5000, Val Loss: 5210343.5000\n",
      "Epoch [8031/50000], Train Loss: 8099592.0000, Val Loss: 5210456.0000\n",
      "Epoch [8032/50000], Train Loss: 8099561.5000, Val Loss: 5210567.5000\n",
      "Epoch [8033/50000], Train Loss: 8099531.0000, Val Loss: 5210680.5000\n",
      "Epoch [8034/50000], Train Loss: 8099500.5000, Val Loss: 5210792.0000\n",
      "Epoch [8035/50000], Train Loss: 8099469.0000, Val Loss: 5210904.5000\n",
      "Epoch [8036/50000], Train Loss: 8099439.5000, Val Loss: 5211016.5000\n",
      "Epoch [8037/50000], Train Loss: 8099409.0000, Val Loss: 5211128.0000\n",
      "Epoch [8038/50000], Train Loss: 8099378.0000, Val Loss: 5211241.5000\n",
      "Epoch [8039/50000], Train Loss: 8099348.0000, Val Loss: 5211353.0000\n",
      "Epoch [8040/50000], Train Loss: 8099317.5000, Val Loss: 5211464.0000\n",
      "Epoch [8041/50000], Train Loss: 8099287.5000, Val Loss: 5211576.5000\n",
      "Epoch [8042/50000], Train Loss: 8099257.5000, Val Loss: 5211688.5000\n",
      "Epoch [8043/50000], Train Loss: 8099227.0000, Val Loss: 5211800.0000\n",
      "Epoch [8044/50000], Train Loss: 8099197.0000, Val Loss: 5211912.0000\n",
      "Epoch [8045/50000], Train Loss: 8099167.5000, Val Loss: 5212022.5000\n",
      "Epoch [8046/50000], Train Loss: 8099137.5000, Val Loss: 5212135.0000\n",
      "Epoch [8047/50000], Train Loss: 8099107.5000, Val Loss: 5212246.5000\n",
      "Epoch [8048/50000], Train Loss: 8099078.0000, Val Loss: 5212358.0000\n",
      "Epoch [8049/50000], Train Loss: 8099049.0000, Val Loss: 5212468.5000\n",
      "Epoch [8050/50000], Train Loss: 8099020.0000, Val Loss: 5212579.5000\n",
      "Epoch [8051/50000], Train Loss: 8098990.0000, Val Loss: 5212691.0000\n",
      "Epoch [8052/50000], Train Loss: 8098959.5000, Val Loss: 5212801.5000\n",
      "Epoch [8053/50000], Train Loss: 8098931.0000, Val Loss: 5212913.0000\n",
      "Epoch [8054/50000], Train Loss: 8098902.0000, Val Loss: 5213024.0000\n",
      "Epoch [8055/50000], Train Loss: 8098872.5000, Val Loss: 5213135.5000\n",
      "Epoch [8056/50000], Train Loss: 8098843.5000, Val Loss: 5213246.0000\n",
      "Epoch [8057/50000], Train Loss: 8098815.0000, Val Loss: 5213357.0000\n",
      "Epoch [8058/50000], Train Loss: 8098785.5000, Val Loss: 5213467.5000\n",
      "Epoch [8059/50000], Train Loss: 8098756.5000, Val Loss: 5213578.5000\n",
      "Epoch [8060/50000], Train Loss: 8098728.5000, Val Loss: 5213689.0000\n",
      "Epoch [8061/50000], Train Loss: 8098699.0000, Val Loss: 5213799.0000\n",
      "Epoch [8062/50000], Train Loss: 8098671.0000, Val Loss: 5213910.5000\n",
      "Epoch [8063/50000], Train Loss: 8098641.0000, Val Loss: 5214020.5000\n",
      "Epoch [8064/50000], Train Loss: 8098613.5000, Val Loss: 5214130.5000\n",
      "Epoch [8065/50000], Train Loss: 8098585.5000, Val Loss: 5214241.5000\n",
      "Epoch [8066/50000], Train Loss: 8098556.5000, Val Loss: 5214351.5000\n",
      "Epoch [8067/50000], Train Loss: 8098528.0000, Val Loss: 5214461.0000\n",
      "Epoch [8068/50000], Train Loss: 8098500.5000, Val Loss: 5214571.5000\n",
      "Epoch [8069/50000], Train Loss: 8098472.5000, Val Loss: 5214681.0000\n",
      "Epoch [8070/50000], Train Loss: 8098444.0000, Val Loss: 5214791.0000\n",
      "Epoch [8071/50000], Train Loss: 8098415.5000, Val Loss: 5214901.5000\n",
      "Epoch [8072/50000], Train Loss: 8098388.0000, Val Loss: 5215010.5000\n",
      "Epoch [8073/50000], Train Loss: 8098360.0000, Val Loss: 5215121.0000\n",
      "Epoch [8074/50000], Train Loss: 8098332.0000, Val Loss: 5215230.0000\n",
      "Epoch [8075/50000], Train Loss: 8098304.0000, Val Loss: 5215339.5000\n",
      "Epoch [8076/50000], Train Loss: 8098277.5000, Val Loss: 5215449.0000\n",
      "Epoch [8077/50000], Train Loss: 8098249.0000, Val Loss: 5215559.0000\n",
      "Epoch [8078/50000], Train Loss: 8098222.0000, Val Loss: 5215668.5000\n",
      "Epoch [8079/50000], Train Loss: 8098194.0000, Val Loss: 5215777.5000\n",
      "Epoch [8080/50000], Train Loss: 8098166.5000, Val Loss: 5215886.0000\n",
      "Epoch [8081/50000], Train Loss: 8098139.0000, Val Loss: 5215995.5000\n",
      "Epoch [8082/50000], Train Loss: 8098112.5000, Val Loss: 5216105.5000\n",
      "Epoch [8083/50000], Train Loss: 8098085.0000, Val Loss: 5216213.5000\n",
      "Epoch [8084/50000], Train Loss: 8098057.5000, Val Loss: 5216323.0000\n",
      "Epoch [8085/50000], Train Loss: 8098030.5000, Val Loss: 5216431.5000\n",
      "Epoch [8086/50000], Train Loss: 8098003.0000, Val Loss: 5216541.0000\n",
      "Epoch [8087/50000], Train Loss: 8097976.5000, Val Loss: 5216649.0000\n",
      "Epoch [8088/50000], Train Loss: 8097950.0000, Val Loss: 5216758.0000\n",
      "Epoch [8089/50000], Train Loss: 8097923.0000, Val Loss: 5216867.0000\n",
      "Epoch [8090/50000], Train Loss: 8097895.5000, Val Loss: 5216975.5000\n",
      "Epoch [8091/50000], Train Loss: 8097869.5000, Val Loss: 5217083.5000\n",
      "Epoch [8092/50000], Train Loss: 8097843.0000, Val Loss: 5217192.5000\n",
      "Epoch [8093/50000], Train Loss: 8097816.5000, Val Loss: 5217300.5000\n",
      "Epoch [8094/50000], Train Loss: 8097790.0000, Val Loss: 5217409.5000\n",
      "Epoch [8095/50000], Train Loss: 8097764.0000, Val Loss: 5217516.5000\n",
      "Epoch [8096/50000], Train Loss: 8097737.5000, Val Loss: 5217625.5000\n",
      "Epoch [8097/50000], Train Loss: 8097710.5000, Val Loss: 5217733.5000\n",
      "Epoch [8098/50000], Train Loss: 8097684.0000, Val Loss: 5217841.0000\n",
      "Epoch [8099/50000], Train Loss: 8097658.5000, Val Loss: 5217949.5000\n",
      "Epoch [8100/50000], Train Loss: 8097632.5000, Val Loss: 5218057.5000\n",
      "Epoch [8101/50000], Train Loss: 8097606.5000, Val Loss: 5218165.0000\n",
      "Epoch [8102/50000], Train Loss: 8097580.5000, Val Loss: 5218273.0000\n",
      "Epoch [8103/50000], Train Loss: 8097554.5000, Val Loss: 5218381.0000\n",
      "Epoch [8104/50000], Train Loss: 8097529.5000, Val Loss: 5218488.5000\n",
      "Epoch [8105/50000], Train Loss: 8097503.0000, Val Loss: 5218595.0000\n",
      "Epoch [8106/50000], Train Loss: 8097477.5000, Val Loss: 5218703.0000\n",
      "Epoch [8107/50000], Train Loss: 8097452.0000, Val Loss: 5218810.0000\n",
      "Epoch [8108/50000], Train Loss: 8097426.0000, Val Loss: 5218918.0000\n",
      "Epoch [8109/50000], Train Loss: 8097400.0000, Val Loss: 5219025.0000\n",
      "Epoch [8110/50000], Train Loss: 8097375.5000, Val Loss: 5219132.5000\n",
      "Epoch [8111/50000], Train Loss: 8097350.0000, Val Loss: 5219239.5000\n",
      "Epoch [8112/50000], Train Loss: 8097324.5000, Val Loss: 5219347.0000\n",
      "Epoch [8113/50000], Train Loss: 8097299.0000, Val Loss: 5219453.0000\n",
      "Epoch [8114/50000], Train Loss: 8097274.0000, Val Loss: 5219560.5000\n",
      "Epoch [8115/50000], Train Loss: 8097249.0000, Val Loss: 5219666.5000\n",
      "Epoch [8116/50000], Train Loss: 8097224.5000, Val Loss: 5219774.0000\n",
      "Epoch [8117/50000], Train Loss: 8097199.0000, Val Loss: 5219880.5000\n",
      "Epoch [8118/50000], Train Loss: 8097173.5000, Val Loss: 5219986.5000\n",
      "Epoch [8119/50000], Train Loss: 8097148.5000, Val Loss: 5220094.0000\n",
      "Epoch [8120/50000], Train Loss: 8097124.5000, Val Loss: 5220200.0000\n",
      "Epoch [8121/50000], Train Loss: 8097099.0000, Val Loss: 5220306.5000\n",
      "Epoch [8122/50000], Train Loss: 8097075.0000, Val Loss: 5220412.5000\n",
      "Epoch [8123/50000], Train Loss: 8097050.0000, Val Loss: 5220519.0000\n",
      "Epoch [8124/50000], Train Loss: 8097026.0000, Val Loss: 5220625.0000\n",
      "Epoch [8125/50000], Train Loss: 8097001.0000, Val Loss: 5220731.5000\n",
      "Epoch [8126/50000], Train Loss: 8096977.0000, Val Loss: 5220837.5000\n",
      "Epoch [8127/50000], Train Loss: 8096952.5000, Val Loss: 5220943.5000\n",
      "Epoch [8128/50000], Train Loss: 8096928.0000, Val Loss: 5221049.0000\n",
      "Epoch [8129/50000], Train Loss: 8096904.5000, Val Loss: 5221154.5000\n",
      "Epoch [8130/50000], Train Loss: 8096880.5000, Val Loss: 5221261.5000\n",
      "Epoch [8131/50000], Train Loss: 8096855.5000, Val Loss: 5221367.0000\n",
      "Epoch [8132/50000], Train Loss: 8096831.5000, Val Loss: 5221472.0000\n",
      "Epoch [8133/50000], Train Loss: 8096807.5000, Val Loss: 5221577.0000\n",
      "Epoch [8134/50000], Train Loss: 8096783.5000, Val Loss: 5221682.5000\n",
      "Epoch [8135/50000], Train Loss: 8096759.5000, Val Loss: 5221789.0000\n",
      "Epoch [8136/50000], Train Loss: 8096736.0000, Val Loss: 5221894.0000\n",
      "Epoch [8137/50000], Train Loss: 8096712.5000, Val Loss: 5221998.5000\n",
      "Epoch [8138/50000], Train Loss: 8096689.0000, Val Loss: 5222104.5000\n",
      "Epoch [8139/50000], Train Loss: 8096665.5000, Val Loss: 5222209.0000\n",
      "Epoch [8140/50000], Train Loss: 8096641.5000, Val Loss: 5222314.0000\n",
      "Epoch [8141/50000], Train Loss: 8096618.0000, Val Loss: 5222419.0000\n",
      "Epoch [8142/50000], Train Loss: 8096594.5000, Val Loss: 5222523.5000\n",
      "Epoch [8143/50000], Train Loss: 8096571.0000, Val Loss: 5222629.0000\n",
      "Epoch [8144/50000], Train Loss: 8096548.0000, Val Loss: 5222733.5000\n",
      "Epoch [8145/50000], Train Loss: 8096524.5000, Val Loss: 5222839.0000\n",
      "Epoch [8146/50000], Train Loss: 8096501.5000, Val Loss: 5222943.0000\n",
      "Epoch [8147/50000], Train Loss: 8096477.0000, Val Loss: 5223047.5000\n",
      "Epoch [8148/50000], Train Loss: 8096454.5000, Val Loss: 5223151.5000\n",
      "Epoch [8149/50000], Train Loss: 8096432.5000, Val Loss: 5223256.5000\n",
      "Epoch [8150/50000], Train Loss: 8096408.5000, Val Loss: 5223360.5000\n",
      "Epoch [8151/50000], Train Loss: 8096386.0000, Val Loss: 5223465.0000\n",
      "Epoch [8152/50000], Train Loss: 8096363.0000, Val Loss: 5223569.0000\n",
      "Epoch [8153/50000], Train Loss: 8096340.0000, Val Loss: 5223673.0000\n",
      "Epoch [8154/50000], Train Loss: 8096318.0000, Val Loss: 5223776.5000\n",
      "Epoch [8155/50000], Train Loss: 8096295.5000, Val Loss: 5223880.5000\n",
      "Epoch [8156/50000], Train Loss: 8096271.5000, Val Loss: 5223985.0000\n",
      "Epoch [8157/50000], Train Loss: 8096250.5000, Val Loss: 5224089.0000\n",
      "Epoch [8158/50000], Train Loss: 8096227.5000, Val Loss: 5224192.0000\n",
      "Epoch [8159/50000], Train Loss: 8096205.5000, Val Loss: 5224296.5000\n",
      "Epoch [8160/50000], Train Loss: 8096182.5000, Val Loss: 5224399.0000\n",
      "Epoch [8161/50000], Train Loss: 8096160.0000, Val Loss: 5224502.5000\n",
      "Epoch [8162/50000], Train Loss: 8096138.0000, Val Loss: 5224606.0000\n",
      "Epoch [8163/50000], Train Loss: 8096116.0000, Val Loss: 5224710.0000\n",
      "Epoch [8164/50000], Train Loss: 8096094.0000, Val Loss: 5224813.5000\n",
      "Epoch [8165/50000], Train Loss: 8096071.5000, Val Loss: 5224916.5000\n",
      "Epoch [8166/50000], Train Loss: 8096049.5000, Val Loss: 5225019.0000\n",
      "Epoch [8167/50000], Train Loss: 8096027.0000, Val Loss: 5225122.0000\n",
      "Epoch [8168/50000], Train Loss: 8096005.5000, Val Loss: 5225225.5000\n",
      "Epoch [8169/50000], Train Loss: 8095983.0000, Val Loss: 5225328.5000\n",
      "Epoch [8170/50000], Train Loss: 8095962.0000, Val Loss: 5225431.5000\n",
      "Epoch [8171/50000], Train Loss: 8095940.0000, Val Loss: 5225534.0000\n",
      "Epoch [8172/50000], Train Loss: 8095918.5000, Val Loss: 5225636.5000\n",
      "Epoch [8173/50000], Train Loss: 8095897.5000, Val Loss: 5225739.0000\n",
      "Epoch [8174/50000], Train Loss: 8095875.0000, Val Loss: 5225841.0000\n",
      "Epoch [8175/50000], Train Loss: 8095853.5000, Val Loss: 5225944.5000\n",
      "Epoch [8176/50000], Train Loss: 8095831.5000, Val Loss: 5226046.5000\n",
      "Epoch [8177/50000], Train Loss: 8095810.0000, Val Loss: 5226148.5000\n",
      "Epoch [8178/50000], Train Loss: 8095789.5000, Val Loss: 5226251.0000\n",
      "Epoch [8179/50000], Train Loss: 8095768.5000, Val Loss: 5226353.0000\n",
      "Epoch [8180/50000], Train Loss: 8095747.5000, Val Loss: 5226455.0000\n",
      "Epoch [8181/50000], Train Loss: 8095726.0000, Val Loss: 5226557.5000\n",
      "Epoch [8182/50000], Train Loss: 8095704.5000, Val Loss: 5226658.5000\n",
      "Epoch [8183/50000], Train Loss: 8095683.5000, Val Loss: 5226761.0000\n",
      "Epoch [8184/50000], Train Loss: 8095661.5000, Val Loss: 5226863.0000\n",
      "Epoch [8185/50000], Train Loss: 8095641.5000, Val Loss: 5226964.5000\n",
      "Epoch [8186/50000], Train Loss: 8095621.0000, Val Loss: 5227066.0000\n",
      "Epoch [8187/50000], Train Loss: 8095599.5000, Val Loss: 5227168.0000\n",
      "Epoch [8188/50000], Train Loss: 8095579.0000, Val Loss: 5227268.5000\n",
      "Epoch [8189/50000], Train Loss: 8095558.5000, Val Loss: 5227370.0000\n",
      "Epoch [8190/50000], Train Loss: 8095537.5000, Val Loss: 5227471.5000\n",
      "Epoch [8191/50000], Train Loss: 8095517.0000, Val Loss: 5227573.0000\n",
      "Epoch [8192/50000], Train Loss: 8095497.0000, Val Loss: 5227674.5000\n",
      "Epoch [8193/50000], Train Loss: 8095475.0000, Val Loss: 5227775.0000\n",
      "Epoch [8194/50000], Train Loss: 8095455.5000, Val Loss: 5227876.5000\n",
      "Epoch [8195/50000], Train Loss: 8095435.0000, Val Loss: 5227977.5000\n",
      "Epoch [8196/50000], Train Loss: 8095414.5000, Val Loss: 5228078.0000\n",
      "Epoch [8197/50000], Train Loss: 8095394.0000, Val Loss: 5228179.0000\n",
      "Epoch [8198/50000], Train Loss: 8095373.5000, Val Loss: 5228279.5000\n",
      "Epoch [8199/50000], Train Loss: 8095353.5000, Val Loss: 5228379.5000\n",
      "Epoch [8200/50000], Train Loss: 8095333.5000, Val Loss: 5228481.0000\n",
      "Epoch [8201/50000], Train Loss: 8095314.0000, Val Loss: 5228581.5000\n",
      "Epoch [8202/50000], Train Loss: 8095292.5000, Val Loss: 5228681.5000\n",
      "Epoch [8203/50000], Train Loss: 8095273.5000, Val Loss: 5228782.0000\n",
      "Epoch [8204/50000], Train Loss: 8095253.0000, Val Loss: 5228882.0000\n",
      "Epoch [8205/50000], Train Loss: 8095233.5000, Val Loss: 5228982.0000\n",
      "Epoch [8206/50000], Train Loss: 8095214.0000, Val Loss: 5229081.5000\n",
      "Epoch [8207/50000], Train Loss: 8095194.5000, Val Loss: 5229182.5000\n",
      "Epoch [8208/50000], Train Loss: 8095174.5000, Val Loss: 5229282.0000\n",
      "Epoch [8209/50000], Train Loss: 8095154.0000, Val Loss: 5229382.0000\n",
      "Epoch [8210/50000], Train Loss: 8095135.5000, Val Loss: 5229481.5000\n",
      "Epoch [8211/50000], Train Loss: 8095114.0000, Val Loss: 5229582.0000\n",
      "Epoch [8212/50000], Train Loss: 8095096.5000, Val Loss: 5229681.0000\n",
      "Epoch [8213/50000], Train Loss: 8095076.5000, Val Loss: 5229781.5000\n",
      "Epoch [8214/50000], Train Loss: 8095056.5000, Val Loss: 5229880.5000\n",
      "Epoch [8215/50000], Train Loss: 8095038.0000, Val Loss: 5229979.0000\n",
      "Epoch [8216/50000], Train Loss: 8095018.0000, Val Loss: 5230078.5000\n",
      "Epoch [8217/50000], Train Loss: 8094999.5000, Val Loss: 5230178.0000\n",
      "Epoch [8218/50000], Train Loss: 8094979.5000, Val Loss: 5230277.0000\n",
      "Epoch [8219/50000], Train Loss: 8094960.5000, Val Loss: 5230375.0000\n",
      "Epoch [8220/50000], Train Loss: 8094941.0000, Val Loss: 5230475.0000\n",
      "Epoch [8221/50000], Train Loss: 8094922.0000, Val Loss: 5230573.5000\n",
      "Epoch [8222/50000], Train Loss: 8094903.0000, Val Loss: 5230672.5000\n",
      "Epoch [8223/50000], Train Loss: 8094884.0000, Val Loss: 5230771.0000\n",
      "Epoch [8224/50000], Train Loss: 8094865.5000, Val Loss: 5230870.0000\n",
      "Epoch [8225/50000], Train Loss: 8094847.0000, Val Loss: 5230968.5000\n",
      "Epoch [8226/50000], Train Loss: 8094826.5000, Val Loss: 5231067.0000\n",
      "Epoch [8227/50000], Train Loss: 8094809.5000, Val Loss: 5231166.0000\n",
      "Epoch [8228/50000], Train Loss: 8094789.5000, Val Loss: 5231264.0000\n",
      "Epoch [8229/50000], Train Loss: 8094771.5000, Val Loss: 5231361.5000\n",
      "Epoch [8230/50000], Train Loss: 8094752.5000, Val Loss: 5231461.0000\n",
      "Epoch [8231/50000], Train Loss: 8094734.5000, Val Loss: 5231559.0000\n",
      "Epoch [8232/50000], Train Loss: 8094715.5000, Val Loss: 5231656.5000\n",
      "Epoch [8233/50000], Train Loss: 8094698.0000, Val Loss: 5231754.0000\n",
      "Epoch [8234/50000], Train Loss: 8094679.0000, Val Loss: 5231852.5000\n",
      "Epoch [8235/50000], Train Loss: 8094660.5000, Val Loss: 5231951.0000\n",
      "Epoch [8236/50000], Train Loss: 8094642.0000, Val Loss: 5232047.5000\n",
      "Epoch [8237/50000], Train Loss: 8094624.5000, Val Loss: 5232145.5000\n",
      "Epoch [8238/50000], Train Loss: 8094605.5000, Val Loss: 5232242.5000\n",
      "Epoch [8239/50000], Train Loss: 8094588.0000, Val Loss: 5232341.5000\n",
      "Epoch [8240/50000], Train Loss: 8094570.5000, Val Loss: 5232438.5000\n",
      "Epoch [8241/50000], Train Loss: 8094551.5000, Val Loss: 5232535.5000\n",
      "Epoch [8242/50000], Train Loss: 8094534.0000, Val Loss: 5232632.5000\n",
      "Epoch [8243/50000], Train Loss: 8094515.5000, Val Loss: 5232730.5000\n",
      "Epoch [8244/50000], Train Loss: 8094497.5000, Val Loss: 5232826.5000\n",
      "Epoch [8245/50000], Train Loss: 8094479.5000, Val Loss: 5232924.5000\n",
      "Epoch [8246/50000], Train Loss: 8094462.5000, Val Loss: 5233021.0000\n",
      "Epoch [8247/50000], Train Loss: 8094444.5000, Val Loss: 5233118.0000\n",
      "Epoch [8248/50000], Train Loss: 8094427.0000, Val Loss: 5233214.5000\n",
      "Epoch [8249/50000], Train Loss: 8094409.0000, Val Loss: 5233311.5000\n",
      "Epoch [8250/50000], Train Loss: 8094391.5000, Val Loss: 5233408.0000\n",
      "Epoch [8251/50000], Train Loss: 8094374.0000, Val Loss: 5233504.0000\n",
      "Epoch [8252/50000], Train Loss: 8094356.0000, Val Loss: 5233601.0000\n",
      "Epoch [8253/50000], Train Loss: 8094339.0000, Val Loss: 5233697.0000\n",
      "Epoch [8254/50000], Train Loss: 8094321.5000, Val Loss: 5233793.5000\n",
      "Epoch [8255/50000], Train Loss: 8094304.5000, Val Loss: 5233889.0000\n",
      "Epoch [8256/50000], Train Loss: 8094286.0000, Val Loss: 5233986.0000\n",
      "Epoch [8257/50000], Train Loss: 8094269.0000, Val Loss: 5234081.5000\n",
      "Epoch [8258/50000], Train Loss: 8094252.5000, Val Loss: 5234178.0000\n",
      "Epoch [8259/50000], Train Loss: 8094235.5000, Val Loss: 5234274.0000\n",
      "Epoch [8260/50000], Train Loss: 8094218.5000, Val Loss: 5234370.0000\n",
      "Epoch [8261/50000], Train Loss: 8094201.5000, Val Loss: 5234465.5000\n",
      "Epoch [8262/50000], Train Loss: 8094184.5000, Val Loss: 5234561.5000\n",
      "Epoch [8263/50000], Train Loss: 8094167.0000, Val Loss: 5234656.5000\n",
      "Epoch [8264/50000], Train Loss: 8094150.0000, Val Loss: 5234752.5000\n",
      "Epoch [8265/50000], Train Loss: 8094134.0000, Val Loss: 5234847.5000\n",
      "Epoch [8266/50000], Train Loss: 8094116.0000, Val Loss: 5234943.5000\n",
      "Epoch [8267/50000], Train Loss: 8094099.0000, Val Loss: 5235038.5000\n",
      "Epoch [8268/50000], Train Loss: 8094082.0000, Val Loss: 5235134.0000\n",
      "Epoch [8269/50000], Train Loss: 8094066.0000, Val Loss: 5235229.0000\n",
      "Epoch [8270/50000], Train Loss: 8094048.5000, Val Loss: 5235323.5000\n",
      "Epoch [8271/50000], Train Loss: 8094033.0000, Val Loss: 5235419.0000\n",
      "Epoch [8272/50000], Train Loss: 8094015.5000, Val Loss: 5235513.5000\n",
      "Epoch [8273/50000], Train Loss: 8094000.5000, Val Loss: 5235608.5000\n",
      "Epoch [8274/50000], Train Loss: 8093983.0000, Val Loss: 5235703.5000\n",
      "Epoch [8275/50000], Train Loss: 8093966.5000, Val Loss: 5235797.5000\n",
      "Epoch [8276/50000], Train Loss: 8093950.0000, Val Loss: 5235892.5000\n",
      "Epoch [8277/50000], Train Loss: 8093934.0000, Val Loss: 5235986.0000\n",
      "Epoch [8278/50000], Train Loss: 8093918.0000, Val Loss: 5236081.0000\n",
      "Epoch [8279/50000], Train Loss: 8093901.0000, Val Loss: 5236175.5000\n",
      "Epoch [8280/50000], Train Loss: 8093884.5000, Val Loss: 5236269.5000\n",
      "Epoch [8281/50000], Train Loss: 8093869.0000, Val Loss: 5236362.5000\n",
      "Epoch [8282/50000], Train Loss: 8093853.0000, Val Loss: 5236457.5000\n",
      "Epoch [8283/50000], Train Loss: 8093837.0000, Val Loss: 5236551.5000\n",
      "Epoch [8284/50000], Train Loss: 8093820.0000, Val Loss: 5236646.0000\n",
      "Epoch [8285/50000], Train Loss: 8093805.0000, Val Loss: 5236739.0000\n",
      "Epoch [8286/50000], Train Loss: 8093789.0000, Val Loss: 5236832.5000\n",
      "Epoch [8287/50000], Train Loss: 8093773.0000, Val Loss: 5236927.0000\n",
      "Epoch [8288/50000], Train Loss: 8093756.5000, Val Loss: 5237020.5000\n",
      "Epoch [8289/50000], Train Loss: 8093741.0000, Val Loss: 5237113.0000\n",
      "Epoch [8290/50000], Train Loss: 8093725.0000, Val Loss: 5237207.0000\n",
      "Epoch [8291/50000], Train Loss: 8093710.0000, Val Loss: 5237300.5000\n",
      "Epoch [8292/50000], Train Loss: 8093694.0000, Val Loss: 5237394.0000\n",
      "Epoch [8293/50000], Train Loss: 8093678.5000, Val Loss: 5237486.5000\n",
      "Epoch [8294/50000], Train Loss: 8093662.5000, Val Loss: 5237579.5000\n",
      "Epoch [8295/50000], Train Loss: 8093647.0000, Val Loss: 5237672.5000\n",
      "Epoch [8296/50000], Train Loss: 8093631.5000, Val Loss: 5237766.0000\n",
      "Epoch [8297/50000], Train Loss: 8093615.5000, Val Loss: 5237858.0000\n",
      "Epoch [8298/50000], Train Loss: 8093600.5000, Val Loss: 5237951.5000\n",
      "Epoch [8299/50000], Train Loss: 8093585.5000, Val Loss: 5238044.5000\n",
      "Epoch [8300/50000], Train Loss: 8093569.5000, Val Loss: 5238136.5000\n",
      "Epoch [8301/50000], Train Loss: 8093555.0000, Val Loss: 5238229.0000\n",
      "Epoch [8302/50000], Train Loss: 8093539.5000, Val Loss: 5238321.0000\n",
      "Epoch [8303/50000], Train Loss: 8093525.0000, Val Loss: 5238414.0000\n",
      "Epoch [8304/50000], Train Loss: 8093509.0000, Val Loss: 5238506.5000\n",
      "Epoch [8305/50000], Train Loss: 8093493.5000, Val Loss: 5238598.5000\n",
      "Epoch [8306/50000], Train Loss: 8093479.5000, Val Loss: 5238690.0000\n",
      "Epoch [8307/50000], Train Loss: 8093464.5000, Val Loss: 5238782.5000\n",
      "Epoch [8308/50000], Train Loss: 8093449.0000, Val Loss: 5238873.5000\n",
      "Epoch [8309/50000], Train Loss: 8093433.5000, Val Loss: 5238966.0000\n",
      "Epoch [8310/50000], Train Loss: 8093419.0000, Val Loss: 5239057.5000\n",
      "Epoch [8311/50000], Train Loss: 8093403.5000, Val Loss: 5239149.5000\n",
      "Epoch [8312/50000], Train Loss: 8093389.0000, Val Loss: 5239241.0000\n",
      "Epoch [8313/50000], Train Loss: 8093373.0000, Val Loss: 5239333.0000\n",
      "Epoch [8314/50000], Train Loss: 8093360.5000, Val Loss: 5239423.5000\n",
      "Epoch [8315/50000], Train Loss: 8093345.0000, Val Loss: 5239515.0000\n",
      "Epoch [8316/50000], Train Loss: 8093330.5000, Val Loss: 5239606.5000\n",
      "Epoch [8317/50000], Train Loss: 8093315.5000, Val Loss: 5239697.5000\n",
      "Epoch [8318/50000], Train Loss: 8093301.0000, Val Loss: 5239789.5000\n",
      "Epoch [8319/50000], Train Loss: 8093286.0000, Val Loss: 5239880.0000\n",
      "Epoch [8320/50000], Train Loss: 8093272.5000, Val Loss: 5239970.5000\n",
      "Epoch [8321/50000], Train Loss: 8093256.5000, Val Loss: 5240062.0000\n",
      "Epoch [8322/50000], Train Loss: 8093243.5000, Val Loss: 5240152.5000\n",
      "Epoch [8323/50000], Train Loss: 8093229.0000, Val Loss: 5240243.5000\n",
      "Epoch [8324/50000], Train Loss: 8093214.5000, Val Loss: 5240334.0000\n",
      "Epoch [8325/50000], Train Loss: 8093200.5000, Val Loss: 5240424.5000\n",
      "Epoch [8326/50000], Train Loss: 8093185.5000, Val Loss: 5240515.0000\n",
      "Epoch [8327/50000], Train Loss: 8093171.5000, Val Loss: 5240605.0000\n",
      "Epoch [8328/50000], Train Loss: 8093158.0000, Val Loss: 5240696.0000\n",
      "Epoch [8329/50000], Train Loss: 8093143.5000, Val Loss: 5240785.5000\n",
      "Epoch [8330/50000], Train Loss: 8093129.0000, Val Loss: 5240875.5000\n",
      "Epoch [8331/50000], Train Loss: 8093115.5000, Val Loss: 5240966.5000\n",
      "Epoch [8332/50000], Train Loss: 8093101.5000, Val Loss: 5241056.0000\n",
      "Epoch [8333/50000], Train Loss: 8093087.5000, Val Loss: 5241145.0000\n",
      "Epoch [8334/50000], Train Loss: 8093074.5000, Val Loss: 5241235.5000\n",
      "Epoch [8335/50000], Train Loss: 8093060.0000, Val Loss: 5241325.5000\n",
      "Epoch [8336/50000], Train Loss: 8093045.5000, Val Loss: 5241414.5000\n",
      "Epoch [8337/50000], Train Loss: 8093032.5000, Val Loss: 5241504.5000\n",
      "Epoch [8338/50000], Train Loss: 8093018.0000, Val Loss: 5241593.5000\n",
      "Epoch [8339/50000], Train Loss: 8093004.5000, Val Loss: 5241682.0000\n",
      "Epoch [8340/50000], Train Loss: 8092991.0000, Val Loss: 5241772.5000\n",
      "Epoch [8341/50000], Train Loss: 8092977.5000, Val Loss: 5241861.5000\n",
      "Epoch [8342/50000], Train Loss: 8092964.0000, Val Loss: 5241951.0000\n",
      "Epoch [8343/50000], Train Loss: 8092950.0000, Val Loss: 5242039.5000\n",
      "Epoch [8344/50000], Train Loss: 8092936.5000, Val Loss: 5242128.5000\n",
      "Epoch [8345/50000], Train Loss: 8092923.5000, Val Loss: 5242217.5000\n",
      "Epoch [8346/50000], Train Loss: 8092909.5000, Val Loss: 5242305.5000\n",
      "Epoch [8347/50000], Train Loss: 8092896.0000, Val Loss: 5242394.5000\n",
      "Epoch [8348/50000], Train Loss: 8092883.0000, Val Loss: 5242483.0000\n",
      "Epoch [8349/50000], Train Loss: 8092869.0000, Val Loss: 5242572.5000\n",
      "Epoch [8350/50000], Train Loss: 8092856.0000, Val Loss: 5242659.5000\n",
      "Epoch [8351/50000], Train Loss: 8092842.5000, Val Loss: 5242749.0000\n",
      "Epoch [8352/50000], Train Loss: 8092829.0000, Val Loss: 5242837.0000\n",
      "Epoch [8353/50000], Train Loss: 8092816.5000, Val Loss: 5242925.0000\n",
      "Epoch [8354/50000], Train Loss: 8092803.5000, Val Loss: 5243013.5000\n",
      "Epoch [8355/50000], Train Loss: 8092790.5000, Val Loss: 5243102.0000\n",
      "Epoch [8356/50000], Train Loss: 8092776.5000, Val Loss: 5243189.0000\n",
      "Epoch [8357/50000], Train Loss: 8092763.5000, Val Loss: 5243277.0000\n",
      "Epoch [8358/50000], Train Loss: 8092750.5000, Val Loss: 5243365.5000\n",
      "Epoch [8359/50000], Train Loss: 8092739.0000, Val Loss: 5243452.5000\n",
      "Epoch [8360/50000], Train Loss: 8092726.0000, Val Loss: 5243540.5000\n",
      "Epoch [8361/50000], Train Loss: 8092712.5000, Val Loss: 5243627.5000\n",
      "Epoch [8362/50000], Train Loss: 8092699.5000, Val Loss: 5243715.0000\n",
      "Epoch [8363/50000], Train Loss: 8092686.5000, Val Loss: 5243802.0000\n",
      "Epoch [8364/50000], Train Loss: 8092674.0000, Val Loss: 5243889.5000\n",
      "Epoch [8365/50000], Train Loss: 8092661.0000, Val Loss: 5243976.5000\n",
      "Epoch [8366/50000], Train Loss: 8092648.5000, Val Loss: 5244064.0000\n",
      "Epoch [8367/50000], Train Loss: 8092635.5000, Val Loss: 5244151.5000\n",
      "Epoch [8368/50000], Train Loss: 8092623.0000, Val Loss: 5244238.0000\n",
      "Epoch [8369/50000], Train Loss: 8092610.0000, Val Loss: 5244325.0000\n",
      "Epoch [8370/50000], Train Loss: 8092598.5000, Val Loss: 5244411.5000\n",
      "Epoch [8371/50000], Train Loss: 8092585.5000, Val Loss: 5244498.0000\n",
      "Epoch [8372/50000], Train Loss: 8092572.0000, Val Loss: 5244585.0000\n",
      "Epoch [8373/50000], Train Loss: 8092561.0000, Val Loss: 5244671.5000\n",
      "Epoch [8374/50000], Train Loss: 8092548.5000, Val Loss: 5244758.5000\n",
      "Epoch [8375/50000], Train Loss: 8092536.0000, Val Loss: 5244844.5000\n",
      "Epoch [8376/50000], Train Loss: 8092524.5000, Val Loss: 5244930.0000\n",
      "Epoch [8377/50000], Train Loss: 8092511.5000, Val Loss: 5245016.5000\n",
      "Epoch [8378/50000], Train Loss: 8092498.5000, Val Loss: 5245103.5000\n",
      "Epoch [8379/50000], Train Loss: 8092486.5000, Val Loss: 5245189.5000\n",
      "Epoch [8380/50000], Train Loss: 8092475.0000, Val Loss: 5245275.0000\n",
      "Epoch [8381/50000], Train Loss: 8092462.0000, Val Loss: 5245361.0000\n",
      "Epoch [8382/50000], Train Loss: 8092450.0000, Val Loss: 5245446.5000\n",
      "Epoch [8383/50000], Train Loss: 8092438.5000, Val Loss: 5245533.0000\n",
      "Epoch [8384/50000], Train Loss: 8092425.5000, Val Loss: 5245618.0000\n",
      "Epoch [8385/50000], Train Loss: 8092414.0000, Val Loss: 5245703.0000\n",
      "Epoch [8386/50000], Train Loss: 8092402.5000, Val Loss: 5245789.0000\n",
      "Epoch [8387/50000], Train Loss: 8092390.0000, Val Loss: 5245874.5000\n",
      "Epoch [8388/50000], Train Loss: 8092378.0000, Val Loss: 5245959.5000\n",
      "Epoch [8389/50000], Train Loss: 8092365.5000, Val Loss: 5246044.5000\n",
      "Epoch [8390/50000], Train Loss: 8092354.0000, Val Loss: 5246129.5000\n",
      "Epoch [8391/50000], Train Loss: 8092342.5000, Val Loss: 5246215.0000\n",
      "Epoch [8392/50000], Train Loss: 8092331.0000, Val Loss: 5246300.5000\n",
      "Epoch [8393/50000], Train Loss: 8092319.5000, Val Loss: 5246384.0000\n",
      "Epoch [8394/50000], Train Loss: 8092307.5000, Val Loss: 5246469.5000\n",
      "Epoch [8395/50000], Train Loss: 8092296.0000, Val Loss: 5246554.5000\n",
      "Epoch [8396/50000], Train Loss: 8092284.5000, Val Loss: 5246639.0000\n",
      "Epoch [8397/50000], Train Loss: 8092272.5000, Val Loss: 5246723.0000\n",
      "Epoch [8398/50000], Train Loss: 8092261.5000, Val Loss: 5246807.0000\n",
      "Epoch [8399/50000], Train Loss: 8092249.5000, Val Loss: 5246891.5000\n",
      "Epoch [8400/50000], Train Loss: 8092238.5000, Val Loss: 5246976.0000\n",
      "Epoch [8401/50000], Train Loss: 8092227.0000, Val Loss: 5247061.0000\n",
      "Epoch [8402/50000], Train Loss: 8092215.5000, Val Loss: 5247144.5000\n",
      "Epoch [8403/50000], Train Loss: 8092204.0000, Val Loss: 5247229.0000\n",
      "Epoch [8404/50000], Train Loss: 8092192.5000, Val Loss: 5247312.0000\n",
      "Epoch [8405/50000], Train Loss: 8092181.5000, Val Loss: 5247396.5000\n",
      "Epoch [8406/50000], Train Loss: 8092169.5000, Val Loss: 5247479.5000\n",
      "Epoch [8407/50000], Train Loss: 8092158.5000, Val Loss: 5247563.0000\n",
      "Epoch [8408/50000], Train Loss: 8092147.5000, Val Loss: 5247647.0000\n",
      "Epoch [8409/50000], Train Loss: 8092136.0000, Val Loss: 5247730.5000\n",
      "Epoch [8410/50000], Train Loss: 8092124.5000, Val Loss: 5247813.5000\n",
      "Epoch [8411/50000], Train Loss: 8092114.5000, Val Loss: 5247898.0000\n",
      "Epoch [8412/50000], Train Loss: 8092102.5000, Val Loss: 5247981.0000\n",
      "Epoch [8413/50000], Train Loss: 8092092.0000, Val Loss: 5248064.0000\n",
      "Epoch [8414/50000], Train Loss: 8092081.0000, Val Loss: 5248146.5000\n",
      "Epoch [8415/50000], Train Loss: 8092069.5000, Val Loss: 5248230.5000\n",
      "Epoch [8416/50000], Train Loss: 8092059.0000, Val Loss: 5248312.5000\n",
      "Epoch [8417/50000], Train Loss: 8092047.5000, Val Loss: 5248395.0000\n",
      "Epoch [8418/50000], Train Loss: 8092037.0000, Val Loss: 5248478.0000\n",
      "Epoch [8419/50000], Train Loss: 8092027.0000, Val Loss: 5248560.5000\n",
      "Epoch [8420/50000], Train Loss: 8092015.5000, Val Loss: 5248642.5000\n",
      "Epoch [8421/50000], Train Loss: 8092005.0000, Val Loss: 5248725.5000\n",
      "Epoch [8422/50000], Train Loss: 8091994.5000, Val Loss: 5248807.5000\n",
      "Epoch [8423/50000], Train Loss: 8091983.5000, Val Loss: 5248889.5000\n",
      "Epoch [8424/50000], Train Loss: 8091972.5000, Val Loss: 5248972.5000\n",
      "Epoch [8425/50000], Train Loss: 8091962.0000, Val Loss: 5249055.0000\n",
      "Epoch [8426/50000], Train Loss: 8091951.0000, Val Loss: 5249136.5000\n",
      "Epoch [8427/50000], Train Loss: 8091941.0000, Val Loss: 5249218.5000\n",
      "Epoch [8428/50000], Train Loss: 8091930.5000, Val Loss: 5249301.5000\n",
      "Epoch [8429/50000], Train Loss: 8091919.0000, Val Loss: 5249382.5000\n",
      "Epoch [8430/50000], Train Loss: 8091909.0000, Val Loss: 5249464.0000\n",
      "Epoch [8431/50000], Train Loss: 8091898.5000, Val Loss: 5249546.0000\n",
      "Epoch [8432/50000], Train Loss: 8091887.5000, Val Loss: 5249626.5000\n",
      "Epoch [8433/50000], Train Loss: 8091878.0000, Val Loss: 5249709.0000\n",
      "Epoch [8434/50000], Train Loss: 8091867.5000, Val Loss: 5249790.5000\n",
      "Epoch [8435/50000], Train Loss: 8091857.0000, Val Loss: 5249872.0000\n",
      "Epoch [8436/50000], Train Loss: 8091846.5000, Val Loss: 5249953.0000\n",
      "Epoch [8437/50000], Train Loss: 8091837.0000, Val Loss: 5250033.5000\n",
      "Epoch [8438/50000], Train Loss: 8091826.5000, Val Loss: 5250115.0000\n",
      "Epoch [8439/50000], Train Loss: 8091816.5000, Val Loss: 5250196.5000\n",
      "Epoch [8440/50000], Train Loss: 8091806.0000, Val Loss: 5250277.0000\n",
      "Epoch [8441/50000], Train Loss: 8091795.5000, Val Loss: 5250357.5000\n",
      "Epoch [8442/50000], Train Loss: 8091785.5000, Val Loss: 5250438.5000\n",
      "Epoch [8443/50000], Train Loss: 8091775.0000, Val Loss: 5250519.5000\n",
      "Epoch [8444/50000], Train Loss: 8091765.0000, Val Loss: 5250599.5000\n",
      "Epoch [8445/50000], Train Loss: 8091756.0000, Val Loss: 5250680.0000\n",
      "Epoch [8446/50000], Train Loss: 8091745.0000, Val Loss: 5250760.0000\n",
      "Epoch [8447/50000], Train Loss: 8091735.5000, Val Loss: 5250840.5000\n",
      "Epoch [8448/50000], Train Loss: 8091726.0000, Val Loss: 5250920.5000\n",
      "Epoch [8449/50000], Train Loss: 8091716.0000, Val Loss: 5251000.5000\n",
      "Epoch [8450/50000], Train Loss: 8091705.5000, Val Loss: 5251081.0000\n",
      "Epoch [8451/50000], Train Loss: 8091696.5000, Val Loss: 5251160.5000\n",
      "Epoch [8452/50000], Train Loss: 8091686.0000, Val Loss: 5251240.0000\n",
      "Epoch [8453/50000], Train Loss: 8091676.5000, Val Loss: 5251320.0000\n",
      "Epoch [8454/50000], Train Loss: 8091666.5000, Val Loss: 5251400.0000\n",
      "Epoch [8455/50000], Train Loss: 8091657.0000, Val Loss: 5251480.0000\n",
      "Epoch [8456/50000], Train Loss: 8091647.5000, Val Loss: 5251559.0000\n",
      "Epoch [8457/50000], Train Loss: 8091638.0000, Val Loss: 5251638.0000\n",
      "Epoch [8458/50000], Train Loss: 8091628.0000, Val Loss: 5251717.5000\n",
      "Epoch [8459/50000], Train Loss: 8091618.0000, Val Loss: 5251797.0000\n",
      "Epoch [8460/50000], Train Loss: 8091608.5000, Val Loss: 5251876.5000\n",
      "Epoch [8461/50000], Train Loss: 8091598.5000, Val Loss: 5251955.0000\n",
      "Epoch [8462/50000], Train Loss: 8091589.5000, Val Loss: 5252034.5000\n",
      "Epoch [8463/50000], Train Loss: 8091580.5000, Val Loss: 5252113.0000\n",
      "Epoch [8464/50000], Train Loss: 8091570.5000, Val Loss: 5252192.0000\n",
      "Epoch [8465/50000], Train Loss: 8091561.0000, Val Loss: 5252271.0000\n",
      "Epoch [8466/50000], Train Loss: 8091551.0000, Val Loss: 5252348.5000\n",
      "Epoch [8467/50000], Train Loss: 8091542.5000, Val Loss: 5252427.0000\n",
      "Epoch [8468/50000], Train Loss: 8091533.5000, Val Loss: 5252506.0000\n",
      "Epoch [8469/50000], Train Loss: 8091524.0000, Val Loss: 5252584.0000\n",
      "Epoch [8470/50000], Train Loss: 8091515.5000, Val Loss: 5252663.0000\n",
      "Epoch [8471/50000], Train Loss: 8091505.5000, Val Loss: 5252741.5000\n",
      "Epoch [8472/50000], Train Loss: 8091496.0000, Val Loss: 5252819.0000\n",
      "Epoch [8473/50000], Train Loss: 8091486.5000, Val Loss: 5252897.0000\n",
      "Epoch [8474/50000], Train Loss: 8091477.5000, Val Loss: 5252975.0000\n",
      "Epoch [8475/50000], Train Loss: 8091469.5000, Val Loss: 5253053.0000\n",
      "Epoch [8476/50000], Train Loss: 8091459.5000, Val Loss: 5253131.0000\n",
      "Epoch [8477/50000], Train Loss: 8091450.0000, Val Loss: 5253209.5000\n",
      "Epoch [8478/50000], Train Loss: 8091441.0000, Val Loss: 5253286.5000\n",
      "Epoch [8479/50000], Train Loss: 8091432.0000, Val Loss: 5253365.0000\n",
      "Epoch [8480/50000], Train Loss: 8091423.5000, Val Loss: 5253441.5000\n",
      "Epoch [8481/50000], Train Loss: 8091414.5000, Val Loss: 5253519.0000\n",
      "Epoch [8482/50000], Train Loss: 8091405.0000, Val Loss: 5253595.5000\n",
      "Epoch [8483/50000], Train Loss: 8091396.0000, Val Loss: 5253673.5000\n",
      "Epoch [8484/50000], Train Loss: 8091387.5000, Val Loss: 5253750.5000\n",
      "Epoch [8485/50000], Train Loss: 8091378.5000, Val Loss: 5253826.5000\n",
      "Epoch [8486/50000], Train Loss: 8091369.5000, Val Loss: 5253904.5000\n",
      "Epoch [8487/50000], Train Loss: 8091361.0000, Val Loss: 5253981.5000\n",
      "Epoch [8488/50000], Train Loss: 8091352.0000, Val Loss: 5254058.0000\n",
      "Epoch [8489/50000], Train Loss: 8091343.5000, Val Loss: 5254134.0000\n",
      "Epoch [8490/50000], Train Loss: 8091334.5000, Val Loss: 5254211.0000\n",
      "Epoch [8491/50000], Train Loss: 8091326.0000, Val Loss: 5254288.0000\n",
      "Epoch [8492/50000], Train Loss: 8091317.5000, Val Loss: 5254363.5000\n",
      "Epoch [8493/50000], Train Loss: 8091309.0000, Val Loss: 5254440.5000\n",
      "Epoch [8494/50000], Train Loss: 8091299.5000, Val Loss: 5254517.5000\n",
      "Epoch [8495/50000], Train Loss: 8091291.5000, Val Loss: 5254593.0000\n",
      "Epoch [8496/50000], Train Loss: 8091283.0000, Val Loss: 5254669.5000\n",
      "Epoch [8497/50000], Train Loss: 8091274.5000, Val Loss: 5254745.0000\n",
      "Epoch [8498/50000], Train Loss: 8091265.5000, Val Loss: 5254821.0000\n",
      "Epoch [8499/50000], Train Loss: 8091258.0000, Val Loss: 5254897.0000\n",
      "Epoch [8500/50000], Train Loss: 8091249.0000, Val Loss: 5254973.0000\n",
      "Epoch [8501/50000], Train Loss: 8091240.0000, Val Loss: 5255047.5000\n",
      "Epoch [8502/50000], Train Loss: 8091233.0000, Val Loss: 5255123.0000\n",
      "Epoch [8503/50000], Train Loss: 8091223.5000, Val Loss: 5255199.0000\n",
      "Epoch [8504/50000], Train Loss: 8091215.5000, Val Loss: 5255274.5000\n",
      "Epoch [8505/50000], Train Loss: 8091207.5000, Val Loss: 5255350.0000\n",
      "Epoch [8506/50000], Train Loss: 8091199.0000, Val Loss: 5255425.5000\n",
      "Epoch [8507/50000], Train Loss: 8091191.0000, Val Loss: 5255500.5000\n",
      "Epoch [8508/50000], Train Loss: 8091182.5000, Val Loss: 5255575.5000\n",
      "Epoch [8509/50000], Train Loss: 8091174.0000, Val Loss: 5255650.5000\n",
      "Epoch [8510/50000], Train Loss: 8091166.0000, Val Loss: 5255726.0000\n",
      "Epoch [8511/50000], Train Loss: 8091157.5000, Val Loss: 5255801.0000\n",
      "Epoch [8512/50000], Train Loss: 8091149.5000, Val Loss: 5255874.5000\n",
      "Epoch [8513/50000], Train Loss: 8091141.0000, Val Loss: 5255950.5000\n",
      "Epoch [8514/50000], Train Loss: 8091134.0000, Val Loss: 5256024.5000\n",
      "Epoch [8515/50000], Train Loss: 8091125.5000, Val Loss: 5256098.5000\n",
      "Epoch [8516/50000], Train Loss: 8091117.5000, Val Loss: 5256173.0000\n",
      "Epoch [8517/50000], Train Loss: 8091109.0000, Val Loss: 5256248.0000\n",
      "Epoch [8518/50000], Train Loss: 8091100.5000, Val Loss: 5256321.5000\n",
      "Epoch [8519/50000], Train Loss: 8091093.5000, Val Loss: 5256395.5000\n",
      "Epoch [8520/50000], Train Loss: 8091085.5000, Val Loss: 5256470.5000\n",
      "Epoch [8521/50000], Train Loss: 8091077.0000, Val Loss: 5256543.5000\n",
      "Epoch [8522/50000], Train Loss: 8091070.0000, Val Loss: 5256617.5000\n",
      "Epoch [8523/50000], Train Loss: 8091062.5000, Val Loss: 5256691.5000\n",
      "Epoch [8524/50000], Train Loss: 8091054.0000, Val Loss: 5256765.5000\n",
      "Epoch [8525/50000], Train Loss: 8091046.0000, Val Loss: 5256839.5000\n",
      "Epoch [8526/50000], Train Loss: 8091038.0000, Val Loss: 5256912.5000\n",
      "Epoch [8527/50000], Train Loss: 8091030.5000, Val Loss: 5256986.5000\n",
      "Epoch [8528/50000], Train Loss: 8091023.0000, Val Loss: 5257059.0000\n",
      "Epoch [8529/50000], Train Loss: 8091014.5000, Val Loss: 5257133.0000\n",
      "Epoch [8530/50000], Train Loss: 8091007.5000, Val Loss: 5257206.0000\n",
      "Epoch [8531/50000], Train Loss: 8091000.5000, Val Loss: 5257279.0000\n",
      "Epoch [8532/50000], Train Loss: 8090991.5000, Val Loss: 5257351.5000\n",
      "Epoch [8533/50000], Train Loss: 8090985.0000, Val Loss: 5257425.0000\n",
      "Epoch [8534/50000], Train Loss: 8090976.5000, Val Loss: 5257497.5000\n",
      "Epoch [8535/50000], Train Loss: 8090969.5000, Val Loss: 5257570.0000\n",
      "Epoch [8536/50000], Train Loss: 8090962.5000, Val Loss: 5257643.0000\n",
      "Epoch [8537/50000], Train Loss: 8090954.5000, Val Loss: 5257715.5000\n",
      "Epoch [8538/50000], Train Loss: 8090947.0000, Val Loss: 5257789.0000\n",
      "Epoch [8539/50000], Train Loss: 8090939.0000, Val Loss: 5257861.5000\n",
      "Epoch [8540/50000], Train Loss: 8090932.0000, Val Loss: 5257933.5000\n",
      "Epoch [8541/50000], Train Loss: 8090925.0000, Val Loss: 5258005.0000\n",
      "Epoch [8542/50000], Train Loss: 8090917.0000, Val Loss: 5258077.5000\n",
      "Epoch [8543/50000], Train Loss: 8090911.0000, Val Loss: 5258149.5000\n",
      "Epoch [8544/50000], Train Loss: 8090902.0000, Val Loss: 5258221.5000\n",
      "Epoch [8545/50000], Train Loss: 8090895.0000, Val Loss: 5258293.5000\n",
      "Epoch [8546/50000], Train Loss: 8090888.5000, Val Loss: 5258366.0000\n",
      "Epoch [8547/50000], Train Loss: 8090881.5000, Val Loss: 5258437.5000\n",
      "Epoch [8548/50000], Train Loss: 8090873.5000, Val Loss: 5258509.0000\n",
      "Epoch [8549/50000], Train Loss: 8090866.0000, Val Loss: 5258581.0000\n",
      "Epoch [8550/50000], Train Loss: 8090858.5000, Val Loss: 5258651.5000\n",
      "Epoch [8551/50000], Train Loss: 8090852.0000, Val Loss: 5258724.5000\n",
      "Epoch [8552/50000], Train Loss: 8090845.0000, Val Loss: 5258794.5000\n",
      "Epoch [8553/50000], Train Loss: 8090837.5000, Val Loss: 5258866.0000\n",
      "Epoch [8554/50000], Train Loss: 8090830.5000, Val Loss: 5258937.5000\n",
      "Epoch [8555/50000], Train Loss: 8090823.5000, Val Loss: 5259008.5000\n",
      "Epoch [8556/50000], Train Loss: 8090815.5000, Val Loss: 5259080.0000\n",
      "Epoch [8557/50000], Train Loss: 8090809.5000, Val Loss: 5259150.0000\n",
      "Epoch [8558/50000], Train Loss: 8090802.0000, Val Loss: 5259221.0000\n",
      "Epoch [8559/50000], Train Loss: 8090796.0000, Val Loss: 5259291.5000\n",
      "Epoch [8560/50000], Train Loss: 8090788.0000, Val Loss: 5259362.0000\n",
      "Epoch [8561/50000], Train Loss: 8090780.5000, Val Loss: 5259433.0000\n",
      "Epoch [8562/50000], Train Loss: 8090775.0000, Val Loss: 5259503.5000\n",
      "Epoch [8563/50000], Train Loss: 8090766.5000, Val Loss: 5259574.5000\n",
      "Epoch [8564/50000], Train Loss: 8090760.5000, Val Loss: 5259643.5000\n",
      "Epoch [8565/50000], Train Loss: 8090753.5000, Val Loss: 5259714.0000\n",
      "Epoch [8566/50000], Train Loss: 8090747.0000, Val Loss: 5259784.5000\n",
      "Epoch [8567/50000], Train Loss: 8090739.5000, Val Loss: 5259855.0000\n",
      "Epoch [8568/50000], Train Loss: 8090733.0000, Val Loss: 5259924.5000\n",
      "Epoch [8569/50000], Train Loss: 8090726.0000, Val Loss: 5259994.5000\n",
      "Epoch [8570/50000], Train Loss: 8090720.0000, Val Loss: 5260065.0000\n",
      "Epoch [8571/50000], Train Loss: 8090712.5000, Val Loss: 5260135.0000\n",
      "Epoch [8572/50000], Train Loss: 8090706.5000, Val Loss: 5260203.5000\n",
      "Epoch [8573/50000], Train Loss: 8090700.0000, Val Loss: 5260274.5000\n",
      "Epoch [8574/50000], Train Loss: 8090692.5000, Val Loss: 5260343.0000\n",
      "Epoch [8575/50000], Train Loss: 8090687.0000, Val Loss: 5260412.5000\n",
      "Epoch [8576/50000], Train Loss: 8090680.0000, Val Loss: 5260481.5000\n",
      "Epoch [8577/50000], Train Loss: 8090673.0000, Val Loss: 5260550.5000\n",
      "Epoch [8578/50000], Train Loss: 8090666.5000, Val Loss: 5260620.5000\n",
      "Epoch [8579/50000], Train Loss: 8090660.0000, Val Loss: 5260689.0000\n",
      "Epoch [8580/50000], Train Loss: 8090654.0000, Val Loss: 5260758.0000\n",
      "Epoch [8581/50000], Train Loss: 8090646.5000, Val Loss: 5260827.5000\n",
      "Epoch [8582/50000], Train Loss: 8090640.5000, Val Loss: 5260896.0000\n",
      "Epoch [8583/50000], Train Loss: 8090633.5000, Val Loss: 5260965.0000\n",
      "Epoch [8584/50000], Train Loss: 8090627.5000, Val Loss: 5261033.5000\n",
      "Epoch [8585/50000], Train Loss: 8090621.0000, Val Loss: 5261102.0000\n",
      "Epoch [8586/50000], Train Loss: 8090614.5000, Val Loss: 5261171.0000\n",
      "Epoch [8587/50000], Train Loss: 8090608.5000, Val Loss: 5261239.0000\n",
      "Epoch [8588/50000], Train Loss: 8090602.0000, Val Loss: 5261306.5000\n",
      "Epoch [8589/50000], Train Loss: 8090595.5000, Val Loss: 5261376.0000\n",
      "Epoch [8590/50000], Train Loss: 8090590.0000, Val Loss: 5261443.0000\n",
      "Epoch [8591/50000], Train Loss: 8090583.5000, Val Loss: 5261511.5000\n",
      "Epoch [8592/50000], Train Loss: 8090576.5000, Val Loss: 5261579.5000\n",
      "Epoch [8593/50000], Train Loss: 8090570.5000, Val Loss: 5261647.5000\n",
      "Epoch [8594/50000], Train Loss: 8090564.5000, Val Loss: 5261715.0000\n",
      "Epoch [8595/50000], Train Loss: 8090557.0000, Val Loss: 5261783.0000\n",
      "Epoch [8596/50000], Train Loss: 8090551.5000, Val Loss: 5261850.5000\n",
      "Epoch [8597/50000], Train Loss: 8090545.5000, Val Loss: 5261919.0000\n",
      "Epoch [8598/50000], Train Loss: 8090540.0000, Val Loss: 5261986.0000\n",
      "Epoch [8599/50000], Train Loss: 8090533.0000, Val Loss: 5262053.0000\n",
      "Epoch [8600/50000], Train Loss: 8090527.0000, Val Loss: 5262120.5000\n",
      "Epoch [8601/50000], Train Loss: 8090521.5000, Val Loss: 5262187.0000\n",
      "Epoch [8602/50000], Train Loss: 8090515.0000, Val Loss: 5262254.5000\n",
      "Epoch [8603/50000], Train Loss: 8090509.5000, Val Loss: 5262321.5000\n",
      "Epoch [8604/50000], Train Loss: 8090502.5000, Val Loss: 5262389.5000\n",
      "Epoch [8605/50000], Train Loss: 8090497.0000, Val Loss: 5262455.0000\n",
      "Epoch [8606/50000], Train Loss: 8090491.0000, Val Loss: 5262522.5000\n",
      "Epoch [8607/50000], Train Loss: 8090485.5000, Val Loss: 5262589.0000\n",
      "Epoch [8608/50000], Train Loss: 8090479.5000, Val Loss: 5262655.5000\n",
      "Epoch [8609/50000], Train Loss: 8090473.0000, Val Loss: 5262722.0000\n",
      "Epoch [8610/50000], Train Loss: 8090467.0000, Val Loss: 5262788.5000\n",
      "Epoch [8611/50000], Train Loss: 8090462.0000, Val Loss: 5262855.5000\n",
      "Epoch [8612/50000], Train Loss: 8090455.5000, Val Loss: 5262921.0000\n",
      "Epoch [8613/50000], Train Loss: 8090449.5000, Val Loss: 5262987.5000\n",
      "Epoch [8614/50000], Train Loss: 8090444.5000, Val Loss: 5263054.0000\n",
      "Epoch [8615/50000], Train Loss: 8090438.0000, Val Loss: 5263119.5000\n",
      "Epoch [8616/50000], Train Loss: 8090432.0000, Val Loss: 5263185.0000\n",
      "Epoch [8617/50000], Train Loss: 8090427.0000, Val Loss: 5263251.5000\n",
      "Epoch [8618/50000], Train Loss: 8090421.0000, Val Loss: 5263318.0000\n",
      "Epoch [8619/50000], Train Loss: 8090415.0000, Val Loss: 5263383.0000\n",
      "Epoch [8620/50000], Train Loss: 8090409.5000, Val Loss: 5263448.5000\n",
      "Epoch [8621/50000], Train Loss: 8090404.0000, Val Loss: 5263514.0000\n",
      "Epoch [8622/50000], Train Loss: 8090398.0000, Val Loss: 5263580.5000\n",
      "Epoch [8623/50000], Train Loss: 8090392.5000, Val Loss: 5263645.0000\n",
      "Epoch [8624/50000], Train Loss: 8090386.5000, Val Loss: 5263710.0000\n",
      "Epoch [8625/50000], Train Loss: 8090381.0000, Val Loss: 5263775.5000\n",
      "Epoch [8626/50000], Train Loss: 8090375.5000, Val Loss: 5263841.0000\n",
      "Epoch [8627/50000], Train Loss: 8090370.0000, Val Loss: 5263906.0000\n",
      "Epoch [8628/50000], Train Loss: 8090364.5000, Val Loss: 5263970.0000\n",
      "Epoch [8629/50000], Train Loss: 8090358.5000, Val Loss: 5264035.0000\n",
      "Epoch [8630/50000], Train Loss: 8090353.5000, Val Loss: 5264100.5000\n",
      "Epoch [8631/50000], Train Loss: 8090348.0000, Val Loss: 5264165.5000\n",
      "Epoch [8632/50000], Train Loss: 8090342.0000, Val Loss: 5264230.0000\n",
      "Epoch [8633/50000], Train Loss: 8090336.5000, Val Loss: 5264294.5000\n",
      "Epoch [8634/50000], Train Loss: 8090332.0000, Val Loss: 5264358.5000\n",
      "Epoch [8635/50000], Train Loss: 8090326.5000, Val Loss: 5264424.0000\n",
      "Epoch [8636/50000], Train Loss: 8090320.5000, Val Loss: 5264488.0000\n",
      "Epoch [8637/50000], Train Loss: 8090314.5000, Val Loss: 5264551.5000\n",
      "Epoch [8638/50000], Train Loss: 8090309.5000, Val Loss: 5264615.5000\n",
      "Epoch [8639/50000], Train Loss: 8090304.0000, Val Loss: 5264680.0000\n",
      "Epoch [8640/50000], Train Loss: 8090299.0000, Val Loss: 5264743.5000\n",
      "Epoch [8641/50000], Train Loss: 8090293.5000, Val Loss: 5264807.5000\n",
      "Epoch [8642/50000], Train Loss: 8090288.5000, Val Loss: 5264871.5000\n",
      "Epoch [8643/50000], Train Loss: 8090283.0000, Val Loss: 5264935.5000\n",
      "Epoch [8644/50000], Train Loss: 8090278.0000, Val Loss: 5264998.5000\n",
      "Epoch [8645/50000], Train Loss: 8090272.5000, Val Loss: 5265063.0000\n",
      "Epoch [8646/50000], Train Loss: 8090267.5000, Val Loss: 5265126.5000\n",
      "Epoch [8647/50000], Train Loss: 8090262.5000, Val Loss: 5265189.5000\n",
      "Epoch [8648/50000], Train Loss: 8090256.5000, Val Loss: 5265252.5000\n",
      "Epoch [8649/50000], Train Loss: 8090252.0000, Val Loss: 5265316.5000\n",
      "Epoch [8650/50000], Train Loss: 8090246.5000, Val Loss: 5265379.0000\n",
      "Epoch [8651/50000], Train Loss: 8090241.5000, Val Loss: 5265442.0000\n",
      "Epoch [8652/50000], Train Loss: 8090236.5000, Val Loss: 5265504.5000\n",
      "Epoch [8653/50000], Train Loss: 8090231.0000, Val Loss: 5265568.0000\n",
      "Epoch [8654/50000], Train Loss: 8090226.0000, Val Loss: 5265631.0000\n",
      "Epoch [8655/50000], Train Loss: 8090221.0000, Val Loss: 5265693.5000\n",
      "Epoch [8656/50000], Train Loss: 8090215.5000, Val Loss: 5265755.5000\n",
      "Epoch [8657/50000], Train Loss: 8090211.0000, Val Loss: 5265818.5000\n",
      "Epoch [8658/50000], Train Loss: 8090205.0000, Val Loss: 5265881.5000\n",
      "Epoch [8659/50000], Train Loss: 8090200.0000, Val Loss: 5265943.5000\n",
      "Epoch [8660/50000], Train Loss: 8090195.5000, Val Loss: 5266006.0000\n",
      "Epoch [8661/50000], Train Loss: 8090191.0000, Val Loss: 5266068.5000\n",
      "Epoch [8662/50000], Train Loss: 8090186.0000, Val Loss: 5266130.5000\n",
      "Epoch [8663/50000], Train Loss: 8090181.5000, Val Loss: 5266192.5000\n",
      "Epoch [8664/50000], Train Loss: 8090176.5000, Val Loss: 5266255.0000\n",
      "Epoch [8665/50000], Train Loss: 8090171.0000, Val Loss: 5266317.0000\n",
      "Epoch [8666/50000], Train Loss: 8090166.0000, Val Loss: 5266378.0000\n",
      "Epoch [8667/50000], Train Loss: 8090161.5000, Val Loss: 5266440.0000\n",
      "Epoch [8668/50000], Train Loss: 8090157.0000, Val Loss: 5266502.0000\n",
      "Epoch [8669/50000], Train Loss: 8090152.0000, Val Loss: 5266563.5000\n",
      "Epoch [8670/50000], Train Loss: 8090147.5000, Val Loss: 5266625.0000\n",
      "Epoch [8671/50000], Train Loss: 8090142.0000, Val Loss: 5266686.5000\n",
      "Epoch [8672/50000], Train Loss: 8090137.5000, Val Loss: 5266747.0000\n",
      "Epoch [8673/50000], Train Loss: 8090133.5000, Val Loss: 5266808.0000\n",
      "Epoch [8674/50000], Train Loss: 8090127.5000, Val Loss: 5266870.0000\n",
      "Epoch [8675/50000], Train Loss: 8090123.0000, Val Loss: 5266930.5000\n",
      "Epoch [8676/50000], Train Loss: 8090118.5000, Val Loss: 5266992.0000\n",
      "Epoch [8677/50000], Train Loss: 8090114.0000, Val Loss: 5267052.5000\n",
      "Epoch [8678/50000], Train Loss: 8090110.0000, Val Loss: 5267114.0000\n",
      "Epoch [8679/50000], Train Loss: 8090103.5000, Val Loss: 5267175.0000\n",
      "Epoch [8680/50000], Train Loss: 8090099.5000, Val Loss: 5267235.0000\n",
      "Epoch [8681/50000], Train Loss: 8090095.0000, Val Loss: 5267295.5000\n",
      "Epoch [8682/50000], Train Loss: 8090091.0000, Val Loss: 5267356.5000\n",
      "Epoch [8683/50000], Train Loss: 8090086.5000, Val Loss: 5267416.5000\n",
      "Epoch [8684/50000], Train Loss: 8090081.5000, Val Loss: 5267477.0000\n",
      "Epoch [8685/50000], Train Loss: 8090077.0000, Val Loss: 5267536.5000\n",
      "Epoch [8686/50000], Train Loss: 8090072.5000, Val Loss: 5267598.0000\n",
      "Epoch [8687/50000], Train Loss: 8090067.0000, Val Loss: 5267657.5000\n",
      "Epoch [8688/50000], Train Loss: 8090063.5000, Val Loss: 5267717.5000\n",
      "Epoch [8689/50000], Train Loss: 8090058.5000, Val Loss: 5267776.5000\n",
      "Epoch [8690/50000], Train Loss: 8090054.0000, Val Loss: 5267837.5000\n",
      "Epoch [8691/50000], Train Loss: 8090049.5000, Val Loss: 5267896.5000\n",
      "Epoch [8692/50000], Train Loss: 8090046.0000, Val Loss: 5267957.0000\n",
      "Epoch [8693/50000], Train Loss: 8090041.5000, Val Loss: 5268016.5000\n",
      "Epoch [8694/50000], Train Loss: 8090036.0000, Val Loss: 5268075.5000\n",
      "Epoch [8695/50000], Train Loss: 8090033.0000, Val Loss: 5268135.5000\n",
      "Epoch [8696/50000], Train Loss: 8090027.0000, Val Loss: 5268194.5000\n",
      "Epoch [8697/50000], Train Loss: 8090024.5000, Val Loss: 5268254.0000\n",
      "Epoch [8698/50000], Train Loss: 8090019.5000, Val Loss: 5268313.5000\n",
      "Epoch [8699/50000], Train Loss: 8090014.5000, Val Loss: 5268373.0000\n",
      "Epoch [8700/50000], Train Loss: 8090010.0000, Val Loss: 5268432.0000\n",
      "Epoch [8701/50000], Train Loss: 8090006.5000, Val Loss: 5268490.5000\n",
      "Epoch [8702/50000], Train Loss: 8090002.0000, Val Loss: 5268549.5000\n",
      "Epoch [8703/50000], Train Loss: 8089997.0000, Val Loss: 5268608.0000\n",
      "Epoch [8704/50000], Train Loss: 8089993.0000, Val Loss: 5268666.5000\n",
      "Epoch [8705/50000], Train Loss: 8089989.0000, Val Loss: 5268725.5000\n",
      "Epoch [8706/50000], Train Loss: 8089984.5000, Val Loss: 5268784.5000\n",
      "Epoch [8707/50000], Train Loss: 8089980.0000, Val Loss: 5268842.5000\n",
      "Epoch [8708/50000], Train Loss: 8089976.0000, Val Loss: 5268901.0000\n",
      "Epoch [8709/50000], Train Loss: 8089972.0000, Val Loss: 5268959.5000\n",
      "Epoch [8710/50000], Train Loss: 8089967.5000, Val Loss: 5269017.0000\n",
      "Epoch [8711/50000], Train Loss: 8089964.5000, Val Loss: 5269075.5000\n",
      "Epoch [8712/50000], Train Loss: 8089960.0000, Val Loss: 5269134.0000\n",
      "Epoch [8713/50000], Train Loss: 8089955.0000, Val Loss: 5269191.5000\n",
      "Epoch [8714/50000], Train Loss: 8089952.0000, Val Loss: 5269249.0000\n",
      "Epoch [8715/50000], Train Loss: 8089947.0000, Val Loss: 5269308.5000\n",
      "Epoch [8716/50000], Train Loss: 8089943.0000, Val Loss: 5269365.5000\n",
      "Epoch [8717/50000], Train Loss: 8089939.0000, Val Loss: 5269423.0000\n",
      "Epoch [8718/50000], Train Loss: 8089934.5000, Val Loss: 5269481.0000\n",
      "Epoch [8719/50000], Train Loss: 8089931.0000, Val Loss: 5269537.5000\n",
      "Epoch [8720/50000], Train Loss: 8089927.5000, Val Loss: 5269596.5000\n",
      "Epoch [8721/50000], Train Loss: 8089922.0000, Val Loss: 5269653.0000\n",
      "Epoch [8722/50000], Train Loss: 8089919.0000, Val Loss: 5269710.5000\n",
      "Epoch [8723/50000], Train Loss: 8089915.0000, Val Loss: 5269767.5000\n",
      "Epoch [8724/50000], Train Loss: 8089911.0000, Val Loss: 5269825.0000\n",
      "Epoch [8725/50000], Train Loss: 8089907.0000, Val Loss: 5269882.0000\n",
      "Epoch [8726/50000], Train Loss: 8089903.0000, Val Loss: 5269938.5000\n",
      "Epoch [8727/50000], Train Loss: 8089900.0000, Val Loss: 5269996.5000\n",
      "Epoch [8728/50000], Train Loss: 8089895.5000, Val Loss: 5270052.5000\n",
      "Epoch [8729/50000], Train Loss: 8089891.0000, Val Loss: 5270109.5000\n",
      "Epoch [8730/50000], Train Loss: 8089887.0000, Val Loss: 5270165.5000\n",
      "Epoch [8731/50000], Train Loss: 8089883.0000, Val Loss: 5270222.5000\n",
      "Epoch [8732/50000], Train Loss: 8089879.5000, Val Loss: 5270278.5000\n",
      "Epoch [8733/50000], Train Loss: 8089875.5000, Val Loss: 5270335.0000\n",
      "Epoch [8734/50000], Train Loss: 8089871.5000, Val Loss: 5270391.5000\n",
      "Epoch [8735/50000], Train Loss: 8089868.5000, Val Loss: 5270447.0000\n",
      "Epoch [8736/50000], Train Loss: 8089864.5000, Val Loss: 5270503.5000\n",
      "Epoch [8737/50000], Train Loss: 8089860.5000, Val Loss: 5270560.0000\n",
      "Epoch [8738/50000], Train Loss: 8089856.5000, Val Loss: 5270616.0000\n",
      "Epoch [8739/50000], Train Loss: 8089853.0000, Val Loss: 5270671.5000\n",
      "Epoch [8740/50000], Train Loss: 8089849.5000, Val Loss: 5270728.0000\n",
      "Epoch [8741/50000], Train Loss: 8089845.0000, Val Loss: 5270784.0000\n",
      "Epoch [8742/50000], Train Loss: 8089842.0000, Val Loss: 5270839.5000\n",
      "Epoch [8743/50000], Train Loss: 8089838.0000, Val Loss: 5270894.0000\n",
      "Epoch [8744/50000], Train Loss: 8089834.5000, Val Loss: 5270950.5000\n",
      "Epoch [8745/50000], Train Loss: 8089830.0000, Val Loss: 5271006.0000\n",
      "Epoch [8746/50000], Train Loss: 8089827.0000, Val Loss: 5271060.5000\n",
      "Epoch [8747/50000], Train Loss: 8089824.0000, Val Loss: 5271117.0000\n",
      "Epoch [8748/50000], Train Loss: 8089820.0000, Val Loss: 5271171.0000\n",
      "Epoch [8749/50000], Train Loss: 8089816.0000, Val Loss: 5271226.5000\n",
      "Epoch [8750/50000], Train Loss: 8089812.0000, Val Loss: 5271281.0000\n",
      "Epoch [8751/50000], Train Loss: 8089809.0000, Val Loss: 5271337.0000\n",
      "Epoch [8752/50000], Train Loss: 8089805.5000, Val Loss: 5271391.0000\n",
      "Epoch [8753/50000], Train Loss: 8089802.0000, Val Loss: 5271446.5000\n",
      "Epoch [8754/50000], Train Loss: 8089798.0000, Val Loss: 5271501.0000\n",
      "Epoch [8755/50000], Train Loss: 8089794.0000, Val Loss: 5271556.5000\n",
      "Epoch [8756/50000], Train Loss: 8089790.5000, Val Loss: 5271610.5000\n",
      "Epoch [8757/50000], Train Loss: 8089787.5000, Val Loss: 5271664.5000\n",
      "Epoch [8758/50000], Train Loss: 8089784.0000, Val Loss: 5271719.5000\n",
      "Epoch [8759/50000], Train Loss: 8089781.0000, Val Loss: 5271774.0000\n",
      "Epoch [8760/50000], Train Loss: 8089777.5000, Val Loss: 5271827.5000\n",
      "Epoch [8761/50000], Train Loss: 8089774.0000, Val Loss: 5271882.0000\n",
      "Epoch [8762/50000], Train Loss: 8089769.5000, Val Loss: 5271936.5000\n",
      "Epoch [8763/50000], Train Loss: 8089766.0000, Val Loss: 5271990.5000\n",
      "Epoch [8764/50000], Train Loss: 8089763.0000, Val Loss: 5272045.0000\n",
      "Epoch [8765/50000], Train Loss: 8089759.5000, Val Loss: 5272098.0000\n",
      "Epoch [8766/50000], Train Loss: 8089756.5000, Val Loss: 5272152.5000\n",
      "Epoch [8767/50000], Train Loss: 8089752.5000, Val Loss: 5272206.5000\n",
      "Epoch [8768/50000], Train Loss: 8089749.5000, Val Loss: 5272259.5000\n",
      "Epoch [8769/50000], Train Loss: 8089746.0000, Val Loss: 5272313.5000\n",
      "Epoch [8770/50000], Train Loss: 8089743.0000, Val Loss: 5272367.0000\n",
      "Epoch [8771/50000], Train Loss: 8089739.0000, Val Loss: 5272420.5000\n",
      "Epoch [8772/50000], Train Loss: 8089736.0000, Val Loss: 5272473.5000\n",
      "Epoch [8773/50000], Train Loss: 8089733.0000, Val Loss: 5272526.5000\n",
      "Epoch [8774/50000], Train Loss: 8089729.0000, Val Loss: 5272579.0000\n",
      "Epoch [8775/50000], Train Loss: 8089726.5000, Val Loss: 5272633.5000\n",
      "Epoch [8776/50000], Train Loss: 8089722.5000, Val Loss: 5272686.5000\n",
      "Epoch [8777/50000], Train Loss: 8089720.0000, Val Loss: 5272739.0000\n",
      "Epoch [8778/50000], Train Loss: 8089715.5000, Val Loss: 5272792.0000\n",
      "Epoch [8779/50000], Train Loss: 8089713.0000, Val Loss: 5272845.0000\n",
      "Epoch [8780/50000], Train Loss: 8089710.0000, Val Loss: 5272898.0000\n",
      "Epoch [8781/50000], Train Loss: 8089706.0000, Val Loss: 5272950.5000\n",
      "Epoch [8782/50000], Train Loss: 8089703.5000, Val Loss: 5273003.0000\n",
      "Epoch [8783/50000], Train Loss: 8089700.5000, Val Loss: 5273055.5000\n",
      "Epoch [8784/50000], Train Loss: 8089697.0000, Val Loss: 5273108.5000\n",
      "Epoch [8785/50000], Train Loss: 8089694.0000, Val Loss: 5273160.0000\n",
      "Epoch [8786/50000], Train Loss: 8089690.5000, Val Loss: 5273213.0000\n",
      "Epoch [8787/50000], Train Loss: 8089687.0000, Val Loss: 5273264.0000\n",
      "Epoch [8788/50000], Train Loss: 8089683.5000, Val Loss: 5273317.0000\n",
      "Epoch [8789/50000], Train Loss: 8089682.0000, Val Loss: 5273369.0000\n",
      "Epoch [8790/50000], Train Loss: 8089678.5000, Val Loss: 5273421.0000\n",
      "Epoch [8791/50000], Train Loss: 8089675.0000, Val Loss: 5273473.0000\n",
      "Epoch [8792/50000], Train Loss: 8089672.0000, Val Loss: 5273524.5000\n",
      "Epoch [8793/50000], Train Loss: 8089669.5000, Val Loss: 5273576.0000\n",
      "Epoch [8794/50000], Train Loss: 8089665.5000, Val Loss: 5273627.5000\n",
      "Epoch [8795/50000], Train Loss: 8089663.0000, Val Loss: 5273679.5000\n",
      "Epoch [8796/50000], Train Loss: 8089659.5000, Val Loss: 5273730.5000\n",
      "Epoch [8797/50000], Train Loss: 8089656.0000, Val Loss: 5273782.0000\n",
      "Epoch [8798/50000], Train Loss: 8089654.0000, Val Loss: 5273833.5000\n",
      "Epoch [8799/50000], Train Loss: 8089650.5000, Val Loss: 5273885.0000\n",
      "Epoch [8800/50000], Train Loss: 8089647.5000, Val Loss: 5273936.0000\n",
      "Epoch [8801/50000], Train Loss: 8089645.0000, Val Loss: 5273987.0000\n",
      "Epoch [8802/50000], Train Loss: 8089641.5000, Val Loss: 5274038.5000\n",
      "Epoch [8803/50000], Train Loss: 8089638.5000, Val Loss: 5274089.0000\n",
      "Epoch [8804/50000], Train Loss: 8089636.0000, Val Loss: 5274139.0000\n",
      "Epoch [8805/50000], Train Loss: 8089632.5000, Val Loss: 5274190.5000\n",
      "Epoch [8806/50000], Train Loss: 8089630.0000, Val Loss: 5274241.5000\n",
      "Epoch [8807/50000], Train Loss: 8089626.5000, Val Loss: 5274292.5000\n",
      "Epoch [8808/50000], Train Loss: 8089623.5000, Val Loss: 5274343.0000\n",
      "Epoch [8809/50000], Train Loss: 8089621.0000, Val Loss: 5274392.5000\n",
      "Epoch [8810/50000], Train Loss: 8089618.0000, Val Loss: 5274442.5000\n",
      "Epoch [8811/50000], Train Loss: 8089615.0000, Val Loss: 5274494.0000\n",
      "Epoch [8812/50000], Train Loss: 8089612.0000, Val Loss: 5274544.0000\n",
      "Epoch [8813/50000], Train Loss: 8089610.0000, Val Loss: 5274594.5000\n",
      "Epoch [8814/50000], Train Loss: 8089606.0000, Val Loss: 5274644.5000\n",
      "Epoch [8815/50000], Train Loss: 8089603.5000, Val Loss: 5274695.0000\n",
      "Epoch [8816/50000], Train Loss: 8089600.0000, Val Loss: 5274744.5000\n",
      "Epoch [8817/50000], Train Loss: 8089598.5000, Val Loss: 5274793.5000\n",
      "Epoch [8818/50000], Train Loss: 8089595.0000, Val Loss: 5274843.5000\n",
      "Epoch [8819/50000], Train Loss: 8089592.0000, Val Loss: 5274894.0000\n",
      "Epoch [8820/50000], Train Loss: 8089590.5000, Val Loss: 5274944.0000\n",
      "Epoch [8821/50000], Train Loss: 8089587.0000, Val Loss: 5274993.0000\n",
      "Epoch [8822/50000], Train Loss: 8089583.0000, Val Loss: 5275042.0000\n",
      "Epoch [8823/50000], Train Loss: 8089580.5000, Val Loss: 5275092.5000\n",
      "Epoch [8824/50000], Train Loss: 8089578.5000, Val Loss: 5275141.5000\n",
      "Epoch [8825/50000], Train Loss: 8089576.0000, Val Loss: 5275191.0000\n",
      "Epoch [8826/50000], Train Loss: 8089573.0000, Val Loss: 5275239.5000\n",
      "Epoch [8827/50000], Train Loss: 8089571.0000, Val Loss: 5275288.5000\n",
      "Epoch [8828/50000], Train Loss: 8089567.5000, Val Loss: 5275337.5000\n",
      "Epoch [8829/50000], Train Loss: 8089564.0000, Val Loss: 5275387.0000\n",
      "Epoch [8830/50000], Train Loss: 8089562.5000, Val Loss: 5275436.5000\n",
      "Epoch [8831/50000], Train Loss: 8089559.0000, Val Loss: 5275485.0000\n",
      "Epoch [8832/50000], Train Loss: 8089557.0000, Val Loss: 5275533.0000\n",
      "Epoch [8833/50000], Train Loss: 8089555.0000, Val Loss: 5275581.5000\n",
      "Epoch [8834/50000], Train Loss: 8089551.5000, Val Loss: 5275630.5000\n",
      "Epoch [8835/50000], Train Loss: 8089549.5000, Val Loss: 5275679.0000\n",
      "Epoch [8836/50000], Train Loss: 8089546.0000, Val Loss: 5275728.0000\n",
      "Epoch [8837/50000], Train Loss: 8089543.5000, Val Loss: 5275776.0000\n",
      "Epoch [8838/50000], Train Loss: 8089541.0000, Val Loss: 5275824.0000\n",
      "Epoch [8839/50000], Train Loss: 8089538.0000, Val Loss: 5275872.5000\n",
      "Epoch [8840/50000], Train Loss: 8089536.0000, Val Loss: 5275921.0000\n",
      "Epoch [8841/50000], Train Loss: 8089532.5000, Val Loss: 5275968.5000\n",
      "Epoch [8842/50000], Train Loss: 8089530.5000, Val Loss: 5276016.5000\n",
      "Epoch [8843/50000], Train Loss: 8089528.0000, Val Loss: 5276064.5000\n",
      "Epoch [8844/50000], Train Loss: 8089526.0000, Val Loss: 5276113.0000\n",
      "Epoch [8845/50000], Train Loss: 8089522.5000, Val Loss: 5276160.0000\n",
      "Epoch [8846/50000], Train Loss: 8089521.0000, Val Loss: 5276207.5000\n",
      "Epoch [8847/50000], Train Loss: 8089517.5000, Val Loss: 5276255.0000\n",
      "Epoch [8848/50000], Train Loss: 8089515.0000, Val Loss: 5276303.0000\n",
      "Epoch [8849/50000], Train Loss: 8089513.0000, Val Loss: 5276350.5000\n",
      "Epoch [8850/50000], Train Loss: 8089510.0000, Val Loss: 5276398.5000\n",
      "Epoch [8851/50000], Train Loss: 8089508.0000, Val Loss: 5276445.0000\n",
      "Epoch [8852/50000], Train Loss: 8089505.5000, Val Loss: 5276492.5000\n",
      "Epoch [8853/50000], Train Loss: 8089502.5000, Val Loss: 5276539.0000\n",
      "Epoch [8854/50000], Train Loss: 8089500.5000, Val Loss: 5276587.0000\n",
      "Epoch [8855/50000], Train Loss: 8089498.0000, Val Loss: 5276634.5000\n",
      "Epoch [8856/50000], Train Loss: 8089495.0000, Val Loss: 5276681.0000\n",
      "Epoch [8857/50000], Train Loss: 8089493.5000, Val Loss: 5276727.5000\n",
      "Epoch [8858/50000], Train Loss: 8089491.0000, Val Loss: 5276775.5000\n",
      "Epoch [8859/50000], Train Loss: 8089489.5000, Val Loss: 5276821.5000\n",
      "Epoch [8860/50000], Train Loss: 8089485.5000, Val Loss: 5276867.5000\n",
      "Epoch [8861/50000], Train Loss: 8089484.0000, Val Loss: 5276914.5000\n",
      "Epoch [8862/50000], Train Loss: 8089481.5000, Val Loss: 5276961.0000\n",
      "Epoch [8863/50000], Train Loss: 8089479.5000, Val Loss: 5277007.5000\n",
      "Epoch [8864/50000], Train Loss: 8089476.5000, Val Loss: 5277054.0000\n",
      "Epoch [8865/50000], Train Loss: 8089475.5000, Val Loss: 5277100.5000\n",
      "Epoch [8866/50000], Train Loss: 8089472.0000, Val Loss: 5277146.5000\n",
      "Epoch [8867/50000], Train Loss: 8089469.0000, Val Loss: 5277192.5000\n",
      "Epoch [8868/50000], Train Loss: 8089467.5000, Val Loss: 5277238.5000\n",
      "Epoch [8869/50000], Train Loss: 8089464.5000, Val Loss: 5277285.0000\n",
      "Epoch [8870/50000], Train Loss: 8089462.0000, Val Loss: 5277330.0000\n",
      "Epoch [8871/50000], Train Loss: 8089460.0000, Val Loss: 5277376.5000\n",
      "Epoch [8872/50000], Train Loss: 8089458.0000, Val Loss: 5277423.0000\n",
      "Epoch [8873/50000], Train Loss: 8089456.5000, Val Loss: 5277467.0000\n",
      "Epoch [8874/50000], Train Loss: 8089454.0000, Val Loss: 5277513.0000\n",
      "Epoch [8875/50000], Train Loss: 8089450.5000, Val Loss: 5277559.0000\n",
      "Epoch [8876/50000], Train Loss: 8089449.5000, Val Loss: 5277604.5000\n",
      "Epoch [8877/50000], Train Loss: 8089447.5000, Val Loss: 5277650.0000\n",
      "Epoch [8878/50000], Train Loss: 8089444.5000, Val Loss: 5277695.0000\n",
      "Epoch [8879/50000], Train Loss: 8089443.0000, Val Loss: 5277741.0000\n",
      "Epoch [8880/50000], Train Loss: 8089440.0000, Val Loss: 5277786.0000\n",
      "Epoch [8881/50000], Train Loss: 8089438.0000, Val Loss: 5277831.5000\n",
      "Epoch [8882/50000], Train Loss: 8089436.0000, Val Loss: 5277876.5000\n",
      "Epoch [8883/50000], Train Loss: 8089434.0000, Val Loss: 5277921.0000\n",
      "Epoch [8884/50000], Train Loss: 8089431.0000, Val Loss: 5277966.5000\n",
      "Epoch [8885/50000], Train Loss: 8089429.5000, Val Loss: 5278011.0000\n",
      "Epoch [8886/50000], Train Loss: 8089426.5000, Val Loss: 5278056.0000\n",
      "Epoch [8887/50000], Train Loss: 8089425.0000, Val Loss: 5278101.0000\n",
      "Epoch [8888/50000], Train Loss: 8089422.5000, Val Loss: 5278145.0000\n",
      "Epoch [8889/50000], Train Loss: 8089421.0000, Val Loss: 5278191.0000\n",
      "Epoch [8890/50000], Train Loss: 8089418.5000, Val Loss: 5278234.0000\n",
      "Epoch [8891/50000], Train Loss: 8089416.5000, Val Loss: 5278279.0000\n",
      "Epoch [8892/50000], Train Loss: 8089414.0000, Val Loss: 5278323.0000\n",
      "Epoch [8893/50000], Train Loss: 8089412.0000, Val Loss: 5278367.0000\n",
      "Epoch [8894/50000], Train Loss: 8089410.0000, Val Loss: 5278411.5000\n",
      "Epoch [8895/50000], Train Loss: 8089408.0000, Val Loss: 5278456.0000\n",
      "Epoch [8896/50000], Train Loss: 8089406.5000, Val Loss: 5278500.5000\n",
      "Epoch [8897/50000], Train Loss: 8089404.0000, Val Loss: 5278544.5000\n",
      "Epoch [8898/50000], Train Loss: 8089402.0000, Val Loss: 5278588.5000\n",
      "Epoch [8899/50000], Train Loss: 8089399.5000, Val Loss: 5278631.5000\n",
      "Epoch [8900/50000], Train Loss: 8089397.5000, Val Loss: 5278675.5000\n",
      "Epoch [8901/50000], Train Loss: 8089395.5000, Val Loss: 5278719.5000\n",
      "Epoch [8902/50000], Train Loss: 8089393.5000, Val Loss: 5278763.0000\n",
      "Epoch [8903/50000], Train Loss: 8089392.5000, Val Loss: 5278806.5000\n",
      "Epoch [8904/50000], Train Loss: 8089389.5000, Val Loss: 5278849.5000\n",
      "Epoch [8905/50000], Train Loss: 8089387.0000, Val Loss: 5278893.5000\n",
      "Epoch [8906/50000], Train Loss: 8089385.5000, Val Loss: 5278936.5000\n",
      "Epoch [8907/50000], Train Loss: 8089384.0000, Val Loss: 5278980.5000\n",
      "Epoch [8908/50000], Train Loss: 8089381.5000, Val Loss: 5279023.5000\n",
      "Epoch [8909/50000], Train Loss: 8089380.0000, Val Loss: 5279066.5000\n",
      "Epoch [8910/50000], Train Loss: 8089377.5000, Val Loss: 5279109.5000\n",
      "Epoch [8911/50000], Train Loss: 8089375.5000, Val Loss: 5279152.0000\n",
      "Epoch [8912/50000], Train Loss: 8089374.5000, Val Loss: 5279195.5000\n",
      "Epoch [8913/50000], Train Loss: 8089371.5000, Val Loss: 5279238.0000\n",
      "Epoch [8914/50000], Train Loss: 8089370.0000, Val Loss: 5279280.5000\n",
      "Epoch [8915/50000], Train Loss: 8089368.0000, Val Loss: 5279323.0000\n",
      "Epoch [8916/50000], Train Loss: 8089366.5000, Val Loss: 5279366.0000\n",
      "Epoch [8917/50000], Train Loss: 8089364.0000, Val Loss: 5279408.0000\n",
      "Epoch [8918/50000], Train Loss: 8089361.5000, Val Loss: 5279450.5000\n",
      "Epoch [8919/50000], Train Loss: 8089361.0000, Val Loss: 5279493.5000\n",
      "Epoch [8920/50000], Train Loss: 8089359.0000, Val Loss: 5279536.0000\n",
      "Epoch [8921/50000], Train Loss: 8089356.5000, Val Loss: 5279578.0000\n",
      "Epoch [8922/50000], Train Loss: 8089354.5000, Val Loss: 5279621.0000\n",
      "Epoch [8923/50000], Train Loss: 8089353.0000, Val Loss: 5279662.0000\n",
      "Epoch [8924/50000], Train Loss: 8089351.5000, Val Loss: 5279704.5000\n",
      "Epoch [8925/50000], Train Loss: 8089349.5000, Val Loss: 5279746.5000\n",
      "Epoch [8926/50000], Train Loss: 8089347.5000, Val Loss: 5279789.0000\n",
      "Epoch [8927/50000], Train Loss: 8089345.0000, Val Loss: 5279831.0000\n",
      "Epoch [8928/50000], Train Loss: 8089344.0000, Val Loss: 5279872.5000\n",
      "Epoch [8929/50000], Train Loss: 8089341.0000, Val Loss: 5279914.5000\n",
      "Epoch [8930/50000], Train Loss: 8089339.5000, Val Loss: 5279956.5000\n",
      "Epoch [8931/50000], Train Loss: 8089338.5000, Val Loss: 5279998.0000\n",
      "Epoch [8932/50000], Train Loss: 8089336.0000, Val Loss: 5280039.0000\n",
      "Epoch [8933/50000], Train Loss: 8089334.5000, Val Loss: 5280080.5000\n",
      "Epoch [8934/50000], Train Loss: 8089332.0000, Val Loss: 5280122.0000\n",
      "Epoch [8935/50000], Train Loss: 8089331.5000, Val Loss: 5280162.5000\n",
      "Epoch [8936/50000], Train Loss: 8089329.5000, Val Loss: 5280205.0000\n",
      "Epoch [8937/50000], Train Loss: 8089327.0000, Val Loss: 5280245.5000\n",
      "Epoch [8938/50000], Train Loss: 8089326.0000, Val Loss: 5280286.5000\n",
      "Epoch [8939/50000], Train Loss: 8089325.0000, Val Loss: 5280327.5000\n",
      "Epoch [8940/50000], Train Loss: 8089321.5000, Val Loss: 5280369.0000\n",
      "Epoch [8941/50000], Train Loss: 8089320.5000, Val Loss: 5280408.5000\n",
      "Epoch [8942/50000], Train Loss: 8089319.5000, Val Loss: 5280450.0000\n",
      "Epoch [8943/50000], Train Loss: 8089317.0000, Val Loss: 5280490.5000\n",
      "Epoch [8944/50000], Train Loss: 8089315.5000, Val Loss: 5280531.5000\n",
      "Epoch [8945/50000], Train Loss: 8089313.5000, Val Loss: 5280572.5000\n",
      "Epoch [8946/50000], Train Loss: 8089312.0000, Val Loss: 5280613.5000\n",
      "Epoch [8947/50000], Train Loss: 8089310.5000, Val Loss: 5280654.0000\n",
      "Epoch [8948/50000], Train Loss: 8089308.5000, Val Loss: 5280694.0000\n",
      "Epoch [8949/50000], Train Loss: 8089307.0000, Val Loss: 5280735.0000\n",
      "Epoch [8950/50000], Train Loss: 8089306.0000, Val Loss: 5280775.0000\n",
      "Epoch [8951/50000], Train Loss: 8089303.5000, Val Loss: 5280815.0000\n",
      "Epoch [8952/50000], Train Loss: 8089302.0000, Val Loss: 5280855.0000\n",
      "Epoch [8953/50000], Train Loss: 8089299.5000, Val Loss: 5280894.5000\n",
      "Epoch [8954/50000], Train Loss: 8089299.0000, Val Loss: 5280935.5000\n",
      "Epoch [8955/50000], Train Loss: 8089298.0000, Val Loss: 5280975.0000\n",
      "Epoch [8956/50000], Train Loss: 8089295.5000, Val Loss: 5281015.0000\n",
      "Epoch [8957/50000], Train Loss: 8089294.0000, Val Loss: 5281054.5000\n",
      "Epoch [8958/50000], Train Loss: 8089292.5000, Val Loss: 5281094.5000\n",
      "Epoch [8959/50000], Train Loss: 8089290.5000, Val Loss: 5281134.0000\n",
      "Epoch [8960/50000], Train Loss: 8089289.5000, Val Loss: 5281174.0000\n",
      "Epoch [8961/50000], Train Loss: 8089286.5000, Val Loss: 5281213.0000\n",
      "Epoch [8962/50000], Train Loss: 8089285.0000, Val Loss: 5281253.0000\n",
      "Epoch [8963/50000], Train Loss: 8089284.0000, Val Loss: 5281292.5000\n",
      "Epoch [8964/50000], Train Loss: 8089283.0000, Val Loss: 5281331.5000\n",
      "Epoch [8965/50000], Train Loss: 8089280.5000, Val Loss: 5281370.5000\n",
      "Epoch [8966/50000], Train Loss: 8089279.5000, Val Loss: 5281409.5000\n",
      "Epoch [8967/50000], Train Loss: 8089277.0000, Val Loss: 5281448.5000\n",
      "Epoch [8968/50000], Train Loss: 8089276.5000, Val Loss: 5281488.0000\n",
      "Epoch [8969/50000], Train Loss: 8089275.0000, Val Loss: 5281527.5000\n",
      "Epoch [8970/50000], Train Loss: 8089273.5000, Val Loss: 5281566.0000\n",
      "Epoch [8971/50000], Train Loss: 8089271.0000, Val Loss: 5281605.5000\n",
      "Epoch [8972/50000], Train Loss: 8089270.0000, Val Loss: 5281644.5000\n",
      "Epoch [8973/50000], Train Loss: 8089269.0000, Val Loss: 5281682.5000\n",
      "Epoch [8974/50000], Train Loss: 8089267.5000, Val Loss: 5281721.5000\n",
      "Epoch [8975/50000], Train Loss: 8089266.0000, Val Loss: 5281760.5000\n",
      "Epoch [8976/50000], Train Loss: 8089264.5000, Val Loss: 5281799.0000\n",
      "Epoch [8977/50000], Train Loss: 8089262.5000, Val Loss: 5281837.5000\n",
      "Epoch [8978/50000], Train Loss: 8089261.0000, Val Loss: 5281875.0000\n",
      "Epoch [8979/50000], Train Loss: 8089261.0000, Val Loss: 5281914.0000\n",
      "Epoch [8980/50000], Train Loss: 8089258.0000, Val Loss: 5281952.0000\n",
      "Epoch [8981/50000], Train Loss: 8089257.0000, Val Loss: 5281990.5000\n",
      "Epoch [8982/50000], Train Loss: 8089254.5000, Val Loss: 5282029.5000\n",
      "Epoch [8983/50000], Train Loss: 8089254.0000, Val Loss: 5282066.5000\n",
      "Epoch [8984/50000], Train Loss: 8089252.0000, Val Loss: 5282104.5000\n",
      "Epoch [8985/50000], Train Loss: 8089251.0000, Val Loss: 5282142.5000\n",
      "Epoch [8986/50000], Train Loss: 8089248.5000, Val Loss: 5282181.0000\n",
      "Epoch [8987/50000], Train Loss: 8089248.5000, Val Loss: 5282218.5000\n",
      "Epoch [8988/50000], Train Loss: 8089246.5000, Val Loss: 5282256.5000\n",
      "Epoch [8989/50000], Train Loss: 8089246.0000, Val Loss: 5282294.0000\n",
      "Epoch [8990/50000], Train Loss: 8089243.5000, Val Loss: 5282330.5000\n",
      "Epoch [8991/50000], Train Loss: 8089242.0000, Val Loss: 5282369.0000\n",
      "Epoch [8992/50000], Train Loss: 8089240.5000, Val Loss: 5282406.5000\n",
      "Epoch [8993/50000], Train Loss: 8089239.0000, Val Loss: 5282445.0000\n",
      "Epoch [8994/50000], Train Loss: 8089238.0000, Val Loss: 5282481.5000\n",
      "Epoch [8995/50000], Train Loss: 8089237.0000, Val Loss: 5282519.5000\n",
      "Epoch [8996/50000], Train Loss: 8089235.5000, Val Loss: 5282555.0000\n",
      "Epoch [8997/50000], Train Loss: 8089234.5000, Val Loss: 5282593.0000\n",
      "Epoch [8998/50000], Train Loss: 8089233.0000, Val Loss: 5282630.5000\n",
      "Epoch [8999/50000], Train Loss: 8089230.5000, Val Loss: 5282667.5000\n",
      "Epoch [9000/50000], Train Loss: 8089229.5000, Val Loss: 5282704.5000\n",
      "Epoch [9001/50000], Train Loss: 8089229.0000, Val Loss: 5282741.5000\n",
      "Epoch [9002/50000], Train Loss: 8089227.0000, Val Loss: 5282778.0000\n",
      "Epoch [9003/50000], Train Loss: 8089225.5000, Val Loss: 5282815.0000\n",
      "Epoch [9004/50000], Train Loss: 8089225.0000, Val Loss: 5282851.5000\n",
      "Epoch [9005/50000], Train Loss: 8089223.5000, Val Loss: 5282888.0000\n",
      "Epoch [9006/50000], Train Loss: 8089221.0000, Val Loss: 5282925.0000\n",
      "Epoch [9007/50000], Train Loss: 8089221.0000, Val Loss: 5282961.0000\n",
      "Epoch [9008/50000], Train Loss: 8089219.0000, Val Loss: 5282997.5000\n",
      "Epoch [9009/50000], Train Loss: 8089218.0000, Val Loss: 5283033.5000\n",
      "Epoch [9010/50000], Train Loss: 8089216.0000, Val Loss: 5283070.5000\n",
      "Epoch [9011/50000], Train Loss: 8089215.5000, Val Loss: 5283106.0000\n",
      "Epoch [9012/50000], Train Loss: 8089213.0000, Val Loss: 5283143.0000\n",
      "Epoch [9013/50000], Train Loss: 8089212.5000, Val Loss: 5283178.0000\n",
      "Epoch [9014/50000], Train Loss: 8089212.0000, Val Loss: 5283214.5000\n",
      "Epoch [9015/50000], Train Loss: 8089210.0000, Val Loss: 5283251.0000\n",
      "Epoch [9016/50000], Train Loss: 8089209.5000, Val Loss: 5283287.5000\n",
      "Epoch [9017/50000], Train Loss: 8089207.0000, Val Loss: 5283322.5000\n",
      "Epoch [9018/50000], Train Loss: 8089206.0000, Val Loss: 5283359.0000\n",
      "Epoch [9019/50000], Train Loss: 8089205.0000, Val Loss: 5283393.5000\n",
      "Epoch [9020/50000], Train Loss: 8089204.0000, Val Loss: 5283429.5000\n",
      "Epoch [9021/50000], Train Loss: 8089202.0000, Val Loss: 5283465.5000\n",
      "Epoch [9022/50000], Train Loss: 8089201.5000, Val Loss: 5283501.0000\n",
      "Epoch [9023/50000], Train Loss: 8089200.5000, Val Loss: 5283536.5000\n",
      "Epoch [9024/50000], Train Loss: 8089198.5000, Val Loss: 5283571.5000\n",
      "Epoch [9025/50000], Train Loss: 8089198.0000, Val Loss: 5283607.0000\n",
      "Epoch [9026/50000], Train Loss: 8089197.0000, Val Loss: 5283642.5000\n",
      "Epoch [9027/50000], Train Loss: 8089195.0000, Val Loss: 5283678.0000\n",
      "Epoch [9028/50000], Train Loss: 8089193.5000, Val Loss: 5283713.0000\n",
      "Epoch [9029/50000], Train Loss: 8089192.5000, Val Loss: 5283747.5000\n",
      "Epoch [9030/50000], Train Loss: 8089191.5000, Val Loss: 5283783.0000\n",
      "Epoch [9031/50000], Train Loss: 8089190.5000, Val Loss: 5283818.5000\n",
      "Epoch [9032/50000], Train Loss: 8089189.0000, Val Loss: 5283853.5000\n",
      "Epoch [9033/50000], Train Loss: 8089188.0000, Val Loss: 5283888.0000\n",
      "Epoch [9034/50000], Train Loss: 8089187.0000, Val Loss: 5283922.5000\n",
      "Epoch [9035/50000], Train Loss: 8089185.0000, Val Loss: 5283958.0000\n",
      "Epoch [9036/50000], Train Loss: 8089184.0000, Val Loss: 5283992.0000\n",
      "Epoch [9037/50000], Train Loss: 8089183.5000, Val Loss: 5284026.0000\n",
      "Epoch [9038/50000], Train Loss: 8089182.5000, Val Loss: 5284061.0000\n",
      "Epoch [9039/50000], Train Loss: 8089180.5000, Val Loss: 5284095.0000\n",
      "Epoch [9040/50000], Train Loss: 8089179.5000, Val Loss: 5284130.5000\n",
      "Epoch [9041/50000], Train Loss: 8089178.5000, Val Loss: 5284164.5000\n",
      "Epoch [9042/50000], Train Loss: 8089178.0000, Val Loss: 5284198.5000\n",
      "Epoch [9043/50000], Train Loss: 8089176.5000, Val Loss: 5284232.0000\n",
      "Epoch [9044/50000], Train Loss: 8089174.5000, Val Loss: 5284266.5000\n",
      "Epoch [9045/50000], Train Loss: 8089174.5000, Val Loss: 5284301.0000\n",
      "Epoch [9046/50000], Train Loss: 8089173.5000, Val Loss: 5284335.0000\n",
      "Epoch [9047/50000], Train Loss: 8089172.0000, Val Loss: 5284369.0000\n",
      "Epoch [9048/50000], Train Loss: 8089171.5000, Val Loss: 5284402.5000\n",
      "Epoch [9049/50000], Train Loss: 8089170.0000, Val Loss: 5284437.0000\n",
      "Epoch [9050/50000], Train Loss: 8089169.0000, Val Loss: 5284470.5000\n",
      "Epoch [9051/50000], Train Loss: 8089167.5000, Val Loss: 5284504.5000\n",
      "Epoch [9052/50000], Train Loss: 8089166.0000, Val Loss: 5284537.5000\n",
      "Epoch [9053/50000], Train Loss: 8089165.5000, Val Loss: 5284571.0000\n",
      "Epoch [9054/50000], Train Loss: 8089164.5000, Val Loss: 5284604.5000\n",
      "Epoch [9055/50000], Train Loss: 8089163.0000, Val Loss: 5284638.0000\n",
      "Epoch [9056/50000], Train Loss: 8089162.0000, Val Loss: 5284671.5000\n",
      "Epoch [9057/50000], Train Loss: 8089161.5000, Val Loss: 5284705.5000\n",
      "Epoch [9058/50000], Train Loss: 8089160.0000, Val Loss: 5284738.0000\n",
      "Epoch [9059/50000], Train Loss: 8089159.5000, Val Loss: 5284771.5000\n",
      "Epoch [9060/50000], Train Loss: 8089158.0000, Val Loss: 5284805.0000\n",
      "Epoch [9061/50000], Train Loss: 8089157.5000, Val Loss: 5284838.5000\n",
      "Epoch [9062/50000], Train Loss: 8089155.5000, Val Loss: 5284871.0000\n",
      "Epoch [9063/50000], Train Loss: 8089154.0000, Val Loss: 5284904.0000\n",
      "Epoch [9064/50000], Train Loss: 8089153.5000, Val Loss: 5284936.5000\n",
      "Epoch [9065/50000], Train Loss: 8089153.0000, Val Loss: 5284969.5000\n",
      "Epoch [9066/50000], Train Loss: 8089151.5000, Val Loss: 5285002.5000\n",
      "Epoch [9067/50000], Train Loss: 8089150.5000, Val Loss: 5285035.0000\n",
      "Epoch [9068/50000], Train Loss: 8089150.0000, Val Loss: 5285069.0000\n",
      "Epoch [9069/50000], Train Loss: 8089148.0000, Val Loss: 5285101.0000\n",
      "Epoch [9070/50000], Train Loss: 8089147.5000, Val Loss: 5285134.0000\n",
      "Epoch [9071/50000], Train Loss: 8089147.0000, Val Loss: 5285166.0000\n",
      "Epoch [9072/50000], Train Loss: 8089146.5000, Val Loss: 5285198.5000\n",
      "Epoch [9073/50000], Train Loss: 8089144.0000, Val Loss: 5285231.0000\n",
      "Epoch [9074/50000], Train Loss: 8089143.0000, Val Loss: 5285262.5000\n",
      "Epoch [9075/50000], Train Loss: 8089143.0000, Val Loss: 5285295.5000\n",
      "Epoch [9076/50000], Train Loss: 8089142.0000, Val Loss: 5285327.5000\n",
      "Epoch [9077/50000], Train Loss: 8089141.0000, Val Loss: 5285359.5000\n",
      "Epoch [9078/50000], Train Loss: 8089139.5000, Val Loss: 5285391.0000\n",
      "Epoch [9079/50000], Train Loss: 8089138.5000, Val Loss: 5285423.0000\n",
      "Epoch [9080/50000], Train Loss: 8089138.0000, Val Loss: 5285455.0000\n",
      "Epoch [9081/50000], Train Loss: 8089137.0000, Val Loss: 5285488.0000\n",
      "Epoch [9082/50000], Train Loss: 8089135.5000, Val Loss: 5285519.0000\n",
      "Epoch [9083/50000], Train Loss: 8089135.0000, Val Loss: 5285551.5000\n",
      "Epoch [9084/50000], Train Loss: 8089134.0000, Val Loss: 5285583.0000\n",
      "Epoch [9085/50000], Train Loss: 8089133.0000, Val Loss: 5285614.5000\n",
      "Epoch [9086/50000], Train Loss: 8089132.0000, Val Loss: 5285646.0000\n",
      "Epoch [9087/50000], Train Loss: 8089131.0000, Val Loss: 5285677.5000\n",
      "Epoch [9088/50000], Train Loss: 8089130.0000, Val Loss: 5285709.0000\n",
      "Epoch [9089/50000], Train Loss: 8089129.5000, Val Loss: 5285741.0000\n",
      "Epoch [9090/50000], Train Loss: 8089128.0000, Val Loss: 5285771.0000\n",
      "Epoch [9091/50000], Train Loss: 8089128.0000, Val Loss: 5285802.5000\n",
      "Epoch [9092/50000], Train Loss: 8089126.0000, Val Loss: 5285834.0000\n",
      "Epoch [9093/50000], Train Loss: 8089125.0000, Val Loss: 5285865.5000\n",
      "Epoch [9094/50000], Train Loss: 8089124.5000, Val Loss: 5285896.5000\n",
      "Epoch [9095/50000], Train Loss: 8089124.0000, Val Loss: 5285928.0000\n",
      "Epoch [9096/50000], Train Loss: 8089122.0000, Val Loss: 5285959.5000\n",
      "Epoch [9097/50000], Train Loss: 8089123.5000, Val Loss: 5285989.5000\n",
      "Epoch [9098/50000], Train Loss: 8089121.5000, Val Loss: 5286020.5000\n",
      "Epoch [9099/50000], Train Loss: 8089120.5000, Val Loss: 5286051.0000\n",
      "Epoch [9100/50000], Train Loss: 8089119.5000, Val Loss: 5286082.0000\n",
      "Epoch [9101/50000], Train Loss: 8089119.0000, Val Loss: 5286113.0000\n",
      "Epoch [9102/50000], Train Loss: 8089118.0000, Val Loss: 5286144.0000\n",
      "Epoch [9103/50000], Train Loss: 8089116.0000, Val Loss: 5286174.0000\n",
      "Epoch [9104/50000], Train Loss: 8089116.0000, Val Loss: 5286205.0000\n",
      "Epoch [9105/50000], Train Loss: 8089115.0000, Val Loss: 5286235.0000\n",
      "Epoch [9106/50000], Train Loss: 8089114.5000, Val Loss: 5286265.5000\n",
      "Epoch [9107/50000], Train Loss: 8089113.5000, Val Loss: 5286295.5000\n",
      "Epoch [9108/50000], Train Loss: 8089112.5000, Val Loss: 5286326.0000\n",
      "Epoch [9109/50000], Train Loss: 8089111.5000, Val Loss: 5286357.0000\n",
      "Epoch [9110/50000], Train Loss: 8089110.5000, Val Loss: 5286387.0000\n",
      "Epoch [9111/50000], Train Loss: 8089109.5000, Val Loss: 5286416.0000\n",
      "Epoch [9112/50000], Train Loss: 8089109.5000, Val Loss: 5286447.0000\n",
      "Epoch [9113/50000], Train Loss: 8089109.0000, Val Loss: 5286477.5000\n",
      "Epoch [9114/50000], Train Loss: 8089107.5000, Val Loss: 5286507.0000\n",
      "Epoch [9115/50000], Train Loss: 8089106.5000, Val Loss: 5286537.0000\n",
      "Epoch [9116/50000], Train Loss: 8089106.0000, Val Loss: 5286566.0000\n",
      "Epoch [9117/50000], Train Loss: 8089105.0000, Val Loss: 5286595.5000\n",
      "Epoch [9118/50000], Train Loss: 8089103.5000, Val Loss: 5286626.5000\n",
      "Epoch [9119/50000], Train Loss: 8089103.5000, Val Loss: 5286656.0000\n",
      "Epoch [9120/50000], Train Loss: 8089102.5000, Val Loss: 5286685.0000\n",
      "Epoch [9121/50000], Train Loss: 8089101.5000, Val Loss: 5286714.0000\n",
      "Epoch [9122/50000], Train Loss: 8089100.5000, Val Loss: 5286744.0000\n",
      "Epoch [9123/50000], Train Loss: 8089100.5000, Val Loss: 5286773.5000\n",
      "Epoch [9124/50000], Train Loss: 8089100.0000, Val Loss: 5286802.0000\n",
      "Epoch [9125/50000], Train Loss: 8089098.0000, Val Loss: 5286832.0000\n",
      "Epoch [9126/50000], Train Loss: 8089097.5000, Val Loss: 5286862.5000\n",
      "Epoch [9127/50000], Train Loss: 8089097.0000, Val Loss: 5286890.0000\n",
      "Epoch [9128/50000], Train Loss: 8089097.0000, Val Loss: 5286920.0000\n",
      "Epoch [9129/50000], Train Loss: 8089095.5000, Val Loss: 5286949.0000\n",
      "Epoch [9130/50000], Train Loss: 8089094.5000, Val Loss: 5286978.5000\n",
      "Epoch [9131/50000], Train Loss: 8089094.0000, Val Loss: 5287007.5000\n",
      "Epoch [9132/50000], Train Loss: 8089093.0000, Val Loss: 5287037.0000\n",
      "Epoch [9133/50000], Train Loss: 8089092.5000, Val Loss: 5287064.5000\n",
      "Epoch [9134/50000], Train Loss: 8089091.5000, Val Loss: 5287094.0000\n",
      "Epoch [9135/50000], Train Loss: 8089091.0000, Val Loss: 5287122.5000\n",
      "Epoch [9136/50000], Train Loss: 8089090.0000, Val Loss: 5287151.5000\n",
      "Epoch [9137/50000], Train Loss: 8089089.0000, Val Loss: 5287179.5000\n",
      "Epoch [9138/50000], Train Loss: 8089089.0000, Val Loss: 5287208.0000\n",
      "Epoch [9139/50000], Train Loss: 8089088.0000, Val Loss: 5287237.5000\n",
      "Epoch [9140/50000], Train Loss: 8089086.5000, Val Loss: 5287265.5000\n",
      "Epoch [9141/50000], Train Loss: 8089087.0000, Val Loss: 5287293.5000\n",
      "Epoch [9142/50000], Train Loss: 8089085.0000, Val Loss: 5287321.5000\n",
      "Epoch [9143/50000], Train Loss: 8089084.5000, Val Loss: 5287350.5000\n",
      "Epoch [9144/50000], Train Loss: 8089084.0000, Val Loss: 5287378.5000\n",
      "Epoch [9145/50000], Train Loss: 8089083.5000, Val Loss: 5287407.0000\n",
      "Epoch [9146/50000], Train Loss: 8089083.0000, Val Loss: 5287435.0000\n",
      "Epoch [9147/50000], Train Loss: 8089082.5000, Val Loss: 5287463.5000\n",
      "Epoch [9148/50000], Train Loss: 8089081.5000, Val Loss: 5287491.0000\n",
      "Epoch [9149/50000], Train Loss: 8089080.0000, Val Loss: 5287520.0000\n",
      "Epoch [9150/50000], Train Loss: 8089080.0000, Val Loss: 5287547.5000\n",
      "Epoch [9151/50000], Train Loss: 8089079.0000, Val Loss: 5287575.5000\n",
      "Epoch [9152/50000], Train Loss: 8089079.0000, Val Loss: 5287602.5000\n",
      "Epoch [9153/50000], Train Loss: 8089078.0000, Val Loss: 5287631.0000\n",
      "Epoch [9154/50000], Train Loss: 8089077.5000, Val Loss: 5287658.0000\n",
      "Epoch [9155/50000], Train Loss: 8089077.0000, Val Loss: 5287686.5000\n",
      "Epoch [9156/50000], Train Loss: 8089075.5000, Val Loss: 5287714.5000\n",
      "Epoch [9157/50000], Train Loss: 8089075.0000, Val Loss: 5287741.5000\n",
      "Epoch [9158/50000], Train Loss: 8089074.5000, Val Loss: 5287769.0000\n",
      "Epoch [9159/50000], Train Loss: 8089074.0000, Val Loss: 5287797.0000\n",
      "Epoch [9160/50000], Train Loss: 8089073.5000, Val Loss: 5287823.5000\n",
      "Epoch [9161/50000], Train Loss: 8089073.0000, Val Loss: 5287851.5000\n",
      "Epoch [9162/50000], Train Loss: 8089072.5000, Val Loss: 5287879.0000\n",
      "Epoch [9163/50000], Train Loss: 8089071.0000, Val Loss: 5287906.0000\n",
      "Epoch [9164/50000], Train Loss: 8089070.0000, Val Loss: 5287933.0000\n",
      "Epoch [9165/50000], Train Loss: 8089069.0000, Val Loss: 5287959.5000\n",
      "Epoch [9166/50000], Train Loss: 8089069.5000, Val Loss: 5287987.0000\n",
      "Epoch [9167/50000], Train Loss: 8089068.0000, Val Loss: 5288013.5000\n",
      "Epoch [9168/50000], Train Loss: 8089068.0000, Val Loss: 5288041.0000\n",
      "Epoch [9169/50000], Train Loss: 8089066.5000, Val Loss: 5288068.5000\n",
      "Epoch [9170/50000], Train Loss: 8089066.5000, Val Loss: 5288095.0000\n",
      "Epoch [9171/50000], Train Loss: 8089065.5000, Val Loss: 5288121.0000\n",
      "Epoch [9172/50000], Train Loss: 8089064.5000, Val Loss: 5288148.5000\n",
      "Epoch [9173/50000], Train Loss: 8089065.0000, Val Loss: 5288175.0000\n",
      "Epoch [9174/50000], Train Loss: 8089063.5000, Val Loss: 5288202.0000\n",
      "Epoch [9175/50000], Train Loss: 8089063.5000, Val Loss: 5288228.5000\n",
      "Epoch [9176/50000], Train Loss: 8089063.5000, Val Loss: 5288255.5000\n",
      "Epoch [9177/50000], Train Loss: 8089062.5000, Val Loss: 5288281.5000\n",
      "Epoch [9178/50000], Train Loss: 8089061.5000, Val Loss: 5288307.5000\n",
      "Epoch [9179/50000], Train Loss: 8089060.5000, Val Loss: 5288334.0000\n",
      "Epoch [9180/50000], Train Loss: 8089061.0000, Val Loss: 5288360.5000\n",
      "Epoch [9181/50000], Train Loss: 8089060.0000, Val Loss: 5288386.5000\n",
      "Epoch [9182/50000], Train Loss: 8089059.5000, Val Loss: 5288412.5000\n",
      "Epoch [9183/50000], Train Loss: 8089059.0000, Val Loss: 5288438.5000\n",
      "Epoch [9184/50000], Train Loss: 8089058.0000, Val Loss: 5288464.5000\n",
      "Epoch [9185/50000], Train Loss: 8089057.5000, Val Loss: 5288491.5000\n",
      "Epoch [9186/50000], Train Loss: 8089057.0000, Val Loss: 5288518.0000\n",
      "Epoch [9187/50000], Train Loss: 8089056.0000, Val Loss: 5288543.0000\n",
      "Epoch [9188/50000], Train Loss: 8089055.5000, Val Loss: 5288569.0000\n",
      "Epoch [9189/50000], Train Loss: 8089054.5000, Val Loss: 5288594.5000\n",
      "Epoch [9190/50000], Train Loss: 8089054.5000, Val Loss: 5288621.0000\n",
      "Epoch [9191/50000], Train Loss: 8089053.0000, Val Loss: 5288646.5000\n",
      "Epoch [9192/50000], Train Loss: 8089053.0000, Val Loss: 5288672.0000\n",
      "Epoch [9193/50000], Train Loss: 8089052.5000, Val Loss: 5288697.5000\n",
      "Epoch [9194/50000], Train Loss: 8089051.5000, Val Loss: 5288723.5000\n",
      "Epoch [9195/50000], Train Loss: 8089052.0000, Val Loss: 5288749.5000\n",
      "Epoch [9196/50000], Train Loss: 8089051.0000, Val Loss: 5288774.0000\n",
      "Epoch [9197/50000], Train Loss: 8089050.5000, Val Loss: 5288799.5000\n",
      "Epoch [9198/50000], Train Loss: 8089049.5000, Val Loss: 5288825.0000\n",
      "Epoch [9199/50000], Train Loss: 8089049.5000, Val Loss: 5288850.0000\n",
      "Epoch [9200/50000], Train Loss: 8089048.5000, Val Loss: 5288875.5000\n",
      "Epoch [9201/50000], Train Loss: 8089047.5000, Val Loss: 5288901.0000\n",
      "Epoch [9202/50000], Train Loss: 8089047.5000, Val Loss: 5288926.5000\n",
      "Epoch [9203/50000], Train Loss: 8089047.0000, Val Loss: 5288951.5000\n",
      "Epoch [9204/50000], Train Loss: 8089045.5000, Val Loss: 5288976.0000\n",
      "Epoch [9205/50000], Train Loss: 8089046.5000, Val Loss: 5289000.5000\n",
      "Epoch [9206/50000], Train Loss: 8089045.0000, Val Loss: 5289026.0000\n",
      "Epoch [9207/50000], Train Loss: 8089045.0000, Val Loss: 5289051.0000\n",
      "Epoch [9208/50000], Train Loss: 8089044.0000, Val Loss: 5289075.5000\n",
      "Epoch [9209/50000], Train Loss: 8089043.0000, Val Loss: 5289101.5000\n",
      "Epoch [9210/50000], Train Loss: 8089043.5000, Val Loss: 5289125.5000\n",
      "Epoch [9211/50000], Train Loss: 8089042.5000, Val Loss: 5289150.5000\n",
      "Epoch [9212/50000], Train Loss: 8089041.5000, Val Loss: 5289175.0000\n",
      "Epoch [9213/50000], Train Loss: 8089042.0000, Val Loss: 5289200.0000\n",
      "Epoch [9214/50000], Train Loss: 8089041.5000, Val Loss: 5289224.5000\n",
      "Epoch [9215/50000], Train Loss: 8089041.0000, Val Loss: 5289249.5000\n",
      "Epoch [9216/50000], Train Loss: 8089039.5000, Val Loss: 5289273.5000\n",
      "Epoch [9217/50000], Train Loss: 8089039.5000, Val Loss: 5289298.0000\n",
      "Epoch [9218/50000], Train Loss: 8089039.0000, Val Loss: 5289322.0000\n",
      "Epoch [9219/50000], Train Loss: 8089038.5000, Val Loss: 5289346.0000\n",
      "Epoch [9220/50000], Train Loss: 8089038.0000, Val Loss: 5289370.5000\n",
      "Epoch [9221/50000], Train Loss: 8089037.5000, Val Loss: 5289394.5000\n",
      "Epoch [9222/50000], Train Loss: 8089037.0000, Val Loss: 5289419.5000\n",
      "Epoch [9223/50000], Train Loss: 8089036.5000, Val Loss: 5289443.0000\n",
      "Epoch [9224/50000], Train Loss: 8089036.5000, Val Loss: 5289467.0000\n",
      "Epoch [9225/50000], Train Loss: 8089035.0000, Val Loss: 5289491.0000\n",
      "Epoch [9226/50000], Train Loss: 8089034.5000, Val Loss: 5289514.5000\n",
      "Epoch [9227/50000], Train Loss: 8089035.0000, Val Loss: 5289539.0000\n",
      "Epoch [9228/50000], Train Loss: 8089034.5000, Val Loss: 5289563.0000\n",
      "Epoch [9229/50000], Train Loss: 8089034.0000, Val Loss: 5289587.0000\n",
      "Epoch [9230/50000], Train Loss: 8089033.5000, Val Loss: 5289609.5000\n",
      "Epoch [9231/50000], Train Loss: 8089032.5000, Val Loss: 5289633.5000\n",
      "Epoch [9232/50000], Train Loss: 8089032.0000, Val Loss: 5289657.5000\n",
      "Epoch [9233/50000], Train Loss: 8089031.5000, Val Loss: 5289681.0000\n",
      "Epoch [9234/50000], Train Loss: 8089030.5000, Val Loss: 5289704.5000\n",
      "Epoch [9235/50000], Train Loss: 8089030.0000, Val Loss: 5289728.5000\n",
      "Epoch [9236/50000], Train Loss: 8089029.5000, Val Loss: 5289752.0000\n",
      "Epoch [9237/50000], Train Loss: 8089030.0000, Val Loss: 5289775.0000\n",
      "Epoch [9238/50000], Train Loss: 8089029.0000, Val Loss: 5289798.5000\n",
      "Epoch [9239/50000], Train Loss: 8089029.0000, Val Loss: 5289822.0000\n",
      "Epoch [9240/50000], Train Loss: 8089028.0000, Val Loss: 5289845.5000\n",
      "Epoch [9241/50000], Train Loss: 8089027.5000, Val Loss: 5289869.0000\n",
      "Epoch [9242/50000], Train Loss: 8089027.5000, Val Loss: 5289892.5000\n",
      "Epoch [9243/50000], Train Loss: 8089026.0000, Val Loss: 5289914.5000\n",
      "Epoch [9244/50000], Train Loss: 8089026.0000, Val Loss: 5289937.5000\n",
      "Epoch [9245/50000], Train Loss: 8089025.5000, Val Loss: 5289961.0000\n",
      "Epoch [9246/50000], Train Loss: 8089025.5000, Val Loss: 5289984.0000\n",
      "Epoch [9247/50000], Train Loss: 8089025.5000, Val Loss: 5290007.0000\n",
      "Epoch [9248/50000], Train Loss: 8089024.5000, Val Loss: 5290029.5000\n",
      "Epoch [9249/50000], Train Loss: 8089025.0000, Val Loss: 5290053.0000\n",
      "Epoch [9250/50000], Train Loss: 8089024.0000, Val Loss: 5290075.0000\n",
      "Epoch [9251/50000], Train Loss: 8089023.0000, Val Loss: 5290098.0000\n",
      "Epoch [9252/50000], Train Loss: 8089023.0000, Val Loss: 5290120.5000\n",
      "Epoch [9253/50000], Train Loss: 8089022.5000, Val Loss: 5290143.0000\n",
      "Epoch [9254/50000], Train Loss: 8089022.5000, Val Loss: 5290166.0000\n",
      "Epoch [9255/50000], Train Loss: 8089021.0000, Val Loss: 5290187.5000\n",
      "Epoch [9256/50000], Train Loss: 8089022.0000, Val Loss: 5290211.0000\n",
      "Epoch [9257/50000], Train Loss: 8089020.5000, Val Loss: 5290233.5000\n",
      "Epoch [9258/50000], Train Loss: 8089019.5000, Val Loss: 5290255.5000\n",
      "Epoch [9259/50000], Train Loss: 8089020.0000, Val Loss: 5290278.5000\n",
      "Epoch [9260/50000], Train Loss: 8089019.5000, Val Loss: 5290300.5000\n",
      "Epoch [9261/50000], Train Loss: 8089019.5000, Val Loss: 5290322.5000\n",
      "Epoch [9262/50000], Train Loss: 8089019.0000, Val Loss: 5290345.0000\n",
      "Epoch [9263/50000], Train Loss: 8089018.5000, Val Loss: 5290368.0000\n",
      "Epoch [9264/50000], Train Loss: 8089018.0000, Val Loss: 5290389.5000\n",
      "Epoch [9265/50000], Train Loss: 8089017.5000, Val Loss: 5290410.5000\n",
      "Epoch [9266/50000], Train Loss: 8089017.5000, Val Loss: 5290433.0000\n",
      "Epoch [9267/50000], Train Loss: 8089017.5000, Val Loss: 5290455.5000\n",
      "Epoch [9268/50000], Train Loss: 8089016.0000, Val Loss: 5290477.5000\n",
      "Epoch [9269/50000], Train Loss: 8089015.5000, Val Loss: 5290498.5000\n",
      "Epoch [9270/50000], Train Loss: 8089015.0000, Val Loss: 5290520.5000\n",
      "Epoch [9271/50000], Train Loss: 8089015.0000, Val Loss: 5290543.0000\n",
      "Epoch [9272/50000], Train Loss: 8089015.5000, Val Loss: 5290563.5000\n",
      "Epoch [9273/50000], Train Loss: 8089014.0000, Val Loss: 5290585.5000\n",
      "Epoch [9274/50000], Train Loss: 8089013.5000, Val Loss: 5290607.5000\n",
      "Epoch [9275/50000], Train Loss: 8089013.5000, Val Loss: 5290629.5000\n",
      "Epoch [9276/50000], Train Loss: 8089013.0000, Val Loss: 5290650.5000\n",
      "Epoch [9277/50000], Train Loss: 8089013.0000, Val Loss: 5290672.0000\n",
      "Epoch [9278/50000], Train Loss: 8089012.0000, Val Loss: 5290693.5000\n",
      "Epoch [9279/50000], Train Loss: 8089011.5000, Val Loss: 5290714.5000\n",
      "Epoch [9280/50000], Train Loss: 8089010.5000, Val Loss: 5290736.5000\n",
      "Epoch [9281/50000], Train Loss: 8089011.5000, Val Loss: 5290757.5000\n",
      "Epoch [9282/50000], Train Loss: 8089011.0000, Val Loss: 5290778.5000\n",
      "Epoch [9283/50000], Train Loss: 8089011.5000, Val Loss: 5290800.0000\n",
      "Epoch [9284/50000], Train Loss: 8089010.0000, Val Loss: 5290821.5000\n",
      "Epoch [9285/50000], Train Loss: 8089009.5000, Val Loss: 5290842.0000\n",
      "Epoch [9286/50000], Train Loss: 8089009.5000, Val Loss: 5290863.5000\n",
      "Epoch [9287/50000], Train Loss: 8089009.0000, Val Loss: 5290884.5000\n",
      "Epoch [9288/50000], Train Loss: 8089009.0000, Val Loss: 5290905.0000\n",
      "Epoch [9289/50000], Train Loss: 8089008.5000, Val Loss: 5290926.5000\n",
      "Epoch [9290/50000], Train Loss: 8089007.5000, Val Loss: 5290946.5000\n",
      "Epoch [9291/50000], Train Loss: 8089007.5000, Val Loss: 5290968.0000\n",
      "Epoch [9292/50000], Train Loss: 8089008.5000, Val Loss: 5290989.0000\n",
      "Epoch [9293/50000], Train Loss: 8089007.0000, Val Loss: 5291010.0000\n",
      "Epoch [9294/50000], Train Loss: 8089006.5000, Val Loss: 5291031.0000\n",
      "Epoch [9295/50000], Train Loss: 8089006.0000, Val Loss: 5291051.0000\n",
      "Epoch [9296/50000], Train Loss: 8089005.5000, Val Loss: 5291072.0000\n",
      "Epoch [9297/50000], Train Loss: 8089005.5000, Val Loss: 5291092.5000\n",
      "Epoch [9298/50000], Train Loss: 8089005.0000, Val Loss: 5291112.5000\n",
      "Epoch [9299/50000], Train Loss: 8089005.0000, Val Loss: 5291133.5000\n",
      "Epoch [9300/50000], Train Loss: 8089005.0000, Val Loss: 5291154.0000\n",
      "Epoch [9301/50000], Train Loss: 8089004.5000, Val Loss: 5291174.5000\n",
      "Epoch [9302/50000], Train Loss: 8089004.0000, Val Loss: 5291194.5000\n",
      "Epoch [9303/50000], Train Loss: 8089003.0000, Val Loss: 5291215.0000\n",
      "Epoch [9304/50000], Train Loss: 8089003.0000, Val Loss: 5291235.5000\n",
      "Epoch [9305/50000], Train Loss: 8089002.5000, Val Loss: 5291255.5000\n",
      "Epoch [9306/50000], Train Loss: 8089002.5000, Val Loss: 5291276.5000\n",
      "Epoch [9307/50000], Train Loss: 8089002.0000, Val Loss: 5291296.0000\n",
      "Epoch [9308/50000], Train Loss: 8089001.5000, Val Loss: 5291315.5000\n",
      "Epoch [9309/50000], Train Loss: 8089001.5000, Val Loss: 5291336.0000\n",
      "Epoch [9310/50000], Train Loss: 8089001.5000, Val Loss: 5291357.0000\n",
      "Epoch [9311/50000], Train Loss: 8089000.5000, Val Loss: 5291376.5000\n",
      "Epoch [9312/50000], Train Loss: 8089000.5000, Val Loss: 5291395.0000\n",
      "Epoch [9313/50000], Train Loss: 8089000.0000, Val Loss: 5291416.5000\n",
      "Epoch [9314/50000], Train Loss: 8089000.5000, Val Loss: 5291434.5000\n",
      "Epoch [9315/50000], Train Loss: 8088999.5000, Val Loss: 5291455.5000\n",
      "Epoch [9316/50000], Train Loss: 8088998.5000, Val Loss: 5291474.0000\n",
      "Epoch [9317/50000], Train Loss: 8088998.5000, Val Loss: 5291494.5000\n",
      "Epoch [9318/50000], Train Loss: 8088998.0000, Val Loss: 5291514.5000\n",
      "Epoch [9319/50000], Train Loss: 8088998.0000, Val Loss: 5291535.0000\n",
      "Epoch [9320/50000], Train Loss: 8088998.5000, Val Loss: 5291553.5000\n",
      "Epoch [9321/50000], Train Loss: 8088997.5000, Val Loss: 5291573.5000\n",
      "Epoch [9322/50000], Train Loss: 8088997.5000, Val Loss: 5291593.0000\n",
      "Epoch [9323/50000], Train Loss: 8088996.5000, Val Loss: 5291612.5000\n",
      "Epoch [9324/50000], Train Loss: 8088996.0000, Val Loss: 5291631.0000\n",
      "Epoch [9325/50000], Train Loss: 8088996.5000, Val Loss: 5291650.5000\n",
      "Epoch [9326/50000], Train Loss: 8088996.0000, Val Loss: 5291670.0000\n",
      "Epoch [9327/50000], Train Loss: 8088995.5000, Val Loss: 5291689.5000\n",
      "Epoch [9328/50000], Train Loss: 8088995.5000, Val Loss: 5291709.0000\n",
      "Epoch [9329/50000], Train Loss: 8088995.5000, Val Loss: 5291728.5000\n",
      "Epoch [9330/50000], Train Loss: 8088995.0000, Val Loss: 5291746.5000\n",
      "Epoch [9331/50000], Train Loss: 8088995.0000, Val Loss: 5291766.0000\n",
      "Epoch [9332/50000], Train Loss: 8088994.0000, Val Loss: 5291785.0000\n",
      "Epoch [9333/50000], Train Loss: 8088994.0000, Val Loss: 5291804.5000\n",
      "Epoch [9334/50000], Train Loss: 8088993.5000, Val Loss: 5291823.5000\n",
      "Epoch [9335/50000], Train Loss: 8088993.5000, Val Loss: 5291842.5000\n",
      "Epoch [9336/50000], Train Loss: 8088992.5000, Val Loss: 5291861.0000\n",
      "Epoch [9337/50000], Train Loss: 8088992.5000, Val Loss: 5291880.0000\n",
      "Epoch [9338/50000], Train Loss: 8088992.0000, Val Loss: 5291898.5000\n",
      "Epoch [9339/50000], Train Loss: 8088992.0000, Val Loss: 5291918.0000\n",
      "Epoch [9340/50000], Train Loss: 8088992.0000, Val Loss: 5291936.0000\n",
      "Epoch [9341/50000], Train Loss: 8088992.0000, Val Loss: 5291955.5000\n",
      "Epoch [9342/50000], Train Loss: 8088991.5000, Val Loss: 5291973.5000\n",
      "Epoch [9343/50000], Train Loss: 8088991.5000, Val Loss: 5291991.5000\n",
      "Epoch [9344/50000], Train Loss: 8088991.0000, Val Loss: 5292010.5000\n",
      "Epoch [9345/50000], Train Loss: 8088990.5000, Val Loss: 5292029.0000\n",
      "Epoch [9346/50000], Train Loss: 8088989.0000, Val Loss: 5292047.5000\n",
      "Epoch [9347/50000], Train Loss: 8088989.0000, Val Loss: 5292065.5000\n",
      "Epoch [9348/50000], Train Loss: 8088990.5000, Val Loss: 5292084.5000\n",
      "Epoch [9349/50000], Train Loss: 8088989.0000, Val Loss: 5292103.0000\n",
      "Epoch [9350/50000], Train Loss: 8088990.0000, Val Loss: 5292121.0000\n",
      "Epoch [9351/50000], Train Loss: 8088988.5000, Val Loss: 5292139.0000\n",
      "Epoch [9352/50000], Train Loss: 8088989.0000, Val Loss: 5292157.0000\n",
      "Epoch [9353/50000], Train Loss: 8088988.5000, Val Loss: 5292175.5000\n",
      "Epoch [9354/50000], Train Loss: 8088988.5000, Val Loss: 5292193.0000\n",
      "Epoch [9355/50000], Train Loss: 8088988.5000, Val Loss: 5292211.5000\n",
      "Epoch [9356/50000], Train Loss: 8088987.5000, Val Loss: 5292230.5000\n",
      "Epoch [9357/50000], Train Loss: 8088988.0000, Val Loss: 5292248.0000\n",
      "Epoch [9358/50000], Train Loss: 8088987.5000, Val Loss: 5292265.5000\n",
      "Epoch [9359/50000], Train Loss: 8088987.0000, Val Loss: 5292283.5000\n",
      "Epoch [9360/50000], Train Loss: 8088987.0000, Val Loss: 5292301.0000\n",
      "Epoch [9361/50000], Train Loss: 8088986.0000, Val Loss: 5292320.0000\n",
      "Epoch [9362/50000], Train Loss: 8088986.0000, Val Loss: 5292337.0000\n",
      "Epoch [9363/50000], Train Loss: 8088986.0000, Val Loss: 5292354.0000\n",
      "Epoch [9364/50000], Train Loss: 8088986.0000, Val Loss: 5292372.5000\n",
      "Epoch [9365/50000], Train Loss: 8088985.5000, Val Loss: 5292390.0000\n",
      "Epoch [9366/50000], Train Loss: 8088985.5000, Val Loss: 5292408.0000\n",
      "Epoch [9367/50000], Train Loss: 8088986.0000, Val Loss: 5292425.5000\n",
      "Epoch [9368/50000], Train Loss: 8088984.5000, Val Loss: 5292442.5000\n",
      "Epoch [9369/50000], Train Loss: 8088984.5000, Val Loss: 5292461.0000\n",
      "Epoch [9370/50000], Train Loss: 8088983.5000, Val Loss: 5292478.0000\n",
      "Epoch [9371/50000], Train Loss: 8088984.0000, Val Loss: 5292495.5000\n",
      "Epoch [9372/50000], Train Loss: 8088983.5000, Val Loss: 5292513.0000\n",
      "Epoch [9373/50000], Train Loss: 8088983.5000, Val Loss: 5292530.0000\n",
      "Epoch [9374/50000], Train Loss: 8088983.0000, Val Loss: 5292547.5000\n",
      "Epoch [9375/50000], Train Loss: 8088983.0000, Val Loss: 5292565.5000\n",
      "Epoch [9376/50000], Train Loss: 8088983.0000, Val Loss: 5292582.5000\n",
      "Epoch [9377/50000], Train Loss: 8088982.5000, Val Loss: 5292599.5000\n",
      "Epoch [9378/50000], Train Loss: 8088982.5000, Val Loss: 5292616.5000\n",
      "Epoch [9379/50000], Train Loss: 8088982.0000, Val Loss: 5292633.0000\n",
      "Epoch [9380/50000], Train Loss: 8088982.0000, Val Loss: 5292650.5000\n",
      "Epoch [9381/50000], Train Loss: 8088981.5000, Val Loss: 5292667.5000\n",
      "Epoch [9382/50000], Train Loss: 8088981.5000, Val Loss: 5292685.5000\n",
      "Epoch [9383/50000], Train Loss: 8088982.0000, Val Loss: 5292702.0000\n",
      "Epoch [9384/50000], Train Loss: 8088981.0000, Val Loss: 5292719.0000\n",
      "Epoch [9385/50000], Train Loss: 8088981.0000, Val Loss: 5292735.0000\n",
      "Epoch [9386/50000], Train Loss: 8088980.0000, Val Loss: 5292752.5000\n",
      "Epoch [9387/50000], Train Loss: 8088980.0000, Val Loss: 5292768.5000\n",
      "Epoch [9388/50000], Train Loss: 8088979.5000, Val Loss: 5292786.0000\n",
      "Epoch [9389/50000], Train Loss: 8088980.0000, Val Loss: 5292802.5000\n",
      "Epoch [9390/50000], Train Loss: 8088979.5000, Val Loss: 5292819.5000\n",
      "Epoch [9391/50000], Train Loss: 8088978.5000, Val Loss: 5292836.5000\n",
      "Epoch [9392/50000], Train Loss: 8088979.0000, Val Loss: 5292853.0000\n",
      "Epoch [9393/50000], Train Loss: 8088978.0000, Val Loss: 5292869.5000\n",
      "Epoch [9394/50000], Train Loss: 8088979.0000, Val Loss: 5292885.5000\n",
      "Epoch [9395/50000], Train Loss: 8088978.0000, Val Loss: 5292902.5000\n",
      "Epoch [9396/50000], Train Loss: 8088978.0000, Val Loss: 5292919.0000\n",
      "Epoch [9397/50000], Train Loss: 8088978.0000, Val Loss: 5292934.5000\n",
      "Epoch [9398/50000], Train Loss: 8088978.0000, Val Loss: 5292951.0000\n",
      "Epoch [9399/50000], Train Loss: 8088977.5000, Val Loss: 5292968.5000\n",
      "Epoch [9400/50000], Train Loss: 8088978.0000, Val Loss: 5292984.0000\n",
      "Epoch [9401/50000], Train Loss: 8088977.5000, Val Loss: 5293000.5000\n",
      "Epoch [9402/50000], Train Loss: 8088977.0000, Val Loss: 5293016.5000\n",
      "Epoch [9403/50000], Train Loss: 8088977.0000, Val Loss: 5293033.5000\n",
      "Epoch [9404/50000], Train Loss: 8088977.0000, Val Loss: 5293049.5000\n",
      "Epoch [9405/50000], Train Loss: 8088977.0000, Val Loss: 5293065.0000\n",
      "Epoch [9406/50000], Train Loss: 8088977.0000, Val Loss: 5293081.0000\n",
      "Epoch [9407/50000], Train Loss: 8088976.5000, Val Loss: 5293097.0000\n",
      "Epoch [9408/50000], Train Loss: 8088976.5000, Val Loss: 5293113.0000\n",
      "Epoch [9409/50000], Train Loss: 8088975.5000, Val Loss: 5293128.5000\n",
      "Epoch [9410/50000], Train Loss: 8088975.5000, Val Loss: 5293145.5000\n",
      "Epoch [9411/50000], Train Loss: 8088975.5000, Val Loss: 5293161.5000\n",
      "Epoch [9412/50000], Train Loss: 8088974.5000, Val Loss: 5293176.5000\n",
      "Epoch [9413/50000], Train Loss: 8088975.0000, Val Loss: 5293193.5000\n",
      "Epoch [9414/50000], Train Loss: 8088974.0000, Val Loss: 5293208.0000\n",
      "Epoch [9415/50000], Train Loss: 8088975.0000, Val Loss: 5293224.0000\n",
      "Epoch [9416/50000], Train Loss: 8088974.5000, Val Loss: 5293239.5000\n",
      "Epoch [9417/50000], Train Loss: 8088974.0000, Val Loss: 5293255.5000\n",
      "Epoch [9418/50000], Train Loss: 8088974.0000, Val Loss: 5293271.0000\n",
      "Epoch [9419/50000], Train Loss: 8088973.5000, Val Loss: 5293286.5000\n",
      "Epoch [9420/50000], Train Loss: 8088973.5000, Val Loss: 5293302.5000\n",
      "Epoch [9421/50000], Train Loss: 8088973.5000, Val Loss: 5293318.0000\n",
      "Epoch [9422/50000], Train Loss: 8088973.0000, Val Loss: 5293333.5000\n",
      "Epoch [9423/50000], Train Loss: 8088973.0000, Val Loss: 5293349.0000\n",
      "Epoch [9424/50000], Train Loss: 8088973.0000, Val Loss: 5293364.5000\n",
      "Epoch [9425/50000], Train Loss: 8088972.5000, Val Loss: 5293379.0000\n",
      "Epoch [9426/50000], Train Loss: 8088973.5000, Val Loss: 5293394.5000\n",
      "Epoch [9427/50000], Train Loss: 8088972.5000, Val Loss: 5293409.5000\n",
      "Epoch [9428/50000], Train Loss: 8088972.5000, Val Loss: 5293425.5000\n",
      "Epoch [9429/50000], Train Loss: 8088972.5000, Val Loss: 5293441.0000\n",
      "Epoch [9430/50000], Train Loss: 8088971.0000, Val Loss: 5293456.0000\n",
      "Epoch [9431/50000], Train Loss: 8088972.0000, Val Loss: 5293471.5000\n",
      "Epoch [9432/50000], Train Loss: 8088972.0000, Val Loss: 5293486.0000\n",
      "Epoch [9433/50000], Train Loss: 8088971.0000, Val Loss: 5293501.0000\n",
      "Epoch [9434/50000], Train Loss: 8088972.0000, Val Loss: 5293516.5000\n",
      "Epoch [9435/50000], Train Loss: 8088971.0000, Val Loss: 5293531.5000\n",
      "Epoch [9436/50000], Train Loss: 8088970.5000, Val Loss: 5293545.5000\n",
      "Epoch [9437/50000], Train Loss: 8088970.0000, Val Loss: 5293561.5000\n",
      "Epoch [9438/50000], Train Loss: 8088971.0000, Val Loss: 5293576.0000\n",
      "Epoch [9439/50000], Train Loss: 8088970.0000, Val Loss: 5293590.5000\n",
      "Epoch [9440/50000], Train Loss: 8088969.5000, Val Loss: 5293606.5000\n",
      "Epoch [9441/50000], Train Loss: 8088970.0000, Val Loss: 5293621.5000\n",
      "Epoch [9442/50000], Train Loss: 8088969.5000, Val Loss: 5293635.0000\n",
      "Epoch [9443/50000], Train Loss: 8088969.0000, Val Loss: 5293650.0000\n",
      "Epoch [9444/50000], Train Loss: 8088970.0000, Val Loss: 5293665.5000\n",
      "Epoch [9445/50000], Train Loss: 8088969.5000, Val Loss: 5293679.0000\n",
      "Epoch [9446/50000], Train Loss: 8088969.0000, Val Loss: 5293694.0000\n",
      "Epoch [9447/50000], Train Loss: 8088969.0000, Val Loss: 5293708.5000\n",
      "Epoch [9448/50000], Train Loss: 8088969.0000, Val Loss: 5293723.0000\n",
      "Epoch [9449/50000], Train Loss: 8088968.5000, Val Loss: 5293737.5000\n",
      "Epoch [9450/50000], Train Loss: 8088968.5000, Val Loss: 5293752.0000\n",
      "Epoch [9451/50000], Train Loss: 8088969.0000, Val Loss: 5293767.0000\n",
      "Epoch [9452/50000], Train Loss: 8088968.0000, Val Loss: 5293781.5000\n",
      "Epoch [9453/50000], Train Loss: 8088968.5000, Val Loss: 5293796.5000\n",
      "Epoch [9454/50000], Train Loss: 8088968.0000, Val Loss: 5293809.5000\n",
      "Epoch [9455/50000], Train Loss: 8088968.0000, Val Loss: 5293824.0000\n",
      "Epoch [9456/50000], Train Loss: 8088968.0000, Val Loss: 5293838.5000\n",
      "Epoch [9457/50000], Train Loss: 8088966.5000, Val Loss: 5293852.5000\n",
      "Epoch [9458/50000], Train Loss: 8088967.5000, Val Loss: 5293867.5000\n",
      "Epoch [9459/50000], Train Loss: 8088967.5000, Val Loss: 5293880.5000\n",
      "Epoch [9460/50000], Train Loss: 8088966.5000, Val Loss: 5293895.5000\n",
      "Epoch [9461/50000], Train Loss: 8088966.0000, Val Loss: 5293909.5000\n",
      "Epoch [9462/50000], Train Loss: 8088966.5000, Val Loss: 5293923.5000\n",
      "Epoch [9463/50000], Train Loss: 8088965.5000, Val Loss: 5293937.5000\n",
      "Epoch [9464/50000], Train Loss: 8088966.5000, Val Loss: 5293951.5000\n",
      "Epoch [9465/50000], Train Loss: 8088966.0000, Val Loss: 5293965.5000\n",
      "Epoch [9466/50000], Train Loss: 8088965.5000, Val Loss: 5293978.5000\n",
      "Epoch [9467/50000], Train Loss: 8088966.0000, Val Loss: 5293992.5000\n",
      "Epoch [9468/50000], Train Loss: 8088965.5000, Val Loss: 5294007.0000\n",
      "Epoch [9469/50000], Train Loss: 8088965.5000, Val Loss: 5294021.0000\n",
      "Epoch [9470/50000], Train Loss: 8088965.0000, Val Loss: 5294034.5000\n",
      "Epoch [9471/50000], Train Loss: 8088965.5000, Val Loss: 5294048.5000\n",
      "Epoch [9472/50000], Train Loss: 8088965.0000, Val Loss: 5294062.0000\n",
      "Epoch [9473/50000], Train Loss: 8088965.5000, Val Loss: 5294075.5000\n",
      "Epoch [9474/50000], Train Loss: 8088964.5000, Val Loss: 5294089.0000\n",
      "Epoch [9475/50000], Train Loss: 8088964.5000, Val Loss: 5294103.0000\n",
      "Epoch [9476/50000], Train Loss: 8088964.5000, Val Loss: 5294116.5000\n",
      "Epoch [9477/50000], Train Loss: 8088964.5000, Val Loss: 5294129.5000\n",
      "Epoch [9478/50000], Train Loss: 8088964.0000, Val Loss: 5294143.0000\n",
      "Epoch [9479/50000], Train Loss: 8088965.0000, Val Loss: 5294157.5000\n",
      "Epoch [9480/50000], Train Loss: 8088964.0000, Val Loss: 5294171.0000\n",
      "Epoch [9481/50000], Train Loss: 8088964.5000, Val Loss: 5294183.5000\n",
      "Epoch [9482/50000], Train Loss: 8088964.0000, Val Loss: 5294198.0000\n",
      "Epoch [9483/50000], Train Loss: 8088964.0000, Val Loss: 5294210.5000\n",
      "Epoch [9484/50000], Train Loss: 8088963.0000, Val Loss: 5294224.0000\n",
      "Epoch [9485/50000], Train Loss: 8088963.0000, Val Loss: 5294238.0000\n",
      "Epoch [9486/50000], Train Loss: 8088963.5000, Val Loss: 5294250.0000\n",
      "Epoch [9487/50000], Train Loss: 8088963.0000, Val Loss: 5294264.0000\n",
      "Epoch [9488/50000], Train Loss: 8088962.0000, Val Loss: 5294276.5000\n",
      "Epoch [9489/50000], Train Loss: 8088964.0000, Val Loss: 5294290.0000\n",
      "Epoch [9490/50000], Train Loss: 8088962.0000, Val Loss: 5294303.0000\n",
      "Epoch [9491/50000], Train Loss: 8088963.0000, Val Loss: 5294317.0000\n",
      "Epoch [9492/50000], Train Loss: 8088963.0000, Val Loss: 5294329.5000\n",
      "Epoch [9493/50000], Train Loss: 8088962.0000, Val Loss: 5294342.5000\n",
      "Epoch [9494/50000], Train Loss: 8088961.5000, Val Loss: 5294355.5000\n",
      "Epoch [9495/50000], Train Loss: 8088962.0000, Val Loss: 5294368.0000\n",
      "Epoch [9496/50000], Train Loss: 8088962.0000, Val Loss: 5294381.0000\n",
      "Epoch [9497/50000], Train Loss: 8088962.0000, Val Loss: 5294394.0000\n",
      "Epoch [9498/50000], Train Loss: 8088961.5000, Val Loss: 5294407.0000\n",
      "Epoch [9499/50000], Train Loss: 8088961.5000, Val Loss: 5294419.0000\n",
      "Epoch [9500/50000], Train Loss: 8088961.0000, Val Loss: 5294432.5000\n",
      "Epoch [9501/50000], Train Loss: 8088961.0000, Val Loss: 5294445.5000\n",
      "Epoch [9502/50000], Train Loss: 8088961.0000, Val Loss: 5294457.5000\n",
      "Epoch [9503/50000], Train Loss: 8088961.0000, Val Loss: 5294471.0000\n",
      "Epoch [9504/50000], Train Loss: 8088961.0000, Val Loss: 5294483.5000\n",
      "Epoch [9505/50000], Train Loss: 8088961.0000, Val Loss: 5294496.0000\n",
      "Epoch [9506/50000], Train Loss: 8088961.0000, Val Loss: 5294508.5000\n",
      "Epoch [9507/50000], Train Loss: 8088960.5000, Val Loss: 5294520.5000\n",
      "Epoch [9508/50000], Train Loss: 8088960.5000, Val Loss: 5294534.0000\n",
      "Epoch [9509/50000], Train Loss: 8088960.5000, Val Loss: 5294545.0000\n",
      "Epoch [9510/50000], Train Loss: 8088960.5000, Val Loss: 5294559.0000\n",
      "Epoch [9511/50000], Train Loss: 8088960.0000, Val Loss: 5294571.0000\n",
      "Epoch [9512/50000], Train Loss: 8088960.5000, Val Loss: 5294583.5000\n",
      "Epoch [9513/50000], Train Loss: 8088960.0000, Val Loss: 5294595.5000\n",
      "Epoch [9514/50000], Train Loss: 8088960.0000, Val Loss: 5294609.0000\n",
      "Epoch [9515/50000], Train Loss: 8088959.5000, Val Loss: 5294621.0000\n",
      "Epoch [9516/50000], Train Loss: 8088960.0000, Val Loss: 5294633.0000\n",
      "Epoch [9517/50000], Train Loss: 8088959.5000, Val Loss: 5294645.5000\n",
      "Epoch [9518/50000], Train Loss: 8088959.0000, Val Loss: 5294657.5000\n",
      "Epoch [9519/50000], Train Loss: 8088959.5000, Val Loss: 5294670.0000\n",
      "Epoch [9520/50000], Train Loss: 8088959.0000, Val Loss: 5294681.0000\n",
      "Epoch [9521/50000], Train Loss: 8088958.5000, Val Loss: 5294694.0000\n",
      "Epoch [9522/50000], Train Loss: 8088959.0000, Val Loss: 5294706.0000\n",
      "Epoch [9523/50000], Train Loss: 8088959.5000, Val Loss: 5294718.0000\n",
      "Epoch [9524/50000], Train Loss: 8088958.5000, Val Loss: 5294730.5000\n",
      "Epoch [9525/50000], Train Loss: 8088959.0000, Val Loss: 5294743.0000\n",
      "Epoch [9526/50000], Train Loss: 8088959.0000, Val Loss: 5294754.5000\n",
      "Epoch [9527/50000], Train Loss: 8088959.0000, Val Loss: 5294767.0000\n",
      "Epoch [9528/50000], Train Loss: 8088959.0000, Val Loss: 5294778.0000\n",
      "Epoch [9529/50000], Train Loss: 8088959.0000, Val Loss: 5294789.5000\n",
      "Epoch [9530/50000], Train Loss: 8088958.5000, Val Loss: 5294802.0000\n",
      "Epoch [9531/50000], Train Loss: 8088958.5000, Val Loss: 5294813.5000\n",
      "Epoch [9532/50000], Train Loss: 8088958.0000, Val Loss: 5294824.5000\n",
      "Epoch [9533/50000], Train Loss: 8088958.0000, Val Loss: 5294837.5000\n",
      "Epoch [9534/50000], Train Loss: 8088958.0000, Val Loss: 5294849.5000\n",
      "Epoch [9535/50000], Train Loss: 8088958.0000, Val Loss: 5294861.0000\n",
      "Epoch [9536/50000], Train Loss: 8088958.0000, Val Loss: 5294872.0000\n",
      "Epoch [9537/50000], Train Loss: 8088958.0000, Val Loss: 5294885.0000\n",
      "Epoch [9538/50000], Train Loss: 8088958.0000, Val Loss: 5294896.0000\n",
      "Epoch [9539/50000], Train Loss: 8088956.5000, Val Loss: 5294906.5000\n",
      "Epoch [9540/50000], Train Loss: 8088956.5000, Val Loss: 5294919.5000\n",
      "Epoch [9541/50000], Train Loss: 8088956.5000, Val Loss: 5294930.5000\n",
      "Epoch [9542/50000], Train Loss: 8088957.0000, Val Loss: 5294942.0000\n",
      "Epoch [9543/50000], Train Loss: 8088957.0000, Val Loss: 5294952.5000\n",
      "Epoch [9544/50000], Train Loss: 8088956.5000, Val Loss: 5294965.0000\n",
      "Epoch [9545/50000], Train Loss: 8088957.0000, Val Loss: 5294976.5000\n",
      "Epoch [9546/50000], Train Loss: 8088957.0000, Val Loss: 5294987.5000\n",
      "Epoch [9547/50000], Train Loss: 8088956.5000, Val Loss: 5294999.0000\n",
      "Epoch [9548/50000], Train Loss: 8088956.0000, Val Loss: 5295010.5000\n",
      "Epoch [9549/50000], Train Loss: 8088956.5000, Val Loss: 5295022.5000\n",
      "Epoch [9550/50000], Train Loss: 8088956.5000, Val Loss: 5295033.0000\n",
      "Epoch [9551/50000], Train Loss: 8088956.5000, Val Loss: 5295044.5000\n",
      "Epoch [9552/50000], Train Loss: 8088956.5000, Val Loss: 5295055.0000\n",
      "Epoch [9553/50000], Train Loss: 8088956.5000, Val Loss: 5295066.0000\n",
      "Epoch [9554/50000], Train Loss: 8088956.5000, Val Loss: 5295078.0000\n",
      "Epoch [9555/50000], Train Loss: 8088955.5000, Val Loss: 5295088.5000\n",
      "Epoch [9556/50000], Train Loss: 8088955.5000, Val Loss: 5295099.0000\n",
      "Epoch [9557/50000], Train Loss: 8088956.0000, Val Loss: 5295111.0000\n",
      "Epoch [9558/50000], Train Loss: 8088955.5000, Val Loss: 5295121.5000\n",
      "Epoch [9559/50000], Train Loss: 8088955.5000, Val Loss: 5295132.5000\n",
      "Epoch [9560/50000], Train Loss: 8088955.5000, Val Loss: 5295143.5000\n",
      "Epoch [9561/50000], Train Loss: 8088956.0000, Val Loss: 5295155.0000\n",
      "Epoch [9562/50000], Train Loss: 8088955.5000, Val Loss: 5295166.0000\n",
      "Epoch [9563/50000], Train Loss: 8088954.5000, Val Loss: 5295176.5000\n",
      "Epoch [9564/50000], Train Loss: 8088955.0000, Val Loss: 5295187.0000\n",
      "Epoch [9565/50000], Train Loss: 8088954.5000, Val Loss: 5295198.5000\n",
      "Epoch [9566/50000], Train Loss: 8088955.5000, Val Loss: 5295209.5000\n",
      "Epoch [9567/50000], Train Loss: 8088955.0000, Val Loss: 5295219.0000\n",
      "Epoch [9568/50000], Train Loss: 8088955.0000, Val Loss: 5295231.0000\n",
      "Epoch [9569/50000], Train Loss: 8088955.5000, Val Loss: 5295241.0000\n",
      "Epoch [9570/50000], Train Loss: 8088955.0000, Val Loss: 5295252.5000\n",
      "Epoch [9571/50000], Train Loss: 8088955.5000, Val Loss: 5295262.5000\n",
      "Epoch [9572/50000], Train Loss: 8088954.5000, Val Loss: 5295273.5000\n",
      "Epoch [9573/50000], Train Loss: 8088954.0000, Val Loss: 5295285.0000\n",
      "Epoch [9574/50000], Train Loss: 8088954.0000, Val Loss: 5295294.5000\n",
      "Epoch [9575/50000], Train Loss: 8088954.0000, Val Loss: 5295306.0000\n",
      "Epoch [9576/50000], Train Loss: 8088954.0000, Val Loss: 5295316.5000\n",
      "Epoch [9577/50000], Train Loss: 8088954.5000, Val Loss: 5295326.5000\n",
      "Epoch [9578/50000], Train Loss: 8088954.5000, Val Loss: 5295337.0000\n",
      "Epoch [9579/50000], Train Loss: 8088953.5000, Val Loss: 5295347.0000\n",
      "Epoch [9580/50000], Train Loss: 8088954.0000, Val Loss: 5295357.5000\n",
      "Epoch [9581/50000], Train Loss: 8088953.5000, Val Loss: 5295367.5000\n",
      "Epoch [9582/50000], Train Loss: 8088954.0000, Val Loss: 5295378.5000\n",
      "Epoch [9583/50000], Train Loss: 8088954.0000, Val Loss: 5295388.5000\n",
      "Epoch [9584/50000], Train Loss: 8088954.0000, Val Loss: 5295398.5000\n",
      "Epoch [9585/50000], Train Loss: 8088954.0000, Val Loss: 5295409.0000\n",
      "Epoch [9586/50000], Train Loss: 8088954.0000, Val Loss: 5295419.5000\n",
      "Epoch [9587/50000], Train Loss: 8088953.5000, Val Loss: 5295430.0000\n",
      "Epoch [9588/50000], Train Loss: 8088953.5000, Val Loss: 5295440.0000\n",
      "Epoch [9589/50000], Train Loss: 8088953.5000, Val Loss: 5295450.0000\n",
      "Epoch [9590/50000], Train Loss: 8088953.5000, Val Loss: 5295461.0000\n",
      "Epoch [9591/50000], Train Loss: 8088952.5000, Val Loss: 5295470.0000\n",
      "Epoch [9592/50000], Train Loss: 8088952.5000, Val Loss: 5295480.5000\n",
      "Epoch [9593/50000], Train Loss: 8088953.5000, Val Loss: 5295490.0000\n",
      "Epoch [9594/50000], Train Loss: 8088953.5000, Val Loss: 5295500.5000\n",
      "Epoch [9595/50000], Train Loss: 8088953.5000, Val Loss: 5295510.5000\n",
      "Epoch [9596/50000], Train Loss: 8088952.5000, Val Loss: 5295520.5000\n",
      "Epoch [9597/50000], Train Loss: 8088952.5000, Val Loss: 5295530.5000\n",
      "Epoch [9598/50000], Train Loss: 8088953.5000, Val Loss: 5295541.5000\n",
      "Epoch [9599/50000], Train Loss: 8088952.5000, Val Loss: 5295550.5000\n",
      "Epoch [9600/50000], Train Loss: 8088952.5000, Val Loss: 5295560.5000\n",
      "Epoch [9601/50000], Train Loss: 8088952.0000, Val Loss: 5295571.0000\n",
      "Epoch [9602/50000], Train Loss: 8088952.5000, Val Loss: 5295580.5000\n",
      "Epoch [9603/50000], Train Loss: 8088951.5000, Val Loss: 5295590.5000\n",
      "Epoch [9604/50000], Train Loss: 8088952.0000, Val Loss: 5295599.5000\n",
      "Epoch [9605/50000], Train Loss: 8088952.0000, Val Loss: 5295609.5000\n",
      "Epoch [9606/50000], Train Loss: 8088952.5000, Val Loss: 5295618.5000\n",
      "Epoch [9607/50000], Train Loss: 8088952.0000, Val Loss: 5295629.0000\n",
      "Epoch [9608/50000], Train Loss: 8088952.0000, Val Loss: 5295638.0000\n",
      "Epoch [9609/50000], Train Loss: 8088951.5000, Val Loss: 5295647.5000\n",
      "Epoch [9610/50000], Train Loss: 8088952.0000, Val Loss: 5295657.5000\n",
      "Epoch [9611/50000], Train Loss: 8088951.5000, Val Loss: 5295667.0000\n",
      "Epoch [9612/50000], Train Loss: 8088952.0000, Val Loss: 5295677.0000\n",
      "Epoch [9613/50000], Train Loss: 8088951.5000, Val Loss: 5295686.5000\n",
      "Epoch [9614/50000], Train Loss: 8088951.5000, Val Loss: 5295696.5000\n",
      "Epoch [9615/50000], Train Loss: 8088951.0000, Val Loss: 5295706.0000\n",
      "Epoch [9616/50000], Train Loss: 8088951.0000, Val Loss: 5295715.0000\n",
      "Epoch [9617/50000], Train Loss: 8088951.5000, Val Loss: 5295724.5000\n",
      "Epoch [9618/50000], Train Loss: 8088952.0000, Val Loss: 5295734.5000\n",
      "Epoch [9619/50000], Train Loss: 8088951.0000, Val Loss: 5295743.5000\n",
      "Epoch [9620/50000], Train Loss: 8088951.0000, Val Loss: 5295752.5000\n",
      "Epoch [9621/50000], Train Loss: 8088951.0000, Val Loss: 5295762.0000\n",
      "Epoch [9622/50000], Train Loss: 8088951.5000, Val Loss: 5295770.5000\n",
      "Epoch [9623/50000], Train Loss: 8088951.5000, Val Loss: 5295780.5000\n",
      "Epoch [9624/50000], Train Loss: 8088951.0000, Val Loss: 5295790.0000\n",
      "Epoch [9625/50000], Train Loss: 8088951.0000, Val Loss: 5295799.0000\n",
      "Epoch [9626/50000], Train Loss: 8088951.0000, Val Loss: 5295808.5000\n",
      "Epoch [9627/50000], Train Loss: 8088950.5000, Val Loss: 5295817.5000\n",
      "Epoch [9628/50000], Train Loss: 8088951.0000, Val Loss: 5295827.0000\n",
      "Epoch [9629/50000], Train Loss: 8088950.5000, Val Loss: 5295836.5000\n",
      "Epoch [9630/50000], Train Loss: 8088950.5000, Val Loss: 5295845.5000\n",
      "Epoch [9631/50000], Train Loss: 8088950.0000, Val Loss: 5295854.0000\n",
      "Epoch [9632/50000], Train Loss: 8088951.0000, Val Loss: 5295862.5000\n",
      "Epoch [9633/50000], Train Loss: 8088950.5000, Val Loss: 5295871.5000\n",
      "Epoch [9634/50000], Train Loss: 8088950.0000, Val Loss: 5295880.5000\n",
      "Epoch [9635/50000], Train Loss: 8088950.5000, Val Loss: 5295890.0000\n",
      "Epoch [9636/50000], Train Loss: 8088950.0000, Val Loss: 5295898.5000\n",
      "Epoch [9637/50000], Train Loss: 8088950.5000, Val Loss: 5295907.5000\n",
      "Epoch [9638/50000], Train Loss: 8088950.0000, Val Loss: 5295916.5000\n",
      "Epoch [9639/50000], Train Loss: 8088950.0000, Val Loss: 5295925.5000\n",
      "Epoch [9640/50000], Train Loss: 8088950.0000, Val Loss: 5295935.0000\n",
      "Epoch [9641/50000], Train Loss: 8088949.5000, Val Loss: 5295944.0000\n",
      "Epoch [9642/50000], Train Loss: 8088950.5000, Val Loss: 5295952.0000\n",
      "Epoch [9643/50000], Train Loss: 8088950.0000, Val Loss: 5295961.5000\n",
      "Epoch [9644/50000], Train Loss: 8088949.5000, Val Loss: 5295970.0000\n",
      "Epoch [9645/50000], Train Loss: 8088949.5000, Val Loss: 5295978.5000\n",
      "Epoch [9646/50000], Train Loss: 8088950.0000, Val Loss: 5295986.5000\n",
      "Epoch [9647/50000], Train Loss: 8088949.5000, Val Loss: 5295995.5000\n",
      "Epoch [9648/50000], Train Loss: 8088950.0000, Val Loss: 5296003.5000\n",
      "Epoch [9649/50000], Train Loss: 8088949.0000, Val Loss: 5296013.0000\n",
      "Epoch [9650/50000], Train Loss: 8088949.5000, Val Loss: 5296022.0000\n",
      "Epoch [9651/50000], Train Loss: 8088950.0000, Val Loss: 5296031.0000\n",
      "Epoch [9652/50000], Train Loss: 8088950.0000, Val Loss: 5296038.5000\n",
      "Epoch [9653/50000], Train Loss: 8088949.5000, Val Loss: 5296047.0000\n",
      "Epoch [9654/50000], Train Loss: 8088949.0000, Val Loss: 5296055.5000\n",
      "Epoch [9655/50000], Train Loss: 8088949.5000, Val Loss: 5296064.5000\n",
      "Epoch [9656/50000], Train Loss: 8088950.0000, Val Loss: 5296073.5000\n",
      "Epoch [9657/50000], Train Loss: 8088949.5000, Val Loss: 5296082.0000\n",
      "Epoch [9658/50000], Train Loss: 8088949.5000, Val Loss: 5296090.5000\n",
      "Epoch [9659/50000], Train Loss: 8088949.0000, Val Loss: 5296098.5000\n",
      "Epoch [9660/50000], Train Loss: 8088949.0000, Val Loss: 5296106.5000\n",
      "Epoch [9661/50000], Train Loss: 8088949.5000, Val Loss: 5296115.0000\n",
      "Epoch [9662/50000], Train Loss: 8088949.0000, Val Loss: 5296123.5000\n",
      "Epoch [9663/50000], Train Loss: 8088949.5000, Val Loss: 5296131.5000\n",
      "Epoch [9664/50000], Train Loss: 8088948.0000, Val Loss: 5296139.0000\n",
      "Epoch [9665/50000], Train Loss: 8088949.0000, Val Loss: 5296148.5000\n",
      "Epoch [9666/50000], Train Loss: 8088948.0000, Val Loss: 5296157.5000\n",
      "Epoch [9667/50000], Train Loss: 8088949.5000, Val Loss: 5296165.0000\n",
      "Epoch [9668/50000], Train Loss: 8088948.0000, Val Loss: 5296173.0000\n",
      "Epoch [9669/50000], Train Loss: 8088948.0000, Val Loss: 5296181.5000\n",
      "Epoch [9670/50000], Train Loss: 8088948.0000, Val Loss: 5296189.5000\n",
      "Epoch [9671/50000], Train Loss: 8088948.0000, Val Loss: 5296198.0000\n",
      "Epoch [9672/50000], Train Loss: 8088948.0000, Val Loss: 5296206.5000\n",
      "Epoch [9673/50000], Train Loss: 8088947.5000, Val Loss: 5296214.0000\n",
      "Epoch [9674/50000], Train Loss: 8088949.0000, Val Loss: 5296222.0000\n",
      "Epoch [9675/50000], Train Loss: 8088948.0000, Val Loss: 5296230.0000\n",
      "Epoch [9676/50000], Train Loss: 8088948.0000, Val Loss: 5296238.5000\n",
      "Epoch [9677/50000], Train Loss: 8088948.0000, Val Loss: 5296246.0000\n",
      "Epoch [9678/50000], Train Loss: 8088947.5000, Val Loss: 5296254.0000\n",
      "Epoch [9679/50000], Train Loss: 8088949.0000, Val Loss: 5296262.0000\n",
      "Epoch [9680/50000], Train Loss: 8088948.0000, Val Loss: 5296270.0000\n",
      "Epoch [9681/50000], Train Loss: 8088948.0000, Val Loss: 5296278.0000\n",
      "Epoch [9682/50000], Train Loss: 8088948.0000, Val Loss: 5296285.5000\n",
      "Epoch [9683/50000], Train Loss: 8088947.5000, Val Loss: 5296293.5000\n",
      "Epoch [9684/50000], Train Loss: 8088948.0000, Val Loss: 5296301.5000\n",
      "Epoch [9685/50000], Train Loss: 8088948.0000, Val Loss: 5296309.5000\n",
      "Epoch [9686/50000], Train Loss: 8088949.0000, Val Loss: 5296317.0000\n",
      "Epoch [9687/50000], Train Loss: 8088947.5000, Val Loss: 5296325.0000\n",
      "Epoch [9688/50000], Train Loss: 8088947.5000, Val Loss: 5296332.5000\n",
      "Epoch [9689/50000], Train Loss: 8088948.0000, Val Loss: 5296341.0000\n",
      "Epoch [9690/50000], Train Loss: 8088947.5000, Val Loss: 5296348.5000\n",
      "Epoch [9691/50000], Train Loss: 8088948.0000, Val Loss: 5296355.5000\n",
      "Epoch [9692/50000], Train Loss: 8088947.5000, Val Loss: 5296363.0000\n",
      "Epoch [9693/50000], Train Loss: 8088947.5000, Val Loss: 5296371.0000\n",
      "Epoch [9694/50000], Train Loss: 8088947.5000, Val Loss: 5296378.0000\n",
      "Epoch [9695/50000], Train Loss: 8088947.0000, Val Loss: 5296386.5000\n",
      "Epoch [9696/50000], Train Loss: 8088947.0000, Val Loss: 5296394.0000\n",
      "Epoch [9697/50000], Train Loss: 8088948.0000, Val Loss: 5296400.5000\n",
      "Epoch [9698/50000], Train Loss: 8088948.0000, Val Loss: 5296408.5000\n",
      "Epoch [9699/50000], Train Loss: 8088947.5000, Val Loss: 5296416.0000\n",
      "Epoch [9700/50000], Train Loss: 8088947.0000, Val Loss: 5296423.5000\n",
      "Epoch [9701/50000], Train Loss: 8088947.5000, Val Loss: 5296431.5000\n",
      "Epoch [9702/50000], Train Loss: 8088947.5000, Val Loss: 5296439.0000\n",
      "Epoch [9703/50000], Train Loss: 8088947.5000, Val Loss: 5296446.5000\n",
      "Epoch [9704/50000], Train Loss: 8088947.5000, Val Loss: 5296453.5000\n",
      "Epoch [9705/50000], Train Loss: 8088946.5000, Val Loss: 5296461.5000\n",
      "Epoch [9706/50000], Train Loss: 8088946.5000, Val Loss: 5296468.5000\n",
      "Epoch [9707/50000], Train Loss: 8088946.5000, Val Loss: 5296476.5000\n",
      "Epoch [9708/50000], Train Loss: 8088947.5000, Val Loss: 5296483.0000\n",
      "Epoch [9709/50000], Train Loss: 8088947.0000, Val Loss: 5296491.0000\n",
      "Epoch [9710/50000], Train Loss: 8088947.5000, Val Loss: 5296497.0000\n",
      "Epoch [9711/50000], Train Loss: 8088947.0000, Val Loss: 5296504.5000\n",
      "Epoch [9712/50000], Train Loss: 8088947.0000, Val Loss: 5296512.5000\n",
      "Epoch [9713/50000], Train Loss: 8088946.5000, Val Loss: 5296518.5000\n",
      "Epoch [9714/50000], Train Loss: 8088947.5000, Val Loss: 5296526.0000\n",
      "Epoch [9715/50000], Train Loss: 8088947.5000, Val Loss: 5296534.0000\n",
      "Epoch [9716/50000], Train Loss: 8088947.0000, Val Loss: 5296541.5000\n",
      "Epoch [9717/50000], Train Loss: 8088947.0000, Val Loss: 5296547.5000\n",
      "Epoch [9718/50000], Train Loss: 8088946.0000, Val Loss: 5296554.0000\n",
      "Epoch [9719/50000], Train Loss: 8088946.5000, Val Loss: 5296562.0000\n",
      "Epoch [9720/50000], Train Loss: 8088946.5000, Val Loss: 5296569.5000\n",
      "Epoch [9721/50000], Train Loss: 8088946.0000, Val Loss: 5296576.0000\n",
      "Epoch [9722/50000], Train Loss: 8088946.5000, Val Loss: 5296584.0000\n",
      "Epoch [9723/50000], Train Loss: 8088947.5000, Val Loss: 5296591.0000\n",
      "Epoch [9724/50000], Train Loss: 8088946.0000, Val Loss: 5296597.5000\n",
      "Epoch [9725/50000], Train Loss: 8088946.0000, Val Loss: 5296605.0000\n",
      "Epoch [9726/50000], Train Loss: 8088946.5000, Val Loss: 5296611.0000\n",
      "Epoch [9727/50000], Train Loss: 8088946.5000, Val Loss: 5296618.0000\n",
      "Epoch [9728/50000], Train Loss: 8088947.0000, Val Loss: 5296625.5000\n",
      "Epoch [9729/50000], Train Loss: 8088946.0000, Val Loss: 5296632.0000\n",
      "Epoch [9730/50000], Train Loss: 8088946.5000, Val Loss: 5296639.0000\n",
      "Epoch [9731/50000], Train Loss: 8088946.5000, Val Loss: 5296646.0000\n",
      "Epoch [9732/50000], Train Loss: 8088946.5000, Val Loss: 5296652.5000\n",
      "Epoch [9733/50000], Train Loss: 8088946.5000, Val Loss: 5296659.0000\n",
      "Epoch [9734/50000], Train Loss: 8088946.0000, Val Loss: 5296666.0000\n",
      "Epoch [9735/50000], Train Loss: 8088947.0000, Val Loss: 5296673.0000\n",
      "Epoch [9736/50000], Train Loss: 8088946.5000, Val Loss: 5296679.5000\n",
      "Epoch [9737/50000], Train Loss: 8088946.0000, Val Loss: 5296686.0000\n",
      "Epoch [9738/50000], Train Loss: 8088946.5000, Val Loss: 5296693.5000\n",
      "Epoch [9739/50000], Train Loss: 8088946.0000, Val Loss: 5296699.5000\n",
      "Epoch [9740/50000], Train Loss: 8088946.0000, Val Loss: 5296707.5000\n",
      "Epoch [9741/50000], Train Loss: 8088946.0000, Val Loss: 5296714.0000\n",
      "Epoch [9742/50000], Train Loss: 8088946.0000, Val Loss: 5296720.5000\n",
      "Epoch [9743/50000], Train Loss: 8088946.0000, Val Loss: 5296727.0000\n",
      "Epoch [9744/50000], Train Loss: 8088945.5000, Val Loss: 5296733.5000\n",
      "Epoch [9745/50000], Train Loss: 8088946.0000, Val Loss: 5296740.5000\n",
      "Epoch [9746/50000], Train Loss: 8088946.5000, Val Loss: 5296746.0000\n",
      "Epoch [9747/50000], Train Loss: 8088946.0000, Val Loss: 5296753.5000\n",
      "Epoch [9748/50000], Train Loss: 8088946.5000, Val Loss: 5296759.5000\n",
      "Epoch [9749/50000], Train Loss: 8088945.5000, Val Loss: 5296766.0000\n",
      "Epoch [9750/50000], Train Loss: 8088946.0000, Val Loss: 5296772.5000\n",
      "Epoch [9751/50000], Train Loss: 8088946.0000, Val Loss: 5296778.0000\n",
      "Epoch [9752/50000], Train Loss: 8088945.5000, Val Loss: 5296785.0000\n",
      "Epoch [9753/50000], Train Loss: 8088945.5000, Val Loss: 5296791.5000\n",
      "Epoch [9754/50000], Train Loss: 8088946.0000, Val Loss: 5296798.5000\n",
      "Epoch [9755/50000], Train Loss: 8088946.0000, Val Loss: 5296804.5000\n",
      "Epoch [9756/50000], Train Loss: 8088946.0000, Val Loss: 5296810.5000\n",
      "Epoch [9757/50000], Train Loss: 8088946.5000, Val Loss: 5296817.0000\n",
      "Epoch [9758/50000], Train Loss: 8088945.5000, Val Loss: 5296824.0000\n",
      "Epoch [9759/50000], Train Loss: 8088945.0000, Val Loss: 5296831.0000\n",
      "Epoch [9760/50000], Train Loss: 8088945.5000, Val Loss: 5296836.5000\n",
      "Epoch [9761/50000], Train Loss: 8088945.5000, Val Loss: 5296843.5000\n",
      "Epoch [9762/50000], Train Loss: 8088945.5000, Val Loss: 5296850.0000\n",
      "Epoch [9763/50000], Train Loss: 8088945.5000, Val Loss: 5296855.5000\n",
      "Epoch [9764/50000], Train Loss: 8088945.5000, Val Loss: 5296862.0000\n",
      "Epoch [9765/50000], Train Loss: 8088945.0000, Val Loss: 5296867.5000\n",
      "Epoch [9766/50000], Train Loss: 8088945.5000, Val Loss: 5296873.5000\n",
      "Epoch [9767/50000], Train Loss: 8088945.5000, Val Loss: 5296880.0000\n",
      "Epoch [9768/50000], Train Loss: 8088946.0000, Val Loss: 5296886.0000\n",
      "Epoch [9769/50000], Train Loss: 8088945.5000, Val Loss: 5296891.5000\n",
      "Epoch [9770/50000], Train Loss: 8088945.5000, Val Loss: 5296898.0000\n",
      "Epoch [9771/50000], Train Loss: 8088945.0000, Val Loss: 5296904.5000\n",
      "Epoch [9772/50000], Train Loss: 8088945.5000, Val Loss: 5296911.0000\n",
      "Epoch [9773/50000], Train Loss: 8088945.0000, Val Loss: 5296916.5000\n",
      "Epoch [9774/50000], Train Loss: 8088945.0000, Val Loss: 5296922.5000\n",
      "Epoch [9775/50000], Train Loss: 8088945.5000, Val Loss: 5296928.5000\n",
      "Epoch [9776/50000], Train Loss: 8088945.5000, Val Loss: 5296934.5000\n",
      "Epoch [9777/50000], Train Loss: 8088945.5000, Val Loss: 5296941.0000\n",
      "Epoch [9778/50000], Train Loss: 8088945.5000, Val Loss: 5296946.5000\n",
      "Epoch [9779/50000], Train Loss: 8088945.5000, Val Loss: 5296953.5000\n",
      "Epoch [9780/50000], Train Loss: 8088945.5000, Val Loss: 5296959.0000\n",
      "Epoch [9781/50000], Train Loss: 8088945.0000, Val Loss: 5296965.0000\n",
      "Epoch [9782/50000], Train Loss: 8088945.0000, Val Loss: 5296970.5000\n",
      "Epoch [9783/50000], Train Loss: 8088945.0000, Val Loss: 5296976.5000\n",
      "Epoch [9784/50000], Train Loss: 8088945.5000, Val Loss: 5296982.5000\n",
      "Epoch [9785/50000], Train Loss: 8088945.5000, Val Loss: 5296988.5000\n",
      "Epoch [9786/50000], Train Loss: 8088945.0000, Val Loss: 5296993.5000\n",
      "Epoch [9787/50000], Train Loss: 8088945.5000, Val Loss: 5296999.0000\n",
      "Epoch [9788/50000], Train Loss: 8088945.0000, Val Loss: 5297005.5000\n",
      "Epoch [9789/50000], Train Loss: 8088944.5000, Val Loss: 5297010.0000\n",
      "Epoch [9790/50000], Train Loss: 8088944.5000, Val Loss: 5297016.5000\n",
      "Epoch [9791/50000], Train Loss: 8088944.5000, Val Loss: 5297022.0000\n",
      "Epoch [9792/50000], Train Loss: 8088945.0000, Val Loss: 5297027.5000\n",
      "Epoch [9793/50000], Train Loss: 8088945.5000, Val Loss: 5297034.0000\n",
      "Epoch [9794/50000], Train Loss: 8088945.0000, Val Loss: 5297040.0000\n",
      "Epoch [9795/50000], Train Loss: 8088945.0000, Val Loss: 5297045.5000\n",
      "Epoch [9796/50000], Train Loss: 8088945.5000, Val Loss: 5297051.0000\n",
      "Epoch [9797/50000], Train Loss: 8088944.5000, Val Loss: 5297056.5000\n",
      "Epoch [9798/50000], Train Loss: 8088943.5000, Val Loss: 5297062.0000\n",
      "Epoch [9799/50000], Train Loss: 8088944.5000, Val Loss: 5297069.0000\n",
      "Epoch [9800/50000], Train Loss: 8088945.5000, Val Loss: 5297074.0000\n",
      "Epoch [9801/50000], Train Loss: 8088944.5000, Val Loss: 5297080.0000\n",
      "Epoch [9802/50000], Train Loss: 8088945.5000, Val Loss: 5297085.5000\n",
      "Epoch [9803/50000], Train Loss: 8088945.0000, Val Loss: 5297090.0000\n",
      "Epoch [9804/50000], Train Loss: 8088944.5000, Val Loss: 5297096.5000\n",
      "Epoch [9805/50000], Train Loss: 8088945.5000, Val Loss: 5297101.0000\n",
      "Epoch [9806/50000], Train Loss: 8088944.5000, Val Loss: 5297106.0000\n",
      "Epoch [9807/50000], Train Loss: 8088944.5000, Val Loss: 5297112.0000\n",
      "Epoch [9808/50000], Train Loss: 8088944.5000, Val Loss: 5297117.5000\n",
      "Epoch [9809/50000], Train Loss: 8088945.0000, Val Loss: 5297123.5000\n",
      "Epoch [9810/50000], Train Loss: 8088945.0000, Val Loss: 5297128.5000\n",
      "Epoch [9811/50000], Train Loss: 8088944.5000, Val Loss: 5297134.0000\n",
      "Epoch [9812/50000], Train Loss: 8088943.5000, Val Loss: 5297139.0000\n",
      "Epoch [9813/50000], Train Loss: 8088944.5000, Val Loss: 5297144.5000\n",
      "Epoch [9814/50000], Train Loss: 8088944.5000, Val Loss: 5297150.5000\n",
      "Epoch [9815/50000], Train Loss: 8088944.5000, Val Loss: 5297155.0000\n",
      "Epoch [9816/50000], Train Loss: 8088944.5000, Val Loss: 5297160.5000\n",
      "Epoch [9817/50000], Train Loss: 8088945.0000, Val Loss: 5297166.5000\n",
      "Epoch [9818/50000], Train Loss: 8088944.5000, Val Loss: 5297171.0000\n",
      "Epoch [9819/50000], Train Loss: 8088945.0000, Val Loss: 5297176.5000\n",
      "Epoch [9820/50000], Train Loss: 8088944.5000, Val Loss: 5297182.0000\n",
      "Epoch [9821/50000], Train Loss: 8088944.5000, Val Loss: 5297187.5000\n",
      "Epoch [9822/50000], Train Loss: 8088945.0000, Val Loss: 5297193.0000\n",
      "Epoch [9823/50000], Train Loss: 8088943.5000, Val Loss: 5297198.5000\n",
      "Epoch [9824/50000], Train Loss: 8088943.5000, Val Loss: 5297203.5000\n",
      "Epoch [9825/50000], Train Loss: 8088943.0000, Val Loss: 5297208.5000\n",
      "Epoch [9826/50000], Train Loss: 8088944.5000, Val Loss: 5297214.0000\n",
      "Epoch [9827/50000], Train Loss: 8088944.5000, Val Loss: 5297218.5000\n",
      "Epoch [9828/50000], Train Loss: 8088945.0000, Val Loss: 5297223.0000\n",
      "Epoch [9829/50000], Train Loss: 8088944.5000, Val Loss: 5297229.0000\n",
      "Epoch [9830/50000], Train Loss: 8088944.5000, Val Loss: 5297233.5000\n",
      "Epoch [9831/50000], Train Loss: 8088943.5000, Val Loss: 5297238.5000\n",
      "Epoch [9832/50000], Train Loss: 8088944.5000, Val Loss: 5297243.0000\n",
      "Epoch [9833/50000], Train Loss: 8088945.0000, Val Loss: 5297248.5000\n",
      "Epoch [9834/50000], Train Loss: 8088944.5000, Val Loss: 5297254.0000\n",
      "Epoch [9835/50000], Train Loss: 8088943.5000, Val Loss: 5297259.0000\n",
      "Epoch [9836/50000], Train Loss: 8088945.0000, Val Loss: 5297263.0000\n",
      "Epoch [9837/50000], Train Loss: 8088944.5000, Val Loss: 5297269.0000\n",
      "Epoch [9838/50000], Train Loss: 8088944.5000, Val Loss: 5297273.5000\n",
      "Epoch [9839/50000], Train Loss: 8088944.5000, Val Loss: 5297278.5000\n",
      "Epoch [9840/50000], Train Loss: 8088943.5000, Val Loss: 5297283.5000\n",
      "Epoch [9841/50000], Train Loss: 8088943.0000, Val Loss: 5297288.5000\n",
      "Epoch [9842/50000], Train Loss: 8088944.5000, Val Loss: 5297293.5000\n",
      "Epoch [9843/50000], Train Loss: 8088943.5000, Val Loss: 5297299.0000\n",
      "Epoch [9844/50000], Train Loss: 8088944.5000, Val Loss: 5297304.0000\n",
      "Epoch [9845/50000], Train Loss: 8088944.5000, Val Loss: 5297309.0000\n",
      "Epoch [9846/50000], Train Loss: 8088943.0000, Val Loss: 5297314.0000\n",
      "Epoch [9847/50000], Train Loss: 8088943.0000, Val Loss: 5297319.5000\n",
      "Epoch [9848/50000], Train Loss: 8088943.5000, Val Loss: 5297323.5000\n",
      "Epoch [9849/50000], Train Loss: 8088943.0000, Val Loss: 5297328.0000\n",
      "Epoch [9850/50000], Train Loss: 8088943.5000, Val Loss: 5297333.5000\n",
      "Epoch [9851/50000], Train Loss: 8088943.0000, Val Loss: 5297337.0000\n",
      "Epoch [9852/50000], Train Loss: 8088943.5000, Val Loss: 5297342.0000\n",
      "Epoch [9853/50000], Train Loss: 8088943.5000, Val Loss: 5297346.0000\n",
      "Epoch [9854/50000], Train Loss: 8088943.5000, Val Loss: 5297352.0000\n",
      "Epoch [9855/50000], Train Loss: 8088943.0000, Val Loss: 5297355.5000\n",
      "Epoch [9856/50000], Train Loss: 8088944.5000, Val Loss: 5297360.5000\n",
      "Epoch [9857/50000], Train Loss: 8088943.5000, Val Loss: 5297365.0000\n",
      "Epoch [9858/50000], Train Loss: 8088944.5000, Val Loss: 5297370.5000\n",
      "Epoch [9859/50000], Train Loss: 8088943.5000, Val Loss: 5297375.0000\n",
      "Epoch [9860/50000], Train Loss: 8088943.5000, Val Loss: 5297378.5000\n",
      "Epoch [9861/50000], Train Loss: 8088943.0000, Val Loss: 5297383.5000\n",
      "Epoch [9862/50000], Train Loss: 8088943.5000, Val Loss: 5297389.0000\n",
      "Epoch [9863/50000], Train Loss: 8088943.5000, Val Loss: 5297393.5000\n",
      "Epoch [9864/50000], Train Loss: 8088943.0000, Val Loss: 5297398.5000\n",
      "Epoch [9865/50000], Train Loss: 8088944.5000, Val Loss: 5297403.0000\n",
      "Epoch [9866/50000], Train Loss: 8088943.5000, Val Loss: 5297407.5000\n",
      "Epoch [9867/50000], Train Loss: 8088943.0000, Val Loss: 5297412.5000\n",
      "Epoch [9868/50000], Train Loss: 8088943.0000, Val Loss: 5297417.0000\n",
      "Epoch [9869/50000], Train Loss: 8088943.5000, Val Loss: 5297421.5000\n",
      "Epoch [9870/50000], Train Loss: 8088943.5000, Val Loss: 5297426.5000\n",
      "Epoch [9871/50000], Train Loss: 8088943.0000, Val Loss: 5297430.5000\n",
      "Epoch [9872/50000], Train Loss: 8088942.5000, Val Loss: 5297435.0000\n",
      "Epoch [9873/50000], Train Loss: 8088943.0000, Val Loss: 5297438.5000\n",
      "Epoch [9874/50000], Train Loss: 8088943.5000, Val Loss: 5297443.0000\n",
      "Epoch [9875/50000], Train Loss: 8088943.5000, Val Loss: 5297447.5000\n",
      "Epoch [9876/50000], Train Loss: 8088943.5000, Val Loss: 5297452.5000\n",
      "Epoch [9877/50000], Train Loss: 8088944.5000, Val Loss: 5297456.5000\n",
      "Epoch [9878/50000], Train Loss: 8088943.0000, Val Loss: 5297461.0000\n",
      "Epoch [9879/50000], Train Loss: 8088943.0000, Val Loss: 5297465.0000\n",
      "Epoch [9880/50000], Train Loss: 8088944.5000, Val Loss: 5297469.5000\n",
      "Epoch [9881/50000], Train Loss: 8088943.0000, Val Loss: 5297474.0000\n",
      "Epoch [9882/50000], Train Loss: 8088943.0000, Val Loss: 5297478.0000\n",
      "Epoch [9883/50000], Train Loss: 8088943.5000, Val Loss: 5297482.0000\n",
      "Epoch [9884/50000], Train Loss: 8088943.5000, Val Loss: 5297487.0000\n",
      "Epoch [9885/50000], Train Loss: 8088943.0000, Val Loss: 5297491.0000\n",
      "Epoch [9886/50000], Train Loss: 8088943.0000, Val Loss: 5297495.5000\n",
      "Epoch [9887/50000], Train Loss: 8088943.5000, Val Loss: 5297499.5000\n",
      "Epoch [9888/50000], Train Loss: 8088942.5000, Val Loss: 5297503.5000\n",
      "Epoch [9889/50000], Train Loss: 8088944.5000, Val Loss: 5297508.5000\n",
      "Epoch [9890/50000], Train Loss: 8088943.0000, Val Loss: 5297512.0000\n",
      "Epoch [9891/50000], Train Loss: 8088943.5000, Val Loss: 5297516.5000\n",
      "Epoch [9892/50000], Train Loss: 8088943.5000, Val Loss: 5297521.5000\n",
      "Epoch [9893/50000], Train Loss: 8088943.5000, Val Loss: 5297525.5000\n",
      "Epoch [9894/50000], Train Loss: 8088943.0000, Val Loss: 5297529.0000\n",
      "Epoch [9895/50000], Train Loss: 8088943.5000, Val Loss: 5297534.5000\n",
      "Epoch [9896/50000], Train Loss: 8088944.5000, Val Loss: 5297538.0000\n",
      "Epoch [9897/50000], Train Loss: 8088943.5000, Val Loss: 5297543.0000\n",
      "Epoch [9898/50000], Train Loss: 8088943.5000, Val Loss: 5297545.5000\n",
      "Epoch [9899/50000], Train Loss: 8088943.0000, Val Loss: 5297550.5000\n",
      "Epoch [9900/50000], Train Loss: 8088943.0000, Val Loss: 5297553.5000\n",
      "Epoch [9901/50000], Train Loss: 8088942.5000, Val Loss: 5297557.5000\n",
      "Epoch [9902/50000], Train Loss: 8088943.0000, Val Loss: 5297562.0000\n",
      "Epoch [9903/50000], Train Loss: 8088943.0000, Val Loss: 5297566.0000\n",
      "Epoch [9904/50000], Train Loss: 8088942.5000, Val Loss: 5297570.0000\n",
      "Epoch [9905/50000], Train Loss: 8088943.0000, Val Loss: 5297573.5000\n",
      "Epoch [9906/50000], Train Loss: 8088943.5000, Val Loss: 5297578.0000\n",
      "Epoch [9907/50000], Train Loss: 8088943.5000, Val Loss: 5297582.5000\n",
      "Epoch [9908/50000], Train Loss: 8088943.0000, Val Loss: 5297586.0000\n",
      "Epoch [9909/50000], Train Loss: 8088943.5000, Val Loss: 5297590.0000\n",
      "Epoch [9910/50000], Train Loss: 8088943.5000, Val Loss: 5297593.5000\n",
      "Epoch [9911/50000], Train Loss: 8088943.0000, Val Loss: 5297597.5000\n",
      "Epoch [9912/50000], Train Loss: 8088943.0000, Val Loss: 5297601.5000\n",
      "Epoch [9913/50000], Train Loss: 8088943.0000, Val Loss: 5297605.0000\n",
      "Epoch [9914/50000], Train Loss: 8088943.0000, Val Loss: 5297609.0000\n",
      "Epoch [9915/50000], Train Loss: 8088943.0000, Val Loss: 5297613.0000\n",
      "Epoch [9916/50000], Train Loss: 8088943.0000, Val Loss: 5297617.0000\n",
      "Epoch [9917/50000], Train Loss: 8088942.5000, Val Loss: 5297621.5000\n",
      "Epoch [9918/50000], Train Loss: 8088942.5000, Val Loss: 5297625.5000\n",
      "Epoch [9919/50000], Train Loss: 8088943.0000, Val Loss: 5297629.0000\n",
      "Epoch [9920/50000], Train Loss: 8088943.5000, Val Loss: 5297633.0000\n",
      "Epoch [9921/50000], Train Loss: 8088943.5000, Val Loss: 5297637.0000\n",
      "Epoch [9922/50000], Train Loss: 8088942.5000, Val Loss: 5297641.5000\n",
      "Epoch [9923/50000], Train Loss: 8088943.5000, Val Loss: 5297645.0000\n",
      "Epoch [9924/50000], Train Loss: 8088942.5000, Val Loss: 5297649.0000\n",
      "Epoch [9925/50000], Train Loss: 8088943.0000, Val Loss: 5297652.5000\n",
      "Epoch [9926/50000], Train Loss: 8088942.5000, Val Loss: 5297655.5000\n",
      "Epoch [9927/50000], Train Loss: 8088942.5000, Val Loss: 5297659.5000\n",
      "Epoch [9928/50000], Train Loss: 8088943.0000, Val Loss: 5297663.0000\n",
      "Epoch [9929/50000], Train Loss: 8088943.0000, Val Loss: 5297666.5000\n",
      "Epoch [9930/50000], Train Loss: 8088943.5000, Val Loss: 5297670.5000\n",
      "Epoch [9931/50000], Train Loss: 8088943.0000, Val Loss: 5297674.0000\n",
      "Epoch [9932/50000], Train Loss: 8088943.0000, Val Loss: 5297678.0000\n",
      "Epoch [9933/50000], Train Loss: 8088943.0000, Val Loss: 5297681.5000\n",
      "Epoch [9934/50000], Train Loss: 8088943.0000, Val Loss: 5297685.5000\n",
      "Epoch [9935/50000], Train Loss: 8088943.5000, Val Loss: 5297688.0000\n",
      "Epoch [9936/50000], Train Loss: 8088943.0000, Val Loss: 5297691.5000\n",
      "Epoch [9937/50000], Train Loss: 8088943.0000, Val Loss: 5297695.5000\n",
      "Epoch [9938/50000], Train Loss: 8088943.0000, Val Loss: 5297699.0000\n",
      "Epoch [9939/50000], Train Loss: 8088942.5000, Val Loss: 5297703.5000\n",
      "Epoch [9940/50000], Train Loss: 8088943.0000, Val Loss: 5297706.5000\n",
      "Epoch [9941/50000], Train Loss: 8088943.0000, Val Loss: 5297709.5000\n",
      "Epoch [9942/50000], Train Loss: 8088942.5000, Val Loss: 5297713.0000\n",
      "Epoch [9943/50000], Train Loss: 8088942.5000, Val Loss: 5297717.5000\n",
      "Epoch [9944/50000], Train Loss: 8088943.0000, Val Loss: 5297720.5000\n",
      "Epoch [9945/50000], Train Loss: 8088943.5000, Val Loss: 5297725.0000\n",
      "Epoch [9946/50000], Train Loss: 8088942.5000, Val Loss: 5297727.0000\n",
      "Epoch [9947/50000], Train Loss: 8088943.0000, Val Loss: 5297731.0000\n",
      "Epoch [9948/50000], Train Loss: 8088943.0000, Val Loss: 5297735.0000\n",
      "Epoch [9949/50000], Train Loss: 8088943.0000, Val Loss: 5297738.5000\n",
      "Epoch [9950/50000], Train Loss: 8088943.0000, Val Loss: 5297742.5000\n",
      "Epoch [9951/50000], Train Loss: 8088942.5000, Val Loss: 5297745.0000\n",
      "Epoch [9952/50000], Train Loss: 8088942.5000, Val Loss: 5297750.0000\n",
      "Epoch [9953/50000], Train Loss: 8088943.0000, Val Loss: 5297752.5000\n",
      "Epoch [9954/50000], Train Loss: 8088942.5000, Val Loss: 5297757.0000\n",
      "Epoch [9955/50000], Train Loss: 8088942.5000, Val Loss: 5297760.0000\n",
      "Epoch [9956/50000], Train Loss: 8088943.0000, Val Loss: 5297763.0000\n",
      "Epoch [9957/50000], Train Loss: 8088943.5000, Val Loss: 5297767.5000\n",
      "Epoch [9958/50000], Train Loss: 8088943.0000, Val Loss: 5297770.0000\n",
      "Epoch [9959/50000], Train Loss: 8088943.0000, Val Loss: 5297773.0000\n",
      "Epoch [9960/50000], Train Loss: 8088942.5000, Val Loss: 5297776.5000\n",
      "Epoch [9961/50000], Train Loss: 8088943.0000, Val Loss: 5297779.5000\n",
      "Epoch [9962/50000], Train Loss: 8088943.0000, Val Loss: 5297783.5000\n",
      "Epoch [9963/50000], Train Loss: 8088942.5000, Val Loss: 5297785.5000\n",
      "Epoch [9964/50000], Train Loss: 8088942.5000, Val Loss: 5297790.0000\n",
      "Epoch [9965/50000], Train Loss: 8088942.5000, Val Loss: 5297793.0000\n",
      "Epoch [9966/50000], Train Loss: 8088943.0000, Val Loss: 5297795.5000\n",
      "Epoch [9967/50000], Train Loss: 8088942.5000, Val Loss: 5297799.0000\n",
      "Epoch [9968/50000], Train Loss: 8088943.0000, Val Loss: 5297802.0000\n",
      "Epoch [9969/50000], Train Loss: 8088942.0000, Val Loss: 5297805.5000\n",
      "Epoch [9970/50000], Train Loss: 8088942.0000, Val Loss: 5297809.0000\n",
      "Epoch [9971/50000], Train Loss: 8088942.5000, Val Loss: 5297812.5000\n",
      "Epoch [9972/50000], Train Loss: 8088943.0000, Val Loss: 5297815.5000\n",
      "Epoch [9973/50000], Train Loss: 8088942.5000, Val Loss: 5297818.0000\n",
      "Epoch [9974/50000], Train Loss: 8088943.0000, Val Loss: 5297822.5000\n",
      "Epoch [9975/50000], Train Loss: 8088942.5000, Val Loss: 5297824.5000\n",
      "Epoch [9976/50000], Train Loss: 8088943.0000, Val Loss: 5297829.0000\n",
      "Epoch [9977/50000], Train Loss: 8088943.0000, Val Loss: 5297831.5000\n",
      "Epoch [9978/50000], Train Loss: 8088942.5000, Val Loss: 5297834.5000\n",
      "Epoch [9979/50000], Train Loss: 8088942.0000, Val Loss: 5297838.0000\n",
      "Epoch [9980/50000], Train Loss: 8088942.0000, Val Loss: 5297840.5000\n",
      "Epoch [9981/50000], Train Loss: 8088942.0000, Val Loss: 5297844.5000\n",
      "Epoch [9982/50000], Train Loss: 8088942.5000, Val Loss: 5297847.0000\n",
      "Epoch [9983/50000], Train Loss: 8088942.5000, Val Loss: 5297850.5000\n",
      "Epoch [9984/50000], Train Loss: 8088943.0000, Val Loss: 5297854.5000\n",
      "Epoch [9985/50000], Train Loss: 8088942.5000, Val Loss: 5297857.0000\n",
      "Epoch [9986/50000], Train Loss: 8088943.0000, Val Loss: 5297860.5000\n",
      "Epoch [9987/50000], Train Loss: 8088942.0000, Val Loss: 5297863.5000\n",
      "Epoch [9988/50000], Train Loss: 8088943.0000, Val Loss: 5297867.0000\n",
      "Epoch [9989/50000], Train Loss: 8088943.0000, Val Loss: 5297870.0000\n",
      "Epoch [9990/50000], Train Loss: 8088942.5000, Val Loss: 5297873.0000\n",
      "Epoch [9991/50000], Train Loss: 8088941.5000, Val Loss: 5297875.5000\n",
      "Epoch [9992/50000], Train Loss: 8088942.5000, Val Loss: 5297878.5000\n",
      "Epoch [9993/50000], Train Loss: 8088943.0000, Val Loss: 5297881.0000\n",
      "Epoch [9994/50000], Train Loss: 8088943.0000, Val Loss: 5297884.5000\n",
      "Epoch [9995/50000], Train Loss: 8088942.0000, Val Loss: 5297887.0000\n",
      "Epoch [9996/50000], Train Loss: 8088942.5000, Val Loss: 5297890.0000\n",
      "Epoch [9997/50000], Train Loss: 8088942.5000, Val Loss: 5297893.0000\n",
      "Epoch [9998/50000], Train Loss: 8088942.5000, Val Loss: 5297895.5000\n",
      "Epoch [9999/50000], Train Loss: 8088943.0000, Val Loss: 5297898.5000\n",
      "Epoch [10000/50000], Train Loss: 8088942.5000, Val Loss: 5297902.0000\n",
      "Epoch [10001/50000], Train Loss: 8088942.0000, Val Loss: 5297904.5000\n",
      "Epoch [10002/50000], Train Loss: 8088942.0000, Val Loss: 5297907.0000\n",
      "Epoch [10003/50000], Train Loss: 8088942.5000, Val Loss: 5297910.5000\n",
      "Epoch [10004/50000], Train Loss: 8088942.5000, Val Loss: 5297912.5000\n",
      "Epoch [10005/50000], Train Loss: 8088942.5000, Val Loss: 5297915.5000\n",
      "Epoch [10006/50000], Train Loss: 8088942.5000, Val Loss: 5297919.0000\n",
      "Epoch [10007/50000], Train Loss: 8088942.5000, Val Loss: 5297922.0000\n",
      "Epoch [10008/50000], Train Loss: 8088943.0000, Val Loss: 5297925.0000\n",
      "Epoch [10009/50000], Train Loss: 8088942.5000, Val Loss: 5297927.5000\n",
      "Epoch [10010/50000], Train Loss: 8088942.5000, Val Loss: 5297930.5000\n",
      "Epoch [10011/50000], Train Loss: 8088942.5000, Val Loss: 5297933.5000\n",
      "Epoch [10012/50000], Train Loss: 8088942.5000, Val Loss: 5297936.0000\n",
      "Epoch [10013/50000], Train Loss: 8088942.5000, Val Loss: 5297939.0000\n",
      "Epoch [10014/50000], Train Loss: 8088942.0000, Val Loss: 5297941.5000\n",
      "Epoch [10015/50000], Train Loss: 8088942.5000, Val Loss: 5297944.5000\n",
      "Epoch [10016/50000], Train Loss: 8088942.5000, Val Loss: 5297947.0000\n",
      "Epoch [10017/50000], Train Loss: 8088942.0000, Val Loss: 5297950.0000\n",
      "Epoch [10018/50000], Train Loss: 8088942.5000, Val Loss: 5297953.5000\n",
      "Epoch [10019/50000], Train Loss: 8088942.0000, Val Loss: 5297955.5000\n",
      "Epoch [10020/50000], Train Loss: 8088943.0000, Val Loss: 5297959.5000\n",
      "Epoch [10021/50000], Train Loss: 8088942.5000, Val Loss: 5297961.5000\n",
      "Epoch [10022/50000], Train Loss: 8088943.0000, Val Loss: 5297965.0000\n",
      "Epoch [10023/50000], Train Loss: 8088942.0000, Val Loss: 5297967.5000\n",
      "Epoch [10024/50000], Train Loss: 8088942.0000, Val Loss: 5297970.5000\n",
      "Epoch [10025/50000], Train Loss: 8088943.0000, Val Loss: 5297973.0000\n",
      "Epoch [10026/50000], Train Loss: 8088942.5000, Val Loss: 5297975.5000\n",
      "Epoch [10027/50000], Train Loss: 8088942.0000, Val Loss: 5297978.0000\n",
      "Epoch [10028/50000], Train Loss: 8088942.0000, Val Loss: 5297980.5000\n",
      "Epoch [10029/50000], Train Loss: 8088942.0000, Val Loss: 5297983.0000\n",
      "Epoch [10030/50000], Train Loss: 8088942.5000, Val Loss: 5297985.5000\n",
      "Epoch [10031/50000], Train Loss: 8088942.5000, Val Loss: 5297988.5000\n",
      "Epoch [10032/50000], Train Loss: 8088942.5000, Val Loss: 5297990.5000\n",
      "Epoch [10033/50000], Train Loss: 8088942.0000, Val Loss: 5297993.0000\n",
      "Epoch [10034/50000], Train Loss: 8088942.5000, Val Loss: 5297995.5000\n",
      "Epoch [10035/50000], Train Loss: 8088942.5000, Val Loss: 5297999.0000\n",
      "Epoch [10036/50000], Train Loss: 8088942.5000, Val Loss: 5298001.0000\n",
      "Epoch [10037/50000], Train Loss: 8088942.0000, Val Loss: 5298003.0000\n",
      "Epoch [10038/50000], Train Loss: 8088943.0000, Val Loss: 5298006.5000\n",
      "Epoch [10039/50000], Train Loss: 8088942.5000, Val Loss: 5298009.0000\n",
      "Epoch [10040/50000], Train Loss: 8088942.0000, Val Loss: 5298010.5000\n",
      "Epoch [10041/50000], Train Loss: 8088942.5000, Val Loss: 5298014.0000\n",
      "Epoch [10042/50000], Train Loss: 8088942.5000, Val Loss: 5298016.5000\n",
      "Epoch [10043/50000], Train Loss: 8088942.0000, Val Loss: 5298018.0000\n",
      "Epoch [10044/50000], Train Loss: 8088942.5000, Val Loss: 5298021.5000\n",
      "Epoch [10045/50000], Train Loss: 8088942.0000, Val Loss: 5298024.0000\n",
      "Epoch [10046/50000], Train Loss: 8088942.0000, Val Loss: 5298026.0000\n",
      "Epoch [10047/50000], Train Loss: 8088942.5000, Val Loss: 5298029.0000\n",
      "Epoch [10048/50000], Train Loss: 8088942.5000, Val Loss: 5298031.0000\n",
      "Epoch [10049/50000], Train Loss: 8088942.0000, Val Loss: 5298033.5000\n",
      "Epoch [10050/50000], Train Loss: 8088942.0000, Val Loss: 5298036.5000\n",
      "Epoch [10051/50000], Train Loss: 8088942.5000, Val Loss: 5298039.0000\n",
      "Epoch [10052/50000], Train Loss: 8088942.5000, Val Loss: 5298041.5000\n",
      "Epoch [10053/50000], Train Loss: 8088942.0000, Val Loss: 5298044.5000\n",
      "Epoch [10054/50000], Train Loss: 8088942.5000, Val Loss: 5298046.5000\n",
      "Epoch [10055/50000], Train Loss: 8088942.5000, Val Loss: 5298049.0000\n",
      "Epoch [10056/50000], Train Loss: 8088942.5000, Val Loss: 5298052.5000\n",
      "Epoch [10057/50000], Train Loss: 8088942.0000, Val Loss: 5298054.0000\n",
      "Epoch [10058/50000], Train Loss: 8088943.0000, Val Loss: 5298056.5000\n",
      "Epoch [10059/50000], Train Loss: 8088942.5000, Val Loss: 5298059.0000\n",
      "Epoch [10060/50000], Train Loss: 8088942.5000, Val Loss: 5298061.5000\n",
      "Epoch [10061/50000], Train Loss: 8088942.0000, Val Loss: 5298063.5000\n",
      "Epoch [10062/50000], Train Loss: 8088941.5000, Val Loss: 5298066.5000\n",
      "Epoch [10063/50000], Train Loss: 8088942.0000, Val Loss: 5298069.0000\n",
      "Epoch [10064/50000], Train Loss: 8088942.5000, Val Loss: 5298071.0000\n",
      "Epoch [10065/50000], Train Loss: 8088942.5000, Val Loss: 5298073.5000\n",
      "Epoch [10066/50000], Train Loss: 8088942.0000, Val Loss: 5298075.5000\n",
      "Epoch [10067/50000], Train Loss: 8088942.0000, Val Loss: 5298078.0000\n",
      "Epoch [10068/50000], Train Loss: 8088942.0000, Val Loss: 5298080.0000\n",
      "Epoch [10069/50000], Train Loss: 8088942.0000, Val Loss: 5298082.0000\n",
      "Epoch [10070/50000], Train Loss: 8088942.5000, Val Loss: 5298085.0000\n",
      "Epoch [10071/50000], Train Loss: 8088943.0000, Val Loss: 5298086.5000\n",
      "Epoch [10072/50000], Train Loss: 8088942.5000, Val Loss: 5298089.0000\n",
      "Epoch [10073/50000], Train Loss: 8088942.5000, Val Loss: 5298090.5000\n",
      "Epoch [10074/50000], Train Loss: 8088942.5000, Val Loss: 5298093.0000\n",
      "Epoch [10075/50000], Train Loss: 8088942.5000, Val Loss: 5298095.5000\n",
      "Epoch [10076/50000], Train Loss: 8088942.5000, Val Loss: 5298097.5000\n",
      "Epoch [10077/50000], Train Loss: 8088942.5000, Val Loss: 5298099.5000\n",
      "Epoch [10078/50000], Train Loss: 8088942.5000, Val Loss: 5298101.5000\n",
      "Epoch [10079/50000], Train Loss: 8088942.5000, Val Loss: 5298104.0000\n",
      "Epoch [10080/50000], Train Loss: 8088942.5000, Val Loss: 5298106.0000\n",
      "Epoch [10081/50000], Train Loss: 8088942.5000, Val Loss: 5298107.5000\n",
      "Epoch [10082/50000], Train Loss: 8088942.5000, Val Loss: 5298110.0000\n",
      "Epoch [10083/50000], Train Loss: 8088942.0000, Val Loss: 5298112.0000\n",
      "Epoch [10084/50000], Train Loss: 8088942.5000, Val Loss: 5298114.5000\n",
      "Epoch [10085/50000], Train Loss: 8088942.0000, Val Loss: 5298116.5000\n",
      "Epoch [10086/50000], Train Loss: 8088942.0000, Val Loss: 5298118.5000\n",
      "Epoch [10087/50000], Train Loss: 8088942.0000, Val Loss: 5298121.0000\n",
      "Epoch [10088/50000], Train Loss: 8088942.0000, Val Loss: 5298123.0000\n",
      "Epoch [10089/50000], Train Loss: 8088942.0000, Val Loss: 5298125.5000\n",
      "Epoch [10090/50000], Train Loss: 8088942.5000, Val Loss: 5298127.0000\n",
      "Epoch [10091/50000], Train Loss: 8088942.5000, Val Loss: 5298130.0000\n",
      "Epoch [10092/50000], Train Loss: 8088942.5000, Val Loss: 5298132.5000\n",
      "Epoch [10093/50000], Train Loss: 8088942.0000, Val Loss: 5298133.5000\n",
      "Epoch [10094/50000], Train Loss: 8088942.5000, Val Loss: 5298136.0000\n",
      "Epoch [10095/50000], Train Loss: 8088942.5000, Val Loss: 5298138.0000\n",
      "Epoch [10096/50000], Train Loss: 8088942.5000, Val Loss: 5298141.0000\n",
      "Epoch [10097/50000], Train Loss: 8088942.0000, Val Loss: 5298143.0000\n",
      "Epoch [10098/50000], Train Loss: 8088942.5000, Val Loss: 5298145.5000\n",
      "Epoch [10099/50000], Train Loss: 8088942.0000, Val Loss: 5298146.5000\n",
      "Epoch [10100/50000], Train Loss: 8088942.5000, Val Loss: 5298149.5000\n",
      "Epoch [10101/50000], Train Loss: 8088942.5000, Val Loss: 5298151.0000\n",
      "Epoch [10102/50000], Train Loss: 8088942.0000, Val Loss: 5298153.0000\n",
      "Epoch [10103/50000], Train Loss: 8088942.5000, Val Loss: 5298155.5000\n",
      "Epoch [10104/50000], Train Loss: 8088942.5000, Val Loss: 5298157.5000\n",
      "Epoch [10105/50000], Train Loss: 8088942.0000, Val Loss: 5298160.0000\n",
      "Epoch [10106/50000], Train Loss: 8088942.5000, Val Loss: 5298162.0000\n",
      "Epoch [10107/50000], Train Loss: 8088942.5000, Val Loss: 5298163.5000\n",
      "Epoch [10108/50000], Train Loss: 8088942.0000, Val Loss: 5298166.5000\n",
      "Epoch [10109/50000], Train Loss: 8088942.0000, Val Loss: 5298168.5000\n",
      "Epoch [10110/50000], Train Loss: 8088942.0000, Val Loss: 5298170.5000\n",
      "Epoch [10111/50000], Train Loss: 8088942.0000, Val Loss: 5298172.5000\n",
      "Epoch [10112/50000], Train Loss: 8088942.0000, Val Loss: 5298174.5000\n",
      "Epoch [10113/50000], Train Loss: 8088942.0000, Val Loss: 5298176.5000\n",
      "Epoch [10114/50000], Train Loss: 8088942.5000, Val Loss: 5298178.5000\n",
      "Epoch [10115/50000], Train Loss: 8088942.0000, Val Loss: 5298180.5000\n",
      "Epoch [10116/50000], Train Loss: 8088942.5000, Val Loss: 5298182.5000\n",
      "Epoch [10117/50000], Train Loss: 8088942.5000, Val Loss: 5298184.0000\n",
      "Epoch [10118/50000], Train Loss: 8088942.0000, Val Loss: 5298185.5000\n",
      "Epoch [10119/50000], Train Loss: 8088943.0000, Val Loss: 5298187.5000\n",
      "Epoch [10120/50000], Train Loss: 8088942.0000, Val Loss: 5298189.5000\n",
      "Epoch [10121/50000], Train Loss: 8088942.5000, Val Loss: 5298191.0000\n",
      "Epoch [10122/50000], Train Loss: 8088942.0000, Val Loss: 5298192.0000\n",
      "Epoch [10123/50000], Train Loss: 8088942.5000, Val Loss: 5298194.5000\n",
      "Epoch [10124/50000], Train Loss: 8088942.5000, Val Loss: 5298195.5000\n",
      "Epoch [10125/50000], Train Loss: 8088942.0000, Val Loss: 5298198.5000\n",
      "Epoch [10126/50000], Train Loss: 8088942.0000, Val Loss: 5298199.5000\n",
      "Epoch [10127/50000], Train Loss: 8088942.0000, Val Loss: 5298202.0000\n",
      "Epoch [10128/50000], Train Loss: 8088942.5000, Val Loss: 5298203.5000\n",
      "Epoch [10129/50000], Train Loss: 8088942.0000, Val Loss: 5298205.5000\n",
      "Epoch [10130/50000], Train Loss: 8088942.0000, Val Loss: 5298207.5000\n",
      "Epoch [10131/50000], Train Loss: 8088942.0000, Val Loss: 5298209.0000\n",
      "Epoch [10132/50000], Train Loss: 8088941.5000, Val Loss: 5298210.5000\n",
      "Epoch [10133/50000], Train Loss: 8088942.0000, Val Loss: 5298212.5000\n",
      "Epoch [10134/50000], Train Loss: 8088942.0000, Val Loss: 5298214.0000\n",
      "Epoch [10135/50000], Train Loss: 8088942.0000, Val Loss: 5298216.5000\n",
      "Epoch [10136/50000], Train Loss: 8088942.0000, Val Loss: 5298217.5000\n",
      "Epoch [10137/50000], Train Loss: 8088942.0000, Val Loss: 5298219.5000\n",
      "Epoch [10138/50000], Train Loss: 8088942.0000, Val Loss: 5298221.5000\n",
      "Epoch [10139/50000], Train Loss: 8088941.5000, Val Loss: 5298223.5000\n",
      "Epoch [10140/50000], Train Loss: 8088942.0000, Val Loss: 5298225.5000\n",
      "Epoch [10141/50000], Train Loss: 8088942.0000, Val Loss: 5298226.5000\n",
      "Epoch [10142/50000], Train Loss: 8088942.5000, Val Loss: 5298229.0000\n",
      "Epoch [10143/50000], Train Loss: 8088942.5000, Val Loss: 5298230.5000\n",
      "Epoch [10144/50000], Train Loss: 8088942.0000, Val Loss: 5298232.0000\n",
      "Epoch [10145/50000], Train Loss: 8088942.0000, Val Loss: 5298233.5000\n",
      "Epoch [10146/50000], Train Loss: 8088942.0000, Val Loss: 5298235.5000\n",
      "Epoch [10147/50000], Train Loss: 8088942.0000, Val Loss: 5298238.0000\n",
      "Epoch [10148/50000], Train Loss: 8088942.5000, Val Loss: 5298240.0000\n",
      "Epoch [10149/50000], Train Loss: 8088942.5000, Val Loss: 5298241.0000\n",
      "Epoch [10150/50000], Train Loss: 8088942.5000, Val Loss: 5298242.5000\n",
      "Epoch [10151/50000], Train Loss: 8088942.5000, Val Loss: 5298245.5000\n",
      "Epoch [10152/50000], Train Loss: 8088942.5000, Val Loss: 5298246.5000\n",
      "Epoch [10153/50000], Train Loss: 8088941.5000, Val Loss: 5298248.5000\n",
      "Epoch [10154/50000], Train Loss: 8088942.0000, Val Loss: 5298250.0000\n",
      "Epoch [10155/50000], Train Loss: 8088942.5000, Val Loss: 5298251.0000\n",
      "Epoch [10156/50000], Train Loss: 8088942.0000, Val Loss: 5298254.0000\n",
      "Epoch [10157/50000], Train Loss: 8088942.5000, Val Loss: 5298255.5000\n",
      "Epoch [10158/50000], Train Loss: 8088941.5000, Val Loss: 5298257.0000\n",
      "Epoch [10159/50000], Train Loss: 8088942.5000, Val Loss: 5298258.5000\n",
      "Epoch [10160/50000], Train Loss: 8088942.0000, Val Loss: 5298261.5000\n",
      "Epoch [10161/50000], Train Loss: 8088942.0000, Val Loss: 5298263.0000\n",
      "Epoch [10162/50000], Train Loss: 8088942.0000, Val Loss: 5298264.5000\n",
      "Epoch [10163/50000], Train Loss: 8088941.5000, Val Loss: 5298266.5000\n",
      "Epoch [10164/50000], Train Loss: 8088942.0000, Val Loss: 5298268.5000\n",
      "Epoch [10165/50000], Train Loss: 8088942.0000, Val Loss: 5298270.0000\n",
      "Epoch [10166/50000], Train Loss: 8088942.5000, Val Loss: 5298271.5000\n",
      "Epoch [10167/50000], Train Loss: 8088942.5000, Val Loss: 5298273.0000\n",
      "Epoch [10168/50000], Train Loss: 8088942.0000, Val Loss: 5298274.5000\n",
      "Epoch [10169/50000], Train Loss: 8088942.5000, Val Loss: 5298275.5000\n",
      "Epoch [10170/50000], Train Loss: 8088942.5000, Val Loss: 5298277.0000\n",
      "Epoch [10171/50000], Train Loss: 8088941.5000, Val Loss: 5298278.5000\n",
      "Epoch [10172/50000], Train Loss: 8088942.0000, Val Loss: 5298280.0000\n",
      "Epoch [10173/50000], Train Loss: 8088942.0000, Val Loss: 5298281.0000\n",
      "Epoch [10174/50000], Train Loss: 8088942.0000, Val Loss: 5298283.0000\n",
      "Epoch [10175/50000], Train Loss: 8088942.0000, Val Loss: 5298285.0000\n",
      "Epoch [10176/50000], Train Loss: 8088942.5000, Val Loss: 5298286.0000\n",
      "Epoch [10177/50000], Train Loss: 8088942.0000, Val Loss: 5298287.5000\n",
      "Epoch [10178/50000], Train Loss: 8088942.5000, Val Loss: 5298288.5000\n",
      "Epoch [10179/50000], Train Loss: 8088942.0000, Val Loss: 5298289.5000\n",
      "Epoch [10180/50000], Train Loss: 8088942.5000, Val Loss: 5298291.0000\n",
      "Epoch [10181/50000], Train Loss: 8088942.0000, Val Loss: 5298292.5000\n",
      "Epoch [10182/50000], Train Loss: 8088942.0000, Val Loss: 5298294.0000\n",
      "Epoch [10183/50000], Train Loss: 8088942.5000, Val Loss: 5298295.5000\n",
      "Epoch [10184/50000], Train Loss: 8088942.5000, Val Loss: 5298297.5000\n",
      "Epoch [10185/50000], Train Loss: 8088942.5000, Val Loss: 5298298.0000\n",
      "Epoch [10186/50000], Train Loss: 8088942.5000, Val Loss: 5298299.5000\n",
      "Epoch [10187/50000], Train Loss: 8088942.0000, Val Loss: 5298301.5000\n",
      "Epoch [10188/50000], Train Loss: 8088942.0000, Val Loss: 5298303.0000\n",
      "Epoch [10189/50000], Train Loss: 8088942.5000, Val Loss: 5298304.0000\n",
      "Epoch [10190/50000], Train Loss: 8088941.5000, Val Loss: 5298305.5000\n",
      "Epoch [10191/50000], Train Loss: 8088941.5000, Val Loss: 5298306.5000\n",
      "Epoch [10192/50000], Train Loss: 8088942.0000, Val Loss: 5298308.5000\n",
      "Epoch [10193/50000], Train Loss: 8088942.0000, Val Loss: 5298310.5000\n",
      "Epoch [10194/50000], Train Loss: 8088942.5000, Val Loss: 5298311.5000\n",
      "Epoch [10195/50000], Train Loss: 8088942.0000, Val Loss: 5298313.0000\n",
      "Epoch [10196/50000], Train Loss: 8088941.5000, Val Loss: 5298314.5000\n",
      "Epoch [10197/50000], Train Loss: 8088941.5000, Val Loss: 5298316.5000\n",
      "Epoch [10198/50000], Train Loss: 8088942.5000, Val Loss: 5298317.5000\n",
      "Epoch [10199/50000], Train Loss: 8088942.0000, Val Loss: 5298319.5000\n",
      "Epoch [10200/50000], Train Loss: 8088942.0000, Val Loss: 5298320.0000\n",
      "Epoch [10201/50000], Train Loss: 8088942.5000, Val Loss: 5298322.0000\n",
      "Epoch [10202/50000], Train Loss: 8088942.0000, Val Loss: 5298323.0000\n",
      "Epoch [10203/50000], Train Loss: 8088941.5000, Val Loss: 5298324.5000\n",
      "Epoch [10204/50000], Train Loss: 8088942.5000, Val Loss: 5298326.5000\n",
      "Epoch [10205/50000], Train Loss: 8088942.0000, Val Loss: 5298327.5000\n",
      "Epoch [10206/50000], Train Loss: 8088942.0000, Val Loss: 5298328.5000\n",
      "Epoch [10207/50000], Train Loss: 8088942.0000, Val Loss: 5298330.0000\n",
      "Epoch [10208/50000], Train Loss: 8088942.0000, Val Loss: 5298331.5000\n",
      "Epoch [10209/50000], Train Loss: 8088942.0000, Val Loss: 5298333.5000\n",
      "Epoch [10210/50000], Train Loss: 8088942.0000, Val Loss: 5298334.0000\n",
      "Epoch [10211/50000], Train Loss: 8088942.5000, Val Loss: 5298336.0000\n",
      "Epoch [10212/50000], Train Loss: 8088942.0000, Val Loss: 5298337.0000\n",
      "Epoch [10213/50000], Train Loss: 8088942.0000, Val Loss: 5298338.5000\n",
      "Epoch [10214/50000], Train Loss: 8088942.0000, Val Loss: 5298341.0000\n",
      "Epoch [10215/50000], Train Loss: 8088941.5000, Val Loss: 5298341.5000\n",
      "Epoch [10216/50000], Train Loss: 8088942.0000, Val Loss: 5298343.5000\n",
      "Epoch [10217/50000], Train Loss: 8088942.0000, Val Loss: 5298344.5000\n",
      "Epoch [10218/50000], Train Loss: 8088942.0000, Val Loss: 5298345.5000\n",
      "Epoch [10219/50000], Train Loss: 8088942.0000, Val Loss: 5298347.5000\n",
      "Epoch [10220/50000], Train Loss: 8088942.5000, Val Loss: 5298349.5000\n",
      "Epoch [10221/50000], Train Loss: 8088942.0000, Val Loss: 5298350.0000\n",
      "Epoch [10222/50000], Train Loss: 8088942.0000, Val Loss: 5298351.5000\n",
      "Epoch [10223/50000], Train Loss: 8088941.5000, Val Loss: 5298352.5000\n",
      "Epoch [10224/50000], Train Loss: 8088942.0000, Val Loss: 5298354.0000\n",
      "Epoch [10225/50000], Train Loss: 8088942.0000, Val Loss: 5298356.5000\n",
      "Epoch [10226/50000], Train Loss: 8088941.5000, Val Loss: 5298357.5000\n",
      "Epoch [10227/50000], Train Loss: 8088941.5000, Val Loss: 5298358.5000\n",
      "Epoch [10228/50000], Train Loss: 8088942.0000, Val Loss: 5298360.5000\n",
      "Epoch [10229/50000], Train Loss: 8088942.0000, Val Loss: 5298362.0000\n",
      "Epoch [10230/50000], Train Loss: 8088942.0000, Val Loss: 5298363.0000\n",
      "Epoch [10231/50000], Train Loss: 8088942.0000, Val Loss: 5298365.0000\n",
      "Epoch [10232/50000], Train Loss: 8088942.0000, Val Loss: 5298365.5000\n",
      "Epoch [10233/50000], Train Loss: 8088942.0000, Val Loss: 5298366.0000\n",
      "Epoch [10234/50000], Train Loss: 8088941.5000, Val Loss: 5298367.5000\n",
      "Epoch [10235/50000], Train Loss: 8088942.0000, Val Loss: 5298368.5000\n",
      "Epoch [10236/50000], Train Loss: 8088942.0000, Val Loss: 5298369.5000\n",
      "Epoch [10237/50000], Train Loss: 8088942.0000, Val Loss: 5298370.0000\n",
      "Epoch [10238/50000], Train Loss: 8088942.0000, Val Loss: 5298372.5000\n",
      "Epoch [10239/50000], Train Loss: 8088942.0000, Val Loss: 5298373.0000\n",
      "Epoch [10240/50000], Train Loss: 8088942.0000, Val Loss: 5298374.0000\n",
      "Epoch [10241/50000], Train Loss: 8088942.0000, Val Loss: 5298375.5000\n",
      "Epoch [10242/50000], Train Loss: 8088941.5000, Val Loss: 5298375.5000\n",
      "Epoch [10243/50000], Train Loss: 8088942.0000, Val Loss: 5298377.5000\n",
      "Epoch [10244/50000], Train Loss: 8088942.5000, Val Loss: 5298377.5000\n",
      "Epoch [10245/50000], Train Loss: 8088942.0000, Val Loss: 5298379.0000\n",
      "Epoch [10246/50000], Train Loss: 8088942.0000, Val Loss: 5298380.5000\n",
      "Epoch [10247/50000], Train Loss: 8088942.0000, Val Loss: 5298381.5000\n",
      "Epoch [10248/50000], Train Loss: 8088942.5000, Val Loss: 5298382.5000\n",
      "Epoch [10249/50000], Train Loss: 8088942.0000, Val Loss: 5298383.0000\n",
      "Epoch [10250/50000], Train Loss: 8088942.5000, Val Loss: 5298385.0000\n",
      "Epoch [10251/50000], Train Loss: 8088942.0000, Val Loss: 5298385.5000\n",
      "Epoch [10252/50000], Train Loss: 8088942.0000, Val Loss: 5298386.5000\n",
      "Epoch [10253/50000], Train Loss: 8088942.0000, Val Loss: 5298387.0000\n",
      "Epoch [10254/50000], Train Loss: 8088942.0000, Val Loss: 5298389.0000\n",
      "Epoch [10255/50000], Train Loss: 8088942.0000, Val Loss: 5298390.5000\n",
      "Epoch [10256/50000], Train Loss: 8088942.0000, Val Loss: 5298390.5000\n",
      "Epoch [10257/50000], Train Loss: 8088942.0000, Val Loss: 5298392.5000\n",
      "Epoch [10258/50000], Train Loss: 8088942.5000, Val Loss: 5298393.0000\n",
      "Epoch [10259/50000], Train Loss: 8088943.0000, Val Loss: 5298394.5000\n",
      "Epoch [10260/50000], Train Loss: 8088941.0000, Val Loss: 5298395.0000\n",
      "Epoch [10261/50000], Train Loss: 8088942.0000, Val Loss: 5298396.5000\n",
      "Epoch [10262/50000], Train Loss: 8088941.5000, Val Loss: 5298398.0000\n",
      "Epoch [10263/50000], Train Loss: 8088942.0000, Val Loss: 5298398.0000\n",
      "Epoch [10264/50000], Train Loss: 8088942.0000, Val Loss: 5298399.5000\n",
      "Epoch [10265/50000], Train Loss: 8088942.0000, Val Loss: 5298400.5000\n",
      "Epoch [10266/50000], Train Loss: 8088941.5000, Val Loss: 5298401.5000\n",
      "Epoch [10267/50000], Train Loss: 8088942.0000, Val Loss: 5298403.0000\n",
      "Epoch [10268/50000], Train Loss: 8088941.5000, Val Loss: 5298403.5000\n",
      "Epoch [10269/50000], Train Loss: 8088942.0000, Val Loss: 5298404.5000\n",
      "Epoch [10270/50000], Train Loss: 8088942.0000, Val Loss: 5298406.0000\n",
      "Epoch [10271/50000], Train Loss: 8088941.5000, Val Loss: 5298407.5000\n",
      "Epoch [10272/50000], Train Loss: 8088942.5000, Val Loss: 5298408.5000\n",
      "Epoch [10273/50000], Train Loss: 8088942.0000, Val Loss: 5298410.0000\n",
      "Epoch [10274/50000], Train Loss: 8088941.5000, Val Loss: 5298410.5000\n",
      "Epoch [10275/50000], Train Loss: 8088941.5000, Val Loss: 5298411.5000\n",
      "Epoch [10276/50000], Train Loss: 8088942.0000, Val Loss: 5298413.0000\n",
      "Epoch [10277/50000], Train Loss: 8088941.5000, Val Loss: 5298414.0000\n",
      "Epoch [10278/50000], Train Loss: 8088942.0000, Val Loss: 5298414.5000\n",
      "Epoch [10279/50000], Train Loss: 8088941.5000, Val Loss: 5298416.0000\n",
      "Epoch [10280/50000], Train Loss: 8088942.0000, Val Loss: 5298417.5000\n",
      "Epoch [10281/50000], Train Loss: 8088942.0000, Val Loss: 5298418.5000\n",
      "Epoch [10282/50000], Train Loss: 8088941.5000, Val Loss: 5298419.0000\n",
      "Epoch [10283/50000], Train Loss: 8088941.5000, Val Loss: 5298420.5000\n",
      "Epoch [10284/50000], Train Loss: 8088942.5000, Val Loss: 5298421.5000\n",
      "Epoch [10285/50000], Train Loss: 8088942.0000, Val Loss: 5298422.5000\n",
      "Epoch [10286/50000], Train Loss: 8088942.0000, Val Loss: 5298423.5000\n",
      "Epoch [10287/50000], Train Loss: 8088941.5000, Val Loss: 5298424.5000\n",
      "Epoch [10288/50000], Train Loss: 8088941.5000, Val Loss: 5298426.0000\n",
      "Epoch [10289/50000], Train Loss: 8088942.0000, Val Loss: 5298426.5000\n",
      "Epoch [10290/50000], Train Loss: 8088942.5000, Val Loss: 5298427.5000\n",
      "Epoch [10291/50000], Train Loss: 8088942.0000, Val Loss: 5298429.0000\n",
      "Epoch [10292/50000], Train Loss: 8088942.0000, Val Loss: 5298430.0000\n",
      "Epoch [10293/50000], Train Loss: 8088942.5000, Val Loss: 5298431.0000\n",
      "Epoch [10294/50000], Train Loss: 8088942.0000, Val Loss: 5298432.0000\n",
      "Epoch [10295/50000], Train Loss: 8088942.5000, Val Loss: 5298433.0000\n",
      "Epoch [10296/50000], Train Loss: 8088942.0000, Val Loss: 5298434.5000\n",
      "Epoch [10297/50000], Train Loss: 8088942.0000, Val Loss: 5298435.0000\n",
      "Epoch [10298/50000], Train Loss: 8088942.5000, Val Loss: 5298437.0000\n",
      "Epoch [10299/50000], Train Loss: 8088942.0000, Val Loss: 5298438.0000\n",
      "Epoch [10300/50000], Train Loss: 8088942.0000, Val Loss: 5298438.5000\n",
      "Epoch [10301/50000], Train Loss: 8088942.0000, Val Loss: 5298439.5000\n",
      "Epoch [10302/50000], Train Loss: 8088942.5000, Val Loss: 5298441.0000\n",
      "Epoch [10303/50000], Train Loss: 8088941.5000, Val Loss: 5298441.5000\n",
      "Epoch [10304/50000], Train Loss: 8088942.0000, Val Loss: 5298442.5000\n",
      "Epoch [10305/50000], Train Loss: 8088942.0000, Val Loss: 5298443.5000\n",
      "Epoch [10306/50000], Train Loss: 8088942.0000, Val Loss: 5298444.5000\n",
      "Epoch [10307/50000], Train Loss: 8088942.5000, Val Loss: 5298446.0000\n",
      "Epoch [10308/50000], Train Loss: 8088942.0000, Val Loss: 5298447.5000\n",
      "Epoch [10309/50000], Train Loss: 8088942.0000, Val Loss: 5298448.5000\n",
      "Epoch [10310/50000], Train Loss: 8088942.5000, Val Loss: 5298449.0000\n",
      "Epoch [10311/50000], Train Loss: 8088942.0000, Val Loss: 5298450.0000\n",
      "Epoch [10312/50000], Train Loss: 8088942.0000, Val Loss: 5298450.5000\n",
      "Epoch [10313/50000], Train Loss: 8088941.5000, Val Loss: 5298453.5000\n",
      "Epoch [10314/50000], Train Loss: 8088941.5000, Val Loss: 5298454.0000\n",
      "Epoch [10315/50000], Train Loss: 8088941.5000, Val Loss: 5298454.5000\n",
      "Epoch [10316/50000], Train Loss: 8088942.0000, Val Loss: 5298455.0000\n",
      "Epoch [10317/50000], Train Loss: 8088942.0000, Val Loss: 5298456.0000\n",
      "Epoch [10318/50000], Train Loss: 8088941.5000, Val Loss: 5298456.0000\n",
      "Epoch [10319/50000], Train Loss: 8088941.5000, Val Loss: 5298456.5000\n",
      "Epoch [10320/50000], Train Loss: 8088942.0000, Val Loss: 5298457.5000\n",
      "Epoch [10321/50000], Train Loss: 8088942.0000, Val Loss: 5298458.5000\n",
      "Epoch [10322/50000], Train Loss: 8088942.0000, Val Loss: 5298459.0000\n",
      "Epoch [10323/50000], Train Loss: 8088942.0000, Val Loss: 5298459.5000\n",
      "Epoch [10324/50000], Train Loss: 8088941.5000, Val Loss: 5298460.5000\n",
      "Epoch [10325/50000], Train Loss: 8088942.5000, Val Loss: 5298462.0000\n",
      "Epoch [10326/50000], Train Loss: 8088942.0000, Val Loss: 5298462.0000\n",
      "Epoch [10327/50000], Train Loss: 8088942.0000, Val Loss: 5298462.5000\n",
      "Epoch [10328/50000], Train Loss: 8088942.0000, Val Loss: 5298464.0000\n",
      "Epoch [10329/50000], Train Loss: 8088942.0000, Val Loss: 5298464.0000\n",
      "Epoch [10330/50000], Train Loss: 8088942.0000, Val Loss: 5298464.5000\n",
      "Epoch [10331/50000], Train Loss: 8088942.0000, Val Loss: 5298465.5000\n",
      "Epoch [10332/50000], Train Loss: 8088941.5000, Val Loss: 5298466.5000\n",
      "Epoch [10333/50000], Train Loss: 8088942.0000, Val Loss: 5298467.0000\n",
      "Epoch [10334/50000], Train Loss: 8088942.0000, Val Loss: 5298467.5000\n",
      "Epoch [10335/50000], Train Loss: 8088941.5000, Val Loss: 5298468.5000\n",
      "Epoch [10336/50000], Train Loss: 8088942.0000, Val Loss: 5298469.0000\n",
      "Epoch [10337/50000], Train Loss: 8088942.0000, Val Loss: 5298470.5000\n",
      "Epoch [10338/50000], Train Loss: 8088942.0000, Val Loss: 5298470.5000\n",
      "Epoch [10339/50000], Train Loss: 8088942.0000, Val Loss: 5298471.0000\n",
      "Epoch [10340/50000], Train Loss: 8088942.0000, Val Loss: 5298472.0000\n",
      "Epoch [10341/50000], Train Loss: 8088942.0000, Val Loss: 5298473.0000\n",
      "Epoch [10342/50000], Train Loss: 8088942.5000, Val Loss: 5298473.0000\n",
      "Epoch [10343/50000], Train Loss: 8088942.5000, Val Loss: 5298473.5000\n",
      "Epoch [10344/50000], Train Loss: 8088942.0000, Val Loss: 5298474.5000\n",
      "Epoch [10345/50000], Train Loss: 8088942.0000, Val Loss: 5298475.0000\n",
      "Epoch [10346/50000], Train Loss: 8088942.0000, Val Loss: 5298476.5000\n",
      "Epoch [10347/50000], Train Loss: 8088942.5000, Val Loss: 5298477.5000\n",
      "Epoch [10348/50000], Train Loss: 8088941.5000, Val Loss: 5298478.0000\n",
      "Epoch [10349/50000], Train Loss: 8088942.0000, Val Loss: 5298479.0000\n",
      "Epoch [10350/50000], Train Loss: 8088942.0000, Val Loss: 5298479.0000\n",
      "Epoch [10351/50000], Train Loss: 8088942.0000, Val Loss: 5298480.0000\n",
      "Epoch [10352/50000], Train Loss: 8088942.0000, Val Loss: 5298480.5000\n",
      "Epoch [10353/50000], Train Loss: 8088941.0000, Val Loss: 5298481.5000\n",
      "Epoch [10354/50000], Train Loss: 8088942.0000, Val Loss: 5298482.0000\n",
      "Epoch [10355/50000], Train Loss: 8088942.0000, Val Loss: 5298483.0000\n",
      "Epoch [10356/50000], Train Loss: 8088943.0000, Val Loss: 5298483.0000\n",
      "Epoch [10357/50000], Train Loss: 8088942.0000, Val Loss: 5298484.5000\n",
      "Epoch [10358/50000], Train Loss: 8088942.0000, Val Loss: 5298485.0000\n",
      "Epoch [10359/50000], Train Loss: 8088942.5000, Val Loss: 5298485.5000\n",
      "Epoch [10360/50000], Train Loss: 8088942.5000, Val Loss: 5298486.5000\n",
      "Epoch [10361/50000], Train Loss: 8088942.0000, Val Loss: 5298487.0000\n",
      "Epoch [10362/50000], Train Loss: 8088942.0000, Val Loss: 5298488.0000\n",
      "Epoch [10363/50000], Train Loss: 8088942.0000, Val Loss: 5298488.5000\n",
      "Epoch [10364/50000], Train Loss: 8088942.0000, Val Loss: 5298488.5000\n",
      "Epoch [10365/50000], Train Loss: 8088942.5000, Val Loss: 5298490.0000\n",
      "Epoch [10366/50000], Train Loss: 8088941.5000, Val Loss: 5298490.5000\n",
      "Epoch [10367/50000], Train Loss: 8088942.0000, Val Loss: 5298490.5000\n",
      "Epoch [10368/50000], Train Loss: 8088942.0000, Val Loss: 5298491.5000\n",
      "Epoch [10369/50000], Train Loss: 8088942.0000, Val Loss: 5298493.0000\n",
      "Epoch [10370/50000], Train Loss: 8088941.5000, Val Loss: 5298493.5000\n",
      "Epoch [10371/50000], Train Loss: 8088941.5000, Val Loss: 5298493.5000\n",
      "Epoch [10372/50000], Train Loss: 8088941.5000, Val Loss: 5298495.0000\n",
      "Epoch [10373/50000], Train Loss: 8088942.0000, Val Loss: 5298496.0000\n",
      "Epoch [10374/50000], Train Loss: 8088941.5000, Val Loss: 5298496.5000\n",
      "Epoch [10375/50000], Train Loss: 8088942.0000, Val Loss: 5298497.0000\n",
      "Epoch [10376/50000], Train Loss: 8088942.0000, Val Loss: 5298497.5000\n",
      "Epoch [10377/50000], Train Loss: 8088942.5000, Val Loss: 5298499.0000\n",
      "Epoch [10378/50000], Train Loss: 8088942.0000, Val Loss: 5298499.5000\n",
      "Epoch [10379/50000], Train Loss: 8088941.5000, Val Loss: 5298501.5000\n",
      "Epoch [10380/50000], Train Loss: 8088942.0000, Val Loss: 5298501.5000\n",
      "Epoch [10381/50000], Train Loss: 8088942.0000, Val Loss: 5298501.5000\n",
      "Epoch [10382/50000], Train Loss: 8088942.0000, Val Loss: 5298502.5000\n",
      "Epoch [10383/50000], Train Loss: 8088941.5000, Val Loss: 5298503.5000\n",
      "Epoch [10384/50000], Train Loss: 8088941.5000, Val Loss: 5298504.0000\n",
      "Epoch [10385/50000], Train Loss: 8088941.5000, Val Loss: 5298504.0000\n",
      "Epoch [10386/50000], Train Loss: 8088941.5000, Val Loss: 5298505.0000\n",
      "Epoch [10387/50000], Train Loss: 8088941.5000, Val Loss: 5298506.0000\n",
      "Epoch [10388/50000], Train Loss: 8088941.5000, Val Loss: 5298506.5000\n",
      "Epoch [10389/50000], Train Loss: 8088942.0000, Val Loss: 5298508.5000\n",
      "Epoch [10390/50000], Train Loss: 8088941.5000, Val Loss: 5298507.5000\n",
      "Epoch [10391/50000], Train Loss: 8088941.5000, Val Loss: 5298509.5000\n",
      "Epoch [10392/50000], Train Loss: 8088942.0000, Val Loss: 5298509.5000\n",
      "Epoch [10393/50000], Train Loss: 8088942.0000, Val Loss: 5298510.5000\n",
      "Epoch [10394/50000], Train Loss: 8088942.0000, Val Loss: 5298511.0000\n",
      "Epoch [10395/50000], Train Loss: 8088942.0000, Val Loss: 5298511.5000\n",
      "Epoch [10396/50000], Train Loss: 8088942.0000, Val Loss: 5298512.5000\n",
      "Epoch [10397/50000], Train Loss: 8088942.0000, Val Loss: 5298513.0000\n",
      "Epoch [10398/50000], Train Loss: 8088942.0000, Val Loss: 5298513.5000\n",
      "Epoch [10399/50000], Train Loss: 8088942.0000, Val Loss: 5298515.0000\n",
      "Epoch [10400/50000], Train Loss: 8088942.0000, Val Loss: 5298515.5000\n",
      "Epoch [10401/50000], Train Loss: 8088942.0000, Val Loss: 5298516.5000\n",
      "Epoch [10402/50000], Train Loss: 8088942.0000, Val Loss: 5298516.5000\n",
      "Epoch [10403/50000], Train Loss: 8088942.5000, Val Loss: 5298517.5000\n",
      "Epoch [10404/50000], Train Loss: 8088942.0000, Val Loss: 5298518.5000\n",
      "Epoch [10405/50000], Train Loss: 8088942.5000, Val Loss: 5298519.0000\n",
      "Epoch [10406/50000], Train Loss: 8088942.0000, Val Loss: 5298519.5000\n",
      "Epoch [10407/50000], Train Loss: 8088942.5000, Val Loss: 5298520.5000\n",
      "Epoch [10408/50000], Train Loss: 8088941.5000, Val Loss: 5298521.0000\n",
      "Epoch [10409/50000], Train Loss: 8088942.0000, Val Loss: 5298521.5000\n",
      "Epoch [10410/50000], Train Loss: 8088942.0000, Val Loss: 5298522.0000\n",
      "Epoch [10411/50000], Train Loss: 8088942.0000, Val Loss: 5298523.0000\n",
      "Epoch [10412/50000], Train Loss: 8088942.0000, Val Loss: 5298524.5000\n",
      "Epoch [10413/50000], Train Loss: 8088942.0000, Val Loss: 5298524.5000\n",
      "Epoch [10414/50000], Train Loss: 8088942.0000, Val Loss: 5298525.5000\n",
      "Epoch [10415/50000], Train Loss: 8088942.0000, Val Loss: 5298526.5000\n",
      "Epoch [10416/50000], Train Loss: 8088942.0000, Val Loss: 5298527.0000\n",
      "Epoch [10417/50000], Train Loss: 8088942.0000, Val Loss: 5298527.5000\n",
      "Epoch [10418/50000], Train Loss: 8088942.5000, Val Loss: 5298528.5000\n",
      "Epoch [10419/50000], Train Loss: 8088942.0000, Val Loss: 5298528.5000\n",
      "Epoch [10420/50000], Train Loss: 8088942.5000, Val Loss: 5298529.0000\n",
      "Epoch [10421/50000], Train Loss: 8088942.5000, Val Loss: 5298530.5000\n",
      "Epoch [10422/50000], Train Loss: 8088942.0000, Val Loss: 5298531.0000\n",
      "Epoch [10423/50000], Train Loss: 8088942.0000, Val Loss: 5298531.0000\n",
      "Epoch [10424/50000], Train Loss: 8088942.5000, Val Loss: 5298532.5000\n",
      "Epoch [10425/50000], Train Loss: 8088941.5000, Val Loss: 5298533.5000\n",
      "Epoch [10426/50000], Train Loss: 8088942.0000, Val Loss: 5298534.5000\n",
      "Epoch [10427/50000], Train Loss: 8088942.0000, Val Loss: 5298534.5000\n",
      "Epoch [10428/50000], Train Loss: 8088942.5000, Val Loss: 5298535.0000\n",
      "Epoch [10429/50000], Train Loss: 8088942.0000, Val Loss: 5298535.5000\n",
      "Epoch [10430/50000], Train Loss: 8088942.0000, Val Loss: 5298536.5000\n",
      "Epoch [10431/50000], Train Loss: 8088942.0000, Val Loss: 5298537.0000\n",
      "Epoch [10432/50000], Train Loss: 8088942.0000, Val Loss: 5298537.0000\n",
      "Epoch [10433/50000], Train Loss: 8088942.0000, Val Loss: 5298537.5000\n",
      "Epoch [10434/50000], Train Loss: 8088941.5000, Val Loss: 5298537.5000\n",
      "Epoch [10435/50000], Train Loss: 8088941.5000, Val Loss: 5298538.5000\n",
      "Epoch [10436/50000], Train Loss: 8088941.5000, Val Loss: 5298539.0000\n",
      "Epoch [10437/50000], Train Loss: 8088941.5000, Val Loss: 5298539.0000\n",
      "Epoch [10438/50000], Train Loss: 8088941.5000, Val Loss: 5298539.0000\n",
      "Epoch [10439/50000], Train Loss: 8088941.5000, Val Loss: 5298539.5000\n",
      "Epoch [10440/50000], Train Loss: 8088942.0000, Val Loss: 5298539.5000\n",
      "Epoch [10441/50000], Train Loss: 8088942.0000, Val Loss: 5298540.5000\n",
      "Epoch [10442/50000], Train Loss: 8088942.0000, Val Loss: 5298541.5000\n",
      "Epoch [10443/50000], Train Loss: 8088942.0000, Val Loss: 5298542.0000\n",
      "Epoch [10444/50000], Train Loss: 8088942.5000, Val Loss: 5298542.0000\n",
      "Epoch [10445/50000], Train Loss: 8088941.5000, Val Loss: 5298542.0000\n",
      "Epoch [10446/50000], Train Loss: 8088941.5000, Val Loss: 5298542.5000\n",
      "Epoch [10447/50000], Train Loss: 8088942.0000, Val Loss: 5298543.0000\n",
      "Epoch [10448/50000], Train Loss: 8088942.0000, Val Loss: 5298543.5000\n",
      "Epoch [10449/50000], Train Loss: 8088941.5000, Val Loss: 5298543.5000\n",
      "Epoch [10450/50000], Train Loss: 8088941.5000, Val Loss: 5298543.5000\n",
      "Epoch [10451/50000], Train Loss: 8088942.0000, Val Loss: 5298544.5000\n",
      "Epoch [10452/50000], Train Loss: 8088942.0000, Val Loss: 5298545.5000\n",
      "Epoch [10453/50000], Train Loss: 8088942.0000, Val Loss: 5298545.5000\n",
      "Epoch [10454/50000], Train Loss: 8088942.0000, Val Loss: 5298546.0000\n",
      "Epoch [10455/50000], Train Loss: 8088942.0000, Val Loss: 5298546.0000\n",
      "Epoch [10456/50000], Train Loss: 8088942.0000, Val Loss: 5298546.5000\n",
      "Epoch [10457/50000], Train Loss: 8088942.0000, Val Loss: 5298547.0000\n",
      "Epoch [10458/50000], Train Loss: 8088942.0000, Val Loss: 5298547.0000\n",
      "Epoch [10459/50000], Train Loss: 8088942.0000, Val Loss: 5298547.0000\n",
      "Epoch [10460/50000], Train Loss: 8088942.0000, Val Loss: 5298548.5000\n",
      "Epoch [10461/50000], Train Loss: 8088941.5000, Val Loss: 5298548.5000\n",
      "Epoch [10462/50000], Train Loss: 8088941.5000, Val Loss: 5298548.5000\n",
      "Epoch [10463/50000], Train Loss: 8088942.0000, Val Loss: 5298549.0000\n",
      "Epoch [10464/50000], Train Loss: 8088941.5000, Val Loss: 5298549.0000\n",
      "Epoch [10465/50000], Train Loss: 8088942.0000, Val Loss: 5298550.0000\n",
      "Epoch [10466/50000], Train Loss: 8088942.0000, Val Loss: 5298550.0000\n",
      "Epoch [10467/50000], Train Loss: 8088942.0000, Val Loss: 5298550.5000\n",
      "Epoch [10468/50000], Train Loss: 8088942.0000, Val Loss: 5298551.0000\n",
      "Epoch [10469/50000], Train Loss: 8088941.5000, Val Loss: 5298551.0000\n",
      "Epoch [10470/50000], Train Loss: 8088941.5000, Val Loss: 5298551.0000\n",
      "Epoch [10471/50000], Train Loss: 8088941.5000, Val Loss: 5298552.0000\n",
      "Epoch [10472/50000], Train Loss: 8088942.0000, Val Loss: 5298552.5000\n",
      "Epoch [10473/50000], Train Loss: 8088942.0000, Val Loss: 5298552.5000\n",
      "Epoch [10474/50000], Train Loss: 8088942.0000, Val Loss: 5298552.5000\n",
      "Epoch [10475/50000], Train Loss: 8088942.0000, Val Loss: 5298553.5000\n",
      "Epoch [10476/50000], Train Loss: 8088941.5000, Val Loss: 5298553.5000\n",
      "Epoch [10477/50000], Train Loss: 8088941.5000, Val Loss: 5298553.5000\n",
      "Epoch [10478/50000], Train Loss: 8088942.0000, Val Loss: 5298554.0000\n",
      "Epoch [10479/50000], Train Loss: 8088942.0000, Val Loss: 5298554.0000\n",
      "Epoch [10480/50000], Train Loss: 8088942.0000, Val Loss: 5298554.0000\n",
      "Epoch [10481/50000], Train Loss: 8088942.0000, Val Loss: 5298555.5000\n",
      "Epoch [10482/50000], Train Loss: 8088942.0000, Val Loss: 5298555.5000\n",
      "Epoch [10483/50000], Train Loss: 8088942.0000, Val Loss: 5298555.5000\n",
      "Epoch [10484/50000], Train Loss: 8088942.5000, Val Loss: 5298557.0000\n",
      "Epoch [10485/50000], Train Loss: 8088942.5000, Val Loss: 5298557.0000\n",
      "Epoch [10486/50000], Train Loss: 8088942.0000, Val Loss: 5298557.0000\n",
      "Epoch [10487/50000], Train Loss: 8088942.0000, Val Loss: 5298557.0000\n",
      "Epoch [10488/50000], Train Loss: 8088942.0000, Val Loss: 5298557.5000\n",
      "Epoch [10489/50000], Train Loss: 8088942.5000, Val Loss: 5298558.0000\n",
      "Epoch [10490/50000], Train Loss: 8088942.0000, Val Loss: 5298558.5000\n",
      "Epoch [10491/50000], Train Loss: 8088942.0000, Val Loss: 5298558.5000\n",
      "Epoch [10492/50000], Train Loss: 8088942.0000, Val Loss: 5298559.5000\n",
      "Epoch [10493/50000], Train Loss: 8088942.5000, Val Loss: 5298559.5000\n",
      "Epoch [10494/50000], Train Loss: 8088942.0000, Val Loss: 5298560.5000\n",
      "Epoch [10495/50000], Train Loss: 8088942.0000, Val Loss: 5298560.5000\n",
      "Epoch [10496/50000], Train Loss: 8088942.0000, Val Loss: 5298561.0000\n",
      "Epoch [10497/50000], Train Loss: 8088941.5000, Val Loss: 5298561.0000\n",
      "Epoch [10498/50000], Train Loss: 8088941.5000, Val Loss: 5298561.0000\n",
      "Epoch [10499/50000], Train Loss: 8088941.5000, Val Loss: 5298562.0000\n",
      "Epoch [10500/50000], Train Loss: 8088942.5000, Val Loss: 5298562.0000\n",
      "Epoch [10501/50000], Train Loss: 8088942.5000, Val Loss: 5298562.0000\n",
      "Epoch [10502/50000], Train Loss: 8088942.5000, Val Loss: 5298563.0000\n",
      "Epoch [10503/50000], Train Loss: 8088941.5000, Val Loss: 5298563.0000\n",
      "Epoch [10504/50000], Train Loss: 8088941.5000, Val Loss: 5298563.0000\n",
      "Epoch [10505/50000], Train Loss: 8088942.0000, Val Loss: 5298563.0000\n",
      "Epoch [10506/50000], Train Loss: 8088942.5000, Val Loss: 5298564.5000\n",
      "Epoch [10507/50000], Train Loss: 8088942.0000, Val Loss: 5298565.0000\n",
      "Epoch [10508/50000], Train Loss: 8088942.0000, Val Loss: 5298565.0000\n",
      "Epoch [10509/50000], Train Loss: 8088942.0000, Val Loss: 5298565.5000\n",
      "Epoch [10510/50000], Train Loss: 8088942.0000, Val Loss: 5298566.0000\n",
      "Epoch [10511/50000], Train Loss: 8088942.0000, Val Loss: 5298566.0000\n",
      "Epoch [10512/50000], Train Loss: 8088942.5000, Val Loss: 5298566.0000\n",
      "Epoch [10513/50000], Train Loss: 8088942.5000, Val Loss: 5298567.0000\n",
      "Epoch [10514/50000], Train Loss: 8088942.0000, Val Loss: 5298567.0000\n",
      "Epoch [10515/50000], Train Loss: 8088941.5000, Val Loss: 5298567.5000\n",
      "Epoch [10516/50000], Train Loss: 8088941.5000, Val Loss: 5298567.5000\n",
      "Epoch [10517/50000], Train Loss: 8088941.5000, Val Loss: 5298568.5000\n",
      "Epoch [10518/50000], Train Loss: 8088942.0000, Val Loss: 5298568.5000\n",
      "Epoch [10519/50000], Train Loss: 8088942.0000, Val Loss: 5298568.5000\n",
      "Epoch [10520/50000], Train Loss: 8088942.0000, Val Loss: 5298569.5000\n",
      "Epoch [10521/50000], Train Loss: 8088942.0000, Val Loss: 5298569.5000\n",
      "Epoch [10522/50000], Train Loss: 8088942.0000, Val Loss: 5298569.5000\n",
      "Epoch [10523/50000], Train Loss: 8088942.0000, Val Loss: 5298570.5000\n",
      "Epoch [10524/50000], Train Loss: 8088942.5000, Val Loss: 5298570.5000\n",
      "Epoch [10525/50000], Train Loss: 8088942.5000, Val Loss: 5298570.5000\n",
      "Epoch [10526/50000], Train Loss: 8088942.5000, Val Loss: 5298571.5000\n",
      "Epoch [10527/50000], Train Loss: 8088942.0000, Val Loss: 5298571.5000\n",
      "Epoch [10528/50000], Train Loss: 8088942.0000, Val Loss: 5298572.5000\n",
      "Epoch [10529/50000], Train Loss: 8088942.0000, Val Loss: 5298572.5000\n",
      "Epoch [10530/50000], Train Loss: 8088942.0000, Val Loss: 5298572.5000\n",
      "Epoch [10531/50000], Train Loss: 8088942.0000, Val Loss: 5298573.5000\n",
      "Epoch [10532/50000], Train Loss: 8088942.0000, Val Loss: 5298573.5000\n",
      "Epoch [10533/50000], Train Loss: 8088942.0000, Val Loss: 5298573.5000\n",
      "Epoch [10534/50000], Train Loss: 8088942.0000, Val Loss: 5298574.0000\n",
      "Epoch [10535/50000], Train Loss: 8088942.5000, Val Loss: 5298574.0000\n",
      "Epoch [10536/50000], Train Loss: 8088942.5000, Val Loss: 5298575.0000\n",
      "Epoch [10537/50000], Train Loss: 8088942.5000, Val Loss: 5298575.0000\n",
      "Epoch [10538/50000], Train Loss: 8088942.5000, Val Loss: 5298576.0000\n",
      "Epoch [10539/50000], Train Loss: 8088942.0000, Val Loss: 5298576.0000\n",
      "Epoch [10540/50000], Train Loss: 8088942.0000, Val Loss: 5298576.0000\n",
      "Epoch [10541/50000], Train Loss: 8088942.0000, Val Loss: 5298577.5000\n",
      "Epoch [10542/50000], Train Loss: 8088942.0000, Val Loss: 5298577.5000\n",
      "Epoch [10543/50000], Train Loss: 8088942.0000, Val Loss: 5298577.5000\n",
      "Epoch [10544/50000], Train Loss: 8088942.0000, Val Loss: 5298578.0000\n",
      "Epoch [10545/50000], Train Loss: 8088942.0000, Val Loss: 5298578.0000\n",
      "Epoch [10546/50000], Train Loss: 8088942.0000, Val Loss: 5298578.5000\n",
      "Epoch [10547/50000], Train Loss: 8088942.0000, Val Loss: 5298578.5000\n",
      "Epoch [10548/50000], Train Loss: 8088941.5000, Val Loss: 5298579.0000\n",
      "Epoch [10549/50000], Train Loss: 8088942.0000, Val Loss: 5298579.5000\n",
      "Epoch [10550/50000], Train Loss: 8088941.5000, Val Loss: 5298579.5000\n",
      "Epoch [10551/50000], Train Loss: 8088941.5000, Val Loss: 5298580.5000\n",
      "Epoch [10552/50000], Train Loss: 8088942.5000, Val Loss: 5298581.0000\n",
      "Epoch [10553/50000], Train Loss: 8088942.5000, Val Loss: 5298581.5000\n",
      "Epoch [10554/50000], Train Loss: 8088942.0000, Val Loss: 5298581.5000\n",
      "Epoch [10555/50000], Train Loss: 8088942.0000, Val Loss: 5298581.5000\n",
      "Epoch [10556/50000], Train Loss: 8088942.0000, Val Loss: 5298581.5000\n",
      "Epoch [10557/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [10558/50000], Train Loss: 8088941.5000, Val Loss: 5298582.5000\n",
      "Epoch [10559/50000], Train Loss: 8088941.5000, Val Loss: 5298583.5000\n",
      "Epoch [10560/50000], Train Loss: 8088942.0000, Val Loss: 5298583.5000\n",
      "Epoch [10561/50000], Train Loss: 8088942.0000, Val Loss: 5298583.5000\n",
      "Epoch [10562/50000], Train Loss: 8088942.0000, Val Loss: 5298584.0000\n",
      "Epoch [10563/50000], Train Loss: 8088942.0000, Val Loss: 5298584.0000\n",
      "Epoch [10564/50000], Train Loss: 8088942.0000, Val Loss: 5298584.0000\n",
      "Epoch [10565/50000], Train Loss: 8088942.0000, Val Loss: 5298585.0000\n",
      "Epoch [10566/50000], Train Loss: 8088942.0000, Val Loss: 5298585.0000\n",
      "Epoch [10567/50000], Train Loss: 8088942.0000, Val Loss: 5298585.5000\n",
      "Epoch [10568/50000], Train Loss: 8088941.0000, Val Loss: 5298586.5000\n",
      "Epoch [10569/50000], Train Loss: 8088941.5000, Val Loss: 5298586.5000\n",
      "Epoch [10570/50000], Train Loss: 8088941.5000, Val Loss: 5298587.5000\n",
      "Epoch [10571/50000], Train Loss: 8088941.5000, Val Loss: 5298587.5000\n",
      "Epoch [10572/50000], Train Loss: 8088941.5000, Val Loss: 5298587.5000\n",
      "Epoch [10573/50000], Train Loss: 8088941.0000, Val Loss: 5298589.0000\n",
      "Epoch [10574/50000], Train Loss: 8088942.0000, Val Loss: 5298588.5000\n",
      "Epoch [10575/50000], Train Loss: 8088941.5000, Val Loss: 5298588.5000\n",
      "Epoch [10576/50000], Train Loss: 8088941.5000, Val Loss: 5298590.0000\n",
      "Epoch [10577/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [10578/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [10579/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [10580/50000], Train Loss: 8088942.0000, Val Loss: 5298591.0000\n",
      "Epoch [10581/50000], Train Loss: 8088942.0000, Val Loss: 5298591.0000\n",
      "Epoch [10582/50000], Train Loss: 8088942.0000, Val Loss: 5298591.0000\n",
      "Epoch [10583/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [10584/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [10585/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [10586/50000], Train Loss: 8088942.0000, Val Loss: 5298593.0000\n",
      "Epoch [10587/50000], Train Loss: 8088942.0000, Val Loss: 5298593.0000\n",
      "Epoch [10588/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [10589/50000], Train Loss: 8088941.5000, Val Loss: 5298594.0000\n",
      "Epoch [10590/50000], Train Loss: 8088942.0000, Val Loss: 5298594.5000\n",
      "Epoch [10591/50000], Train Loss: 8088942.0000, Val Loss: 5298595.0000\n",
      "Epoch [10592/50000], Train Loss: 8088942.0000, Val Loss: 5298595.0000\n",
      "Epoch [10593/50000], Train Loss: 8088942.0000, Val Loss: 5298595.5000\n",
      "Epoch [10594/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [10595/50000], Train Loss: 8088942.5000, Val Loss: 5298596.5000\n",
      "Epoch [10596/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [10597/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [10598/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [10599/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [10600/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [10601/50000], Train Loss: 8088942.0000, Val Loss: 5298598.5000\n",
      "Epoch [10602/50000], Train Loss: 8088942.0000, Val Loss: 5298598.5000\n",
      "Epoch [10603/50000], Train Loss: 8088942.0000, Val Loss: 5298598.5000\n",
      "Epoch [10604/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [10605/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [10606/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [10607/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [10608/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [10609/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [10610/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [10611/50000], Train Loss: 8088941.5000, Val Loss: 5298602.0000\n",
      "Epoch [10612/50000], Train Loss: 8088941.5000, Val Loss: 5298602.5000\n",
      "Epoch [10613/50000], Train Loss: 8088942.0000, Val Loss: 5298602.5000\n",
      "Epoch [10614/50000], Train Loss: 8088942.0000, Val Loss: 5298603.0000\n",
      "Epoch [10615/50000], Train Loss: 8088942.0000, Val Loss: 5298603.0000\n",
      "Epoch [10616/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [10617/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [10618/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [10619/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [10620/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [10621/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [10622/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [10623/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [10624/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [10625/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [10626/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [10627/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [10628/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [10629/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [10630/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [10631/50000], Train Loss: 8088942.5000, Val Loss: 5298609.0000\n",
      "Epoch [10632/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [10633/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [10634/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [10635/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [10636/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [10637/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [10638/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [10639/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [10640/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [10641/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [10642/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [10643/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10644/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10645/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10646/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10647/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10648/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10649/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10650/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10651/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10652/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10653/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10654/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10655/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10656/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10657/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10658/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10659/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10660/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10661/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10662/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10663/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10664/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10665/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10666/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10667/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10668/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10669/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10670/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10671/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10672/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10673/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10674/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10675/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10676/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10677/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10678/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10679/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10680/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10681/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10682/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10683/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10684/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10685/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10686/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10687/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10688/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10689/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10690/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10691/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10692/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10693/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10694/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10695/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10696/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10697/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10698/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10699/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10700/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10701/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10702/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10703/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10704/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10705/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10706/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10707/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10708/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10709/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10710/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10711/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10712/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10713/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10714/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10715/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10716/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10717/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10718/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10719/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10720/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10721/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10722/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10723/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10724/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10725/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10726/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10727/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10728/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10729/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10730/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10731/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10732/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10733/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10734/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10735/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10736/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10737/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10738/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10739/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10740/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10741/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10742/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10743/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10744/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10745/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10746/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10747/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10748/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10749/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10750/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10751/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10752/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10753/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10754/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10755/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10756/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10757/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10758/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10759/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10760/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10761/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10762/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10763/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10764/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10765/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10766/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10767/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10768/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10769/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10770/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10771/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10772/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10773/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10774/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10775/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10776/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10777/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10778/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10779/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10780/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10781/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10782/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10783/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10784/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10785/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10786/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10787/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10788/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10789/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10790/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10791/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10792/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10793/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10794/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10795/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10796/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10797/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10798/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10799/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10800/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10801/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10802/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10803/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10804/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10805/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10806/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10807/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10808/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10809/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10810/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10811/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10812/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10813/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10814/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10815/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10816/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10817/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10818/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10819/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10820/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10821/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10822/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10823/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10824/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10825/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10826/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10827/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10828/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10829/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10830/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10831/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10832/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10833/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10834/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10835/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10836/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10837/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10838/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10839/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10840/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10841/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10842/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10843/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10844/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10845/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10846/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10847/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10848/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10849/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10850/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10851/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10852/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10853/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10854/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10855/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10856/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10857/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10858/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10859/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10860/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10861/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10862/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10863/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10864/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10865/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10866/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10867/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10868/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10869/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10870/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10871/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10872/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10873/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10874/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10875/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10876/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10877/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10878/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10879/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10880/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10881/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10882/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10883/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10884/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10885/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10886/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10887/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [10888/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [10889/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10890/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10891/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10892/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10893/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10894/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10895/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10896/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10897/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10898/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10899/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10900/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10901/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10902/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10903/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10904/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10905/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10906/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10907/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10908/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10909/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10910/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10911/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10912/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10913/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10914/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10915/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10916/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10917/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10918/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10919/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10920/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10921/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10922/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10923/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10924/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10925/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10926/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10927/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10928/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10929/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10930/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10931/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10932/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10933/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10934/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10935/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10936/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10937/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10938/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10939/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10940/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10941/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10942/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10943/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10944/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10945/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10946/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10947/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10948/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10949/50000], Train Loss: 8088943.0000, Val Loss: 5298615.0000\n",
      "Epoch [10950/50000], Train Loss: 8088943.0000, Val Loss: 5298615.5000\n",
      "Epoch [10951/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10952/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10953/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10954/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10955/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10956/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10957/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10958/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10959/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10960/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10961/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10962/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10963/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10964/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10965/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10966/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10967/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10968/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10969/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10970/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10971/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10972/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10973/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10974/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10975/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10976/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10977/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10978/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10979/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10980/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10981/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10982/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10983/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10984/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10985/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10986/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10987/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10988/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10989/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10990/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10991/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10992/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10993/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10994/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10995/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10996/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10997/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10998/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [10999/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11000/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11001/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11002/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11003/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11004/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11005/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11006/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11007/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11008/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11009/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11010/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11011/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11012/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11013/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11014/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11015/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [11016/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11017/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11018/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11019/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11020/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11021/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11022/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11023/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11024/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11025/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11026/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11027/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11028/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11029/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11030/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11031/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11032/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11033/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11034/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11035/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11036/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11037/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11038/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11039/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11040/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11041/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11042/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [11043/50000], Train Loss: 8088941.5000, Val Loss: 5298617.0000\n",
      "Epoch [11044/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11045/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11046/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11047/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11048/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11049/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11050/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11051/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11052/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11053/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11054/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11055/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11056/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11057/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11058/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11059/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11060/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11061/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11062/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11063/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11064/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11065/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11066/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11067/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11068/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11069/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11070/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11071/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11072/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11073/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11074/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11075/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11076/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11077/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11078/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11079/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11080/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11081/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11082/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11083/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11084/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11085/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11086/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11087/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11088/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11089/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11090/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11091/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11092/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11093/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11094/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11095/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11096/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11097/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11098/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11099/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11100/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11101/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11102/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11103/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11104/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11105/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11106/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11107/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11108/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11109/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11110/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [11111/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [11112/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11113/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11114/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11115/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11116/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11117/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11118/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11119/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11120/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11121/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11122/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11123/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11124/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11125/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11126/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11127/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11128/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11129/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11130/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11131/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11132/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11133/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11134/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11135/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11136/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11137/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11138/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11139/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11140/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11141/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11142/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11143/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11144/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11145/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11146/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11147/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [11148/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11149/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11150/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11151/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11152/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11153/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11154/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11155/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11156/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11157/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11158/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11159/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11160/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11161/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11162/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11163/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11164/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11165/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11166/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11167/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11168/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11169/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11170/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11171/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11172/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11173/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11174/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11175/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11176/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11177/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11178/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11179/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11180/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11181/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [11182/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11183/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11184/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11185/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11186/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11187/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11188/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11189/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11190/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11191/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11192/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11193/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11194/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11195/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11196/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11197/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11198/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11199/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11200/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11201/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11202/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11203/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11204/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11205/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11206/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11207/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11208/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11209/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11210/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11211/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11212/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11213/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11214/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11215/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11216/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11217/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11218/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [11219/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11220/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11221/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11222/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11223/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11224/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11225/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11226/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11227/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11228/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11229/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11230/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11231/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11232/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11233/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11234/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11235/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11236/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11237/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11238/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11239/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11240/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11241/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11242/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11243/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11244/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11245/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11246/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11247/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11248/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11249/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11250/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11251/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11252/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11253/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11254/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11255/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11256/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11257/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11258/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11259/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [11260/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11261/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11262/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11263/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11264/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11265/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11266/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11267/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11268/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11269/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11270/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11271/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11272/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11273/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11274/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11275/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11276/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11277/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11278/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11279/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11280/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11281/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11282/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11283/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11284/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11285/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11286/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11287/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11288/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11289/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11290/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11291/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11292/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [11293/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11294/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11295/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11296/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11297/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11298/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11299/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11300/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11301/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11302/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11303/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11304/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11305/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11306/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11307/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11308/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11309/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11310/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11311/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11312/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11313/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11314/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11315/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11316/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11317/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11318/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11319/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11320/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11321/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11322/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11323/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11324/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11325/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11326/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11327/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11328/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11329/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11330/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11331/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11332/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11333/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11334/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11335/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11336/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11337/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11338/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11339/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11340/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11341/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11342/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11343/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11344/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11345/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11346/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11347/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11348/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11349/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11350/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11351/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11352/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11353/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11354/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11355/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11356/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11357/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11358/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11359/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11360/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11361/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11362/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11363/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11364/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11365/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11366/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11367/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11368/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11369/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11370/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11371/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [11372/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [11373/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11374/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11375/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11376/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11377/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11378/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11379/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11380/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11381/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11382/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11383/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11384/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11385/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11386/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11387/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11388/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11389/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11390/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11391/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11392/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11393/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11394/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11395/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11396/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11397/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11398/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11399/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11400/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11401/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11402/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11403/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11404/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11405/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11406/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11407/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11408/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11409/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11410/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11411/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11412/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11413/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11414/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11415/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11416/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11417/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11418/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11419/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11420/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11421/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11422/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11423/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11424/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11425/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11426/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11427/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11428/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11429/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11430/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11431/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11432/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11433/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11434/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11435/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11436/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11437/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11438/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11439/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11440/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11441/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11442/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11443/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11444/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11445/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11446/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11447/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11448/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11449/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11450/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11451/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11452/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11453/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11454/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11455/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11456/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [11457/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [11458/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11459/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11460/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11461/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11462/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11463/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11464/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11465/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11466/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11467/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11468/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11469/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11470/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11471/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11472/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11473/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11474/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11475/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11476/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11477/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11478/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11479/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11480/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11481/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11482/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11483/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11484/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11485/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11486/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11487/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11488/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11489/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11490/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11491/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11492/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11493/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11494/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11495/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11496/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11497/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11498/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11499/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11500/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11501/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11502/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11503/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11504/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11505/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11506/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11507/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11508/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11509/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11510/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11511/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11512/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11513/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11514/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11515/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11516/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11517/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11518/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11519/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11520/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11521/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11522/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11523/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11524/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11525/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11526/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11527/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11528/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11529/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11530/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11531/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11532/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11533/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11534/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11535/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11536/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11537/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11538/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [11539/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [11540/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11541/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11542/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11543/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11544/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11545/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11546/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11547/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11548/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11549/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11550/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11551/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11552/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11553/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11554/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11555/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11556/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11557/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11558/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11559/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11560/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11561/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11562/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11563/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11564/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11565/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11566/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11567/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11568/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11569/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11570/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11571/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11572/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11573/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11574/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11575/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11576/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11577/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11578/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11579/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11580/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11581/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11582/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11583/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11584/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11585/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11586/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11587/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11588/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11589/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11590/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11591/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11592/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11593/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11594/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11595/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11596/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11597/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11598/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11599/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11600/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11601/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11602/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11603/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11604/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11605/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11606/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11607/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11608/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11609/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11610/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11611/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11612/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11613/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11614/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11615/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11616/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11617/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11618/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11619/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11620/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11621/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11622/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11623/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11624/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11625/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11626/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11627/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11628/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11629/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11630/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11631/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11632/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11633/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11634/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11635/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11636/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11637/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11638/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11639/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11640/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11641/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11642/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11643/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11644/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11645/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11646/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11647/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11648/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11649/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11650/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11651/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11652/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11653/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11654/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11655/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11656/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11657/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11658/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11659/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11660/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11661/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11662/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11663/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11664/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [11665/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [11666/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11667/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11668/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11669/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11670/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11671/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11672/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11673/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11674/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11675/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11676/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11677/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11678/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11679/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11680/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11681/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11682/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11683/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11684/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11685/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11686/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11687/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11688/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11689/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11690/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11691/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11692/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11693/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11694/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11695/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11696/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11697/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11698/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11699/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11700/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11701/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11702/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11703/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11704/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11705/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11706/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11707/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [11708/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11709/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11710/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11711/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11712/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11713/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11714/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11715/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11716/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11717/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11718/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11719/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11720/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11721/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11722/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11723/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11724/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11725/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11726/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11727/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11728/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11729/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11730/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11731/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11732/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11733/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11734/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11735/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11736/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11737/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11738/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11739/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11740/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11741/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11742/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11743/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11744/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11745/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11746/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11747/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11748/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11749/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11750/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11751/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11752/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11753/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11754/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11755/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11756/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [11757/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11758/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11759/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11760/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11761/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11762/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11763/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11764/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11765/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11766/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11767/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11768/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11769/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11770/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11771/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11772/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11773/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11774/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11775/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11776/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11777/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11778/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11779/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11780/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11781/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11782/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11783/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11784/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11785/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11786/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11787/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11788/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11789/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11790/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11791/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11792/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11793/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11794/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11795/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11796/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11797/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11798/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11799/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11800/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11801/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11802/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11803/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11804/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [11805/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [11806/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11807/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11808/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11809/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11810/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11811/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11812/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11813/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11814/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11815/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11816/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11817/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11818/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11819/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11820/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11821/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11822/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11823/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11824/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11825/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11826/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11827/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11828/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11829/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11830/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11831/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11832/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11833/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11834/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11835/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11836/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11837/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11838/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11839/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11840/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11841/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11842/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11843/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11844/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11845/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11846/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11847/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11848/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11849/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11850/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11851/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11852/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11853/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11854/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11855/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11856/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11857/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11858/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11859/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11860/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11861/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11862/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [11863/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11864/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11865/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11866/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11867/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11868/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11869/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11870/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11871/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11872/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11873/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11874/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11875/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11876/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11877/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11878/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11879/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11880/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11881/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11882/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11883/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11884/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11885/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11886/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11887/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11888/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11889/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11890/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11891/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11892/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11893/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11894/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11895/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11896/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11897/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11898/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11899/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11900/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11901/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11902/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11903/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11904/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11905/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [11906/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11907/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11908/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11909/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11910/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11911/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11912/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11913/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11914/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11915/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11916/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11917/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11918/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11919/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11920/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11921/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11922/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11923/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11924/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11925/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11926/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11927/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11928/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11929/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11930/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11931/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11932/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11933/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11934/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11935/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11936/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11937/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11938/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11939/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11940/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11941/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11942/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11943/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11944/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11945/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11946/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11947/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11948/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11949/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11950/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11951/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11952/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11953/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11954/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11955/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11956/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11957/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11958/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11959/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [11960/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [11961/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11962/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11963/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11964/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11965/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11966/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11967/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11968/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11969/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11970/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11971/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11972/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11973/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11974/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11975/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11976/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11977/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11978/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11979/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11980/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11981/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11982/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11983/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11984/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11985/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11986/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11987/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11988/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11989/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11990/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11991/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11992/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11993/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11994/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11995/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11996/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11997/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11998/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [11999/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12000/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12001/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12002/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12003/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12004/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12005/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12006/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12007/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12008/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12009/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12010/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12011/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12012/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [12013/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12014/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12015/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12016/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12017/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12018/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12019/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12020/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12021/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12022/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12023/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12024/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12025/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12026/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12027/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12028/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12029/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12030/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12031/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12032/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12033/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12034/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12035/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12036/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12037/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12038/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12039/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12040/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12041/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12042/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12043/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12044/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12045/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12046/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12047/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12048/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12049/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12050/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12051/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12052/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12053/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12054/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12055/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12056/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12057/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12058/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12059/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12060/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12061/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12062/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12063/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12064/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12065/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12066/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12067/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12068/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12069/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12070/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12071/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12072/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12073/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12074/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12075/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12076/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12077/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12078/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12079/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12080/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12081/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12082/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12083/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12084/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12085/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12086/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12087/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12088/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12089/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12090/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12091/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12092/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12093/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12094/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12095/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12096/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12097/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12098/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12099/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12100/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12101/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12102/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12103/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12104/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12105/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12106/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12107/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12108/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12109/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12110/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12111/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12112/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12113/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12114/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12115/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12116/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12117/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12118/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12119/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12120/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [12121/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [12122/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12123/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12124/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12125/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12126/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12127/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12128/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12129/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12130/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12131/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12132/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12133/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12134/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12135/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12136/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12137/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12138/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12139/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12140/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12141/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12142/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12143/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12144/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12145/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12146/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12147/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12148/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12149/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12150/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12151/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12152/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12153/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12154/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12155/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12156/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12157/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12158/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12159/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12160/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12161/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12162/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12163/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12164/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12165/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12166/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12167/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12168/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12169/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12170/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12171/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12172/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12173/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12174/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12175/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12176/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12177/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12178/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12179/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12180/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12181/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12182/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12183/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12184/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12185/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12186/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12187/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12188/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12189/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12190/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12191/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12192/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12193/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12194/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12195/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12196/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12197/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12198/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12199/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12200/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12201/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12202/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12203/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12204/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12205/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12206/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12207/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12208/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12209/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12210/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12211/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12212/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12213/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12214/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12215/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12216/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12217/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12218/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12219/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12220/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12221/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12222/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12223/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12224/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12225/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12226/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12227/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12228/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12229/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12230/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12231/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12232/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12233/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12234/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12235/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12236/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12237/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12238/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12239/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12240/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12241/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12242/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12243/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [12244/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [12245/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12246/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12247/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12248/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12249/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12250/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12251/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12252/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12253/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12254/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12255/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12256/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12257/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12258/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12259/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12260/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12261/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12262/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12263/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12264/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12265/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12266/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12267/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12268/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12269/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12270/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12271/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12272/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12273/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12274/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12275/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12276/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12277/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12278/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12279/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12280/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12281/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12282/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12283/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12284/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12285/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12286/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12287/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12288/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12289/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12290/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12291/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12292/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12293/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12294/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12295/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12296/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12297/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12298/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12299/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12300/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12301/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12302/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12303/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12304/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12305/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12306/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12307/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12308/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12309/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12310/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12311/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12312/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12313/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12314/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12315/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12316/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12317/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12318/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12319/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12320/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12321/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12322/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12323/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12324/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12325/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12326/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12327/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12328/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12329/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12330/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12331/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12332/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12333/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12334/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12335/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12336/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12337/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12338/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12339/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12340/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12341/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12342/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12343/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12344/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12345/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12346/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12347/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12348/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12349/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12350/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12351/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12352/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12353/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12354/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12355/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12356/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12357/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12358/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12359/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12360/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12361/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [12362/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12363/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12364/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12365/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12366/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12367/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12368/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12369/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12370/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12371/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12372/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12373/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12374/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12375/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12376/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12377/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12378/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12379/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12380/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12381/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12382/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12383/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12384/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12385/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12386/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12387/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12388/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12389/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12390/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12391/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12392/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12393/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12394/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12395/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12396/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12397/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12398/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12399/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12400/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12401/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12402/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12403/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12404/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12405/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12406/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12407/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12408/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12409/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12410/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12411/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12412/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12413/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12414/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12415/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12416/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12417/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12418/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12419/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12420/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12421/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12422/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12423/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12424/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12425/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12426/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12427/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12428/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12429/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12430/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12431/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12432/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12433/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [12434/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [12435/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12436/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12437/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12438/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12439/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12440/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12441/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12442/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12443/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12444/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12445/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12446/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12447/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12448/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12449/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12450/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12451/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12452/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12453/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12454/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12455/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12456/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12457/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12458/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12459/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12460/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12461/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12462/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12463/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12464/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12465/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12466/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12467/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12468/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12469/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12470/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12471/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12472/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12473/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12474/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12475/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12476/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12477/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12478/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12479/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12480/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12481/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12482/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12483/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12484/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12485/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12486/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12487/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12488/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12489/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12490/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12491/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12492/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12493/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12494/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12495/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12496/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12497/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12498/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12499/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12500/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12501/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12502/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12503/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12504/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12505/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12506/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12507/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12508/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12509/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12510/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12511/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12512/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12513/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12514/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12515/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12516/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12517/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12518/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12519/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12520/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12521/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12522/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12523/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12524/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12525/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12526/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12527/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12528/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12529/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12530/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12531/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12532/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12533/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12534/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12535/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12536/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12537/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12538/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12539/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12540/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12541/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12542/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12543/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12544/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12545/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12546/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12547/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12548/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12549/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12550/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12551/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12552/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12553/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12554/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12555/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12556/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12557/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12558/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12559/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12560/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12561/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12562/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12563/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12564/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12565/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12566/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12567/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12568/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12569/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [12570/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [12571/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12572/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12573/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12574/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12575/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12576/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12577/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12578/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12579/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12580/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12581/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12582/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12583/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12584/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12585/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12586/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12587/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12588/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12589/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12590/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12591/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12592/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12593/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12594/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12595/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12596/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12597/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12598/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12599/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12600/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12601/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12602/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12603/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12604/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12605/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12606/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12607/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12608/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12609/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12610/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12611/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12612/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12613/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12614/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12615/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12616/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12617/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12618/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12619/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12620/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12621/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12622/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12623/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12624/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12625/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12626/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12627/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12628/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12629/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12630/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12631/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12632/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12633/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12634/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12635/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12636/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12637/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12638/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12639/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12640/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12641/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12642/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12643/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12644/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12645/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12646/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12647/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12648/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12649/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12650/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12651/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12652/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12653/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12654/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12655/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12656/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12657/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12658/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12659/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12660/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12661/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12662/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12663/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12664/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12665/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12666/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12667/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12668/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12669/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12670/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12671/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12672/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12673/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12674/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12675/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12676/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12677/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12678/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12679/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12680/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12681/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12682/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12683/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12684/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12685/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12686/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12687/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12688/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12689/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12690/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12691/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12692/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12693/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12694/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12695/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12696/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12697/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12698/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12699/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12700/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12701/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12702/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12703/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12704/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12705/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12706/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12707/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12708/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12709/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12710/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12711/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12712/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12713/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12714/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12715/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12716/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12717/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12718/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12719/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12720/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12721/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12722/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [12723/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [12724/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12725/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12726/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12727/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12728/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12729/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12730/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12731/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12732/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12733/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12734/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12735/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12736/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12737/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12738/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12739/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12740/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12741/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12742/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12743/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12744/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12745/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12746/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12747/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12748/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12749/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12750/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12751/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12752/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12753/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12754/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12755/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12756/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12757/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12758/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12759/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12760/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12761/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12762/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12763/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12764/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12765/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12766/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12767/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12768/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12769/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12770/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12771/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12772/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12773/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12774/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12775/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12776/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12777/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12778/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12779/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12780/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12781/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12782/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12783/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12784/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12785/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12786/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12787/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12788/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12789/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12790/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12791/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12792/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12793/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12794/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12795/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12796/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12797/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12798/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12799/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12800/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12801/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12802/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12803/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12804/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12805/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12806/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12807/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12808/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12809/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12810/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12811/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12812/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12813/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12814/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12815/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12816/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12817/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12818/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12819/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12820/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12821/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12822/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12823/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12824/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12825/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12826/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12827/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12828/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12829/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12830/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12831/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12832/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12833/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12834/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12835/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12836/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12837/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12838/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12839/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12840/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12841/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12842/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12843/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12844/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12845/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12846/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12847/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12848/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12849/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12850/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12851/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12852/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12853/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12854/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12855/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12856/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12857/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12858/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12859/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12860/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12861/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12862/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12863/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12864/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12865/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12866/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12867/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12868/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12869/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12870/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12871/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12872/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12873/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12874/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12875/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12876/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12877/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12878/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12879/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12880/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12881/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12882/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12883/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12884/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [12885/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12886/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12887/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12888/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12889/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12890/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12891/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12892/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12893/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12894/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12895/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12896/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12897/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12898/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12899/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12900/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12901/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12902/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12903/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12904/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12905/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12906/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12907/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12908/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12909/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12910/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12911/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12912/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12913/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12914/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12915/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12916/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12917/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12918/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12919/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12920/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12921/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12922/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12923/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12924/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12925/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12926/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12927/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12928/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12929/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12930/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12931/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12932/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12933/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12934/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12935/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12936/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12937/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12938/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12939/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12940/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12941/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12942/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12943/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12944/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12945/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12946/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12947/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12948/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12949/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12950/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12951/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12952/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12953/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12954/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12955/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12956/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12957/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12958/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12959/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12960/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12961/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12962/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12963/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12964/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [12965/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12966/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12967/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12968/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12969/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12970/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12971/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12972/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12973/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12974/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12975/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12976/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12977/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12978/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12979/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12980/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12981/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12982/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12983/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12984/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12985/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12986/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12987/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12988/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12989/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12990/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12991/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12992/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12993/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12994/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12995/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12996/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12997/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12998/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [12999/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13000/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13001/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13002/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13003/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13004/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13005/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13006/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13007/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13008/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13009/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13010/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13011/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13012/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13013/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13014/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13015/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13016/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13017/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13018/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13019/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13020/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13021/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13022/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13023/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13024/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13025/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13026/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13027/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13028/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13029/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13030/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13031/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13032/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13033/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13034/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13035/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13036/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13037/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13038/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13039/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13040/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13041/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13042/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13043/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13044/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13045/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13046/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13047/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [13048/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13049/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13050/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13051/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13052/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13053/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13054/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13055/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13056/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13057/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13058/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13059/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13060/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13061/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13062/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13063/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13064/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13065/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13066/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13067/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13068/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13069/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13070/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13071/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13072/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13073/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13074/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13075/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13076/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13077/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13078/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13079/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13080/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13081/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13082/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13083/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13084/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13085/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13086/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13087/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13088/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13089/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13090/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13091/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13092/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13093/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13094/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13095/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13096/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13097/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13098/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13099/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13100/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13101/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13102/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13103/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13104/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13105/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13106/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13107/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13108/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13109/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13110/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13111/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13112/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13113/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13114/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13115/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13116/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13117/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13118/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13119/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13120/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13121/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13122/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13123/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13124/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13125/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13126/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13127/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13128/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13129/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13130/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13131/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13132/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13133/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13134/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13135/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13136/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13137/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13138/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13139/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13140/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13141/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13142/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13143/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13144/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13145/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13146/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13147/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13148/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13149/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [13150/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13151/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13152/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13153/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13154/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13155/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13156/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13157/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13158/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13159/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13160/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13161/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13162/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13163/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13164/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13165/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13166/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13167/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13168/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13169/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13170/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13171/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13172/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13173/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13174/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13175/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13176/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13177/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13178/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13179/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13180/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13181/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13182/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13183/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13184/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13185/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13186/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13187/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13188/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13189/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13190/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13191/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13192/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13193/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13194/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13195/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13196/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13197/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13198/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13199/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13200/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13201/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13202/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13203/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13204/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13205/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13206/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13207/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13208/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13209/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13210/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13211/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13212/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13213/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13214/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13215/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13216/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13217/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13218/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13219/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13220/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13221/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13222/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13223/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13224/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13225/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13226/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13227/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13228/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13229/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13230/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13231/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13232/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13233/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13234/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13235/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13236/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13237/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13238/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13239/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13240/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13241/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13242/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [13243/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [13244/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13245/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13246/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13247/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13248/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13249/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13250/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13251/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13252/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13253/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13254/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13255/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13256/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13257/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13258/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13259/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13260/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13261/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13262/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13263/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13264/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13265/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13266/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13267/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13268/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13269/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13270/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13271/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13272/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13273/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13274/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13275/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13276/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13277/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13278/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13279/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13280/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13281/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13282/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13283/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13284/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13285/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13286/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13287/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13288/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13289/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13290/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13291/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13292/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13293/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13294/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13295/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13296/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13297/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13298/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13299/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13300/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13301/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13302/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13303/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13304/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13305/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13306/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13307/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13308/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13309/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13310/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13311/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13312/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13313/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13314/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13315/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13316/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13317/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13318/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13319/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13320/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13321/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13322/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13323/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13324/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13325/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13326/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13327/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13328/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13329/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13330/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13331/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13332/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13333/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13334/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13335/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13336/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13337/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13338/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13339/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13340/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13341/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13342/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13343/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13344/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13345/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13346/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13347/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13348/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13349/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13350/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13351/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13352/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13353/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13354/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13355/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13356/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13357/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13358/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13359/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13360/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13361/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13362/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13363/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13364/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13365/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13366/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13367/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13368/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13369/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13370/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13371/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13372/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13373/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13374/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13375/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13376/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13377/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13378/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13379/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13380/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13381/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13382/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13383/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13384/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13385/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13386/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13387/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13388/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13389/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13390/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13391/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13392/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13393/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13394/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13395/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13396/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13397/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13398/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13399/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13400/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13401/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13402/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13403/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13404/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13405/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13406/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13407/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13408/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13409/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13410/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13411/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13412/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13413/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13414/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13415/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13416/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13417/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13418/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13419/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13420/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13421/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13422/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13423/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13424/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13425/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13426/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13427/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13428/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13429/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13430/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13431/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13432/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13433/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13434/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13435/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13436/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13437/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13438/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13439/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13440/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13441/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13442/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13443/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13444/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13445/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13446/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13447/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13448/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13449/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13450/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13451/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13452/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13453/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13454/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13455/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13456/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13457/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [13458/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13459/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13460/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13461/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13462/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13463/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13464/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13465/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13466/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13467/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13468/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13469/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13470/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13471/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13472/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13473/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13474/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13475/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13476/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13477/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13478/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13479/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13480/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13481/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13482/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13483/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13484/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13485/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13486/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13487/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13488/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13489/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13490/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13491/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13492/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13493/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13494/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13495/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13496/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13497/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13498/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13499/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13500/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13501/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13502/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13503/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13504/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13505/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13506/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13507/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13508/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13509/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13510/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13511/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13512/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13513/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13514/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13515/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13516/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13517/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13518/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13519/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13520/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13521/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13522/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13523/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13524/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13525/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13526/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13527/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13528/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13529/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13530/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13531/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13532/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13533/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13534/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13535/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13536/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13537/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13538/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13539/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13540/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13541/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13542/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13543/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13544/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13545/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13546/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13547/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13548/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13549/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13550/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13551/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13552/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13553/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13554/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13555/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13556/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13557/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13558/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13559/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13560/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13561/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13562/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13563/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13564/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13565/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13566/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13567/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13568/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13569/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13570/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13571/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13572/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13573/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13574/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13575/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13576/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13577/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13578/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13579/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13580/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13581/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13582/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13583/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13584/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13585/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13586/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13587/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13588/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13589/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13590/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13591/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13592/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13593/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13594/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13595/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13596/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13597/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13598/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13599/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13600/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13601/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13602/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13603/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13604/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13605/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13606/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13607/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13608/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13609/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13610/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13611/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13612/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13613/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13614/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13615/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13616/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13617/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13618/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13619/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13620/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13621/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13622/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13623/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13624/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13625/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13626/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13627/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13628/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13629/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13630/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13631/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13632/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13633/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13634/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13635/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13636/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13637/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13638/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13639/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13640/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13641/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13642/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13643/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13644/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13645/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13646/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13647/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13648/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13649/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13650/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13651/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13652/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13653/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13654/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13655/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13656/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13657/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13658/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13659/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13660/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13661/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13662/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13663/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13664/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13665/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13666/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13667/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13668/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13669/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13670/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13671/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13672/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13673/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13674/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13675/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13676/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13677/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13678/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13679/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13680/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13681/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13682/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13683/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13684/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13685/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13686/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13687/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13688/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13689/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13690/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13691/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13692/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13693/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13694/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13695/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13696/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13697/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13698/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13699/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13700/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13701/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [13702/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [13703/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13704/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13705/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13706/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13707/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13708/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13709/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13710/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13711/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13712/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13713/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13714/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13715/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13716/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13717/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13718/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13719/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13720/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13721/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13722/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13723/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13724/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13725/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13726/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13727/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13728/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13729/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13730/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13731/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13732/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13733/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13734/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13735/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13736/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13737/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13738/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13739/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13740/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13741/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13742/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13743/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13744/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13745/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13746/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13747/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13748/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13749/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13750/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13751/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13752/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13753/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13754/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13755/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13756/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13757/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13758/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13759/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13760/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13761/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13762/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13763/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13764/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13765/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13766/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13767/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13768/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13769/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13770/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13771/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13772/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13773/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13774/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13775/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13776/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13777/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13778/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13779/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13780/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13781/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13782/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13783/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13784/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13785/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13786/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13787/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13788/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13789/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13790/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13791/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13792/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13793/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13794/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13795/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13796/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13797/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13798/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13799/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13800/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13801/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13802/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13803/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13804/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13805/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13806/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13807/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13808/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13809/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13810/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13811/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13812/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13813/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13814/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13815/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13816/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13817/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13818/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13819/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13820/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13821/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13822/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13823/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13824/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13825/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13826/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13827/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13828/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13829/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13830/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13831/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13832/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13833/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13834/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13835/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13836/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13837/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13838/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13839/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13840/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13841/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13842/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13843/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13844/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13845/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13846/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13847/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13848/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13849/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13850/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13851/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13852/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13853/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13854/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13855/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13856/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13857/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13858/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13859/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13860/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13861/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13862/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13863/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13864/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13865/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13866/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13867/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13868/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13869/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13870/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13871/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13872/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13873/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13874/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13875/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13876/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13877/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13878/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13879/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13880/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13881/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13882/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13883/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13884/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13885/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13886/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13887/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13888/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13889/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13890/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13891/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13892/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13893/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13894/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13895/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13896/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13897/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13898/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13899/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13900/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13901/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13902/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13903/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13904/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13905/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13906/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13907/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13908/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13909/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13910/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13911/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13912/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13913/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13914/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13915/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13916/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13917/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13918/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13919/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13920/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13921/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13922/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13923/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13924/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13925/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13926/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13927/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13928/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13929/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13930/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13931/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13932/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13933/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13934/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13935/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13936/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13937/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13938/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13939/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13940/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13941/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13942/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13943/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13944/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13945/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13946/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13947/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13948/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13949/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13950/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13951/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13952/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13953/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13954/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [13955/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13956/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13957/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13958/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13959/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13960/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13961/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13962/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13963/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13964/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13965/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13966/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13967/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13968/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13969/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13970/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13971/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13972/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13973/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13974/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13975/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13976/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13977/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13978/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13979/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13980/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13981/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13982/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13983/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13984/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13985/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13986/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13987/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13988/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13989/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13990/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13991/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13992/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13993/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13994/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13995/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13996/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13997/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13998/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [13999/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14000/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14001/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14002/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14003/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14004/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14005/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14006/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14007/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14008/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14009/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14010/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14011/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14012/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14013/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14014/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14015/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14016/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14017/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14018/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14019/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14020/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14021/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14022/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14023/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14024/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14025/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14026/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14027/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14028/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14029/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14030/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14031/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14032/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14033/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14034/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14035/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14036/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14037/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14038/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14039/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14040/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14041/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14042/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14043/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14044/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14045/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14046/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14047/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14048/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14049/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14050/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14051/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14052/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14053/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14054/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14055/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14056/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14057/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14058/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14059/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14060/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14061/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14062/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14063/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14064/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14065/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14066/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14067/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14068/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14069/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14070/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14071/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14072/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14073/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14074/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14075/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14076/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14077/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14078/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14079/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14080/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14081/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14082/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14083/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14084/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14085/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14086/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14087/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14088/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14089/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14090/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14091/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14092/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14093/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14094/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14095/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14096/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14097/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14098/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14099/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14100/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14101/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14102/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14103/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14104/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14105/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14106/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14107/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14108/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14109/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14110/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14111/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14112/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14113/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14114/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14115/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14116/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14117/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14118/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14119/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14120/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14121/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14122/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14123/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14124/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14125/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14126/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14127/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14128/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14129/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14130/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14131/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14132/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14133/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14134/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14135/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14136/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14137/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14138/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14139/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14140/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14141/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14142/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14143/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14144/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14145/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14146/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14147/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14148/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14149/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14150/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14151/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14152/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14153/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14154/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14155/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14156/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14157/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14158/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14159/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14160/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14161/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14162/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14163/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14164/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14165/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14166/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14167/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14168/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14169/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14170/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14171/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14172/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14173/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14174/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14175/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14176/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14177/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14178/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14179/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14180/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14181/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14182/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14183/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14184/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14185/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14186/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14187/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14188/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14189/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14190/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14191/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14192/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14193/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14194/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14195/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14196/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14197/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14198/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14199/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14200/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14201/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14202/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14203/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14204/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14205/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14206/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14207/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14208/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14209/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14210/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14211/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14212/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14213/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14214/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14215/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14216/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14217/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14218/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14219/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14220/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14221/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14222/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14223/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14224/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14225/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14226/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14227/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14228/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14229/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14230/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14231/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14232/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14233/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14234/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14235/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14236/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14237/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14238/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14239/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14240/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14241/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14242/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14243/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14244/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14245/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14246/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14247/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14248/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14249/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14250/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14251/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14252/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14253/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14254/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14255/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14256/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14257/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14258/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14259/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14260/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14261/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14262/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14263/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14264/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14265/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14266/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14267/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14268/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14269/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14270/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14271/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14272/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [14273/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14274/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14275/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14276/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14277/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14278/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14279/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14280/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14281/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14282/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14283/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14284/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14285/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14286/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14287/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14288/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14289/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14290/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14291/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14292/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14293/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14294/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14295/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14296/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14297/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14298/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14299/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14300/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14301/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14302/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14303/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14304/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14305/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14306/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14307/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14308/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14309/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14310/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14311/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14312/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14313/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14314/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14315/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14316/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14317/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14318/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14319/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14320/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14321/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14322/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14323/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14324/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14325/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14326/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14327/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14328/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14329/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14330/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14331/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14332/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14333/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14334/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14335/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14336/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14337/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14338/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14339/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14340/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14341/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14342/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14343/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14344/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14345/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14346/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14347/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14348/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14349/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14350/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14351/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14352/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14353/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14354/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14355/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14356/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14357/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14358/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14359/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14360/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14361/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14362/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14363/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14364/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14365/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14366/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14367/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14368/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14369/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14370/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14371/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14372/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14373/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14374/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14375/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14376/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14377/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14378/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14379/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14380/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14381/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14382/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14383/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14384/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14385/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14386/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14387/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14388/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14389/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14390/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14391/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14392/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14393/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14394/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14395/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14396/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14397/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14398/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14399/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14400/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14401/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14402/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14403/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14404/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14405/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14406/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14407/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14408/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14409/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14410/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14411/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14412/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14413/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14414/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14415/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14416/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14417/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14418/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14419/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14420/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14421/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14422/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14423/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14424/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [14425/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14426/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14427/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14428/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14429/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14430/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14431/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14432/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14433/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14434/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14435/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14436/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14437/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14438/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14439/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14440/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14441/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14442/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14443/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14444/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14445/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14446/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14447/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14448/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14449/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14450/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14451/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14452/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14453/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14454/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14455/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14456/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14457/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14458/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14459/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14460/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14461/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14462/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14463/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14464/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14465/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14466/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14467/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14468/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14469/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14470/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14471/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14472/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14473/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14474/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14475/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14476/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14477/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14478/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14479/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14480/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14481/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14482/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14483/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14484/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14485/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14486/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14487/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14488/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14489/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14490/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14491/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14492/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14493/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14494/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14495/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14496/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14497/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14498/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14499/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14500/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14501/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14502/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14503/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14504/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14505/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14506/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14507/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14508/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14509/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14510/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14511/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14512/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14513/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14514/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14515/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14516/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14517/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14518/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14519/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14520/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14521/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14522/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14523/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14524/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14525/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14526/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14527/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14528/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14529/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14530/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14531/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14532/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14533/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14534/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14535/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14536/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14537/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14538/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14539/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14540/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14541/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14542/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14543/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14544/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14545/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14546/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14547/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14548/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14549/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14550/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14551/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14552/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14553/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14554/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14555/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14556/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14557/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14558/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14559/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14560/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14561/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14562/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14563/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14564/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14565/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14566/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14567/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14568/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14569/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14570/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14571/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14572/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14573/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14574/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14575/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14576/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14577/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14578/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14579/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14580/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14581/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14582/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14583/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14584/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14585/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14586/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14587/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14588/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14589/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14590/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14591/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14592/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14593/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14594/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14595/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14596/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14597/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14598/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14599/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14600/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14601/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14602/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14603/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14604/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14605/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14606/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14607/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14608/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14609/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14610/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14611/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14612/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14613/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14614/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14615/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14616/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14617/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14618/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14619/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14620/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14621/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14622/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14623/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14624/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14625/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14626/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14627/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [14628/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14629/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14630/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14631/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14632/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14633/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14634/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14635/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14636/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14637/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14638/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14639/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14640/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14641/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14642/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14643/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14644/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14645/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14646/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14647/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14648/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14649/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14650/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14651/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14652/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14653/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14654/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14655/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14656/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14657/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14658/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14659/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14660/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14661/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14662/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14663/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14664/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14665/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14666/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14667/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14668/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14669/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14670/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14671/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14672/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14673/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14674/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14675/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14676/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14677/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14678/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14679/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14680/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14681/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14682/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14683/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14684/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14685/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14686/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14687/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14688/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14689/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14690/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14691/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14692/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14693/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14694/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14695/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14696/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14697/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14698/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14699/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14700/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14701/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14702/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14703/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14704/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14705/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14706/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14707/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14708/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14709/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14710/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14711/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14712/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14713/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14714/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14715/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14716/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14717/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14718/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14719/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14720/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14721/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14722/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14723/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14724/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14725/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14726/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14727/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14728/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14729/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14730/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14731/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14732/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14733/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14734/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14735/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14736/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14737/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14738/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14739/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14740/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14741/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14742/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14743/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14744/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14745/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14746/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14747/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14748/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14749/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14750/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14751/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14752/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14753/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14754/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14755/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14756/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14757/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14758/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14759/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14760/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14761/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14762/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14763/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14764/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14765/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14766/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14767/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14768/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14769/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14770/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14771/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14772/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14773/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14774/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14775/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14776/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14777/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14778/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14779/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14780/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14781/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14782/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14783/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14784/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14785/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14786/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14787/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14788/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14789/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14790/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14791/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14792/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14793/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14794/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14795/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14796/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14797/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14798/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14799/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14800/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14801/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14802/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14803/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14804/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14805/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14806/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14807/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14808/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14809/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14810/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14811/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14812/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14813/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14814/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14815/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14816/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14817/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14818/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14819/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14820/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14821/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14822/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14823/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14824/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14825/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14826/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14827/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14828/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14829/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14830/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14831/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14832/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14833/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14834/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14835/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14836/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14837/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14838/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14839/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14840/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14841/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14842/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [14843/50000], Train Loss: 8088941.0000, Val Loss: 5298641.0000\n",
      "Epoch [14844/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14845/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14846/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14847/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14848/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14849/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14850/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14851/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14852/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14853/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14854/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14855/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14856/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14857/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14858/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14859/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14860/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14861/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14862/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14863/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14864/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14865/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14866/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14867/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14868/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14869/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14870/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14871/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14872/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14873/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14874/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14875/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14876/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14877/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14878/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14879/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14880/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14881/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14882/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14883/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14884/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14885/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14886/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14887/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14888/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14889/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14890/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14891/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14892/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14893/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14894/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14895/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14896/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14897/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14898/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14899/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14900/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14901/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14902/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14903/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14904/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14905/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14906/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14907/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14908/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14909/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14910/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14911/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14912/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14913/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14914/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14915/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14916/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14917/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14918/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14919/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14920/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14921/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14922/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14923/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14924/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14925/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14926/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14927/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14928/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14929/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14930/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14931/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14932/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14933/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14934/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14935/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14936/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14937/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14938/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14939/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14940/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14941/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14942/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14943/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14944/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14945/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14946/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14947/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14948/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14949/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14950/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14951/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14952/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14953/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14954/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14955/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14956/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14957/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14958/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14959/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14960/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14961/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14962/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14963/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14964/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14965/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14966/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14967/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14968/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14969/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14970/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14971/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14972/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14973/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14974/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14975/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14976/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14977/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14978/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14979/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14980/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14981/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14982/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14983/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14984/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14985/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14986/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14987/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14988/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14989/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14990/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14991/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14992/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14993/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14994/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14995/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14996/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14997/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14998/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [14999/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15000/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15001/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15002/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15003/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15004/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15005/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15006/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15007/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15008/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15009/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15010/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15011/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15012/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15013/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15014/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15015/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15016/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15017/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15018/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15019/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15020/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15021/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15022/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15023/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15024/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15025/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15026/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15027/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15028/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15029/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15030/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15031/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15032/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15033/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15034/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15035/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15036/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15037/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15038/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15039/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15040/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15041/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15042/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15043/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15044/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15045/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15046/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15047/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15048/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15049/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15050/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15051/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15052/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15053/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15054/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15055/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15056/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15057/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15058/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15059/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15060/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15061/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15062/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15063/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15064/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15065/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15066/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15067/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15068/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15069/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15070/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15071/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15072/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15073/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15074/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15075/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15076/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15077/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15078/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15079/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15080/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15081/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15082/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15083/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15084/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15085/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15086/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15087/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15088/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15089/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15090/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15091/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15092/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15093/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15094/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15095/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15096/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15097/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15098/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15099/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15100/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15101/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15102/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15103/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15104/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15105/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15106/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15107/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15108/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15109/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15110/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15111/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15112/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15113/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15114/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15115/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15116/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15117/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15118/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15119/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15120/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15121/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15122/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15123/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15124/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15125/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15126/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15127/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15128/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15129/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15130/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15131/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15132/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15133/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15134/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15135/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15136/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15137/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15138/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15139/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15140/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15141/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15142/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15143/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15144/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15145/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15146/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15147/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15148/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15149/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15150/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15151/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15152/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15153/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15154/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15155/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15156/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15157/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15158/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15159/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15160/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15161/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15162/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15163/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15164/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15165/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15166/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15167/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15168/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15169/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15170/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15171/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15172/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15173/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15174/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15175/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15176/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15177/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15178/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15179/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15180/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15181/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15182/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15183/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15184/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15185/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15186/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15187/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15188/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15189/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15190/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15191/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15192/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15193/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15194/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15195/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15196/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15197/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15198/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15199/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15200/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15201/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15202/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15203/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15204/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15205/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15206/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15207/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15208/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15209/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15210/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15211/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15212/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15213/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15214/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15215/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15216/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15217/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15218/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15219/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15220/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15221/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15222/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15223/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15224/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15225/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15226/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15227/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15228/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15229/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15230/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15231/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15232/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15233/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15234/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15235/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15236/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15237/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15238/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15239/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15240/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15241/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15242/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15243/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15244/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15245/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15246/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15247/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15248/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15249/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15250/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15251/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15252/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15253/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15254/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15255/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15256/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15257/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15258/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15259/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15260/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15261/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15262/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15263/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15264/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15265/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15266/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15267/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15268/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15269/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15270/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15271/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15272/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15273/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15274/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15275/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15276/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15277/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15278/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15279/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15280/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15281/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15282/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15283/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15284/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15285/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15286/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15287/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15288/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15289/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15290/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15291/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15292/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15293/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15294/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15295/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15296/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15297/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15298/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15299/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15300/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15301/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15302/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15303/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15304/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15305/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15306/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15307/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15308/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15309/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15310/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15311/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15312/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15313/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15314/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15315/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15316/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15317/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15318/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15319/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15320/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15321/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15322/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15323/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15324/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15325/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15326/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15327/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15328/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15329/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15330/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15331/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15332/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15333/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15334/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15335/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15336/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15337/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15338/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15339/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15340/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15341/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15342/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15343/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15344/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15345/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15346/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15347/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15348/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15349/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15350/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15351/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15352/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15353/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15354/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15355/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15356/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15357/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15358/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15359/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15360/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15361/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15362/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15363/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15364/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15365/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15366/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15367/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15368/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15369/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15370/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15371/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15372/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15373/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15374/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15375/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15376/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15377/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15378/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15379/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15380/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15381/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15382/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15383/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15384/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15385/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15386/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15387/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15388/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15389/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15390/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15391/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15392/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15393/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15394/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15395/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15396/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15397/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15398/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15399/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15400/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15401/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15402/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15403/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15404/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15405/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15406/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15407/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15408/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15409/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15410/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15411/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15412/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15413/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15414/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15415/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15416/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15417/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15418/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15419/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15420/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15421/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15422/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15423/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15424/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15425/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15426/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15427/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15428/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15429/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15430/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15431/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15432/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15433/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15434/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15435/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15436/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15437/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15438/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15439/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15440/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15441/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15442/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15443/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15444/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15445/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15446/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15447/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15448/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15449/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15450/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15451/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15452/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15453/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15454/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15455/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15456/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15457/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15458/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15459/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15460/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15461/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15462/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15463/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15464/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15465/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15466/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15467/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15468/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15469/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15470/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15471/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15472/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15473/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15474/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15475/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15476/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15477/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15478/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15479/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15480/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15481/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15482/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15483/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15484/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15485/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15486/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15487/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15488/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15489/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15490/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15491/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15492/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15493/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15494/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15495/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15496/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15497/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15498/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15499/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15500/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15501/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15502/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15503/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15504/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15505/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15506/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15507/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15508/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15509/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15510/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15511/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15512/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15513/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15514/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15515/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15516/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15517/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15518/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15519/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15520/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15521/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15522/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15523/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15524/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15525/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15526/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15527/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15528/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15529/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15530/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15531/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15532/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15533/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15534/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15535/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15536/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15537/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15538/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15539/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15540/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15541/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15542/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15543/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15544/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15545/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15546/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15547/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15548/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15549/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15550/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15551/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15552/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15553/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15554/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15555/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15556/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15557/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15558/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15559/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15560/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15561/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15562/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15563/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15564/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15565/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15566/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15567/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15568/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15569/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15570/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15571/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15572/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15573/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15574/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15575/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15576/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15577/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15578/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15579/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15580/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15581/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15582/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15583/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15584/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15585/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15586/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15587/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15588/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15589/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15590/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15591/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15592/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15593/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15594/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15595/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15596/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15597/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15598/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15599/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15600/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15601/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15602/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15603/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15604/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15605/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15606/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15607/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15608/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15609/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15610/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15611/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15612/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15613/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15614/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15615/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15616/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15617/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15618/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15619/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15620/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15621/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15622/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15623/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15624/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15625/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15626/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15627/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15628/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15629/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15630/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15631/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15632/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15633/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15634/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15635/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15636/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15637/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15638/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15639/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15640/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15641/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15642/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15643/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15644/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15645/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15646/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15647/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15648/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15649/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15650/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15651/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15652/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15653/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [15654/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [15655/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15656/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15657/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15658/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15659/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15660/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15661/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15662/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15663/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15664/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15665/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15666/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15667/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15668/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15669/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15670/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15671/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15672/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15673/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15674/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15675/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15676/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15677/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15678/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15679/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15680/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15681/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15682/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15683/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15684/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15685/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15686/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15687/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15688/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15689/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15690/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15691/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15692/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15693/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15694/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15695/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15696/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15697/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15698/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15699/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15700/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15701/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15702/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15703/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15704/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15705/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15706/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15707/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15708/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15709/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15710/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15711/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15712/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15713/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15714/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15715/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15716/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15717/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15718/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15719/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15720/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15721/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15722/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15723/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15724/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15725/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15726/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15727/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15728/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15729/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15730/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15731/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15732/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15733/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15734/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15735/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15736/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15737/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15738/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15739/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15740/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15741/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15742/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15743/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15744/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15745/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15746/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15747/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15748/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15749/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15750/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15751/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15752/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15753/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15754/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15755/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15756/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15757/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15758/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15759/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15760/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15761/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15762/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15763/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15764/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15765/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15766/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15767/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15768/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15769/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15770/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15771/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15772/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15773/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15774/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15775/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15776/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15777/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15778/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15779/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15780/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15781/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15782/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15783/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15784/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15785/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15786/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15787/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15788/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15789/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15790/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15791/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15792/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15793/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15794/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15795/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15796/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15797/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15798/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15799/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15800/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15801/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15802/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15803/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15804/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15805/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15806/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15807/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15808/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15809/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15810/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15811/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15812/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15813/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15814/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15815/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15816/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15817/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15818/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15819/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15820/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15821/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15822/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15823/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15824/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15825/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15826/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15827/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15828/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15829/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15830/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15831/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15832/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15833/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15834/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15835/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15836/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15837/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15838/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15839/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15840/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15841/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15842/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15843/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15844/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15845/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15846/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15847/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15848/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15849/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15850/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15851/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15852/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15853/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15854/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15855/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15856/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15857/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15858/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15859/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15860/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15861/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15862/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15863/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15864/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15865/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15866/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15867/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15868/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15869/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15870/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15871/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15872/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15873/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15874/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15875/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15876/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15877/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15878/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15879/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15880/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15881/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15882/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15883/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15884/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15885/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15886/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15887/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15888/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15889/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15890/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15891/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15892/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15893/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15894/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15895/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15896/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15897/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15898/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15899/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15900/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15901/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15902/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15903/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15904/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15905/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15906/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15907/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15908/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15909/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15910/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15911/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15912/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15913/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15914/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15915/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15916/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15917/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15918/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15919/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15920/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15921/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15922/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15923/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15924/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15925/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15926/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15927/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15928/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15929/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15930/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15931/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15932/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15933/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15934/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15935/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15936/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15937/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15938/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15939/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15940/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15941/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15942/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15943/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15944/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15945/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15946/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15947/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15948/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15949/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15950/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15951/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15952/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15953/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15954/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15955/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15956/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15957/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15958/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15959/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15960/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15961/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15962/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15963/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15964/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15965/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15966/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15967/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15968/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15969/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15970/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15971/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15972/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15973/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15974/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15975/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15976/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15977/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15978/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15979/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15980/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15981/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15982/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15983/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15984/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15985/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15986/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15987/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15988/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15989/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15990/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15991/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15992/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15993/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15994/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15995/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15996/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15997/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15998/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [15999/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16000/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16001/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16002/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16003/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16004/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16005/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16006/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16007/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16008/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16009/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16010/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16011/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16012/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16013/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16014/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16015/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16016/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16017/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16018/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16019/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16020/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16021/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16022/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16023/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16024/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16025/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16026/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16027/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16028/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16029/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16030/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16031/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16032/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16033/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16034/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16035/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [16036/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [16037/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16038/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16039/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16040/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16041/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16042/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16043/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16044/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16045/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16046/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16047/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16048/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16049/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16050/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16051/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16052/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16053/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16054/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16055/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16056/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16057/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16058/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16059/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16060/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16061/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16062/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16063/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16064/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16065/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16066/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16067/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16068/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16069/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16070/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16071/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16072/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16073/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16074/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16075/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16076/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16077/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16078/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16079/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16080/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16081/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16082/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16083/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16084/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16085/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16086/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16087/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16088/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16089/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16090/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16091/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16092/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16093/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16094/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16095/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16096/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16097/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16098/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16099/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16100/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16101/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16102/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16103/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16104/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16105/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16106/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16107/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16108/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16109/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16110/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16111/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16112/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16113/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16114/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16115/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16116/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16117/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16118/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16119/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16120/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16121/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16122/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16123/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16124/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16125/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16126/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16127/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16128/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16129/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16130/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16131/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16132/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16133/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16134/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16135/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16136/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16137/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16138/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16139/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16140/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16141/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16142/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16143/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16144/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16145/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16146/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16147/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16148/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16149/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16150/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16151/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16152/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16153/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16154/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16155/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16156/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16157/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16158/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16159/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16160/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16161/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16162/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16163/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16164/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16165/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16166/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16167/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16168/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16169/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16170/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16171/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16172/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16173/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16174/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16175/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16176/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16177/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16178/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16179/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16180/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16181/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16182/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16183/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16184/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16185/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16186/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16187/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16188/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16189/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16190/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16191/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16192/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16193/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16194/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16195/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16196/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16197/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16198/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16199/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16200/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16201/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16202/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16203/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16204/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16205/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16206/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16207/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16208/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16209/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16210/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16211/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16212/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16213/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16214/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16215/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16216/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16217/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16218/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16219/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16220/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16221/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16222/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16223/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16224/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16225/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16226/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16227/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16228/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16229/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16230/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16231/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16232/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16233/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16234/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16235/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16236/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16237/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16238/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16239/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16240/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16241/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16242/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16243/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16244/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16245/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16246/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16247/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16248/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16249/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16250/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16251/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16252/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16253/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16254/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16255/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16256/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16257/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16258/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16259/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16260/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16261/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16262/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16263/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16264/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16265/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16266/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16267/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16268/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16269/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16270/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16271/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16272/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16273/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16274/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16275/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16276/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16277/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16278/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16279/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16280/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16281/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16282/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16283/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16284/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16285/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16286/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16287/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16288/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16289/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16290/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16291/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16292/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16293/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16294/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16295/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16296/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16297/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16298/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16299/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16300/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16301/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16302/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16303/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16304/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16305/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16306/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16307/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16308/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16309/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16310/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16311/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16312/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16313/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16314/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16315/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16316/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16317/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16318/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16319/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16320/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16321/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16322/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16323/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16324/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16325/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16326/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16327/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16328/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16329/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16330/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16331/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16332/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16333/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16334/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16335/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16336/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16337/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16338/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16339/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16340/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16341/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16342/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16343/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16344/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16345/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16346/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16347/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16348/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16349/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16350/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16351/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16352/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16353/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16354/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16355/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16356/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16357/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16358/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16359/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16360/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16361/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16362/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16363/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16364/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16365/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16366/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16367/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16368/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16369/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16370/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16371/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16372/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16373/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16374/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16375/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16376/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16377/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16378/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16379/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16380/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16381/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16382/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16383/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16384/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16385/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16386/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16387/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16388/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16389/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16390/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16391/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16392/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16393/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16394/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16395/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16396/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16397/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16398/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16399/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16400/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16401/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16402/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16403/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16404/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16405/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16406/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16407/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16408/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16409/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16410/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16411/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16412/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16413/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16414/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16415/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16416/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16417/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16418/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16419/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16420/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16421/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16422/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16423/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16424/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16425/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16426/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16427/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16428/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16429/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16430/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16431/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16432/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16433/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16434/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16435/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16436/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16437/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16438/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16439/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16440/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16441/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16442/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16443/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16444/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16445/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16446/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16447/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16448/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16449/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16450/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16451/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16452/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16453/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16454/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16455/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16456/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16457/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16458/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16459/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [16460/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16461/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16462/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16463/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16464/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16465/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16466/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16467/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16468/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16469/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16470/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16471/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16472/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16473/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16474/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16475/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16476/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16477/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16478/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16479/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16480/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16481/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16482/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16483/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16484/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16485/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16486/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16487/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16488/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16489/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16490/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16491/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16492/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16493/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16494/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16495/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16496/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16497/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16498/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16499/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16500/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16501/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16502/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16503/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16504/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16505/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16506/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16507/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16508/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16509/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16510/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16511/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16512/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16513/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16514/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16515/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16516/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16517/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16518/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16519/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16520/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16521/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16522/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16523/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16524/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16525/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16526/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16527/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16528/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16529/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16530/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16531/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16532/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16533/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16534/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16535/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16536/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16537/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16538/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16539/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16540/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16541/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16542/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16543/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16544/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16545/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16546/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16547/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16548/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16549/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16550/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16551/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16552/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16553/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16554/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16555/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16556/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16557/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16558/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16559/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16560/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16561/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16562/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16563/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16564/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16565/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16566/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16567/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16568/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16569/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16570/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16571/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16572/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16573/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16574/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16575/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16576/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16577/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16578/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16579/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16580/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16581/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16582/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16583/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16584/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16585/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16586/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16587/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16588/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16589/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16590/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16591/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16592/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16593/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16594/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16595/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16596/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16597/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16598/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16599/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16600/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16601/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16602/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16603/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16604/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16605/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16606/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16607/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16608/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16609/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16610/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16611/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16612/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16613/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16614/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16615/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16616/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16617/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16618/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16619/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16620/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16621/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16622/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16623/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16624/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16625/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16626/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16627/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16628/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16629/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16630/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16631/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16632/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16633/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16634/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16635/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16636/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16637/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16638/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16639/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16640/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16641/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16642/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16643/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16644/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16645/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16646/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16647/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16648/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16649/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16650/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16651/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16652/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16653/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16654/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16655/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16656/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16657/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16658/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16659/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16660/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16661/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16662/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16663/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16664/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16665/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16666/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16667/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16668/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16669/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16670/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16671/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16672/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16673/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16674/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16675/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16676/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16677/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16678/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16679/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16680/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16681/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16682/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16683/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16684/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16685/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16686/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16687/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16688/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16689/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16690/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16691/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16692/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16693/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16694/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16695/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16696/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16697/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16698/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16699/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16700/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16701/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16702/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16703/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16704/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16705/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16706/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16707/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16708/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16709/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16710/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16711/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16712/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16713/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16714/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16715/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16716/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16717/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16718/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16719/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16720/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16721/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16722/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16723/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16724/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16725/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16726/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16727/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16728/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16729/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16730/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16731/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16732/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16733/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16734/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16735/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16736/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16737/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16738/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16739/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16740/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16741/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16742/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16743/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16744/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16745/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16746/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16747/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16748/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16749/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16750/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16751/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16752/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16753/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16754/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16755/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16756/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16757/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16758/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16759/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16760/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16761/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16762/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16763/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16764/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16765/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16766/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16767/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16768/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16769/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16770/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16771/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16772/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16773/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16774/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16775/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16776/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16777/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16778/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16779/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16780/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16781/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16782/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16783/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16784/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16785/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16786/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16787/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16788/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16789/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16790/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16791/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16792/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16793/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16794/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16795/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16796/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16797/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16798/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16799/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16800/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16801/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16802/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16803/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16804/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16805/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16806/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16807/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16808/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16809/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16810/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16811/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16812/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16813/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16814/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16815/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16816/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16817/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16818/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16819/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16820/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16821/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16822/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16823/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16824/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16825/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16826/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16827/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16828/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16829/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16830/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16831/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16832/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16833/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16834/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16835/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16836/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16837/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16838/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16839/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16840/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16841/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16842/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16843/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16844/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16845/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16846/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16847/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16848/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16849/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16850/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16851/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16852/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16853/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16854/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16855/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16856/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16857/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16858/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16859/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16860/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16861/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16862/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16863/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16864/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16865/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16866/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16867/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16868/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16869/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16870/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16871/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16872/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16873/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16874/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16875/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16876/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16877/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16878/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16879/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16880/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16881/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16882/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16883/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16884/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16885/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16886/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16887/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16888/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16889/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16890/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16891/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16892/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16893/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16894/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16895/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16896/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16897/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16898/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16899/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16900/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16901/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16902/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16903/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16904/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16905/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16906/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16907/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16908/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16909/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16910/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16911/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16912/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16913/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16914/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16915/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16916/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16917/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16918/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16919/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16920/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16921/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16922/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16923/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16924/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16925/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16926/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16927/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16928/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16929/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16930/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16931/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16932/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16933/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16934/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16935/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16936/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16937/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16938/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16939/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16940/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16941/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16942/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16943/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16944/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16945/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16946/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16947/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16948/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16949/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16950/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16951/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16952/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16953/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16954/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16955/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16956/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16957/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16958/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16959/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16960/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16961/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16962/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16963/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16964/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16965/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16966/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16967/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16968/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16969/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16970/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16971/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16972/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16973/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16974/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16975/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16976/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16977/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16978/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16979/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16980/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16981/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16982/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16983/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16984/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16985/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16986/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16987/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16988/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16989/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16990/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16991/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16992/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16993/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16994/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16995/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16996/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16997/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16998/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [16999/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17000/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17001/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17002/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17003/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17004/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17005/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17006/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17007/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17008/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17009/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17010/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17011/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17012/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17013/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17014/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17015/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17016/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17017/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17018/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17019/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17020/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17021/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17022/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17023/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17024/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17025/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17026/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17027/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17028/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17029/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17030/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17031/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17032/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17033/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17034/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17035/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17036/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17037/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17038/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17039/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17040/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17041/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17042/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17043/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17044/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17045/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17046/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17047/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17048/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17049/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17050/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17051/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [17052/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17053/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17054/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17055/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17056/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17057/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17058/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17059/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17060/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17061/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17062/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17063/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17064/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17065/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17066/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17067/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17068/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17069/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17070/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17071/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17072/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17073/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17074/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17075/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17076/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17077/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17078/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17079/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17080/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17081/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17082/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17083/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17084/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17085/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17086/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17087/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17088/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17089/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17090/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17091/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17092/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17093/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17094/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17095/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17096/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17097/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17098/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17099/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17100/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17101/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17102/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17103/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17104/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17105/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17106/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17107/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17108/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17109/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17110/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17111/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17112/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17113/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17114/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17115/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17116/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17117/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17118/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17119/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17120/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17121/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17122/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17123/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17124/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17125/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17126/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17127/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17128/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17129/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17130/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17131/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17132/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17133/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17134/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17135/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17136/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17137/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17138/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17139/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17140/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17141/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17142/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17143/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17144/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17145/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17146/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17147/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17148/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17149/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17150/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17151/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17152/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17153/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17154/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17155/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17156/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17157/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17158/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17159/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17160/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17161/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17162/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17163/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17164/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17165/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17166/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17167/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17168/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17169/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17170/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17171/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17172/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17173/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17174/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17175/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17176/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17177/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17178/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17179/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17180/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17181/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17182/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17183/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17184/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17185/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17186/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17187/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17188/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17189/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17190/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17191/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17192/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17193/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17194/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17195/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17196/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17197/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17198/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17199/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17200/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17201/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17202/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17203/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17204/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17205/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17206/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17207/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17208/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17209/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17210/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17211/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17212/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17213/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17214/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17215/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17216/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17217/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17218/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17219/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17220/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17221/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17222/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17223/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17224/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17225/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17226/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17227/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17228/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17229/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17230/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17231/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17232/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17233/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17234/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17235/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17236/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17237/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17238/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17239/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17240/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17241/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17242/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17243/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17244/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17245/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17246/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17247/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17248/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17249/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17250/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17251/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17252/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17253/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17254/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17255/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17256/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17257/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17258/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17259/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17260/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17261/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17262/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17263/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17264/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17265/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17266/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17267/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17268/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17269/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17270/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17271/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17272/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17273/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17274/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17275/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17276/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17277/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17278/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17279/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17280/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17281/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17282/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17283/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17284/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17285/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17286/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17287/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17288/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17289/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17290/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17291/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17292/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17293/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17294/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17295/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17296/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17297/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17298/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17299/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17300/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17301/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17302/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17303/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17304/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17305/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17306/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17307/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17308/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17309/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17310/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17311/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17312/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17313/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17314/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17315/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17316/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17317/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17318/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17319/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17320/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17321/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17322/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17323/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17324/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17325/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17326/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17327/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17328/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17329/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17330/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17331/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17332/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17333/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17334/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17335/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17336/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17337/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17338/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17339/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17340/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17341/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17342/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17343/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17344/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17345/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17346/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17347/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17348/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17349/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17350/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17351/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17352/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17353/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17354/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17355/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17356/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17357/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17358/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17359/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17360/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17361/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17362/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17363/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17364/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17365/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17366/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17367/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17368/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17369/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17370/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17371/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17372/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17373/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17374/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17375/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17376/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17377/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17378/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17379/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17380/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17381/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17382/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17383/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17384/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17385/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17386/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17387/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17388/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17389/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17390/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17391/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17392/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17393/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17394/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17395/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17396/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17397/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17398/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17399/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17400/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17401/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17402/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17403/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17404/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17405/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17406/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17407/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17408/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17409/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17410/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17411/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17412/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17413/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17414/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17415/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17416/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17417/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17418/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17419/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17420/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17421/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17422/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17423/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17424/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17425/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17426/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17427/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17428/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17429/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17430/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17431/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17432/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17433/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17434/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17435/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17436/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17437/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17438/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17439/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17440/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17441/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17442/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17443/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17444/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17445/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17446/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17447/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17448/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17449/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17450/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17451/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17452/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17453/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17454/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17455/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17456/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17457/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17458/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17459/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17460/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17461/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17462/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17463/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17464/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17465/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17466/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17467/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17468/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17469/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17470/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17471/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17472/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17473/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17474/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17475/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17476/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17477/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17478/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17479/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17480/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17481/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17482/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17483/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17484/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17485/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17486/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17487/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17488/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17489/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17490/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17491/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17492/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17493/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17494/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17495/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17496/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17497/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17498/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17499/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17500/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17501/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17502/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17503/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17504/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17505/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17506/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17507/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17508/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17509/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17510/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17511/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17512/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17513/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17514/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17515/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17516/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17517/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17518/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17519/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17520/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17521/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17522/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17523/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17524/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17525/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17526/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17527/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17528/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17529/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17530/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17531/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17532/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17533/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17534/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17535/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17536/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17537/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17538/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17539/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17540/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17541/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17542/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17543/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17544/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17545/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17546/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17547/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17548/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17549/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17550/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17551/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17552/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17553/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17554/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17555/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17556/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17557/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17558/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17559/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17560/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17561/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17562/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17563/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17564/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17565/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17566/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17567/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17568/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17570/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17571/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17572/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17573/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17574/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17575/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17576/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17577/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17578/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17579/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17580/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17581/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17582/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17583/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17585/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17586/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17587/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17588/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17589/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17590/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17591/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17592/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17593/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17594/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17595/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17596/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17597/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17598/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17599/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17600/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17601/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17602/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17603/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17604/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17605/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17606/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17607/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17608/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17609/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17610/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17611/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17612/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17613/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17614/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17615/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17616/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17617/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17618/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17619/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17620/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17621/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17622/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17623/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17624/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17625/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17626/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17627/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17628/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17629/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17630/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17631/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17632/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17633/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17634/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17635/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17636/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17637/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17638/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17639/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17640/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17641/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17642/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17643/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17644/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17645/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17646/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17647/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17648/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17649/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17650/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17651/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17652/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17653/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17654/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17655/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17656/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17657/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17658/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17659/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17660/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17661/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17662/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17663/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17664/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17665/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17666/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17667/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17668/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17669/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17670/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17671/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17672/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17673/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17674/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17675/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17676/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17677/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17678/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17679/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17680/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17681/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17682/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17683/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17684/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17685/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17686/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17687/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17688/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17689/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17690/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17691/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17692/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17693/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17694/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17695/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17696/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17697/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17698/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17699/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17700/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17701/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17702/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17703/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17704/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17705/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17706/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17707/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17708/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17709/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17710/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17711/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17712/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17713/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17714/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17715/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17716/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17717/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17718/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17719/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17720/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17721/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17722/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17723/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17724/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17725/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17726/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17727/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17728/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17729/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17730/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17731/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17732/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17733/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17734/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17735/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17736/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17737/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17738/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17739/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17740/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17741/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17742/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17743/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17744/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17745/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17746/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17747/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17748/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17749/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17750/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17751/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17752/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17753/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17754/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17755/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17756/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17757/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17758/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17759/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17760/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17761/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17762/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17763/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17764/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17765/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17766/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17767/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17768/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17769/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17770/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17771/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17772/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17773/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17774/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17775/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17776/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17777/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17778/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17779/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17780/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17781/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17782/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17783/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17784/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17785/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17786/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17787/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17788/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17789/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17790/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17791/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17792/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17793/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17794/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17795/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17796/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17797/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17798/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17799/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17800/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17801/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17802/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17803/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17804/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17805/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17806/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17807/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17808/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17809/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17810/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17811/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17812/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17813/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17814/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17815/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17816/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17817/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17818/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17819/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17820/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17821/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17822/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17823/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17824/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17825/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17826/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17827/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17828/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17829/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17830/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17831/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17832/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17833/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17834/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17835/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17836/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17837/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17838/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17839/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17840/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17841/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17842/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17843/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17844/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17845/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17846/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17847/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17848/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17849/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17850/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17851/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17852/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17853/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17854/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17855/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17856/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17857/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17858/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17859/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17860/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17861/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17862/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17863/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17864/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17865/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17866/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17867/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17868/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17869/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17870/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17871/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17872/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17873/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17874/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17875/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17876/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17877/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17878/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17879/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17880/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17881/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17882/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17883/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17884/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17885/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17886/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17887/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17888/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17889/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17890/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17891/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17892/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17893/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17894/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17895/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17896/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17897/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17898/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17899/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17900/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17901/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17902/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17903/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17904/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17905/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17906/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17907/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17908/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17909/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17910/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17911/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17912/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17913/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17914/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17915/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17916/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17917/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17918/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17919/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17920/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17921/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17922/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17923/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17924/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17925/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17926/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17927/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17928/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17929/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17930/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17931/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17932/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17933/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17934/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17935/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17936/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17937/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17938/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17939/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17940/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17941/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17942/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17943/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17945/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17946/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17947/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17948/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17949/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17950/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17951/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17952/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17953/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17954/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17955/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17956/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17957/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17958/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17959/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17960/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17961/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17962/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17963/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17964/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17965/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17966/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17967/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17968/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17969/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17970/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17971/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17972/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17973/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17974/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17975/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17976/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17977/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17978/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17979/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17980/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17981/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17982/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17983/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17984/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17985/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17986/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17987/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17988/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17989/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17990/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17991/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17992/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17993/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17994/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17995/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17996/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17997/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17998/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [17999/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18000/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18001/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18002/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18003/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18004/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18005/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18006/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18007/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18008/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18009/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18010/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18011/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18012/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18013/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18014/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18015/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18016/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18017/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18018/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18019/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18020/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18021/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18022/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18023/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18024/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18025/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18026/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18027/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18028/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18029/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18030/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18031/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18032/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18033/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18034/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18035/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18036/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18037/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18038/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18039/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18040/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18041/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18042/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18043/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18044/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18045/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18046/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18047/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18048/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18049/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18050/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18051/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18052/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18053/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18054/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18055/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18056/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18057/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18058/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18059/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18060/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18061/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18062/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18063/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18064/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18065/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18066/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18067/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18068/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18069/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18070/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18071/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18072/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18073/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18074/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18075/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18076/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18077/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18078/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18079/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18080/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18081/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18082/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18083/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18084/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18085/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18086/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18087/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18088/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18089/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18090/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18091/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18092/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18093/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18094/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18095/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18096/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18097/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18098/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18099/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18100/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18101/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18102/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18103/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18104/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18105/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18106/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18107/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18108/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18109/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18110/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18111/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18112/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18113/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18114/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18115/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18116/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18117/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18118/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18119/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18120/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18121/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18122/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18123/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18124/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18125/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18126/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18127/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18128/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18129/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18130/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18131/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18132/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18133/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18134/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18135/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18136/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18137/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18138/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18139/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18140/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18141/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18142/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18143/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18144/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18145/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18146/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18147/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18148/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18149/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18150/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18151/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18152/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18153/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18154/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18155/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18156/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18157/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18158/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18159/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18160/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18161/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18162/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18163/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18164/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18165/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18166/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18167/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18168/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18169/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18170/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18171/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18172/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18173/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18174/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18175/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18176/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18177/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18178/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18179/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18180/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18181/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18182/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18183/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18184/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18185/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18186/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18187/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18188/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18189/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18190/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18191/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18192/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18193/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18194/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18195/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18196/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18197/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18198/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18199/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18200/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18201/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18202/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18203/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18204/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18205/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18206/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18207/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18208/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18209/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18210/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18211/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18212/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18213/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18214/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18215/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18216/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18217/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18218/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18219/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18220/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18221/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18222/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18223/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18224/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18225/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18226/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18227/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18228/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18229/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18230/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18231/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18232/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18233/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18234/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18235/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18236/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18237/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18238/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18239/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18240/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18241/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18242/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18243/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18244/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18245/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18246/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18247/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18248/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18249/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18250/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18251/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18252/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18253/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18254/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18255/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18256/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18257/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18258/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18259/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18260/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18261/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18262/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18263/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18264/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18265/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18266/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18267/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18268/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18269/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18270/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18271/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18272/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18273/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18274/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18275/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18276/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18277/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18278/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18279/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18280/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18281/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18282/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18283/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18284/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18285/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18286/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18287/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18288/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18289/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18290/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18291/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18292/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18293/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18294/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18295/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18296/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18297/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18298/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18299/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18300/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18301/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18302/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18303/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18304/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18305/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18306/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18307/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18308/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18309/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18310/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18311/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18312/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18313/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18314/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18315/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18316/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18317/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18318/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18319/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18320/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18321/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18322/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18323/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18324/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18325/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18326/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18327/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18328/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18329/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18330/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18331/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18332/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18333/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18334/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18335/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18336/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18337/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18338/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18339/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18340/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18341/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18342/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18343/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18344/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18345/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18346/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18347/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18348/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18349/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18350/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18351/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18352/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18353/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18354/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18355/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18356/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18357/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18358/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18359/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18360/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18361/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18362/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18363/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18364/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18365/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18366/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18367/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18368/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18369/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18370/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18371/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18372/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18373/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18374/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18375/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18376/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18377/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18378/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18379/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18380/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18381/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18382/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18383/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18384/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18385/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18386/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18387/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18388/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18389/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18390/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18391/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18392/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18393/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18394/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18395/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18396/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18397/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18398/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18399/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18400/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18401/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18402/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18403/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18404/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18405/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18406/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18407/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18408/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18409/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18410/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18411/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18412/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18413/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18414/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18415/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18416/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18417/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18418/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18419/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18420/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18421/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18422/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18423/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18424/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18425/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18426/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18427/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18428/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18429/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18430/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18431/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18432/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18433/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18434/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18435/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18436/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18437/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18438/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18439/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18440/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18441/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18442/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18443/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18444/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18445/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18446/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18447/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18448/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18449/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18450/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18451/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18452/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18453/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18454/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18455/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18456/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18457/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18458/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18459/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18460/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18461/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18462/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18463/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18464/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18465/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18466/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18467/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18468/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18469/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18470/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18471/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18472/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18473/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18474/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18475/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18476/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18477/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18478/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18479/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18480/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18481/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18482/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18483/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18484/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18485/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18486/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18487/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18488/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18489/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18490/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18491/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18492/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18493/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18494/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18495/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18496/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18497/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18498/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18499/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18500/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18501/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18502/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18503/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18504/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18505/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18506/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18507/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18508/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18509/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18510/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18511/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18512/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18513/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18514/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18515/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18516/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18517/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18518/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18519/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18520/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18521/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18522/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18523/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18524/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18525/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18526/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18527/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18528/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18529/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18530/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18531/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18532/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18533/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18534/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18535/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18536/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18537/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18538/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18539/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18540/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18541/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18542/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18543/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18544/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18545/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18546/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18547/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18548/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18549/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18550/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18551/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18552/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18553/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18554/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18555/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18556/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18557/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18558/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18559/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18560/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18561/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18562/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18563/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18564/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18565/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18566/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18567/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18568/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18570/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18571/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18572/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18573/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18574/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18575/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18576/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18577/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18578/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18579/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18580/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18581/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18582/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18583/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18585/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18586/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18587/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18588/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18589/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18590/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18591/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18592/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18593/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18594/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18595/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18596/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18597/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18598/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18599/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18600/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18601/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18602/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18603/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18604/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18605/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18606/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18607/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18608/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18609/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18610/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18611/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18612/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18613/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18614/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18615/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18616/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18617/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18618/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18619/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18620/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18621/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18622/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18623/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18624/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18625/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18626/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18627/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18628/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18629/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18630/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18631/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18632/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18633/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18634/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18635/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18636/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18637/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18638/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18639/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18640/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18641/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18642/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18643/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18644/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18645/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18646/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18647/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18648/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18649/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18650/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18651/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18652/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18653/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18654/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18655/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18656/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18657/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18658/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18659/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18660/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18661/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18662/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18663/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18664/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18665/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18666/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18667/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18668/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18669/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18670/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18671/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18672/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18673/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18674/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18675/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18676/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18677/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18678/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18679/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18680/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18681/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18682/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18683/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18684/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18685/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18686/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18687/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18688/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18689/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18690/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18691/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18692/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18693/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18694/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18695/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18696/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18697/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18698/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18699/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18700/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18701/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18702/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18703/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18704/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18705/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18706/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18707/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18708/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18709/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18710/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18711/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18712/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18713/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18714/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18715/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18716/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18717/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18718/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18719/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18720/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18721/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18722/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18723/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18724/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18725/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18726/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18727/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18728/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18729/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18730/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18731/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18732/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18733/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18734/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18735/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18736/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18737/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18738/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18739/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18740/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18741/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18742/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18743/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18744/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18745/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18746/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18747/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18748/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18749/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18750/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18751/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18752/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18753/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18754/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18755/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18756/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18757/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18758/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18759/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18760/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18761/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18762/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18763/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18764/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18765/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18766/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18767/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18768/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18769/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18770/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18771/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18772/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18773/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18774/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18775/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18776/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18777/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18778/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18779/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18780/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18781/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18782/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18783/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18784/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18785/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18786/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18787/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18788/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18789/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18790/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18791/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18792/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18793/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18794/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18795/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18796/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18797/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18798/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18799/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18800/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18801/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18802/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18803/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18804/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18805/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18806/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18807/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18808/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18809/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18810/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18811/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18812/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18813/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18814/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18815/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18816/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18817/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18818/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18819/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18820/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18821/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18822/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18823/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18824/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18825/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18826/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18827/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18828/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18829/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18830/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18831/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18832/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18833/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18834/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18835/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18836/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18837/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18838/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18839/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18840/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18841/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18842/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18843/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18844/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18845/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18846/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18847/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18848/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18849/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18850/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18851/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18852/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18853/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18854/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18855/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18856/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18857/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18858/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18859/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18860/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18861/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18862/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18863/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18864/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18865/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18866/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18867/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18868/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18869/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18870/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18871/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18872/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18873/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18874/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18875/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18876/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18877/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18878/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18879/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18880/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18881/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18882/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18883/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18884/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18885/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18886/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18887/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18888/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18889/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18890/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18891/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18892/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18893/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18894/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18895/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18896/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18897/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18898/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18899/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18900/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18901/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18902/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18903/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18904/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18905/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18906/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18907/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18908/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18909/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18910/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18911/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18912/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18913/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18914/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18915/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18916/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18917/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18918/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18919/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18920/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18921/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18922/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18923/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18924/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18925/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18926/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18927/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18928/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18929/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18930/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18931/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18932/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18933/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18934/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18935/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18936/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18937/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18938/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18939/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18940/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18941/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18942/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18943/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18945/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18946/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18947/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18948/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18949/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18950/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18951/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18952/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18953/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18954/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18955/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18956/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18957/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18958/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18959/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18960/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18961/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18962/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18963/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18964/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18965/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18966/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18967/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18968/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18969/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18970/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18971/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18972/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18973/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18974/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18975/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18976/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18977/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18978/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18979/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18980/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18981/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18982/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18983/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18984/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18985/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18986/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18987/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18988/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18989/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18990/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18991/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18992/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18993/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18994/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18995/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18996/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18997/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18998/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [18999/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19000/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19001/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19002/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19003/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19004/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19005/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19006/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19007/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19008/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19009/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19010/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19011/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19012/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19013/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19014/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19015/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19016/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19017/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19018/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19019/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19020/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19021/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19022/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19023/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19024/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19025/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19026/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19027/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19028/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19029/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19030/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19031/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19032/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19033/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19034/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19035/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19036/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19037/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19038/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19039/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19040/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19041/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19042/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19043/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19044/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19045/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19046/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19047/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19048/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19049/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19050/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19051/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19052/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19053/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19054/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19055/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19056/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19057/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19058/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19059/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19060/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19061/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19062/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19063/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19064/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19065/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19066/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19067/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19068/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19069/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19070/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19071/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19072/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19073/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19074/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19075/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19076/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19077/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19078/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19079/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19080/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19081/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19082/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19083/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19084/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19085/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19086/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19087/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19088/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19089/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19090/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19091/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19092/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19093/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19094/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19095/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19096/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19097/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19098/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19099/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19100/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19101/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19102/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19103/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19104/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19105/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19106/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19107/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19108/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19109/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19110/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19111/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19112/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19113/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19114/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19115/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19116/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19117/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19118/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19119/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19120/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19121/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19122/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19123/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19124/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19125/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19126/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19127/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19128/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19129/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19130/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19131/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19132/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19133/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19134/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19135/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19136/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19137/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19138/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19139/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19140/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19141/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19142/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19143/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19144/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19145/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19146/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19147/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19148/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19149/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19150/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19151/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19152/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19153/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19154/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19155/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19156/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19157/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19158/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19159/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19160/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19161/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19162/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19163/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19164/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19165/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19166/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19167/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19168/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19169/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19170/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19171/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19172/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19173/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19174/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19175/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19176/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19177/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19178/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19179/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19180/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19181/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19182/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19183/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19184/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19185/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19186/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19187/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19188/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19189/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19190/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19191/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19192/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19193/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19194/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19195/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19196/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19197/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19198/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19199/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19200/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19201/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19202/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19203/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19204/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19205/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19206/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19207/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19208/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19209/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19210/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19211/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19212/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19213/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19214/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19215/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19216/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19217/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19218/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19219/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19220/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19221/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19222/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19223/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19224/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19225/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19226/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19227/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19228/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19229/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19230/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19231/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19232/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19233/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19234/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19235/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19236/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19237/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19238/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19239/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19240/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19241/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19242/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19243/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19244/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19245/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19246/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19247/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19248/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19249/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19250/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19251/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19252/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19253/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19254/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19255/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19256/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19257/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19258/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19259/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19260/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19261/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19262/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19263/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19264/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19265/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19266/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19267/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19268/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19269/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19270/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19271/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19272/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19273/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19274/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19275/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19276/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19277/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19278/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19279/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19280/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19281/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19282/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19283/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19284/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19285/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19286/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19287/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19288/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19289/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19290/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19291/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19292/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19293/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19294/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19295/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19296/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19297/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19298/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19299/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19300/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19301/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19302/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19303/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19304/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19305/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19306/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19307/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19308/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19309/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19310/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19311/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19312/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19313/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19314/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19315/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19316/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19317/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19318/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19319/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19320/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19321/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19322/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19323/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19324/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19325/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19326/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19327/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19328/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19329/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19330/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19331/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19332/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19333/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19334/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19335/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19336/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19337/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19338/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19339/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19340/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19341/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19342/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19343/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19344/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19345/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19346/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19347/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19348/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19349/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19350/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19351/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19352/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19353/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19354/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19355/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19356/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19357/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19358/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19359/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19360/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19361/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19362/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19363/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19364/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19365/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19366/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19367/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19368/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19369/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19370/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19371/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19372/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19373/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19374/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19375/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19376/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19377/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19378/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19379/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19380/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19381/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19382/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19383/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19384/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19385/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19386/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19387/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19388/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19389/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19390/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19391/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19392/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19393/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19394/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19395/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19396/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19397/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19398/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19399/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19400/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19401/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19402/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19403/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19404/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19405/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19406/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19407/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19408/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19409/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19410/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19411/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19412/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19413/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19414/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19415/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19416/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19417/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19418/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19419/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19420/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19421/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19422/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19423/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19424/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19425/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19426/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19427/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19428/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19429/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19430/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19431/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19432/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19433/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19434/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19435/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19436/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19437/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19438/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19439/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19440/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19441/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19442/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19443/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19444/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19445/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19446/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19447/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19448/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19449/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19450/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19451/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19452/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19453/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19454/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19455/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19456/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19457/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19458/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19459/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19460/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19461/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19462/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19463/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19464/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19465/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19466/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19467/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19468/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19469/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19470/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19471/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19472/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19473/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19474/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19475/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19476/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19477/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19478/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19479/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19480/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19481/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19482/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19483/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19484/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19485/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19486/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19487/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19488/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19489/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19490/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19491/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19492/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19493/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19494/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19495/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19496/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19497/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19498/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19499/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19500/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19501/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19502/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19503/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19504/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19505/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19506/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19507/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19508/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19509/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19510/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19511/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19512/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19513/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19514/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19515/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19516/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19517/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19518/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19519/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19520/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19521/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19522/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19523/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19524/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19525/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19526/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19527/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19528/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19529/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19530/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19531/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19532/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19533/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19534/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19535/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19536/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19537/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19538/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19539/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19540/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19541/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19542/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19543/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19544/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19545/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19546/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19547/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19548/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19549/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19550/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19551/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19552/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19553/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19554/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19555/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19556/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19557/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19558/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19559/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19560/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19561/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19562/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19563/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19564/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19565/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19566/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19567/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19568/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19570/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19571/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19572/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19573/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19574/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19575/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19576/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19577/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19578/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19579/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19580/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19581/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19582/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19583/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19585/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19586/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19587/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19588/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19589/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19590/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19591/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19592/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19593/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19594/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19595/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19596/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19597/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19598/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19599/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19600/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19601/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19602/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19603/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19604/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19605/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19606/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19607/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19608/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19609/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19610/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19611/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19612/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19613/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19614/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19615/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19616/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19617/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19618/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19619/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19620/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19621/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19622/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19623/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19624/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19625/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19626/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19627/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19628/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19629/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19630/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19631/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19632/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19633/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19634/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19635/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19636/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19637/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19638/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19639/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19640/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19641/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19642/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19643/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19644/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19645/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19646/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19647/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19648/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19649/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19650/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19651/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19652/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19653/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19654/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19655/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19656/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19657/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19658/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19659/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19660/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19661/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19662/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19663/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19664/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19665/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19666/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19667/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19668/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19669/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19670/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19671/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19672/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19673/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19674/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19675/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19676/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19677/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19678/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19679/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19680/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19681/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19682/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19683/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19684/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19685/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19686/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19687/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19688/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19689/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19690/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19691/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19692/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19693/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19694/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19695/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19696/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19697/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19698/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19699/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19700/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19701/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19702/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19703/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19704/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19705/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19706/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19707/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19708/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19709/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19710/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19711/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19712/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19713/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19714/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19715/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19716/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19717/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19718/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19719/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19720/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19721/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19722/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19723/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19724/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19725/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19726/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19727/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19728/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19729/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19730/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19731/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19732/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19733/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19734/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19735/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19736/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19737/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19738/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19739/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19740/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19741/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19742/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19743/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19744/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19745/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19746/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19747/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19748/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19749/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19750/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19751/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19752/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19753/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19754/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19755/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19756/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19757/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19758/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19759/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19760/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19761/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19762/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19763/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19764/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19765/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19766/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19767/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19768/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19769/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19770/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19771/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19772/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19773/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19774/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19775/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19776/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19777/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19778/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19779/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19780/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19781/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19782/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19783/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19784/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19785/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19786/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19787/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19788/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19789/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19790/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19791/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19792/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19793/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19794/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19795/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19796/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19797/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19798/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19799/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19800/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19801/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19802/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19803/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19804/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19805/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19806/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19807/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19808/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19809/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19810/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19811/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19812/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19813/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19814/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19815/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19816/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19817/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19818/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19819/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19820/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19821/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19822/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19823/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19824/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19825/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19826/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19827/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19828/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19829/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19830/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19831/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19832/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19833/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19834/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19835/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19836/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19837/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19838/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19839/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19840/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19841/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19842/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19843/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19844/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19845/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19846/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19847/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19848/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19849/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19850/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19851/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19852/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19853/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19854/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19855/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19856/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19857/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19858/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19859/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19860/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19861/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19862/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19863/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19864/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19865/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19866/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19867/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19868/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19869/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19870/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19871/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19872/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19873/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19874/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19875/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19876/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19877/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19878/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19879/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19880/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19881/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19882/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19883/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19884/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19885/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19886/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19887/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19888/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19889/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19890/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19891/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19892/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19893/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19894/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19895/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19896/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19897/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19898/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19899/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19900/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19901/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19902/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19903/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19904/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19905/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19906/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19907/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19908/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19909/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19910/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19911/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19912/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19913/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19914/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19915/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19916/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19917/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19918/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19919/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19920/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19921/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19922/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19923/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19924/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19925/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19926/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19927/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19928/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19929/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19930/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19931/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19932/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19933/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19934/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19935/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19936/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19937/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19938/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19939/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19940/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19941/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19942/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19943/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19945/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19946/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19947/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19948/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19949/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19950/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19951/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19952/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19953/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19954/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19955/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19956/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19957/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19958/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19959/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19960/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19961/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19962/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19963/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19964/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19965/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19966/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19967/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19968/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19969/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19970/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19971/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19972/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19973/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19974/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19975/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19976/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19977/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19978/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19979/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19980/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [19981/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19982/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19983/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19984/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19985/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19986/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19987/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19988/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19989/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19990/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19991/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19992/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19993/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19994/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19995/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [19996/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [19997/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19998/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [19999/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20000/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20001/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20002/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20003/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20004/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20005/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20006/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20007/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20008/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20009/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20010/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20011/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20012/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20013/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20014/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20015/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20016/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20017/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20018/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20019/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20020/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20021/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20022/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20023/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20024/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20025/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20026/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20027/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20028/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20029/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20030/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20031/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20032/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20033/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20034/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20035/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20036/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20037/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20038/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20039/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20040/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20041/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20042/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20043/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20044/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20045/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20046/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20047/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20048/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20049/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20050/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20051/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20052/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20053/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20054/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20055/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20056/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20057/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20058/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20059/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20060/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20061/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20062/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20063/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20064/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20065/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20066/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20067/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20068/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20069/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20070/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20071/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20072/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20073/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20074/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20075/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20076/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20077/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20078/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20079/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20080/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20081/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20082/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20083/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20084/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20085/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20086/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20087/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20088/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20089/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20090/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20091/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20092/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20093/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20094/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20095/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20096/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20097/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20098/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20099/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20100/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20101/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20102/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20103/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20104/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20105/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20106/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20107/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20108/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20109/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20110/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20111/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20112/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20113/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20114/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20115/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20116/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20117/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20118/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20119/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20120/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20121/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20122/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20123/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20124/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20125/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20126/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20127/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20128/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20129/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20130/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20131/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20132/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20133/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20134/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20135/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20136/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20137/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20138/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20139/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20140/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20141/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20142/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20143/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20144/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20145/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20146/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20147/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20148/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20149/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20150/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20151/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20152/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20153/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20154/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20155/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20156/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20157/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20158/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20159/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20160/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20161/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20162/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20163/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20164/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20165/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20166/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20167/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20168/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20169/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20170/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20171/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20172/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20173/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20174/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20175/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20176/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20177/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20178/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20179/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20180/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20181/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20182/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20183/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20184/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20185/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20186/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20187/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20188/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20189/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20190/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20191/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20192/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20193/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20194/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20195/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20196/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20197/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20198/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20199/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20200/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20201/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20202/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20203/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20204/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20205/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20206/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20207/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20208/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20209/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20210/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20211/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20212/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20213/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20214/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20215/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20216/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20217/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20218/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20219/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20220/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20221/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20222/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20223/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20224/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20225/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20226/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20227/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20228/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20229/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20230/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20231/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20232/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20233/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20234/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20235/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20236/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20237/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20238/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20239/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20240/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20241/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20242/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20243/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20244/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20245/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20246/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20247/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20248/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20249/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20250/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20251/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20252/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20253/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20254/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20255/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20256/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20257/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20258/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20259/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20260/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20261/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20262/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20263/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20264/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20265/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20266/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20267/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20268/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20269/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20270/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20271/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20272/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20273/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20274/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20275/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20276/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20277/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20278/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20279/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20280/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20281/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20282/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20283/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20284/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20285/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20286/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20287/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20288/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20289/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20290/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20291/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20292/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20293/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20294/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20295/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20296/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20297/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20298/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20299/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20300/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20301/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20302/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20303/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20304/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20305/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20306/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20307/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20308/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20309/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20310/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20311/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20312/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20313/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20314/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20315/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20316/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20317/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20318/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20319/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20320/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20321/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20322/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20323/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20324/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20325/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20326/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20327/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20328/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20329/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20330/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20331/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20332/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20333/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20334/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20335/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20336/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20337/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20338/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20339/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20340/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20341/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20342/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20343/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20344/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20345/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20346/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20347/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20348/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20349/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20350/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20351/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20352/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20353/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20354/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20355/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20356/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20357/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20358/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20359/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20360/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20361/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20362/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20363/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20364/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20365/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20366/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20367/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20368/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20369/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20370/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20371/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20372/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20373/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20374/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20375/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20376/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20377/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20378/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20379/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20380/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20381/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20382/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20383/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20384/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20385/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20386/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20387/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20388/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20389/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20390/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20391/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20392/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20393/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20394/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20395/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20396/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20397/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20398/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20399/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20400/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20401/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20402/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20403/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20404/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20405/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20406/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20407/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20408/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20409/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20410/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20411/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20412/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20413/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20414/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20415/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20416/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20417/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20418/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20419/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20420/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20421/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20422/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20423/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20424/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20425/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20426/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20427/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20428/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20429/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20430/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20431/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20432/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20433/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20434/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20435/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20436/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20437/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20438/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20439/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20440/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20441/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20442/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20443/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20444/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20445/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20446/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20447/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20448/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20449/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20450/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20451/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20452/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20453/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20454/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20455/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20456/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20457/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20458/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20459/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20460/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20461/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20462/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20463/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20464/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20465/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20466/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20467/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20468/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20469/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20470/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20471/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20472/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20473/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20474/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20475/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20476/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20477/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20478/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20479/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20480/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20481/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20482/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20483/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20484/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20485/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20486/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20487/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20488/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20489/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20490/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20491/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20492/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20493/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20494/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20495/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20496/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20497/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20498/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20499/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20500/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20501/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20502/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20503/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20504/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20505/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20506/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20507/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20508/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20509/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20510/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20511/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20512/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20513/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20514/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20515/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20516/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20517/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20518/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20519/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20520/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20521/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20522/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20523/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20524/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20525/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20526/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20527/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20528/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20529/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20530/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20531/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20532/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20533/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20534/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20535/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20536/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20537/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20538/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20539/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20540/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20541/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20542/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20543/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20544/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20545/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20546/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20547/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20548/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20549/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20550/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20551/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20552/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20553/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20554/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20555/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20556/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20557/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20558/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20559/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20560/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20561/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20562/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20563/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20564/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20565/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20566/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20567/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20568/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20570/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20571/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20572/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20573/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20574/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20575/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20576/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20577/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20578/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20579/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20580/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20581/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20582/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20583/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20585/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20586/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20587/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20588/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20589/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20590/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20591/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20592/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20593/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20594/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20595/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20596/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20597/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20598/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20599/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20600/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20601/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20602/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20603/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20604/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20605/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20606/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20607/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20608/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20609/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20610/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20611/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20612/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20613/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20614/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20615/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20616/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20617/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20618/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20619/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20620/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20621/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20622/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20623/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20624/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20625/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20626/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20627/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20628/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20629/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20630/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20631/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20632/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20633/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20634/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20635/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20636/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20637/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20638/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20639/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20640/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20641/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20642/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20643/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20644/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20645/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20646/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20647/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20648/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20649/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20650/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20651/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20652/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20653/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20654/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20655/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20656/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20657/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20658/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20659/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20660/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20661/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20662/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20663/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20664/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20665/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20666/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20667/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20668/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20669/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20670/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20671/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20672/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20673/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20674/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20675/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20676/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20677/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20678/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20679/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20680/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20681/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20682/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20683/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20684/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20685/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20686/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20687/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20688/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20689/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20690/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20691/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20692/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20693/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20694/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20695/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20696/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20697/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20698/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20699/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20700/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20701/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20702/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20703/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20704/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20705/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20706/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20707/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20708/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20709/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20710/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20711/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20712/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20713/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20714/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20715/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20716/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20717/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20718/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20719/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20720/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20721/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20722/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20723/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20724/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20725/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20726/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20727/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20728/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20729/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20730/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20731/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20732/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20733/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20734/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20735/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20736/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20737/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20738/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20739/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20740/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20741/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20742/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20743/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20744/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20745/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20746/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20747/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20748/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20749/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20750/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20751/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20752/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20753/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20754/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20755/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20756/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20757/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20758/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20759/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20760/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20761/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20762/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20763/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20764/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20765/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20766/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20767/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20768/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20769/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20770/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20771/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20772/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20773/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20774/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20775/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20776/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20777/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20778/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20779/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20780/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20781/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20782/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20783/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20784/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20785/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20786/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20787/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20788/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20789/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20790/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20791/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20792/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20793/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20794/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20795/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20796/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20797/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20798/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20799/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20800/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20801/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20802/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20803/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20804/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20805/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20806/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20807/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20808/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20809/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20810/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20811/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20812/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20813/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20814/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20815/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20816/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20817/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20818/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20819/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20820/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20821/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20822/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20823/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20824/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20825/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20826/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20827/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20828/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20829/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20830/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20831/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20832/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20833/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20834/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20835/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20836/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20837/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20838/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20839/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20840/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20841/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20842/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20843/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20844/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20845/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20846/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20847/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20848/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20849/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20850/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20851/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20852/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20853/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20854/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20855/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20856/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20857/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20858/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20859/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20860/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20861/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20862/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20863/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20864/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20865/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20866/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20867/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20868/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20869/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20870/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20871/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20872/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20873/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20874/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20875/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20876/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20877/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20878/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20879/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20880/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20881/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20882/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20883/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20884/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20885/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20886/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20887/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20888/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20889/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20890/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20891/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20892/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20893/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20894/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20895/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20896/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20897/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20898/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20899/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20900/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20901/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20902/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20903/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20904/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20905/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20906/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20907/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20908/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20909/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20910/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20911/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20912/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20913/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20914/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20915/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20916/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20917/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20918/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20919/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20920/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20921/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20922/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20923/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20924/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20925/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20926/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20927/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20928/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20929/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20930/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20931/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20932/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20933/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20934/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20935/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20936/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20937/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20938/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20939/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20940/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20941/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20942/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20943/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20944/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20945/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20946/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20947/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20948/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20949/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20950/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20951/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20952/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20953/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20954/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20955/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20956/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20957/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20958/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20959/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20960/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20961/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20962/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20963/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20964/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20965/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20966/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20967/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20968/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20969/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20970/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20971/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20972/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20973/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20974/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20975/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20976/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20977/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20978/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20979/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20980/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20981/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20982/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20983/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20984/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20985/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20986/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20987/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20988/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [20989/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20990/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20991/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20992/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20993/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20994/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20995/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20996/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [20997/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [20998/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [20999/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21000/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21001/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21002/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21003/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21004/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21005/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21006/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21007/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21008/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21009/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21010/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21011/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21012/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21013/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21014/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21015/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21016/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21017/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21018/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21019/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21020/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21021/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21022/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21023/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21024/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21025/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21026/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21027/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21028/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21029/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21030/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21031/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21032/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21033/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21034/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21035/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21036/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21037/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21038/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21039/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21040/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21041/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21042/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21043/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21044/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21045/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21046/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21047/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21048/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21049/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21050/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21051/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21052/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21053/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21054/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21055/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21056/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21057/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21058/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21059/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21060/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21061/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21062/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21063/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21064/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21065/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21066/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21067/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21068/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21069/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21070/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21071/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21072/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21073/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21074/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21075/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21076/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21077/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21078/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21079/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21080/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21081/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21082/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21083/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21084/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21085/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21086/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21087/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21088/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21089/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21090/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21091/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21092/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21093/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21094/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21095/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21096/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21097/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21098/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21099/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21100/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21101/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21102/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21103/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21104/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21105/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21106/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21107/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21108/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21109/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21110/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21111/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21112/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21113/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21114/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21115/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21116/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21117/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21118/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21119/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21120/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21121/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21122/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21123/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21124/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21125/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21126/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21127/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21128/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21129/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21130/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21131/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21132/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21133/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21134/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21135/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21136/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21137/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21138/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21139/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21140/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21141/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21142/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21143/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21144/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21145/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21146/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21147/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21148/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21149/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21150/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21151/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21152/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21153/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21154/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21155/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21156/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21157/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21158/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21159/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21160/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21161/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21162/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21163/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21164/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21165/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21166/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21167/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21168/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21169/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21170/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21171/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21172/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21173/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21174/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21175/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21176/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21177/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21178/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21179/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21180/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21181/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21182/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21183/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21184/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21185/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21186/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21187/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21188/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21189/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21190/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21191/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21192/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21193/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21194/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21195/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21196/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21197/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21198/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21199/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21200/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21201/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21202/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21203/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21204/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21205/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21206/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21207/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21208/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21209/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21210/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21211/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21212/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21213/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21214/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21215/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21216/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21217/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21218/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21219/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21220/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21221/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21222/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21223/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21224/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21225/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21226/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21227/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21228/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21229/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21230/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21231/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21232/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21233/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21234/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21235/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21236/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21237/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21238/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21239/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21240/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21241/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21242/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21243/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21244/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21245/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21246/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21247/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21248/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21249/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21250/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21251/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21252/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21253/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21254/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21255/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21256/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21257/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21258/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21259/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21260/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21261/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21262/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21263/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21264/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21265/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21266/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21267/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21268/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21269/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21270/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21271/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21272/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21273/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21274/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21275/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21276/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21277/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21278/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21279/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21280/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21281/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21282/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21283/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21284/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21285/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21286/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21287/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21288/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21289/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21290/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21291/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21292/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21293/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21294/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21295/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21296/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21297/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21298/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21299/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21300/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21301/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21302/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21303/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21304/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21305/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21306/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21307/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21308/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21309/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21310/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21311/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21312/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21313/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21314/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21315/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21316/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21317/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21318/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21319/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21320/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21321/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21322/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21323/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21324/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21325/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21326/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21327/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21328/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21329/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21330/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21331/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21332/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21333/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21334/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21335/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21336/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21337/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21338/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21339/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21340/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21341/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21342/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21343/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21344/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21345/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21346/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21347/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21348/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21349/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21350/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21351/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21352/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21353/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21354/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21355/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21356/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21357/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21358/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21359/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21360/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21361/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21362/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21363/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21364/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21365/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21366/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21367/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21368/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21369/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21370/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21371/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21372/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21373/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21374/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21375/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21376/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21377/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21378/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21379/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21380/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21381/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21382/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21383/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21384/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21385/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21386/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21387/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21388/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21389/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21390/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21391/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21392/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21393/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21394/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21395/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21396/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21397/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21398/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21399/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21400/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21401/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21402/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21403/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21404/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21405/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21406/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21407/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21408/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21409/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21410/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21411/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21412/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21413/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21414/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21415/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21416/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21417/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21418/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21419/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21420/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21421/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21422/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21423/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21424/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21425/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21426/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21427/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21428/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21429/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21430/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21431/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21432/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21433/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21434/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21435/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21436/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21437/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21438/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21439/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21440/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21441/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21442/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21443/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21444/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21445/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21446/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21447/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21448/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21449/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21450/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21451/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21452/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21453/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21454/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21455/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21456/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21457/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21458/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21459/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21460/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21461/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21462/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21463/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21464/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21465/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21466/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21467/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21468/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21469/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21470/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21471/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21472/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21473/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21474/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21475/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21476/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21477/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21478/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21479/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21480/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21481/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21482/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21483/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21484/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21485/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21486/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21487/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21488/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21489/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21490/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21491/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21492/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21493/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21494/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21495/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21496/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21497/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21498/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21499/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21500/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21501/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21502/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21503/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21504/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21505/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21506/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21507/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21508/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21509/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21510/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21511/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21512/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21513/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21514/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21515/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21516/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21517/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21518/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21519/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21520/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21521/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21522/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21523/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21524/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21525/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21526/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21527/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21528/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21529/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21530/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21531/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21532/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21533/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21534/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21535/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21536/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21537/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21538/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21539/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21540/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21541/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21542/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21543/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21544/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21545/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21546/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21547/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21548/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21549/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21550/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21551/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21552/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21553/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21554/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21555/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21556/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21557/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21558/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21559/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21560/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21561/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21562/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21563/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21564/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21565/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21566/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21567/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21568/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21570/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21571/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21572/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21573/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21574/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21575/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21576/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21577/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21578/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21579/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21580/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21581/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21582/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21583/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21585/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21586/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21587/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21588/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21589/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21590/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21591/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21592/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21593/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21594/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21595/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21596/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21597/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21598/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21599/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21600/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21601/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21602/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21603/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21604/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21605/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21606/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21607/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21608/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21609/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21610/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21611/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21612/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21613/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21614/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21615/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21616/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21617/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21618/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21619/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21620/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21621/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21622/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21623/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21624/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21625/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21626/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21627/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21628/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21629/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21630/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21631/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21632/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21633/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21634/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21635/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21636/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21637/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21638/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21639/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21640/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21641/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21642/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21643/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21644/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21645/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21646/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21647/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21648/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21649/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21650/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21651/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21652/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21653/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21654/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21655/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21656/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21657/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21658/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21659/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21660/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21661/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21662/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21663/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21664/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21665/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21666/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21667/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21668/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21669/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21670/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21671/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21672/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21673/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21674/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21675/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21676/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21677/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21678/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21679/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21680/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21681/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21682/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21683/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21684/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21685/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21686/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21687/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21688/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21689/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21690/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21691/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21692/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21693/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21694/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21695/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21696/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21697/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21698/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21699/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21700/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21701/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21702/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21703/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21704/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21705/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21706/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21707/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21708/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21709/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21710/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21711/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21712/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21713/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21714/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21715/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21716/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21717/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21718/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21719/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21720/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21721/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21722/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21723/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21724/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21725/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21726/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21727/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21728/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21729/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21730/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21731/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21732/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21733/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21734/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21735/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21736/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21737/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21738/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21739/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21740/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21741/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21742/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21743/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21744/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21745/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21746/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21747/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21748/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21749/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21750/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21751/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21752/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21753/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21754/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21755/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21756/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21757/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21758/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21759/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21760/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21761/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21762/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21763/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21764/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21765/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21766/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21767/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21768/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21769/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21770/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21771/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21772/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21773/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21774/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21775/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21776/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21777/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21778/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21779/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21780/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21781/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21782/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21783/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21784/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21785/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21786/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21787/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21788/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21789/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21790/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21791/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21792/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21793/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21794/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21795/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21796/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21797/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21798/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21799/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21800/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21801/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21802/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21803/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21804/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21805/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21806/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21807/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21808/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21809/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21810/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21811/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21812/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21813/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21814/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21815/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21816/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21817/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21818/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21819/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21820/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21821/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21822/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21823/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21824/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21825/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21826/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21827/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21828/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21829/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21830/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21831/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21832/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21833/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21834/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21835/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21836/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21837/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21838/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21839/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21840/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21841/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21842/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21843/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21844/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21845/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21846/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21847/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21848/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21849/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21850/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21851/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21852/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21853/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21854/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21855/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21856/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21857/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21858/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21859/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21860/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21861/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21862/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21863/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21864/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21865/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21866/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21867/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21868/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21869/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21870/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21871/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21872/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21873/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21874/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21875/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21876/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21877/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21878/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21879/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21880/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21881/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21882/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21883/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21884/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21885/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21886/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21887/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21888/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21889/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21890/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21891/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21892/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21893/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21894/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21895/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21896/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21897/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21898/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21899/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21900/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21901/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21902/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21903/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21904/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21905/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21906/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21907/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21908/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21909/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21910/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21911/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21912/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21913/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21914/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21915/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21916/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21917/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21918/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21919/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21920/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21921/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21922/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21923/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21924/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21925/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21926/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21927/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21928/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21929/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21930/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21931/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21932/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21933/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21934/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21935/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21936/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21937/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21938/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21939/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21940/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21941/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21942/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21943/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21945/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21946/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21947/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21948/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21949/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21950/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21951/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21952/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21953/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21954/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21955/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21956/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21957/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21958/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21959/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21960/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21961/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21962/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21963/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21964/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21965/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21966/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21967/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21968/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21969/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21970/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21971/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21972/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21973/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21974/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21975/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21976/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21977/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21978/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21979/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21980/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21981/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21982/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21983/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21984/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21985/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21986/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21987/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21988/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [21989/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21990/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21991/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21992/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21993/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [21994/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [21995/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21996/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21997/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21998/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [21999/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22000/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22001/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22002/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22003/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22004/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22005/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22006/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22007/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22008/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22009/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22010/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22011/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22012/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22013/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22014/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22015/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22016/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22017/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22018/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22019/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22020/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22021/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22022/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22023/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22024/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22025/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22026/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22027/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22028/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22029/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22030/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22031/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22032/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22033/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22034/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22035/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22036/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22037/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22038/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22039/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22040/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22041/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22042/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22043/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22044/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22045/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22046/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22047/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22048/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22049/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22050/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22051/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22052/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22053/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22054/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22055/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22056/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22057/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22058/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22059/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22060/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22061/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22062/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22063/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22064/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22065/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22066/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22067/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22068/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22069/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22070/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22071/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22072/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22073/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22074/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22075/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22076/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22077/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22078/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22079/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22080/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22081/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22082/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22083/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22084/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22085/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22086/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22087/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22088/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22089/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22090/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22091/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22092/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22093/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22094/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22095/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22096/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22097/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22098/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22099/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22100/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22101/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22102/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22103/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22104/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22105/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22106/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22107/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22108/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22109/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22110/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22111/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22112/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22113/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22114/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22115/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22116/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22117/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22118/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22119/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22120/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22121/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22122/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22123/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22124/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22125/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22126/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22127/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22128/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22129/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22130/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22131/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22132/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22133/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22134/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22135/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22136/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22137/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22138/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22139/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22140/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22141/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22142/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22143/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22144/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22145/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22146/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22147/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22148/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22149/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22150/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22151/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22152/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22153/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22154/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22155/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22156/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22157/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22158/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22159/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22160/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22161/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22162/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22163/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22164/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22165/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22166/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22167/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22168/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22169/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22170/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22171/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22172/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22173/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22174/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22175/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22176/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22177/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22178/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22179/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22180/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22181/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22182/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22183/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22184/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22185/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22186/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22187/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22188/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22189/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22190/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22191/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22192/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22193/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22194/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22195/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22196/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22197/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22198/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22199/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22200/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22201/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22202/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22203/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22204/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22205/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22206/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22207/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22208/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22209/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22210/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22211/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22212/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22213/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22214/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22215/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22216/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22217/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22218/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22219/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22220/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22221/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22222/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22223/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22224/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22225/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22226/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22227/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22228/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22229/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22230/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22231/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22232/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22233/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22234/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22235/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22236/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22237/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22238/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22239/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22240/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22241/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22242/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22243/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22244/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22245/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22246/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22247/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22248/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22249/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22250/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22251/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22252/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22253/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22254/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22255/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22256/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22257/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22258/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22259/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22260/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22261/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22262/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22263/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22264/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22265/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22266/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22267/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22268/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22269/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22270/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22271/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22272/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22273/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22274/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22275/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22276/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22277/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22278/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22279/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22280/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22281/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22282/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22283/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22284/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22285/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22286/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22287/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22288/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22289/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22290/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22291/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22292/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22293/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22294/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22295/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22296/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22297/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22298/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22299/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22300/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22301/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22302/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22303/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22304/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22305/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22306/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22307/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22308/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22309/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22310/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22311/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22312/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22313/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22314/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22315/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22316/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22317/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22318/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22319/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22320/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22321/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22322/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22323/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22324/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22325/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22326/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22327/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22328/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22329/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22330/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22331/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22332/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22333/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22334/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22335/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22336/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22337/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22338/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22339/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22340/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22341/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22342/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22343/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22344/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22345/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22346/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22347/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22348/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22349/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22350/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22351/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22352/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22353/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22354/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22355/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22356/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22357/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22358/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22359/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22360/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22361/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22362/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22363/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22364/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22365/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22366/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22367/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22368/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22369/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22370/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22371/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22372/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22373/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22374/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22375/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22376/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22377/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22378/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22379/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22380/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22381/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22382/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22383/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22384/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22385/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22386/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22387/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22388/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22389/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22390/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22391/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22392/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22393/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22394/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22395/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22396/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22397/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22398/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22399/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22400/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22401/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22402/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22403/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22404/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22405/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22406/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22407/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22408/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22409/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22410/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22411/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22412/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22413/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22414/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22415/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22416/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22417/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22418/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22419/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22420/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22421/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22422/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22423/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22424/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22425/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22426/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22427/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22428/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22429/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22430/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22431/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22432/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22433/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22434/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22435/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22436/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [22437/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22438/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22439/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22440/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [22441/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22442/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22443/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22444/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22445/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22446/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22447/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22448/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22449/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22450/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22451/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22452/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22453/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22454/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22455/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [22456/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22457/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [22458/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [22459/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22460/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22461/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22462/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22463/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22464/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22465/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22466/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22467/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22468/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22469/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22470/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22471/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22472/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22473/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22474/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22475/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22476/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22477/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22478/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22479/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22480/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22481/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22482/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22483/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22484/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22485/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22486/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22487/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22488/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22489/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22490/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22491/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22492/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22493/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22494/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22495/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22496/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22497/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22498/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22499/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22500/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22501/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22502/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22503/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22504/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22505/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22506/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22507/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22508/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22509/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22510/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22511/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22512/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22513/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22514/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22515/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22516/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22517/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22518/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22519/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22520/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22521/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22522/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22523/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22524/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22525/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22526/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22527/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22528/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22529/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22530/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22531/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22532/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22533/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22534/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22535/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22536/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22537/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22538/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22539/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22540/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22541/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22542/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22543/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22544/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22545/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22546/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22547/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22548/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22549/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22550/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22551/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22552/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22553/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22554/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22555/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22556/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22557/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22558/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22559/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22560/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22561/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22562/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22563/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22564/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22565/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22566/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22567/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22568/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22570/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22571/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22572/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22573/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22574/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22575/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22576/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22577/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22578/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22579/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22580/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22581/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22582/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22583/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22585/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22586/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22587/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22588/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22589/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22590/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22591/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22592/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22593/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22594/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22595/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22596/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22597/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22598/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22599/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22600/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22601/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22602/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22603/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22604/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22605/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22606/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22607/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22608/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22609/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22610/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22611/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22612/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22613/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22614/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22615/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22616/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22617/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22618/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22619/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22620/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22621/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22622/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22623/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22624/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22625/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22626/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22627/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22628/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22629/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22630/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22631/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22632/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22633/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22634/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22635/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22636/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22637/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22638/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22639/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22640/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22641/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22642/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22643/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22644/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22645/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22646/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22647/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22648/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22649/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22650/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22651/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22652/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22653/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22654/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22655/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22656/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22657/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22658/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22659/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22660/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22661/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22662/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22663/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22664/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22665/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22666/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22667/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22668/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22669/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22670/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22671/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22672/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22673/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22674/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22675/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22676/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22677/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22678/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22679/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22680/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22681/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22682/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22683/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22684/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22685/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22686/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22687/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22688/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22689/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22690/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22691/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22692/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22693/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22694/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22695/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22696/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22697/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22698/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22699/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22700/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22701/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22702/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22703/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22704/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22705/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22706/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22707/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22708/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22709/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22710/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22711/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22712/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22713/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22714/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22715/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22716/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22717/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22718/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22719/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22720/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22721/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22722/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22723/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22724/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22725/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22726/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22727/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22728/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22729/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22730/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22731/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22732/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22733/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22734/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22735/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22736/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22737/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22738/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22739/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22740/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22741/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22742/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22743/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22744/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22745/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22746/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22747/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22748/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22749/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22750/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22751/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22752/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22753/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22754/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22755/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22756/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22757/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22758/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22759/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22760/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22761/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22762/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22763/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22764/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22765/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22766/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22767/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22768/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22769/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22770/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22771/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22772/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22773/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22774/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22775/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22776/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22777/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22778/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22779/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22780/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22781/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22782/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22783/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22784/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22785/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22786/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22787/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22788/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22789/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22790/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22791/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22792/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22793/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22794/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22795/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22796/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22797/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22798/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22799/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22800/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22801/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22802/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22803/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22804/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22805/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22806/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22807/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22808/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22809/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22810/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22811/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22812/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22813/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22814/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22815/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22816/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22817/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22818/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22819/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22820/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22821/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22822/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22823/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22824/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22825/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22826/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22827/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22828/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22829/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22830/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22831/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22832/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22833/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22834/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22835/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22836/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22837/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22838/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22839/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22840/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22841/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22842/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22843/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22844/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22845/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22846/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22847/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22848/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22849/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22850/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22851/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22852/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22853/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22854/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22855/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22856/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22857/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22858/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22859/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22860/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22861/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22862/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22863/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22864/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22865/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22866/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22867/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22868/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22869/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22870/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22871/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22872/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22873/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22874/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22875/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22876/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22877/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22878/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22879/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22880/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22881/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22882/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22883/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22884/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22885/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22886/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22887/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22888/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22889/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22890/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22891/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22892/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22893/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22894/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22895/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22896/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22897/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22898/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22899/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22900/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22901/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22902/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22903/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22904/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22905/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22906/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22907/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22908/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22909/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22910/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22911/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22912/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22913/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22914/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22915/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22916/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22917/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22918/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22919/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22920/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22921/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22922/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22923/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22924/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22925/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22926/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22927/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22928/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22929/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22930/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22931/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22932/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22933/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22934/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22935/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22936/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22937/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22938/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22939/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22940/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22941/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22942/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22943/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22945/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22946/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22947/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22948/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22949/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22950/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22951/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22952/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22953/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22954/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22955/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22956/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22957/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22958/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22959/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22960/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22961/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22962/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22963/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22964/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22965/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22966/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22967/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22968/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22969/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22970/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22971/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22972/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22973/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22974/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22975/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22976/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22977/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22978/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22979/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22980/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22981/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22982/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22983/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22984/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22985/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22986/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22987/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22988/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22989/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22990/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22991/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22992/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22993/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22994/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22995/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22996/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22997/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22998/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [22999/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23000/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23001/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23002/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23003/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23004/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23005/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23006/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23007/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23008/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23009/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23010/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23011/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23012/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23013/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23014/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23015/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23016/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23017/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23018/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23019/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23020/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23021/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23022/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23023/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23024/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23025/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23026/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23027/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23028/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23029/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23030/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23031/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23032/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23033/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23034/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23035/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23036/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23037/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23038/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23039/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23040/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23041/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23042/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23043/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23044/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23045/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23046/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23047/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23048/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23049/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23050/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23051/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23052/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23053/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23054/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23055/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23056/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23057/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23058/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23059/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23060/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23061/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23062/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23063/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23064/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23065/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23066/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23067/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23068/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23069/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23070/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23071/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23072/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23073/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23074/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23075/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23076/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23077/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23078/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23079/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23080/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23081/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23082/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23083/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23084/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23085/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23086/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23087/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23088/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23089/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23090/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23091/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23092/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23093/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23094/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23095/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23096/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23097/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23098/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23099/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23100/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23101/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23102/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23103/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23104/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23105/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23106/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23107/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23108/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23109/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23110/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23111/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23112/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23113/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23114/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23115/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23116/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23117/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23118/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23119/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23120/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23121/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23122/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23123/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23124/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23125/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23126/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23127/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23128/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23129/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23130/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23131/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23132/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23133/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23134/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23135/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23136/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23137/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23138/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23139/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23140/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23141/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23142/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23143/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23144/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23145/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23146/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23147/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23148/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23149/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23150/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23151/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23152/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23153/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23154/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23155/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23156/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23157/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23158/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23159/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23160/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23161/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23162/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23163/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23164/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23165/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23166/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23167/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23168/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23169/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23170/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23171/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23172/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23173/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23174/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23175/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23176/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23177/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23178/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23179/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23180/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23181/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23182/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23183/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23184/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23185/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23186/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23187/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23188/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23189/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23190/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23191/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23192/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23193/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23194/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23195/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23196/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23197/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23198/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23199/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23200/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23201/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23202/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23203/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23204/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23205/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23206/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23207/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23208/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23209/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23210/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23211/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23212/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23213/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23214/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23215/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23216/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23217/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23218/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23219/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23220/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23221/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23222/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23223/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23224/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23225/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23226/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23227/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23228/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23229/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23230/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23231/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23232/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23233/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23234/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23235/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23236/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23237/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23238/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23239/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23240/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23241/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23242/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23243/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23244/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23245/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23246/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23247/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23248/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23249/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23250/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23251/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23252/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23253/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23254/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23255/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23256/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23257/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23258/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23259/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23260/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23261/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23262/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23263/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23264/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23265/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23266/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23267/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23268/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23269/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23270/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23271/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23272/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23273/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23274/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23275/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23276/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23277/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23278/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23279/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23280/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23281/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23282/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23283/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23284/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23285/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23286/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23287/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23288/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23289/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23290/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23291/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23292/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23293/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23294/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23295/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23296/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23297/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23298/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23299/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23300/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23301/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23302/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23303/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23304/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23305/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23306/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23307/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23308/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23309/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23310/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23311/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23312/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23313/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23314/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23315/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23316/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23317/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23318/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23319/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23320/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23321/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23322/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23323/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23324/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23325/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23326/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23327/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23328/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23329/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23330/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23331/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23332/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23333/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23334/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23335/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23336/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23337/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23338/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23339/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23340/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23341/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23342/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23343/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23344/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23345/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23346/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23347/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23348/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23349/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23350/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23351/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23352/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23353/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23354/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23355/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23356/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23357/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23358/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23359/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23360/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23361/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23362/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23363/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23364/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23365/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23366/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23367/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23368/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23369/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23370/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23371/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23372/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23373/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23374/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23375/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23376/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23377/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23378/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23379/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23380/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23381/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23382/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23383/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23384/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23385/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23386/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23387/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23388/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23389/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23390/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23391/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23392/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23393/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23394/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23395/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23396/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23397/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23398/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23399/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23400/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23401/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23402/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23403/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23404/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23405/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23406/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23407/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23408/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23409/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23410/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23411/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23412/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23413/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23414/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23415/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23416/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23417/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23418/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23419/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23420/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23421/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23422/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23423/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23424/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23425/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23426/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23427/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23428/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23429/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23430/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23431/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23432/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23433/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23434/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23435/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23436/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23437/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23438/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23439/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23440/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23441/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23442/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23443/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23444/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23445/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23446/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23447/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23448/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23449/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23450/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23451/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23452/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23453/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23454/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23455/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23456/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23457/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23458/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23459/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23460/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23461/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23462/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23463/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23464/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23465/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23466/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23467/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23468/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23469/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23470/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23471/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23472/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23473/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23474/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23475/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23476/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23477/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23478/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23479/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23480/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23481/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23482/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23483/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23484/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23485/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23486/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23487/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23488/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23489/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23490/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23491/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23492/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23493/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23494/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23495/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23496/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23497/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23498/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23499/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23500/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23501/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23502/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23503/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23504/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23505/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23506/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23507/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23508/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23509/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23510/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23511/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23512/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23513/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23514/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23515/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23516/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23517/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23518/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23519/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23520/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23521/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23522/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23523/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23524/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23525/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23526/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23527/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23528/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23529/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23530/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23531/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23532/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23533/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23534/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23535/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23536/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23537/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23538/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23539/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23540/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23541/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23542/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23543/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23544/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23545/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23546/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23547/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23548/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23549/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23550/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23551/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23552/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23553/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23554/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23555/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23556/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23557/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23558/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23559/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23560/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23561/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23562/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23563/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23564/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23565/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23566/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23567/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23568/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23570/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23571/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23572/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23573/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23574/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23575/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23576/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23577/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23578/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23579/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23580/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23581/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23582/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23583/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23585/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23586/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23587/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23588/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23589/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23590/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23591/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23592/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23593/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23594/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23595/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23596/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23597/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23598/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23599/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23600/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23601/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23602/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23603/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23604/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23605/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23606/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23607/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23608/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23609/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23610/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23611/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23612/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23613/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23614/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23615/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23616/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23617/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23618/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23619/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23620/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23621/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23622/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23623/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23624/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23625/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23626/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23627/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23628/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23629/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23630/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23631/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23632/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23633/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23634/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23635/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23636/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23637/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23638/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23639/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23640/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23641/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23642/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23643/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23644/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23645/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23646/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23647/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23648/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23649/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23650/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23651/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23652/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23653/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23654/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23655/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23656/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23657/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23658/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23659/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23660/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23661/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23662/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23663/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23664/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23665/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23666/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23667/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23668/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23669/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23670/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23671/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23672/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23673/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23674/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23675/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23676/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23677/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23678/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23679/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23680/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23681/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23682/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23683/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23684/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23685/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23686/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23687/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23688/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23689/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23690/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23691/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23692/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23693/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23694/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23695/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23696/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23697/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23698/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23699/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23700/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23701/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23702/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23703/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23704/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23705/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23706/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23707/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23708/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23709/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23710/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23711/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23712/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23713/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23714/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23715/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23716/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23717/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23718/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23719/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23720/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23721/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23722/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23723/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23724/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23725/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23726/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23727/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23728/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23729/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23730/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23731/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23732/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23733/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23734/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23735/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23736/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23737/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23738/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23739/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23740/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23741/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23742/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23743/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23744/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23745/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23746/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23747/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23748/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23749/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23750/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23751/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23752/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23753/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23754/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23755/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23756/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23757/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23758/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23759/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23760/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23761/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23762/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23763/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23764/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23765/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23766/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23767/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23768/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23769/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23770/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23771/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23772/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23773/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23774/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23775/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23776/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23777/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23778/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23779/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23780/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23781/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23782/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23783/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23784/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23785/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23786/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23787/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23788/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23789/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23790/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23791/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23792/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23793/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23794/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23795/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23796/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23797/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23798/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23799/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23800/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23801/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23802/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23803/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23804/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23805/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23806/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23807/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23808/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23809/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23810/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23811/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23812/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23813/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23814/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23815/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23816/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23817/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23818/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23819/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23820/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23821/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23822/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23823/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23824/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23825/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23826/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23827/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23828/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23829/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23830/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23831/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23832/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23833/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23834/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23835/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23836/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23837/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23838/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23839/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23840/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23841/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23842/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23843/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23844/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23845/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23846/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23847/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23848/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23849/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23850/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23851/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23852/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23853/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23854/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23855/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23856/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23857/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23858/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23859/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23860/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23861/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23862/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23863/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23864/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23865/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23866/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23867/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23868/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23869/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23870/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23871/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23872/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23873/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23874/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23875/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23876/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23877/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23878/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23879/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23880/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23881/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23882/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23883/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23884/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23885/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23886/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23887/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23888/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23889/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23890/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23891/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23892/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23893/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23894/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23895/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23896/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23897/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23898/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23899/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23900/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23901/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23902/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23903/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23904/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23905/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23906/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23907/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23908/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23909/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23910/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23911/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23912/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23913/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23914/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23915/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23916/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23917/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23918/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23919/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23920/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23921/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23922/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23923/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23924/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23925/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23926/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23927/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23928/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23929/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23930/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23931/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23932/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23933/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23934/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23935/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23936/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23937/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23938/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23939/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23940/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23941/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23942/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23943/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23945/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23946/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23947/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23948/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23949/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23950/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23951/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23952/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23953/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23954/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23955/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23956/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23957/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23958/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23959/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23960/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23961/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23962/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23963/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23964/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23965/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23966/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23967/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23968/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23969/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23970/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23971/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23972/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23973/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23974/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23975/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23976/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23977/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23978/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23979/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23980/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23981/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23982/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23983/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23984/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23985/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23986/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23987/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23988/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23989/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23990/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23991/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23992/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23993/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23994/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23995/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23996/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23997/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23998/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [23999/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24000/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24001/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24002/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24003/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24004/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24005/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24006/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24007/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24008/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24009/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24010/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24011/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24012/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24013/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24014/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24015/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24016/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24017/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24018/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24019/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24020/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24021/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24022/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24023/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24024/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24025/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24026/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24027/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24028/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24029/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24030/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24031/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24032/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24033/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24034/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24035/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24036/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24037/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24038/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24039/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24040/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24041/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24042/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24043/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24044/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24045/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24046/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24047/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24048/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24049/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24050/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24051/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24052/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24053/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24054/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24055/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24056/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24057/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24058/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24059/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24060/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24061/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24062/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24063/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24064/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24065/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24066/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24067/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24068/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24069/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24070/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24071/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24072/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24073/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24074/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24075/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24076/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24077/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24078/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24079/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24080/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24081/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24082/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24083/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24084/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24085/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24086/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24087/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24088/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24089/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24090/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24091/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24092/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24093/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24094/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24095/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24096/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24097/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24098/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24099/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24100/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24101/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24102/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24103/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24104/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24105/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24106/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24107/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24108/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24109/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24110/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24111/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24112/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24113/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24114/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24115/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24116/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24117/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24118/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24119/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24120/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24121/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24122/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24123/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24124/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24125/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24126/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24127/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24128/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24129/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24130/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24131/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24132/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24133/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24134/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24135/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24136/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24137/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24138/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24139/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24140/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24141/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24142/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24143/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24144/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24145/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24146/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24147/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24148/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24149/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24150/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24151/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24152/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24153/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24154/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24155/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24156/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24157/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24158/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24159/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24160/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24161/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24162/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24163/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24164/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24165/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24166/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24167/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24168/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24169/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24170/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24171/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24172/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24173/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24174/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24175/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24176/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24177/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24178/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24179/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24180/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24181/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24182/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24183/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24184/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24185/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24186/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24187/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24188/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24189/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24190/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24191/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24192/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24193/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24194/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24195/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24196/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24197/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24198/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24199/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24200/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24201/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24202/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24203/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24204/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24205/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24206/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24207/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24208/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24209/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24210/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24211/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24212/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24213/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24214/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24215/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24216/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24217/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24218/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24219/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24220/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24221/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24222/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24223/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24224/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24225/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24226/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24227/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24228/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24229/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24230/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24231/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24232/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24233/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24234/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24235/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24236/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24237/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24238/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24239/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24240/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24241/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24242/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24243/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24244/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24245/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24246/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24247/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24248/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24249/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24250/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24251/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24252/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24253/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24254/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24255/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24256/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24257/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24258/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24259/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24260/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24261/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24262/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24263/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24264/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24265/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24266/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24267/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24268/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24269/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24270/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24271/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24272/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24273/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24274/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24275/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24276/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24277/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24278/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24279/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24280/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24281/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24282/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24283/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24284/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24285/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24286/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24287/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24288/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24289/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24290/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24291/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24292/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24293/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24294/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24295/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24296/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24297/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24298/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24299/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24300/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24301/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24302/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24303/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24304/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24305/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24306/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24307/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24308/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24309/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24310/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24311/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24312/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24313/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24314/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24315/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24316/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24317/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24318/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24319/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24320/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24321/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24322/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24323/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24324/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24325/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24326/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24327/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24328/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24329/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24330/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24331/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24332/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24333/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24334/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24335/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24336/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24337/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24338/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24339/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24340/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24341/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24342/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24343/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24344/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24345/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24346/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24347/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24348/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24349/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24350/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24351/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24352/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24353/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24354/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24355/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24356/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24357/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24358/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24359/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24360/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24361/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24362/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24363/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24364/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24365/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24366/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24367/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24368/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24369/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24370/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24371/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24372/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24373/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24374/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24375/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24376/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24377/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24378/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24379/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24380/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24381/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24382/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24383/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24384/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24385/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24386/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24387/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24388/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24389/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24390/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24391/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24392/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24393/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24394/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24395/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24396/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24397/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24398/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24399/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24400/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24401/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24402/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24403/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24404/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24405/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24406/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24407/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24408/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24409/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24410/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24411/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24412/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24413/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24414/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24415/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24416/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24417/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24418/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24419/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24420/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24421/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24422/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24423/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24424/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24425/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24426/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24427/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24428/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24429/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24430/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24431/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24432/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24433/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24434/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24435/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24436/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24437/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24438/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24439/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24440/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24441/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24442/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24443/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24444/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24445/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24446/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24447/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24448/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24449/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24450/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24451/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24452/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24453/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24454/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24455/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24456/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24457/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24458/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24459/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24460/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24461/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24462/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24463/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24464/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24465/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24466/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24467/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24468/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24469/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24470/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24471/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24472/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24473/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24474/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24475/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24476/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24477/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24478/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24479/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24480/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24481/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24482/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24483/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24484/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24485/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24486/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24487/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24488/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24489/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24490/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24491/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24492/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24493/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24494/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24495/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24496/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24497/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24498/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24499/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24500/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24501/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24502/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24503/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24504/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24505/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24506/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24507/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24508/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24509/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24510/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24511/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24512/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24513/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24514/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24515/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24516/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24517/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24518/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24519/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24520/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24521/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24522/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24523/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24524/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24525/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24526/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24527/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24528/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24529/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24530/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24531/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24532/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24533/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24534/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24535/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24536/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24537/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24538/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24539/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24540/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24541/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24542/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24543/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24544/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24545/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24546/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24547/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24548/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24549/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24550/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24551/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24552/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24553/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24554/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24555/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24556/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24557/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24558/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24559/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24560/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24561/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24562/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24563/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24564/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24565/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24566/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24567/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24568/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24570/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24571/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24572/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24573/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24574/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24575/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24576/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24577/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24578/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24579/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24580/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24581/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24582/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24583/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24585/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24586/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24587/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24588/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24589/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24590/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24591/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24592/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24593/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24594/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24595/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24596/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24597/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24598/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24599/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24600/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24601/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24602/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24603/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24604/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24605/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24606/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24607/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24608/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24609/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24610/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24611/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24612/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24613/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24614/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24615/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24616/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24617/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24618/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24619/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24620/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24621/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24622/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24623/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24624/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24625/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24626/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24627/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24628/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24629/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24630/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24631/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24632/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24633/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24634/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24635/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24636/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24637/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24638/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24639/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24640/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24641/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24642/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24643/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24644/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24645/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24646/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24647/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24648/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24649/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24650/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24651/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24652/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24653/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24654/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24655/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24656/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24657/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24658/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24659/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24660/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24661/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24662/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24663/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24664/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24665/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24666/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24667/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24668/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24669/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24670/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24671/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24672/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24673/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24674/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24675/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24676/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24677/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24678/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24679/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24680/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24681/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24682/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24683/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24684/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24685/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24686/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24687/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24688/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24689/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24690/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24691/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24692/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24693/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24694/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24695/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24696/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24697/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24698/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24699/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24700/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24701/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24702/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24703/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24704/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24705/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24706/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24707/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24708/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24709/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24710/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24711/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24712/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24713/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24714/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24715/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24716/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24717/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24718/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24719/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24720/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24721/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24722/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24723/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24724/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24725/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24726/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24727/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24728/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24729/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24730/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24731/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24732/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24733/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24734/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24735/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24736/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24737/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24738/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24739/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24740/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24741/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24742/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24743/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24744/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24745/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24746/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24747/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24748/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24749/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24750/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24751/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24752/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24753/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24754/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24755/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24756/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24757/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24758/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24759/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24760/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24761/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24762/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24763/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24764/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24765/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24766/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24767/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24768/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24769/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24770/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24771/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24772/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24773/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24774/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24775/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24776/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24777/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24778/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24779/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24780/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24781/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24782/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24783/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24784/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24785/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24786/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24787/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24788/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24789/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24790/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24791/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24792/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24793/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24794/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24795/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24796/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24797/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24798/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24799/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24800/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24801/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24802/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24803/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24804/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24805/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24806/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24807/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24808/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24809/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24810/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24811/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24812/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24813/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24814/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24815/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24816/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24817/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24818/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24819/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24820/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24821/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24822/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24823/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24824/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24825/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24826/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24827/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24828/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24829/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24830/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24831/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24832/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24833/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24834/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24835/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24836/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24837/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24838/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24839/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24840/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24841/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24842/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24843/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24844/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24845/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24846/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24847/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24848/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24849/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24850/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24851/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24852/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24853/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24854/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24855/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24856/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24857/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24858/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24859/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24860/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24861/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24862/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24863/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24864/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24865/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24866/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24867/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24868/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24869/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24870/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24871/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24872/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24873/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24874/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24875/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24876/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24877/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24878/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24879/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24880/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24881/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24882/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24883/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24884/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24885/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24886/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24887/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24888/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24889/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24890/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24891/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24892/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24893/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24894/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24895/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24896/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24897/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24898/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24899/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24900/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24901/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24902/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24903/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24904/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24905/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24906/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24907/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24908/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24909/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24910/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24911/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24912/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24913/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24914/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24915/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24916/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24917/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24918/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24919/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24920/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24921/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24922/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24923/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24924/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24925/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24926/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24927/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24928/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24929/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24930/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24931/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24932/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24933/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24934/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24935/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24936/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24937/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24938/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24939/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24940/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24941/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24942/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24943/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24944/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24945/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24946/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24947/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24948/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24949/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24950/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24951/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24952/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24953/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24954/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24955/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24956/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24957/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24958/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24959/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24960/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24961/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24962/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24963/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24964/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24965/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24966/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24967/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24968/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24969/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24970/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24971/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24972/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24973/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24974/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24975/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24976/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24977/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24978/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24979/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24980/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24981/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24982/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24983/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24984/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24985/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24986/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24987/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24988/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24989/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24990/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24991/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24992/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24993/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24994/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24995/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [24996/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [24997/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [24998/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [24999/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25000/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25001/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25002/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25003/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25004/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25005/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25006/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25007/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25008/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25009/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25010/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25011/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25012/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25013/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25014/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25015/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25016/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25017/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25018/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25019/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25020/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25021/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25022/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25023/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25024/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25025/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25026/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25027/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25028/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25029/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25030/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25031/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25032/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25033/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25034/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25035/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25036/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25037/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25038/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25039/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25040/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25041/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25042/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25043/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25044/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25045/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25046/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25047/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25048/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25049/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25050/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25051/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25052/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25053/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25054/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25055/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25056/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25057/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25058/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25059/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25060/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25061/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25062/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25063/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25064/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25065/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25066/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25067/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25068/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25069/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25070/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25071/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25072/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25073/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25074/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25075/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25076/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25077/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25078/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25079/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25080/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25081/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25082/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25083/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25084/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25085/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25086/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25087/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25088/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25089/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25090/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25091/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25092/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25093/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25094/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25095/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25096/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25097/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25098/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25099/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25100/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25101/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25102/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25103/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25104/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25105/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25106/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25107/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25108/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25109/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25110/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25111/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25112/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25113/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25114/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25115/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25116/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25117/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25118/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25119/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25120/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25121/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25122/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25123/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25124/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25125/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25126/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25127/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25128/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25129/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25130/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25131/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25132/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25133/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25134/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25135/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25136/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25137/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25138/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25139/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25140/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25141/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25142/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25143/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25144/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25145/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25146/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25147/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25148/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25149/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25150/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25151/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25152/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25153/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25154/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25155/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25156/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25157/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25158/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25159/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25160/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25161/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25162/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25163/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25164/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25165/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25166/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25167/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25168/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25169/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25170/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25171/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25172/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25173/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25174/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25175/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25176/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25177/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25178/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25179/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25180/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25181/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25182/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25183/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25184/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25185/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25186/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25187/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25188/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25189/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25190/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25191/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25192/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25193/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25194/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25195/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25196/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25197/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25198/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25199/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25200/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25201/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25202/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25203/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25204/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25205/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25206/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25207/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25208/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25209/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25210/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25211/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25212/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25213/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25214/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25215/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25216/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25217/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25218/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25219/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25220/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25221/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25222/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25223/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25224/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25225/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25226/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25227/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25228/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25229/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25230/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25231/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25232/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25233/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25234/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25235/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25236/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25237/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25238/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25239/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25240/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25241/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25242/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25243/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25244/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25245/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25246/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25247/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25248/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25249/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25250/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25251/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25252/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25253/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25254/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25255/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25256/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25257/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25258/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25259/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25260/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25261/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25262/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25263/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25264/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25265/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25266/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25267/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25268/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25269/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25270/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25271/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25272/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25273/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25274/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25275/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25276/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25277/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25278/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25279/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25280/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25281/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25282/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25283/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25284/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25285/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25286/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25287/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25288/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25289/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25290/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25291/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25292/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25293/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25294/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25295/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25296/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25297/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25298/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25299/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25300/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25301/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25302/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25303/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25304/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25305/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25306/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25307/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25308/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25309/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25310/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25311/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25312/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25313/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25314/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25315/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25316/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25317/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25318/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25319/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25320/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25321/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25322/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25323/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25324/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25325/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25326/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25327/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25328/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25329/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25330/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25331/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25332/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25333/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25334/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25335/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25336/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25337/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25338/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25339/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25340/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25341/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25342/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25343/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25344/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25345/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25346/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25347/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25348/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25349/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25350/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25351/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25352/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25353/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25354/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25355/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25356/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25357/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25358/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25359/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25360/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25361/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25362/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25363/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25364/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25365/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25366/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25367/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25368/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25369/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25370/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25371/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25372/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25373/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25374/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25375/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25376/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25377/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25378/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25379/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25380/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25381/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25382/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25383/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25384/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25385/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25386/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25387/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25388/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25389/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25390/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25391/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25392/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25393/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25394/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25395/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25396/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25397/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25398/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25399/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25400/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25401/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25402/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25403/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25404/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25405/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25406/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25407/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25408/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25409/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25410/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25411/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25412/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25413/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25414/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25415/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25416/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25417/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25418/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25419/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25420/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25421/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25422/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25423/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25424/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25425/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25426/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25427/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25428/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25429/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25430/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25431/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25432/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25433/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25434/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25435/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25436/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25437/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25438/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25439/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25440/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25441/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25442/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25443/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25444/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25445/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25446/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25447/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25448/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25449/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25450/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25451/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25452/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25453/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25454/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25455/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25456/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25457/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25458/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25459/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [25460/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25461/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25462/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25463/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25464/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25465/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25466/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25467/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25468/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25469/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25470/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25471/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25472/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25473/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25474/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25475/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25476/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25477/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25478/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25479/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25480/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25481/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25482/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25483/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25484/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25485/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25486/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25487/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25488/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25489/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25490/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25491/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25492/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25493/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25494/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25495/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25496/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25497/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25498/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25499/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25500/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25501/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25502/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25503/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25504/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25505/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25506/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25507/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25508/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25509/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25510/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25511/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25512/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25513/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25514/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25515/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25516/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25517/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25518/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25519/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25520/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25521/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25522/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25523/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25524/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25525/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25526/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25527/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25528/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25529/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25530/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25531/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25532/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25533/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25534/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25535/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25536/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25537/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25538/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25539/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25540/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25541/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25542/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25543/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25544/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25545/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25546/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25547/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25548/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25549/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25550/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25551/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25552/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25553/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25554/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25555/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25556/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25557/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25558/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25559/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25560/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25561/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25562/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25563/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25564/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25565/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25566/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25567/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25568/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25570/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25571/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25572/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25573/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25574/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25575/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25576/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25577/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25578/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25579/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25580/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25581/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25582/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25583/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25585/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25586/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25587/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25588/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25589/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25590/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25591/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25592/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25593/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25594/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25595/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25596/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25597/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25598/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25599/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25600/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25601/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25602/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25603/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25604/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25605/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25606/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25607/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25608/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25609/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25610/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25611/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25612/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25613/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25614/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25615/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25616/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25617/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25618/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25619/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25620/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25621/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25622/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25623/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25624/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25625/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25626/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25627/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25628/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25629/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25630/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25631/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25632/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25633/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25634/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25635/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25636/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25637/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25638/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25639/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25640/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25641/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25642/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25643/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25644/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25645/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25646/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25647/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25648/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25649/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25650/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25651/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25652/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25653/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25654/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25655/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25656/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25657/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25658/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25659/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25660/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25661/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25662/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25663/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25664/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25665/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25666/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25667/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25668/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25669/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25670/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25671/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25672/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25673/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25674/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25675/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25676/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25677/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25678/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25679/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25680/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25681/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25682/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25683/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25684/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25685/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25686/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25687/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25688/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25689/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25690/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25691/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25692/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25693/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25694/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25695/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25696/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25697/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25698/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25699/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25700/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25701/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25702/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25703/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25704/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25705/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25706/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25707/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25708/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25709/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25710/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25711/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25712/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25713/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25714/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25715/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25716/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25717/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25718/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25719/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25720/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25721/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25722/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25723/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25724/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25725/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25726/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25727/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25728/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25729/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25730/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25731/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25732/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25733/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25734/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25735/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25736/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25737/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25738/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25739/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25740/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25741/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25742/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25743/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25744/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25745/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25746/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25747/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25748/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25749/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25750/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25751/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25752/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25753/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25754/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25755/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25756/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25757/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25758/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25759/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25760/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25761/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25762/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25763/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25764/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25765/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25766/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25767/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25768/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25769/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25770/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25771/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25772/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25773/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25774/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25775/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25776/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25777/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25778/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25779/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25780/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25781/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25782/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25783/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25784/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25785/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25786/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25787/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25788/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25789/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25790/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25791/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25792/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25793/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25794/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25795/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25796/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25797/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25798/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25799/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25800/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25801/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25802/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25803/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25804/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25805/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25806/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25807/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25808/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25809/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25810/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25811/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25812/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25813/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25814/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25815/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25816/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25817/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25818/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25819/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25820/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25821/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25822/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25823/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25824/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25825/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25826/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25827/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25828/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25829/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25830/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25831/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25832/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25833/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25834/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25835/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25836/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25837/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25838/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25839/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25840/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25841/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25842/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25843/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25844/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25845/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25846/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25847/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25848/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25849/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25850/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25851/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25852/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25853/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25854/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25855/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25856/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25857/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25858/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25859/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25860/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25861/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25862/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25863/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25864/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25865/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25866/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25867/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25868/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25869/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25870/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25871/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25872/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25873/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25874/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25875/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25876/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25877/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25878/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25879/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25880/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25881/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25882/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25883/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25884/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25885/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25886/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25887/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25888/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25889/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25890/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25891/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25892/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25893/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25894/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25895/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25896/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25897/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25898/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25899/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25900/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25901/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25902/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25903/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25904/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25905/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25906/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25907/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25908/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25909/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25910/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25911/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25912/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25913/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25914/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25915/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [25916/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25917/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [25918/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25919/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25920/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25921/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25922/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25923/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25924/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [25925/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [25926/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25927/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [25928/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25929/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [25930/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [25931/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [25932/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [25933/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [25934/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [25935/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25936/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25937/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25938/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25939/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [25940/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25941/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25942/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25943/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25944/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [25945/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25946/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25947/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25948/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [25949/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [25950/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25951/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25952/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25953/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [25954/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [25955/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25956/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25957/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25958/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25959/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [25960/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [25961/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25962/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25963/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25964/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25965/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [25966/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [25967/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25968/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25969/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25970/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [25971/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [25972/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25973/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25974/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25975/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [25976/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25977/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25978/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25979/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [25980/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [25981/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25982/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25983/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25984/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25985/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [25986/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [25987/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25988/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25989/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25990/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [25991/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [25992/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [25993/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [25994/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25995/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [25996/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [25997/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [25998/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [25999/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26000/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26001/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26002/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [26003/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26004/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26005/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26006/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26007/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26008/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26009/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26010/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26011/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26012/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26013/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26014/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26015/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26016/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26017/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26018/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26019/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26020/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26021/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26022/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26023/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [26024/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26025/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26026/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26027/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26028/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26029/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26030/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26031/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26032/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26033/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26034/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26035/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26036/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26037/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26038/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26039/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26040/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26041/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26042/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26043/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26044/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26045/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26046/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26047/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26048/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26049/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26050/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26051/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26052/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26053/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26054/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26055/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26056/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26057/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26058/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26059/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26060/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26061/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26062/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26063/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26064/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26065/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26066/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26067/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26068/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26069/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26070/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26071/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26072/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26073/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26074/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26075/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26076/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26077/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26078/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26079/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26080/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26081/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26082/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26083/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26084/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26085/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26086/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26087/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26088/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26089/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26090/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26091/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26092/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26093/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26094/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26095/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26096/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26097/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26098/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26099/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26100/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26101/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26102/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26103/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26104/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26105/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26106/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26107/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26108/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26109/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26110/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26111/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26112/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26113/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26114/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26115/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26116/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26117/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26118/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26119/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26120/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26121/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26122/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26123/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26124/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26125/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26126/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26127/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26128/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26129/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26130/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26131/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26132/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26133/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26134/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26135/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26136/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26137/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26138/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26139/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26140/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26141/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26142/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26143/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26144/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26145/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26146/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26147/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26148/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [26149/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26150/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26151/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26152/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26153/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26154/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26155/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26156/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26157/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26158/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26159/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26160/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26161/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26162/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26163/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26164/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26165/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26166/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26167/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26168/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26169/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26170/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26171/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26172/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26173/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [26174/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26175/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26176/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26177/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26178/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26179/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26180/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26181/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26182/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26183/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26184/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26185/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [26186/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26187/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26188/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26189/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26190/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26191/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26192/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26193/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26194/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26195/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26196/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26197/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26198/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26199/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26200/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26201/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26202/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26203/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26204/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26205/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26206/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26207/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26208/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26209/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26210/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [26211/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26212/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26213/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26214/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26215/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26216/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26217/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26218/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26219/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26220/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26221/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26222/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [26223/50000], Train Loss: 8088942.5000, Val Loss: 5298645.5000\n",
      "Epoch [26224/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [26225/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26226/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26227/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26228/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26229/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26230/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26231/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26232/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26233/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26234/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26235/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26236/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26237/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26238/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26239/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26240/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26241/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26242/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26243/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26244/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26245/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26246/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26247/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26248/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26249/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26250/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26251/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26252/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26253/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26254/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26255/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26256/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26257/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26258/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26259/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26260/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26261/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26262/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26263/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26264/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26265/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26266/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26267/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26268/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26269/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26270/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26271/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26272/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26273/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26274/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26275/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26276/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26277/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26278/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26279/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26280/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26281/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26282/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26283/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26284/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26285/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26286/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26287/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26288/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26289/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26290/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26291/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26292/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26293/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26294/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26295/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26296/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26297/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26298/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26299/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26300/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26301/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26302/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26303/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26304/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26305/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26306/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26307/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26308/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26309/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26310/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26311/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26312/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26313/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26314/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26315/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26316/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26317/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26318/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26319/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26320/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26321/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26322/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26323/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26324/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26325/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26326/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26327/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26328/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26329/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26330/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26331/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26332/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26333/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26334/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26335/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26336/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26337/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26338/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26339/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26340/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26341/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26342/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26343/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26344/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26345/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26346/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26347/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26348/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26349/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26350/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26351/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26352/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26353/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26354/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26355/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26356/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26357/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26358/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26359/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26360/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26361/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26362/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26363/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26364/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26365/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26366/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26367/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26368/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26369/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26370/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26371/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26372/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26373/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26374/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26375/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26376/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26377/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26378/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26379/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26380/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26381/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26382/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26383/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26384/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26385/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26386/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26387/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26388/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26389/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26390/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26391/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26392/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26393/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26394/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26395/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26396/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26397/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26398/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26399/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26400/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26401/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26402/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26403/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26404/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26405/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26406/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26407/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26408/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26409/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26410/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26411/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26412/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26413/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26414/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26415/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26416/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26417/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26418/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26419/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26420/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26421/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26422/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26423/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26424/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26425/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26426/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26427/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26428/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26429/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26430/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26431/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26432/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26433/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26434/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26435/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26436/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26437/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26438/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26439/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26440/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26441/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26442/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26443/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26444/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26445/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26446/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26447/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26448/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26449/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26450/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26451/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26452/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26453/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26454/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26455/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26456/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26457/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26458/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26459/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26460/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26461/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26462/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26463/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26464/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26465/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26466/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26467/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26468/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26469/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26470/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26471/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26472/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26473/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26474/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26475/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26476/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26477/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26478/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26479/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26480/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26481/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26482/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26483/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26484/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26485/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26486/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26487/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26488/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26489/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26490/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26491/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26492/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26493/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26494/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26495/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26496/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26497/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26498/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26499/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26500/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26501/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26502/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26503/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26504/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26505/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26506/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26507/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26508/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26509/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26510/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26511/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26512/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26513/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26514/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26515/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26516/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26517/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26518/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26519/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26520/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26521/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26522/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26523/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26524/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26525/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26526/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26527/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26528/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26529/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26530/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26531/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26532/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26533/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26534/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26535/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26536/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26537/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26538/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26539/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26540/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26541/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26542/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26543/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26544/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26545/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26546/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26547/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26548/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26549/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26550/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26551/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26552/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26553/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26554/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26555/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26556/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26557/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26558/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26559/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26560/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26561/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26562/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26563/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26564/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26565/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26566/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26567/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26568/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26570/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26571/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26572/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26573/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26574/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26575/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26576/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26577/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26578/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26579/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26580/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26581/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26582/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26583/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26585/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26586/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26587/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26588/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26589/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26590/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26591/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26592/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26593/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26594/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26595/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26596/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26597/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26598/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26599/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26600/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26601/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26602/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26603/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26604/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26605/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26606/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26607/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26608/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26609/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26610/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26611/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26612/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26613/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26614/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26615/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26616/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26617/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26618/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26619/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26620/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26621/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26622/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26623/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26624/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26625/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26626/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26627/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26628/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26629/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26630/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26631/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26632/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26633/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26634/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26635/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26636/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26637/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26638/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26639/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26640/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26641/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26642/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26643/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26644/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26645/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26646/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26647/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26648/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26649/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26650/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26651/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26652/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26653/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26654/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26655/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26656/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26657/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26658/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26659/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26660/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26661/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26662/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26663/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26664/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26665/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26666/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26667/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26668/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26669/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26670/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26671/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26672/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26673/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26674/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26675/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26676/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26677/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26678/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26679/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26680/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26681/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26682/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26683/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26684/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26685/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26686/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [26687/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [26688/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26689/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26690/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26691/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26692/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26693/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26694/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26695/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26696/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26697/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26698/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26699/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26700/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26701/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26702/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26703/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26704/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26705/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26706/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26707/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26708/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26709/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26710/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26711/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26712/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26713/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26714/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26715/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26716/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26717/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26718/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26719/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26720/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26721/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26722/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [26723/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [26724/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26725/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26726/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26727/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26728/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26729/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26730/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26731/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26732/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26733/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26734/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26735/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26736/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26737/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26738/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26739/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26740/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26741/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26742/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26743/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26744/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26745/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26746/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26747/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26748/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26749/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26750/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26751/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26752/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26753/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26754/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26755/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26756/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26757/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26758/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26759/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26760/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26761/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26762/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26763/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26764/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26765/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26766/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26767/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26768/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26769/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26770/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26771/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26772/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26773/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26774/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26775/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26776/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26777/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26778/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26779/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26780/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26781/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26782/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26783/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26784/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26785/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26786/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26787/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26788/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26789/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26790/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26791/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26792/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26793/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26794/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26795/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26796/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26797/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26798/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26799/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26800/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26801/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26802/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26803/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26804/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26805/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26806/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26807/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26808/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26809/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26810/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26811/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26812/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26813/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26814/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26815/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26816/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26817/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26818/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26819/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26820/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26821/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26822/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26823/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26824/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26825/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26826/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26827/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26828/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26829/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26830/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26831/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26832/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26833/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26834/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26835/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26836/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26837/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26838/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26839/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26840/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26841/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26842/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26843/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [26844/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26845/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26846/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26847/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26848/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26849/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26850/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26851/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26852/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26853/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26854/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26855/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26856/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26857/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26858/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26859/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26860/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26861/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26862/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26863/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26864/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26865/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26866/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26867/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26868/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26869/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26870/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26871/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26872/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26873/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26874/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26875/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26876/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26877/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26878/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26879/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26880/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26881/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26882/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26883/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26884/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26885/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26886/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26887/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26888/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26889/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26890/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26891/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26892/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26893/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26894/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26895/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26896/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26897/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26898/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26899/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26900/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26901/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26902/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26903/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26904/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26905/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26906/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26907/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26908/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26909/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26910/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26911/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26912/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26913/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26914/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26915/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26916/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26917/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26918/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26919/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26920/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26921/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26922/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26923/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26924/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26925/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26926/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26927/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26928/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26929/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26930/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26931/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26932/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26933/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26934/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26935/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26936/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26937/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26938/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26939/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26940/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26941/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26942/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [26943/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26944/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26945/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26946/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26947/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26948/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [26949/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26950/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26951/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [26952/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26953/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26954/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26955/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26956/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26957/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26958/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26959/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26960/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26961/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26962/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26963/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26964/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26965/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26966/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26967/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26968/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [26969/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26970/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [26971/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26972/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26973/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [26974/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26975/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [26976/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [26977/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26978/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26979/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26980/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26981/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26982/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [26983/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [26984/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26985/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26986/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [26987/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [26988/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [26989/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [26990/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [26991/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [26992/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [26993/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [26994/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [26995/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [26996/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [26997/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [26998/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [26999/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [27000/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27001/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [27002/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27003/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [27004/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27005/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [27006/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27007/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27008/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27009/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27010/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27011/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27012/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27013/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27014/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27015/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27016/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27017/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27018/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27019/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27020/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27021/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27022/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27023/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27024/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27025/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [27026/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27027/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27028/50000], Train Loss: 8088941.0000, Val Loss: 5298643.5000\n",
      "Epoch [27029/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [27030/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [27031/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [27032/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27033/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27034/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27035/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27036/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27037/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27038/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27039/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27040/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [27041/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27042/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27043/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27044/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27045/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27046/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27047/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27048/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27049/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27050/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27051/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27052/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [27053/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27054/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27055/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27056/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27057/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27058/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27059/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27060/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27061/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27062/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27063/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27064/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27065/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27066/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27067/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27068/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27069/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27070/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27071/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27072/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27073/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27074/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27075/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27076/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27077/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27078/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27079/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27080/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27081/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27082/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27083/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27084/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27085/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27086/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27087/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27088/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27089/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27090/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27091/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27092/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27093/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27094/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27095/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27096/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27097/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27098/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27099/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27100/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27101/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27102/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27103/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27104/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27105/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27106/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27107/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27108/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27109/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27110/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27111/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27112/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27113/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27114/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27115/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27116/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27117/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27118/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27119/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27120/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27121/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27122/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27123/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27124/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27125/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27126/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27127/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27128/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27129/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27130/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27131/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [27132/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27133/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27134/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27135/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27136/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27137/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27138/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27139/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27140/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27141/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27142/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [27143/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [27144/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27145/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27146/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27147/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27148/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27149/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27150/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27151/50000], Train Loss: 8088941.0000, Val Loss: 5298644.5000\n",
      "Epoch [27152/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [27153/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27154/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [27155/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27156/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [27157/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27158/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [27159/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27160/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27161/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27162/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27163/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27164/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27165/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27166/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27167/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27168/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27169/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27170/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27171/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27172/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27173/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27174/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27175/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27176/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27177/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [27178/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [27179/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27180/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27181/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27182/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27183/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [27184/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27185/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [27186/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27187/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [27188/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27189/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27190/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27191/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27192/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27193/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27194/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27195/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27196/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27197/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27198/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27199/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27200/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27201/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27202/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27203/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27204/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27205/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27206/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27207/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27208/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27209/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [27210/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27211/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27212/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27213/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27214/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27215/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27216/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27217/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27218/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27219/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27220/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27221/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27222/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27223/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27224/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27225/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27226/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27227/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27228/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27229/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27230/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27231/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27232/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27233/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27234/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27235/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27236/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27237/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27238/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27239/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27240/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27241/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27242/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27243/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27244/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27245/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27246/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27247/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27248/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27249/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27250/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27251/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27252/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27253/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27254/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27255/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27256/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27257/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27258/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27259/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27260/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27261/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27262/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27263/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27264/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27265/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27266/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27267/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27268/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27269/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27270/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27271/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27272/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27273/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27274/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27275/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27276/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27277/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27278/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27279/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27280/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [27281/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27282/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27283/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27284/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27285/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27286/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27287/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27288/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27289/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27290/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27291/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [27292/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27293/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27294/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27295/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27296/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27297/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27298/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27299/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27300/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27301/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27302/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27303/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27304/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27305/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27306/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27307/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27308/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27309/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27310/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27311/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27312/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27313/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27314/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27315/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27316/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27317/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [27318/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27319/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27320/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27321/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27322/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27323/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27324/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27325/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27326/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27327/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27328/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27329/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27330/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27331/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27332/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27333/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27334/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27335/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27336/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27337/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27338/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27339/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27340/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27341/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27342/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27343/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27344/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27345/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27346/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27347/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27348/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27349/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27350/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27351/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27352/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27353/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27354/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27355/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27356/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27357/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27358/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27359/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27360/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27361/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27362/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [27363/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27364/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27365/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [27366/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [27367/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27368/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27369/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27370/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27371/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27372/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27373/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27374/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27375/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27376/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27377/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27378/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27379/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27380/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27381/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27382/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27383/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27384/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27385/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27386/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27387/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27388/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27389/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27390/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27391/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27392/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27393/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27394/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27395/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27396/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27397/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27398/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27399/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27400/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27401/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27402/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27403/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27404/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [27405/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27406/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [27407/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [27408/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27409/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27410/50000], Train Loss: 8088941.0000, Val Loss: 5298645.5000\n",
      "Epoch [27411/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [27412/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27413/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27414/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [27415/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [27416/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [27417/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27418/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27419/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27420/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27421/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27422/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27423/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27424/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27425/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27426/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27427/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27428/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27429/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27430/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27431/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27432/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27433/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27434/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [27435/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27436/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27437/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27438/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27439/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27440/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27441/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27442/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27443/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27444/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27445/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27446/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27447/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27448/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27449/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27450/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27451/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27452/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27453/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27454/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27455/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27456/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27457/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27458/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27459/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27460/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27461/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27462/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27463/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27464/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27465/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [27466/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27467/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27468/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27469/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27470/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27471/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27472/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27473/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27474/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27475/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [27476/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27477/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27478/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27479/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [27480/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27481/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27482/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27483/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27484/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27485/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27486/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27487/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27488/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27489/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27490/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27491/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27492/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27493/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27494/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27495/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27496/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [27497/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27498/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27499/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27500/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27501/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27502/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27503/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27504/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27505/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [27506/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [27507/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27508/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27509/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27510/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27511/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27512/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [27513/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [27514/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27515/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27516/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27517/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27518/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27519/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27520/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27521/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27522/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27523/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27524/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27525/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27526/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27527/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27528/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27529/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27530/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27531/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27532/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27533/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27534/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27535/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27536/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27537/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27538/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27539/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [27540/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27541/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27542/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [27543/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27544/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27545/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27546/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27547/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27548/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27549/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27550/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27551/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27552/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27553/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27554/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27555/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27556/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27557/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27558/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27559/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27560/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27561/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27562/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27563/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [27564/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27565/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27566/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27567/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27568/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27569/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27570/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27571/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27572/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [27573/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27574/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27575/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27576/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27577/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27578/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27579/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27580/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27581/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [27582/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27583/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27584/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27585/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27586/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27587/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27588/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27589/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27590/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27591/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27592/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27593/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27594/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27595/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27596/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27597/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27598/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [27599/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27600/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27601/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [27602/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27603/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27604/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [27605/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [27606/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27607/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27608/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [27609/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27610/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [27611/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27612/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27613/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27614/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [27615/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27616/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27617/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [27618/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [27619/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27620/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27621/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [27622/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27623/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27624/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [27625/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27626/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27627/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27628/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27629/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27630/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27631/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27632/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27633/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27634/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27635/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27636/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27637/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27638/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27639/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27640/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27641/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [27642/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27643/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27644/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27645/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27646/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27647/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27648/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27649/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27650/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27651/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27652/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27653/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27654/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27655/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27656/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27657/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27658/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27659/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27660/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27661/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27662/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27663/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27664/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27665/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27666/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27667/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27668/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27669/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27670/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27671/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27672/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27673/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27674/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27675/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27676/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27677/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27678/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27679/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27680/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27681/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27682/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27683/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27684/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27685/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27686/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27687/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27688/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27689/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27690/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27691/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27692/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27693/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27694/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27695/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27696/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27697/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27698/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27699/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27700/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27701/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27702/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27703/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27704/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27705/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27706/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27707/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27708/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27709/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27710/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27711/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27712/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27713/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27714/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27715/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27716/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27717/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27718/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27719/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27720/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27721/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27722/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27723/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27724/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27725/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27726/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27727/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27728/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27729/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27730/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27731/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27732/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27733/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27734/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27735/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27736/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27737/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27738/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27739/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27740/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27741/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27742/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27743/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27744/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27745/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27746/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27747/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27748/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27749/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27750/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [27751/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27752/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27753/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27754/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27755/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27756/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27757/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27758/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27759/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27760/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27761/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27762/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27763/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27764/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27765/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27766/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27767/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27768/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27769/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27770/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27771/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27772/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27773/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27774/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27775/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27776/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27777/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27778/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27779/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27780/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [27781/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27782/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27783/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27784/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27785/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27786/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27787/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27788/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27789/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27790/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27791/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27792/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27793/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27794/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27795/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27796/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27797/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27798/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27799/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27800/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27801/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27802/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [27803/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27804/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27805/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27806/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27807/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27808/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [27809/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27810/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27811/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27812/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27813/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27814/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27815/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27816/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27817/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27818/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27819/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27820/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27821/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27822/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27823/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27824/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27825/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27826/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27827/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27828/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27829/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27830/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27831/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27832/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27833/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27834/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27835/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27836/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27837/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27838/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27839/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27840/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27841/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27842/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27843/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27844/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27845/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27846/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27847/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27848/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27849/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27850/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27851/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [27852/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [27853/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27854/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27855/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27856/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27857/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27858/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27859/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27860/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27861/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27862/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27863/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27864/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27865/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27866/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27867/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27868/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27869/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27870/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27871/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27872/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27873/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27874/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27875/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27876/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27877/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27878/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27879/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [27880/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27881/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27882/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27883/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27884/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27885/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27886/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27887/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27888/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27889/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27890/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27891/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27892/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27893/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27894/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27895/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27896/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27897/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [27898/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27899/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27900/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27901/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27902/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27903/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27904/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27905/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27906/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27907/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27908/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27909/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27910/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27911/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27912/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27913/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27914/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27915/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [27916/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27917/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27918/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27919/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27920/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27921/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27922/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27923/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27924/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [27925/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27926/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [27927/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27928/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27929/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27930/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27931/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [27932/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27933/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27934/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27935/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27936/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27937/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27938/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [27939/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27940/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27941/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27942/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [27943/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [27945/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27946/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27947/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27948/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27949/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27950/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [27951/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27952/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27953/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [27954/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27955/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [27956/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [27957/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27958/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27959/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27960/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27961/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [27962/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [27963/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [27964/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [27965/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [27966/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27967/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [27968/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27969/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [27970/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27971/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27972/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [27973/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [27974/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [27975/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [27976/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [27977/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27978/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27979/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [27980/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [27981/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [27982/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27983/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [27984/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [27985/50000], Train Loss: 8088941.0000, Val Loss: 5298644.5000\n",
      "Epoch [27986/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [27987/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [27988/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [27989/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27990/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [27991/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27992/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [27993/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [27994/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [27995/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27996/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [27997/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [27998/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [27999/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28000/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [28001/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28002/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [28003/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28004/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [28005/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28006/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [28007/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28008/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28009/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28010/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28011/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28012/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28013/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28014/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [28015/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28016/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28017/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28018/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [28019/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28020/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [28021/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28022/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [28023/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [28024/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28025/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [28026/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28027/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [28028/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [28029/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28030/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28031/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28032/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28033/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [28034/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28035/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [28036/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28037/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28038/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28039/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28040/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [28041/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28042/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [28043/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [28044/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28045/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28046/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28047/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28048/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [28049/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28050/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28051/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28052/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28053/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28054/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28055/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28056/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28057/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28058/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28059/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28060/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28061/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [28062/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28063/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28064/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28065/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28066/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28067/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28068/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28069/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [28070/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28071/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28072/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28073/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [28074/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28075/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28076/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28077/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28078/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [28079/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28080/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28081/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [28082/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28083/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28084/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28085/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28086/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28087/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28088/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28089/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28090/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28091/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28092/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28093/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28094/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28095/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [28096/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28097/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [28098/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [28099/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28100/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28101/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28102/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28103/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28104/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28105/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28106/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28107/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28108/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28109/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28110/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28111/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [28112/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28113/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28114/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28115/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28116/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28117/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28118/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28119/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28120/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28121/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28122/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28123/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28124/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [28125/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28126/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28127/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [28128/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [28129/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28130/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28131/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28132/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28133/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [28134/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28135/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28136/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28137/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28138/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28139/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [28140/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28141/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28142/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28143/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28144/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28145/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [28146/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28147/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [28148/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28149/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [28150/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28151/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [28152/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28153/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [28154/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28155/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28156/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28157/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28158/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28159/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28160/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28161/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28162/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28163/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [28164/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [28165/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28166/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28167/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28168/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [28169/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28170/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28171/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [28172/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28173/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [28174/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28175/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28176/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28177/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28178/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28179/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28180/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28181/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28182/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [28183/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28184/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28185/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [28186/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28187/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [28188/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28189/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [28190/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28191/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [28192/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28193/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28194/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [28195/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28196/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28197/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28198/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28199/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28200/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28201/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28202/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28203/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28204/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [28205/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28206/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [28207/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28208/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28209/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28210/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28211/50000], Train Loss: 8088941.0000, Val Loss: 5298644.5000\n",
      "Epoch [28212/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [28213/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28214/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [28215/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28216/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28217/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [28218/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [28219/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28220/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28221/50000], Train Loss: 8088941.0000, Val Loss: 5298644.5000\n",
      "Epoch [28222/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [28223/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [28224/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28225/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28226/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28227/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28228/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [28229/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [28230/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [28231/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28232/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28233/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [28234/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28235/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28236/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28237/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28238/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28239/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [28240/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28241/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [28242/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [28243/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28244/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28245/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28246/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28247/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [28248/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28249/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28250/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [28251/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [28252/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [28253/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28254/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28255/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28256/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28257/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [28258/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [28259/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28260/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28261/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28262/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [28263/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28264/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [28265/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [28266/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28267/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28268/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28269/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28270/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28271/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28272/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28273/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28274/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28275/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [28276/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [28277/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28278/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [28279/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28280/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28281/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28282/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28283/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28284/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [28285/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [28286/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28287/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [28288/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28289/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28290/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28291/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28292/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28293/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28294/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [28295/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28296/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28297/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [28298/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28299/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28300/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [28301/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [28302/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28303/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [28304/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28305/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28306/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [28307/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28308/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28309/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [28310/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28311/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28312/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [28313/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28314/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [28315/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [28316/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [28317/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28318/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [28319/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28320/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28321/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28322/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [28323/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28324/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [28325/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28326/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [28327/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [28328/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28329/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [28330/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28331/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28332/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28333/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28334/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28335/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28336/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [28337/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28338/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28339/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28340/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28341/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28342/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28343/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [28344/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28345/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28346/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28347/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28348/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28349/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28350/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28351/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28352/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28353/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [28354/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28355/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28356/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [28357/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28358/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28359/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28360/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [28361/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28362/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [28363/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28364/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28365/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28366/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [28367/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28368/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28369/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [28370/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28371/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [28372/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28373/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28374/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28375/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28376/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28377/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28378/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [28379/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [28380/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28381/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [28382/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [28383/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28384/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28385/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28386/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28387/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [28388/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [28389/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [28390/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [28391/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28392/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [28393/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28394/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28395/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28396/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [28397/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28398/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28399/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28400/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28401/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [28402/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [28403/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [28404/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [28405/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [28406/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [28407/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [28408/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [28409/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [28410/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [28411/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [28412/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28413/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28414/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [28415/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [28416/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28417/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28418/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28419/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28420/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28421/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28422/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28423/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [28424/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28425/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28426/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [28427/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28428/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28429/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28430/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28431/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28432/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28433/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28434/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28435/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28436/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [28437/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28438/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28439/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28440/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28441/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28442/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28443/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28444/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28445/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [28446/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28447/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28448/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [28449/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28450/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [28451/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28452/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [28453/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28454/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [28455/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28456/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28457/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28458/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28459/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28460/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28461/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [28462/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [28463/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [28464/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [28465/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [28466/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [28467/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [28468/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [28469/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [28470/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [28471/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [28472/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [28473/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [28474/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [28475/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [28476/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [28477/50000], Train Loss: 8088941.5000, Val Loss: 5298597.5000\n",
      "Epoch [28478/50000], Train Loss: 8088942.0000, Val Loss: 5298698.5000\n",
      "Epoch [28479/50000], Train Loss: 8088942.5000, Val Loss: 5298582.5000\n",
      "Epoch [28480/50000], Train Loss: 8088941.5000, Val Loss: 5298711.0000\n",
      "Epoch [28481/50000], Train Loss: 8088941.5000, Val Loss: 5298575.0000\n",
      "Epoch [28482/50000], Train Loss: 8088942.5000, Val Loss: 5298713.5000\n",
      "Epoch [28483/50000], Train Loss: 8088942.0000, Val Loss: 5298579.0000\n",
      "Epoch [28484/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [28485/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [28486/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [28487/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28488/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [28489/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [28490/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [28491/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [28492/50000], Train Loss: 8088942.0000, Val Loss: 5298605.0000\n",
      "Epoch [28493/50000], Train Loss: 8088942.5000, Val Loss: 5298679.0000\n",
      "Epoch [28494/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [28495/50000], Train Loss: 8088941.5000, Val Loss: 5298664.0000\n",
      "Epoch [28496/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [28497/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28498/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [28499/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [28500/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [28501/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [28502/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [28503/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [28504/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [28505/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28506/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28507/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28508/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28509/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28510/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28511/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28512/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [28513/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [28514/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28515/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28516/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28517/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28518/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [28519/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28520/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28521/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [28522/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [28523/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28524/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28525/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28526/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28527/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [28528/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28529/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28530/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [28531/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28532/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28533/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28534/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28535/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28536/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28537/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [28538/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [28539/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [28540/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [28541/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28542/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [28543/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [28544/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28545/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [28546/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [28547/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28548/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28549/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [28550/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [28551/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [28552/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [28553/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [28554/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [28555/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [28556/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28557/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [28558/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28559/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [28560/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [28561/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [28562/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [28563/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [28564/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [28565/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [28566/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [28567/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [28568/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [28569/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [28570/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [28571/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [28572/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [28573/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [28574/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [28575/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [28576/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [28577/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [28578/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [28579/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [28580/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28581/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [28582/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28583/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28584/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28585/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [28586/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28587/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [28588/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [28589/50000], Train Loss: 8088941.5000, Val Loss: 5298631.5000\n",
      "Epoch [28590/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [28591/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [28592/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [28593/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [28594/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [28595/50000], Train Loss: 8088942.5000, Val Loss: 5298610.5000\n",
      "Epoch [28596/50000], Train Loss: 8088942.5000, Val Loss: 5298678.0000\n",
      "Epoch [28597/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [28598/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [28599/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [28600/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [28601/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [28602/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [28603/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [28604/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28605/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28606/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [28607/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [28608/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [28609/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28610/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [28611/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [28612/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [28613/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [28614/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [28615/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [28616/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [28617/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [28618/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [28619/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [28620/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [28621/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [28622/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [28623/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [28624/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [28625/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28626/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28627/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [28628/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [28629/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [28630/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [28631/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [28632/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [28633/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [28634/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [28635/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [28636/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [28637/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [28638/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [28639/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [28640/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [28641/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28642/50000], Train Loss: 8088941.0000, Val Loss: 5298643.5000\n",
      "Epoch [28643/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [28644/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [28645/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [28646/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [28647/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [28648/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [28649/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [28650/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [28651/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [28652/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [28653/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [28654/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [28655/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [28656/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [28657/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [28658/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [28659/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [28660/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [28661/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28662/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [28663/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28664/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [28665/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [28666/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [28667/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [28668/50000], Train Loss: 8088942.5000, Val Loss: 5298630.5000\n",
      "Epoch [28669/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [28670/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [28671/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [28672/50000], Train Loss: 8088941.5000, Val Loss: 5298629.5000\n",
      "Epoch [28673/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [28674/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [28675/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [28676/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [28677/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [28678/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [28679/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [28680/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [28681/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [28682/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [28683/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [28684/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [28685/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [28686/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [28687/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [28688/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [28689/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [28690/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [28691/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28692/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28693/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28694/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28695/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28696/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [28697/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28698/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28699/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [28700/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [28701/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28702/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28703/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [28704/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [28705/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28706/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28707/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [28708/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28709/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [28710/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28711/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28712/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28713/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28714/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28715/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28716/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28717/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28718/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28719/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [28720/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28721/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28722/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [28723/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28724/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28725/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [28726/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28727/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28728/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [28729/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [28730/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [28731/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [28732/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [28733/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [28734/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [28735/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [28736/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [28737/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [28738/50000], Train Loss: 8088941.5000, Val Loss: 5298598.0000\n",
      "Epoch [28739/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [28740/50000], Train Loss: 8088942.0000, Val Loss: 5298578.0000\n",
      "Epoch [28741/50000], Train Loss: 8088942.0000, Val Loss: 5298720.5000\n",
      "Epoch [28742/50000], Train Loss: 8088942.0000, Val Loss: 5298560.5000\n",
      "Epoch [28743/50000], Train Loss: 8088942.0000, Val Loss: 5298732.5000\n",
      "Epoch [28744/50000], Train Loss: 8088941.5000, Val Loss: 5298558.5000\n",
      "Epoch [28745/50000], Train Loss: 8088942.0000, Val Loss: 5298720.0000\n",
      "Epoch [28746/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [28747/50000], Train Loss: 8088941.0000, Val Loss: 5298675.5000\n",
      "Epoch [28748/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [28749/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [28750/50000], Train Loss: 8088942.5000, Val Loss: 5298683.0000\n",
      "Epoch [28751/50000], Train Loss: 8088941.5000, Val Loss: 5298594.0000\n",
      "Epoch [28752/50000], Train Loss: 8088941.5000, Val Loss: 5298698.0000\n",
      "Epoch [28753/50000], Train Loss: 8088942.5000, Val Loss: 5298596.5000\n",
      "Epoch [28754/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [28755/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [28756/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28757/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [28758/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [28759/50000], Train Loss: 8088942.5000, Val Loss: 5298685.5000\n",
      "Epoch [28760/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [28761/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [28762/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [28763/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [28764/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [28765/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [28766/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [28767/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [28768/50000], Train Loss: 8088943.0000, Val Loss: 5298666.5000\n",
      "Epoch [28769/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [28770/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28771/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [28772/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [28773/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [28774/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [28775/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [28776/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28777/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [28778/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [28779/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [28780/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [28781/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28782/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28783/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [28784/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [28785/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28786/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28787/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28788/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28789/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [28790/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [28791/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [28792/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28793/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28794/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28795/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28796/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28797/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28798/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28799/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28800/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28801/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28802/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [28803/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [28804/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [28805/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28806/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28807/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [28808/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28809/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28810/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28811/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [28812/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28813/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28814/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [28815/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28816/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28817/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28818/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28819/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28820/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28821/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28822/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28823/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [28824/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28825/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28826/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [28827/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [28828/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [28829/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28830/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28831/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [28832/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28833/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28834/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [28835/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28836/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28837/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [28838/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28839/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [28840/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [28841/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28842/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28843/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28844/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [28845/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28846/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28847/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28848/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28849/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [28850/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28851/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [28852/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28853/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [28854/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28855/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28856/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [28857/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28858/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [28859/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [28860/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [28861/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28862/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28863/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [28864/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28865/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28866/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28867/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [28868/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28869/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28870/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28871/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [28872/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [28873/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [28874/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [28875/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [28876/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [28877/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [28878/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [28879/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [28880/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [28881/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [28882/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [28883/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [28884/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28885/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [28886/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [28887/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [28888/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28889/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28890/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [28891/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28892/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [28893/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28894/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [28895/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [28896/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [28897/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [28898/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28899/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28900/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [28901/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [28902/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [28903/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28904/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28905/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28906/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [28907/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [28908/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28909/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28910/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28911/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28912/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [28913/50000], Train Loss: 8088941.5000, Val Loss: 5298661.5000\n",
      "Epoch [28914/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [28915/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [28916/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [28917/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [28918/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [28919/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [28920/50000], Train Loss: 8088942.0000, Val Loss: 5298585.0000\n",
      "Epoch [28921/50000], Train Loss: 8088942.0000, Val Loss: 5298703.0000\n",
      "Epoch [28922/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [28923/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [28924/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [28925/50000], Train Loss: 8088941.5000, Val Loss: 5298672.5000\n",
      "Epoch [28926/50000], Train Loss: 8088942.5000, Val Loss: 5298627.0000\n",
      "Epoch [28927/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [28928/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28929/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [28930/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [28931/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [28932/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [28933/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [28934/50000], Train Loss: 8088943.0000, Val Loss: 5298671.5000\n",
      "Epoch [28935/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [28936/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [28937/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [28938/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [28939/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [28940/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [28941/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [28942/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [28943/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [28944/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [28945/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [28946/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [28947/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [28948/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [28949/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [28950/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [28951/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [28952/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [28953/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28954/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28955/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28956/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28957/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28958/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28959/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [28960/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [28961/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [28962/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [28963/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [28964/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28965/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [28966/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28967/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [28968/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [28969/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [28970/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [28971/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [28972/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [28973/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [28974/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [28975/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [28976/50000], Train Loss: 8088941.0000, Val Loss: 5298656.0000\n",
      "Epoch [28977/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [28978/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28979/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [28980/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [28981/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [28982/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [28983/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [28984/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [28985/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [28986/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [28987/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [28988/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [28989/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [28990/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28991/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28992/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [28993/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28994/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [28995/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [28996/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [28997/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [28998/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [28999/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29000/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29001/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29002/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [29003/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29004/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [29005/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [29006/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29007/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [29008/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29009/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29010/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29011/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29012/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [29013/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29014/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [29015/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [29016/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29017/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [29018/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [29019/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [29020/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [29021/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29022/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [29023/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [29024/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [29025/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [29026/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [29027/50000], Train Loss: 8088941.5000, Val Loss: 5298618.5000\n",
      "Epoch [29028/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [29029/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [29030/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [29031/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [29032/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [29033/50000], Train Loss: 8088941.5000, Val Loss: 5298602.0000\n",
      "Epoch [29034/50000], Train Loss: 8088941.5000, Val Loss: 5298693.5000\n",
      "Epoch [29035/50000], Train Loss: 8088941.5000, Val Loss: 5298590.0000\n",
      "Epoch [29036/50000], Train Loss: 8088942.0000, Val Loss: 5298705.0000\n",
      "Epoch [29037/50000], Train Loss: 8088942.0000, Val Loss: 5298579.5000\n",
      "Epoch [29038/50000], Train Loss: 8088941.5000, Val Loss: 5298710.0000\n",
      "Epoch [29039/50000], Train Loss: 8088942.0000, Val Loss: 5298579.5000\n",
      "Epoch [29040/50000], Train Loss: 8088941.5000, Val Loss: 5298703.0000\n",
      "Epoch [29041/50000], Train Loss: 8088942.0000, Val Loss: 5298595.0000\n",
      "Epoch [29042/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [29043/50000], Train Loss: 8088941.5000, Val Loss: 5298623.5000\n",
      "Epoch [29044/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [29045/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [29046/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [29047/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [29048/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [29049/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [29050/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [29051/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [29052/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [29053/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [29054/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29055/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29056/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [29057/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [29058/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [29059/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [29060/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [29061/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [29062/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29063/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [29064/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [29065/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [29066/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29067/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [29068/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [29069/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [29070/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29071/50000], Train Loss: 8088941.0000, Val Loss: 5298641.5000\n",
      "Epoch [29072/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [29073/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [29074/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [29075/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [29076/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [29077/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29078/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [29079/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [29080/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29081/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29082/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29083/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [29084/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [29085/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29086/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [29087/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29088/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [29089/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29090/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29091/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29092/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [29093/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29094/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29095/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [29096/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29097/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29098/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [29099/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29100/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29101/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29102/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29103/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [29104/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [29105/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29106/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [29107/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [29108/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29109/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29110/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29111/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29112/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [29113/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29114/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29115/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29116/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29117/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29118/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [29119/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29120/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29121/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29122/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29123/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [29124/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29125/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29126/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29127/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29128/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [29129/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29130/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [29131/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29132/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29133/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29134/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29135/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29136/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [29137/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29138/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29139/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29140/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29141/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [29142/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29143/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29144/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [29145/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29146/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [29147/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [29148/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [29149/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29150/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29151/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [29152/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29153/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29154/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [29155/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [29156/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [29157/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29158/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29159/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29160/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29161/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29162/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [29163/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [29164/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [29165/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [29166/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [29167/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [29168/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [29169/50000], Train Loss: 8088942.5000, Val Loss: 5298690.5000\n",
      "Epoch [29170/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [29171/50000], Train Loss: 8088941.5000, Val Loss: 5298714.5000\n",
      "Epoch [29172/50000], Train Loss: 8088942.0000, Val Loss: 5298562.5000\n",
      "Epoch [29173/50000], Train Loss: 8088942.5000, Val Loss: 5298737.5000\n",
      "Epoch [29174/50000], Train Loss: 8088942.5000, Val Loss: 5298543.5000\n",
      "Epoch [29175/50000], Train Loss: 8088941.5000, Val Loss: 5298745.0000\n",
      "Epoch [29176/50000], Train Loss: 8088942.0000, Val Loss: 5298555.5000\n",
      "Epoch [29177/50000], Train Loss: 8088942.5000, Val Loss: 5298708.5000\n",
      "Epoch [29178/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [29179/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29180/50000], Train Loss: 8088941.0000, Val Loss: 5298679.0000\n",
      "Epoch [29181/50000], Train Loss: 8088942.0000, Val Loss: 5298588.5000\n",
      "Epoch [29182/50000], Train Loss: 8088941.5000, Val Loss: 5298707.5000\n",
      "Epoch [29183/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [29184/50000], Train Loss: 8088941.5000, Val Loss: 5298683.0000\n",
      "Epoch [29185/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [29186/50000], Train Loss: 8088941.0000, Val Loss: 5298633.5000\n",
      "Epoch [29187/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [29188/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [29189/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [29190/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [29191/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [29192/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29193/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29194/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [29195/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [29196/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [29197/50000], Train Loss: 8088942.5000, Val Loss: 5298634.0000\n",
      "Epoch [29198/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29199/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [29200/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [29201/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [29202/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [29203/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [29204/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29205/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [29206/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [29207/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [29208/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [29209/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [29210/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29211/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29212/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [29213/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [29214/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [29215/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [29216/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29217/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [29218/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29219/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [29220/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [29221/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [29222/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29223/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [29224/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29225/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [29226/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [29227/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29228/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29229/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29230/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29231/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [29232/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29233/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [29234/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29235/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29236/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29237/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29238/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29239/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [29240/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29241/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29242/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29243/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [29244/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [29245/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29246/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29247/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [29248/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [29249/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [29250/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29251/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29252/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29253/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29254/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29255/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29256/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29257/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [29258/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [29259/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29260/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29261/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [29262/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29263/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29264/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29265/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [29266/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29267/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [29268/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [29269/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29270/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29271/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [29272/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29273/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29274/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29275/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29276/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29277/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29278/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [29279/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29280/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29281/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29282/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29283/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [29284/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29285/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29286/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [29287/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [29288/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29289/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29290/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29291/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29292/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [29293/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29294/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29295/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29296/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29297/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [29298/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [29299/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29300/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29301/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29302/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29303/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29304/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [29305/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29306/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29307/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [29308/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29309/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29310/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29311/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29312/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29313/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [29314/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [29315/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29316/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29317/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [29318/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [29319/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [29320/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [29321/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [29322/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [29323/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [29324/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [29325/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [29326/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [29327/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29328/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29329/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29330/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [29331/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29332/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29333/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29334/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29335/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [29336/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29337/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [29338/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [29339/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [29340/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29341/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [29342/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [29343/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29344/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [29345/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29346/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [29347/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [29348/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29349/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29350/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [29351/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29352/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [29353/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29354/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29355/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29356/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29357/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [29358/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29359/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29360/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29361/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [29362/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [29363/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29364/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29365/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [29366/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29367/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [29368/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29369/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29370/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [29371/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29372/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [29373/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [29374/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [29375/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [29376/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [29377/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [29378/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [29379/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [29380/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [29381/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [29382/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [29383/50000], Train Loss: 8088941.5000, Val Loss: 5298680.0000\n",
      "Epoch [29384/50000], Train Loss: 8088942.5000, Val Loss: 5298598.0000\n",
      "Epoch [29385/50000], Train Loss: 8088942.0000, Val Loss: 5298703.0000\n",
      "Epoch [29386/50000], Train Loss: 8088942.0000, Val Loss: 5298571.5000\n",
      "Epoch [29387/50000], Train Loss: 8088942.0000, Val Loss: 5298731.0000\n",
      "Epoch [29388/50000], Train Loss: 8088942.0000, Val Loss: 5298546.5000\n",
      "Epoch [29389/50000], Train Loss: 8088942.0000, Val Loss: 5298747.0000\n",
      "Epoch [29390/50000], Train Loss: 8088942.5000, Val Loss: 5298548.5000\n",
      "Epoch [29391/50000], Train Loss: 8088941.5000, Val Loss: 5298722.5000\n",
      "Epoch [29392/50000], Train Loss: 8088942.0000, Val Loss: 5298594.5000\n",
      "Epoch [29393/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [29394/50000], Train Loss: 8088941.5000, Val Loss: 5298665.0000\n",
      "Epoch [29395/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [29396/50000], Train Loss: 8088942.5000, Val Loss: 5298707.0000\n",
      "Epoch [29397/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [29398/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [29399/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [29400/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29401/50000], Train Loss: 8088941.0000, Val Loss: 5298671.0000\n",
      "Epoch [29402/50000], Train Loss: 8088942.0000, Val Loss: 5298603.0000\n",
      "Epoch [29403/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [29404/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [29405/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [29406/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29407/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [29408/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [29409/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [29410/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [29411/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [29412/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29413/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [29414/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29415/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [29416/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [29417/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [29418/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29419/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [29420/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [29421/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [29422/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [29423/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [29424/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29425/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29426/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [29427/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [29428/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29429/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [29430/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29431/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29432/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [29433/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [29434/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29435/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [29436/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [29437/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29438/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29439/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29440/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29441/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [29442/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [29443/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [29444/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29445/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29446/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29447/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29448/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29449/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29450/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29451/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [29452/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29453/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [29454/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29455/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [29456/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29457/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [29458/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29459/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29460/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29461/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29462/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29463/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [29464/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [29465/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [29466/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29467/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29468/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29469/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [29470/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [29471/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29472/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29473/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29474/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [29475/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29476/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29477/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29478/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [29479/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29480/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29481/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [29482/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29483/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29484/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29485/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [29486/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29487/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [29488/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29489/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29490/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29491/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29492/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29493/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29494/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [29495/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [29496/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29497/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [29498/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29499/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29500/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29501/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29502/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29503/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29504/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [29505/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29506/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [29507/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29508/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29509/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29510/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29511/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [29512/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29513/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [29514/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29515/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29516/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [29517/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29518/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [29519/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29520/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [29521/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29522/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29523/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29524/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29525/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29526/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29527/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29528/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29529/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29530/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [29531/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29532/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [29533/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29534/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29535/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29536/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29537/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [29538/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29539/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [29540/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29541/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29542/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29543/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29544/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29545/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29546/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [29547/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [29548/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29549/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29550/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [29551/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [29552/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [29553/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [29554/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [29555/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [29556/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [29557/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [29558/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [29559/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [29560/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [29561/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [29562/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [29563/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [29564/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [29565/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [29566/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [29567/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [29568/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [29569/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [29570/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [29571/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [29572/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [29573/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29574/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29575/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [29576/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29577/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [29578/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29579/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [29580/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [29581/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [29582/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [29583/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [29584/50000], Train Loss: 8088941.5000, Val Loss: 5298659.0000\n",
      "Epoch [29585/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29586/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [29587/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [29588/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [29589/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [29590/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [29591/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [29592/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [29593/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [29594/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [29595/50000], Train Loss: 8088942.5000, Val Loss: 5298607.0000\n",
      "Epoch [29596/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [29597/50000], Train Loss: 8088941.5000, Val Loss: 5298609.0000\n",
      "Epoch [29598/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [29599/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [29600/50000], Train Loss: 8088943.0000, Val Loss: 5298670.0000\n",
      "Epoch [29601/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [29602/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [29603/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [29604/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [29605/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29606/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29607/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29608/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29609/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29610/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29611/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29612/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29613/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [29614/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29615/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29616/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [29617/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29618/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29619/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [29620/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [29621/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [29622/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [29623/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [29624/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [29625/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [29626/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [29627/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [29628/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [29629/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [29630/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [29631/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [29632/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [29633/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [29634/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [29635/50000], Train Loss: 8088941.5000, Val Loss: 5298607.0000\n",
      "Epoch [29636/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [29637/50000], Train Loss: 8088941.5000, Val Loss: 5298610.0000\n",
      "Epoch [29638/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [29639/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [29640/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [29641/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [29642/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [29643/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [29644/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29645/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [29646/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29647/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [29648/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29649/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29650/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29651/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [29652/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [29653/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29654/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29655/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [29656/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29657/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [29658/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29659/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29660/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29661/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29662/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [29663/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29664/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [29665/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [29666/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [29667/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29668/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [29669/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [29670/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [29671/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [29672/50000], Train Loss: 8088943.0000, Val Loss: 5298677.0000\n",
      "Epoch [29673/50000], Train Loss: 8088942.5000, Val Loss: 5298609.0000\n",
      "Epoch [29674/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [29675/50000], Train Loss: 8088941.5000, Val Loss: 5298608.5000\n",
      "Epoch [29676/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [29677/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [29678/50000], Train Loss: 8088942.5000, Val Loss: 5298675.5000\n",
      "Epoch [29679/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [29680/50000], Train Loss: 8088941.5000, Val Loss: 5298669.0000\n",
      "Epoch [29681/50000], Train Loss: 8088941.5000, Val Loss: 5298623.5000\n",
      "Epoch [29682/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [29683/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29684/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [29685/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [29686/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [29687/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [29688/50000], Train Loss: 8088941.0000, Val Loss: 5298659.0000\n",
      "Epoch [29689/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [29690/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [29691/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [29692/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [29693/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [29694/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [29695/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [29696/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [29697/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [29698/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [29699/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [29700/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [29701/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [29702/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [29703/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [29704/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [29705/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [29706/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [29707/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29708/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [29709/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [29710/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [29711/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [29712/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [29713/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [29714/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [29715/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [29716/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [29717/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [29718/50000], Train Loss: 8088942.5000, Val Loss: 5298659.0000\n",
      "Epoch [29719/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [29720/50000], Train Loss: 8088941.0000, Val Loss: 5298655.5000\n",
      "Epoch [29721/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [29722/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [29723/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [29724/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29725/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29726/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29727/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [29728/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29729/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [29730/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29731/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [29732/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29733/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29734/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [29735/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [29736/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [29737/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [29738/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29739/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [29740/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [29741/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [29742/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [29743/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [29744/50000], Train Loss: 8088942.5000, Val Loss: 5298608.0000\n",
      "Epoch [29745/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [29746/50000], Train Loss: 8088941.5000, Val Loss: 5298603.0000\n",
      "Epoch [29747/50000], Train Loss: 8088941.5000, Val Loss: 5298687.0000\n",
      "Epoch [29748/50000], Train Loss: 8088941.5000, Val Loss: 5298601.5000\n",
      "Epoch [29749/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [29750/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [29751/50000], Train Loss: 8088941.5000, Val Loss: 5298684.5000\n",
      "Epoch [29752/50000], Train Loss: 8088942.5000, Val Loss: 5298607.0000\n",
      "Epoch [29753/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [29754/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [29755/50000], Train Loss: 8088943.0000, Val Loss: 5298669.0000\n",
      "Epoch [29756/50000], Train Loss: 8088941.5000, Val Loss: 5298625.5000\n",
      "Epoch [29757/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [29758/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [29759/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29760/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29761/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29762/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [29763/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [29764/50000], Train Loss: 8088941.0000, Val Loss: 5298663.0000\n",
      "Epoch [29765/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [29766/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [29767/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [29768/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [29769/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [29770/50000], Train Loss: 8088943.0000, Val Loss: 5298672.5000\n",
      "Epoch [29771/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [29772/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [29773/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [29774/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [29775/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29776/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [29777/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29778/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29779/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [29780/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [29781/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [29782/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [29783/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [29784/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29785/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [29786/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29787/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [29788/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [29789/50000], Train Loss: 8088942.5000, Val Loss: 5298659.0000\n",
      "Epoch [29790/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [29791/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [29792/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [29793/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [29794/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [29795/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [29796/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [29797/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [29798/50000], Train Loss: 8088941.5000, Val Loss: 5298627.0000\n",
      "Epoch [29799/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [29800/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [29801/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [29802/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [29803/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [29804/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [29805/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [29806/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [29807/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [29808/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [29809/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [29810/50000], Train Loss: 8088941.5000, Val Loss: 5298623.5000\n",
      "Epoch [29811/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [29812/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [29813/50000], Train Loss: 8088941.0000, Val Loss: 5298656.0000\n",
      "Epoch [29814/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [29815/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29816/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29817/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29818/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [29819/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29820/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29821/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29822/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29823/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [29824/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [29825/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [29826/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [29827/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [29828/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29829/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [29830/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [29831/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [29832/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29833/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29834/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [29835/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [29836/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [29837/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [29838/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [29839/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29840/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [29841/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [29842/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29843/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [29844/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [29845/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29846/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [29847/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [29848/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [29849/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [29850/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [29851/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [29852/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [29853/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [29854/50000], Train Loss: 8088942.5000, Val Loss: 5298675.5000\n",
      "Epoch [29855/50000], Train Loss: 8088942.5000, Val Loss: 5298607.0000\n",
      "Epoch [29856/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [29857/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [29858/50000], Train Loss: 8088942.0000, Val Loss: 5298703.0000\n",
      "Epoch [29859/50000], Train Loss: 8088942.0000, Val Loss: 5298578.5000\n",
      "Epoch [29860/50000], Train Loss: 8088941.5000, Val Loss: 5298714.0000\n",
      "Epoch [29861/50000], Train Loss: 8088942.0000, Val Loss: 5298574.0000\n",
      "Epoch [29862/50000], Train Loss: 8088942.5000, Val Loss: 5298710.0000\n",
      "Epoch [29863/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [29864/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [29865/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [29866/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [29867/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [29868/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [29869/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [29870/50000], Train Loss: 8088942.5000, Val Loss: 5298599.5000\n",
      "Epoch [29871/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [29872/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [29873/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [29874/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [29875/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [29876/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [29877/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [29878/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [29879/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [29880/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [29881/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29882/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [29883/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29884/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [29885/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [29886/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [29887/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [29888/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [29889/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29890/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [29891/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [29892/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [29893/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [29894/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [29895/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [29896/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29897/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29898/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [29899/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29900/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [29901/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [29902/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29903/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [29904/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [29905/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29906/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [29907/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [29908/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29909/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [29910/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29911/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29912/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29913/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [29914/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [29915/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [29916/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29917/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [29918/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29919/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [29920/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29921/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29922/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29923/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29924/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29925/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29926/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [29927/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29928/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [29929/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [29930/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29931/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [29932/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29933/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29934/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [29935/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29936/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29937/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29938/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29939/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29940/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [29941/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [29942/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29943/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29944/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [29945/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [29946/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [29947/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [29948/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [29949/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29950/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [29951/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [29952/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [29953/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [29954/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29955/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29956/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [29957/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [29958/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29959/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [29960/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [29961/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29962/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [29963/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [29964/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [29965/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [29966/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [29967/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [29968/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [29969/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [29970/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [29971/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [29972/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [29973/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [29974/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [29975/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [29976/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [29977/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [29978/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [29979/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [29980/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [29981/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [29982/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [29983/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [29984/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [29985/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [29986/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [29987/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [29988/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [29989/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [29990/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [29991/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [29992/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [29993/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [29994/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [29995/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [29996/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [29997/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [29998/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [29999/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [30000/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [30001/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30002/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30003/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [30004/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [30005/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [30006/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30007/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30008/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30009/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30010/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [30011/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30012/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30013/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30014/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [30015/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30016/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30017/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30018/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30019/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30020/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30021/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30022/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30023/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30024/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30025/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30026/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30027/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30028/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [30029/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30030/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [30031/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30032/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30033/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30034/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [30035/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30036/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30037/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30038/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [30039/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [30040/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [30041/50000], Train Loss: 8088941.5000, Val Loss: 5298659.0000\n",
      "Epoch [30042/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [30043/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [30044/50000], Train Loss: 8088942.5000, Val Loss: 5298608.0000\n",
      "Epoch [30045/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [30046/50000], Train Loss: 8088942.0000, Val Loss: 5298577.5000\n",
      "Epoch [30047/50000], Train Loss: 8088942.0000, Val Loss: 5298733.5000\n",
      "Epoch [30048/50000], Train Loss: 8088941.5000, Val Loss: 5298531.0000\n",
      "Epoch [30049/50000], Train Loss: 8088942.5000, Val Loss: 5298779.0000\n",
      "Epoch [30050/50000], Train Loss: 8088942.0000, Val Loss: 5298498.5000\n",
      "Epoch [30051/50000], Train Loss: 8088941.0000, Val Loss: 5298777.5000\n",
      "Epoch [30052/50000], Train Loss: 8088942.5000, Val Loss: 5298552.5000\n",
      "Epoch [30053/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [30054/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [30055/50000], Train Loss: 8088942.5000, Val Loss: 5298559.5000\n",
      "Epoch [30056/50000], Train Loss: 8088942.0000, Val Loss: 5298747.0000\n",
      "Epoch [30057/50000], Train Loss: 8088942.5000, Val Loss: 5298559.5000\n",
      "Epoch [30058/50000], Train Loss: 8088942.5000, Val Loss: 5298683.0000\n",
      "Epoch [30059/50000], Train Loss: 8088941.5000, Val Loss: 5298663.0000\n",
      "Epoch [30060/50000], Train Loss: 8088942.0000, Val Loss: 5298581.0000\n",
      "Epoch [30061/50000], Train Loss: 8088942.5000, Val Loss: 5298723.0000\n",
      "Epoch [30062/50000], Train Loss: 8088942.0000, Val Loss: 5298583.5000\n",
      "Epoch [30063/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [30064/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [30065/50000], Train Loss: 8088942.5000, Val Loss: 5298590.0000\n",
      "Epoch [30066/50000], Train Loss: 8088942.0000, Val Loss: 5298703.0000\n",
      "Epoch [30067/50000], Train Loss: 8088942.0000, Val Loss: 5298609.0000\n",
      "Epoch [30068/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30069/50000], Train Loss: 8088942.5000, Val Loss: 5298679.0000\n",
      "Epoch [30070/50000], Train Loss: 8088942.0000, Val Loss: 5298595.0000\n",
      "Epoch [30071/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [30072/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [30073/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30074/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [30075/50000], Train Loss: 8088941.5000, Val Loss: 5298606.0000\n",
      "Epoch [30076/50000], Train Loss: 8088941.5000, Val Loss: 5298666.5000\n",
      "Epoch [30077/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30078/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [30079/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [30080/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [30081/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [30082/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [30083/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [30084/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [30085/50000], Train Loss: 8088942.5000, Val Loss: 5298633.5000\n",
      "Epoch [30086/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30087/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [30088/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [30089/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [30090/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30091/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [30092/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [30093/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30094/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [30095/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30096/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [30097/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [30098/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [30099/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [30100/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30101/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30102/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30103/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30104/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [30105/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30106/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30107/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [30108/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30109/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [30110/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30111/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30112/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [30113/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30114/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30115/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [30116/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30117/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30118/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [30119/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30120/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30121/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [30122/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [30123/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30124/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30125/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30126/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [30127/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30128/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30129/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30130/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30131/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30132/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30133/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30134/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30135/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30136/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30137/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30138/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30139/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30140/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30141/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30142/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30143/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30144/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30145/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30146/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30147/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30148/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30149/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30150/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30151/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30152/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30153/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30154/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30155/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30156/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [30157/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30158/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30159/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30160/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30161/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30162/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30163/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30164/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30165/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30166/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30167/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30168/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [30169/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30170/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30171/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30172/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [30173/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [30174/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30175/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30176/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [30177/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30178/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30179/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30180/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30181/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30182/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30183/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30184/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [30185/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30186/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30187/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30188/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30189/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30190/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [30191/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30192/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30193/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30194/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30195/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30196/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30197/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30198/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30199/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30200/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30201/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30202/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30203/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30204/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30205/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30206/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30207/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30208/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [30209/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30210/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [30211/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30212/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30213/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30214/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30215/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30216/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30217/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30218/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30219/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [30220/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30221/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30222/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [30223/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [30224/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30225/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [30226/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30227/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [30228/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30229/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30230/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30231/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30232/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30233/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30234/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30235/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30236/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [30237/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30238/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30239/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [30240/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30241/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30242/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30243/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30244/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30245/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30246/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30247/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30248/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30249/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30250/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30251/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30252/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [30253/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [30254/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30255/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [30256/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [30257/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30258/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30259/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30260/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30261/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [30262/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30263/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [30264/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30265/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [30266/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [30267/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30268/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30269/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30270/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30271/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30272/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30273/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30274/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30275/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [30276/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [30277/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [30278/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30279/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [30280/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30281/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30282/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30283/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [30284/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30285/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [30286/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30287/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30288/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30289/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30290/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30291/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30292/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [30293/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [30294/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [30295/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [30296/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [30297/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30298/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [30299/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [30300/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30301/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30302/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [30303/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30304/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [30305/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [30306/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30307/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30308/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [30309/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [30310/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [30311/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30312/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [30313/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30314/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30315/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30316/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30317/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30318/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30319/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30320/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30321/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30322/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30323/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30324/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [30325/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30326/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30327/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30328/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30329/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [30330/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30331/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30332/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30333/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30334/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30335/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30336/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30337/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30338/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30339/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [30340/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30341/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30342/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30343/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30344/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30345/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30346/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [30347/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30348/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30349/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [30350/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30351/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30352/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [30353/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [30354/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [30355/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30356/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [30357/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [30358/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [30359/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30360/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30361/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30362/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [30363/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [30364/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [30365/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30366/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30367/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30368/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [30369/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30370/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30371/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [30372/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [30373/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30374/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [30375/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [30376/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30377/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [30378/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30379/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30380/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30381/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30382/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [30383/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30384/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [30385/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [30386/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30387/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [30388/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30389/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [30390/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30391/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30392/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [30393/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [30394/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [30395/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [30396/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [30397/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [30398/50000], Train Loss: 8088941.5000, Val Loss: 5298691.5000\n",
      "Epoch [30399/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [30400/50000], Train Loss: 8088941.5000, Val Loss: 5298713.5000\n",
      "Epoch [30401/50000], Train Loss: 8088942.0000, Val Loss: 5298563.0000\n",
      "Epoch [30402/50000], Train Loss: 8088942.5000, Val Loss: 5298733.5000\n",
      "Epoch [30403/50000], Train Loss: 8088942.0000, Val Loss: 5298552.5000\n",
      "Epoch [30404/50000], Train Loss: 8088942.0000, Val Loss: 5298731.0000\n",
      "Epoch [30405/50000], Train Loss: 8088942.0000, Val Loss: 5298572.5000\n",
      "Epoch [30406/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [30407/50000], Train Loss: 8088941.5000, Val Loss: 5298624.5000\n",
      "Epoch [30408/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30409/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [30410/50000], Train Loss: 8088942.5000, Val Loss: 5298594.5000\n",
      "Epoch [30411/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [30412/50000], Train Loss: 8088942.0000, Val Loss: 5298591.5000\n",
      "Epoch [30413/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [30414/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [30415/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30416/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [30417/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [30418/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [30419/50000], Train Loss: 8088941.5000, Val Loss: 5298611.0000\n",
      "Epoch [30420/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [30421/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [30422/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30423/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [30424/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [30425/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [30426/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [30427/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [30428/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [30429/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30430/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [30431/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [30432/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [30433/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [30434/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [30435/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30436/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [30437/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [30438/50000], Train Loss: 8088941.5000, Val Loss: 5298625.5000\n",
      "Epoch [30439/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [30440/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [30441/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [30442/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [30443/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [30444/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [30445/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [30446/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [30447/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [30448/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [30449/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [30450/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [30451/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30452/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [30453/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [30454/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [30455/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30456/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30457/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30458/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30459/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30460/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [30461/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [30462/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30463/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [30464/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [30465/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [30466/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30467/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [30468/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30469/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [30470/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [30471/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [30472/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30473/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30474/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30475/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [30476/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [30477/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30478/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [30479/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30480/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30481/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30482/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30483/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30484/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30485/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30486/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30487/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30488/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30489/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30490/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30491/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30492/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30493/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30494/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [30495/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30496/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [30497/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30498/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [30499/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30500/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30501/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30502/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [30503/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [30504/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [30505/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [30506/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30507/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30508/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [30509/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30510/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [30511/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30512/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [30513/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [30514/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30515/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [30516/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30517/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30518/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [30519/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30520/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30521/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30522/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [30523/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30524/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30525/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30526/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30527/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30528/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30529/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30530/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30531/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [30532/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30533/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [30534/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [30535/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30536/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [30537/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30538/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [30539/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [30540/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [30541/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30542/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [30543/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30544/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30545/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30546/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30547/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30548/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30549/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30550/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30551/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [30552/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30553/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30554/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [30555/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [30556/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [30557/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30558/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [30559/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [30560/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30561/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [30562/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30563/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30564/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30565/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30566/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [30567/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30568/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30569/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30570/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30571/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30572/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30573/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [30574/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30575/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30576/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30577/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [30578/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [30579/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [30580/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [30581/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [30582/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [30583/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [30584/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [30585/50000], Train Loss: 8088941.5000, Val Loss: 5298679.0000\n",
      "Epoch [30586/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [30587/50000], Train Loss: 8088942.0000, Val Loss: 5298697.0000\n",
      "Epoch [30588/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [30589/50000], Train Loss: 8088941.5000, Val Loss: 5298713.0000\n",
      "Epoch [30590/50000], Train Loss: 8088942.0000, Val Loss: 5298568.5000\n",
      "Epoch [30591/50000], Train Loss: 8088942.0000, Val Loss: 5298723.0000\n",
      "Epoch [30592/50000], Train Loss: 8088942.0000, Val Loss: 5298567.0000\n",
      "Epoch [30593/50000], Train Loss: 8088942.0000, Val Loss: 5298714.0000\n",
      "Epoch [30594/50000], Train Loss: 8088942.0000, Val Loss: 5298589.5000\n",
      "Epoch [30595/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [30596/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [30597/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30598/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [30599/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [30600/50000], Train Loss: 8088941.5000, Val Loss: 5298691.5000\n",
      "Epoch [30601/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [30602/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [30603/50000], Train Loss: 8088941.5000, Val Loss: 5298618.5000\n",
      "Epoch [30604/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30605/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30606/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [30607/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [30608/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [30609/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [30610/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [30611/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [30612/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30613/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [30614/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [30615/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [30616/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [30617/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [30618/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [30619/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30620/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30621/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30622/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30623/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [30624/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [30625/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30626/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30627/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30628/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30629/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30630/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30631/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [30632/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [30633/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30634/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [30635/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30636/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30637/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30638/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30639/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30640/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30641/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30642/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30643/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30644/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30645/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30646/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30647/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [30648/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30649/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30650/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [30651/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30652/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30653/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30654/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30655/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30656/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30657/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30658/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [30659/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30660/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30661/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30662/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30663/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30664/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30665/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30666/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30667/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30668/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30669/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30670/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30671/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30672/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [30673/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30674/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [30675/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30676/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30677/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30678/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30679/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30680/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30681/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [30682/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30683/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [30684/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30685/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30686/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30687/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [30688/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30689/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30690/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [30691/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30692/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [30693/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30694/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [30695/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [30696/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [30697/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30698/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30699/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [30700/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30701/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [30702/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [30703/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [30704/50000], Train Loss: 8088941.0000, Val Loss: 5298661.5000\n",
      "Epoch [30705/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [30706/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [30707/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [30708/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [30709/50000], Train Loss: 8088941.5000, Val Loss: 5298621.0000\n",
      "Epoch [30710/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [30711/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [30712/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [30713/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [30714/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [30715/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [30716/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [30717/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [30718/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [30719/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [30720/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [30721/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [30722/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [30723/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [30724/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [30725/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [30726/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [30727/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [30728/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [30729/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [30730/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [30731/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [30732/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [30733/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [30734/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [30735/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [30736/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [30737/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [30738/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [30739/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [30740/50000], Train Loss: 8088941.0000, Val Loss: 5298655.0000\n",
      "Epoch [30741/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [30742/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30743/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30744/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30745/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30746/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [30747/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [30748/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [30749/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [30750/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30751/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [30752/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [30753/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [30754/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30755/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [30756/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [30757/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [30758/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30759/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [30760/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30761/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30762/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30763/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30764/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30765/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30766/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [30767/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [30768/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [30769/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [30770/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [30771/50000], Train Loss: 8088942.5000, Val Loss: 5298602.0000\n",
      "Epoch [30772/50000], Train Loss: 8088941.5000, Val Loss: 5298693.5000\n",
      "Epoch [30773/50000], Train Loss: 8088941.5000, Val Loss: 5298590.0000\n",
      "Epoch [30774/50000], Train Loss: 8088942.0000, Val Loss: 5298703.0000\n",
      "Epoch [30775/50000], Train Loss: 8088942.0000, Val Loss: 5298583.5000\n",
      "Epoch [30776/50000], Train Loss: 8088942.0000, Val Loss: 5298705.0000\n",
      "Epoch [30777/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [30778/50000], Train Loss: 8088941.5000, Val Loss: 5298695.5000\n",
      "Epoch [30779/50000], Train Loss: 8088942.0000, Val Loss: 5298603.0000\n",
      "Epoch [30780/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [30781/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30782/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30783/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [30784/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [30785/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [30786/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [30787/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [30788/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [30789/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [30790/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [30791/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [30792/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30793/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30794/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30795/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30796/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [30797/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [30798/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [30799/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [30800/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [30801/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [30802/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30803/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30804/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30805/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30806/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [30807/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [30808/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30809/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [30810/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [30811/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30812/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30813/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [30814/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [30815/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [30816/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [30817/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [30818/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30819/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [30820/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30821/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30822/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30823/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30824/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [30825/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30826/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [30827/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30828/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30829/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30830/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30831/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [30832/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30833/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [30834/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [30835/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [30836/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30837/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30838/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [30839/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [30840/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30841/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [30842/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [30843/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [30844/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [30845/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [30846/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [30847/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [30848/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [30849/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [30850/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [30851/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [30852/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [30853/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [30854/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30855/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [30856/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30857/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [30858/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30859/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [30860/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [30861/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [30862/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [30863/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [30864/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [30865/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [30866/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [30867/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [30868/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [30869/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [30870/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [30871/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [30872/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [30873/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [30874/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [30875/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [30876/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30877/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [30878/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30879/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30880/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [30881/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30882/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [30883/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30884/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [30885/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [30886/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30887/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30888/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30889/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30890/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30891/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [30892/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30893/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30894/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [30895/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [30896/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30897/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30898/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [30899/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [30900/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [30901/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30902/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [30903/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [30904/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [30905/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30906/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30907/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [30908/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [30909/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30910/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30911/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [30912/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30913/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30914/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [30915/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30916/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [30917/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [30918/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [30919/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [30920/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [30921/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30922/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30923/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [30924/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [30925/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [30926/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [30927/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [30928/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [30929/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [30930/50000], Train Loss: 8088941.5000, Val Loss: 5298677.0000\n",
      "Epoch [30931/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [30932/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [30933/50000], Train Loss: 8088941.5000, Val Loss: 5298590.0000\n",
      "Epoch [30934/50000], Train Loss: 8088942.0000, Val Loss: 5298708.5000\n",
      "Epoch [30935/50000], Train Loss: 8088942.0000, Val Loss: 5298571.5000\n",
      "Epoch [30936/50000], Train Loss: 8088942.0000, Val Loss: 5298725.0000\n",
      "Epoch [30937/50000], Train Loss: 8088941.5000, Val Loss: 5298559.5000\n",
      "Epoch [30938/50000], Train Loss: 8088942.0000, Val Loss: 5298728.0000\n",
      "Epoch [30939/50000], Train Loss: 8088941.5000, Val Loss: 5298569.5000\n",
      "Epoch [30940/50000], Train Loss: 8088942.0000, Val Loss: 5298702.5000\n",
      "Epoch [30941/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [30942/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30943/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [30944/50000], Train Loss: 8088941.5000, Val Loss: 5298610.5000\n",
      "Epoch [30945/50000], Train Loss: 8088942.5000, Val Loss: 5298690.5000\n",
      "Epoch [30946/50000], Train Loss: 8088941.5000, Val Loss: 5298592.5000\n",
      "Epoch [30947/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [30948/50000], Train Loss: 8088941.5000, Val Loss: 5298610.0000\n",
      "Epoch [30949/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [30950/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [30951/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [30952/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [30953/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [30954/50000], Train Loss: 8088942.5000, Val Loss: 5298678.5000\n",
      "Epoch [30955/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [30956/50000], Train Loss: 8088941.5000, Val Loss: 5298662.0000\n",
      "Epoch [30957/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [30958/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30959/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [30960/50000], Train Loss: 8088941.5000, Val Loss: 5298625.0000\n",
      "Epoch [30961/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [30962/50000], Train Loss: 8088942.5000, Val Loss: 5298626.5000\n",
      "Epoch [30963/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [30964/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30965/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30966/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [30967/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [30968/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [30969/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [30970/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30971/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30972/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30973/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [30974/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [30975/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30976/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [30977/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30978/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [30979/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [30980/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [30981/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [30982/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [30983/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [30984/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [30985/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [30986/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [30987/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30988/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [30989/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [30990/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [30991/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [30992/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [30993/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [30994/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [30995/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [30996/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [30997/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [30998/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [30999/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31000/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31001/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31002/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [31003/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31004/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31005/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [31006/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31007/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31008/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [31009/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31010/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31011/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31012/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31013/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [31014/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31015/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31016/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [31017/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31018/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31019/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [31020/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31021/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31022/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31023/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [31024/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31025/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31026/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [31027/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [31028/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31029/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [31030/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31031/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31032/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [31033/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [31034/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [31035/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [31036/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31037/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31038/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31039/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [31040/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31041/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31042/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31043/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [31044/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31045/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31046/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31047/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31048/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31049/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31050/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31051/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31052/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [31053/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31054/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31055/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31056/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [31057/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31058/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31059/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31060/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31061/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31062/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31063/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31064/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31065/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31066/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31067/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31068/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [31069/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31070/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [31071/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31072/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31073/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31074/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [31075/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31076/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31077/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [31078/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31079/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31080/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31081/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31082/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31083/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31084/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31085/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31086/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31087/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31088/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [31089/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31090/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [31091/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [31092/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [31093/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31094/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [31095/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [31096/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [31097/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [31098/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31099/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31100/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [31101/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31102/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31103/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31104/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31105/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31106/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31107/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31108/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [31109/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [31110/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31111/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [31112/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [31113/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [31114/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [31115/50000], Train Loss: 8088943.0000, Val Loss: 5298680.5000\n",
      "Epoch [31116/50000], Train Loss: 8088941.5000, Val Loss: 5298599.5000\n",
      "Epoch [31117/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [31118/50000], Train Loss: 8088942.0000, Val Loss: 5298577.5000\n",
      "Epoch [31119/50000], Train Loss: 8088942.0000, Val Loss: 5298725.0000\n",
      "Epoch [31120/50000], Train Loss: 8088941.5000, Val Loss: 5298551.0000\n",
      "Epoch [31121/50000], Train Loss: 8088941.5000, Val Loss: 5298747.0000\n",
      "Epoch [31122/50000], Train Loss: 8088942.5000, Val Loss: 5298538.5000\n",
      "Epoch [31123/50000], Train Loss: 8088941.5000, Val Loss: 5298740.5000\n",
      "Epoch [31124/50000], Train Loss: 8088942.0000, Val Loss: 5298572.5000\n",
      "Epoch [31125/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [31126/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31127/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [31128/50000], Train Loss: 8088942.0000, Val Loss: 5298704.0000\n",
      "Epoch [31129/50000], Train Loss: 8088942.0000, Val Loss: 5298576.0000\n",
      "Epoch [31130/50000], Train Loss: 8088942.0000, Val Loss: 5298706.5000\n",
      "Epoch [31131/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [31132/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [31133/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [31134/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [31135/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [31136/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [31137/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [31138/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31139/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31140/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [31141/50000], Train Loss: 8088941.5000, Val Loss: 5298611.0000\n",
      "Epoch [31142/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [31143/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [31144/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [31145/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31146/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [31147/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [31148/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [31149/50000], Train Loss: 8088942.5000, Val Loss: 5298659.0000\n",
      "Epoch [31150/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31151/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [31152/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [31153/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [31154/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [31155/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [31156/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31157/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31158/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [31159/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [31160/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [31161/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31162/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31163/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [31164/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31165/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31166/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31167/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [31168/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [31169/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [31170/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [31171/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31172/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31173/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31174/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31175/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31176/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31177/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31178/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31179/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31180/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [31181/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31182/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31183/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31184/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31185/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31186/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31187/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31188/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [31189/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31190/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31191/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31192/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31193/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31194/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31195/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31196/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31197/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31198/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [31199/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31200/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [31201/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31202/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31203/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [31204/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31205/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31206/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31207/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31208/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31209/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31210/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31211/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31212/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31213/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [31214/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31215/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31216/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [31217/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [31218/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31219/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31220/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31221/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [31222/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31223/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31224/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [31225/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31226/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [31227/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [31228/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [31229/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31230/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [31231/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31232/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [31233/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31234/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31235/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31236/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31237/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31238/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31239/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [31240/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [31241/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [31242/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31243/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [31244/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31245/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31246/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [31247/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31248/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31249/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [31250/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31251/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [31252/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31253/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [31254/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31255/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31256/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [31257/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31258/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [31259/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31260/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31261/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31262/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31263/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31264/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31265/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31266/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31267/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [31268/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31269/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31270/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31271/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [31272/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31273/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31274/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31275/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [31276/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [31277/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31278/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [31279/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31280/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31281/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [31282/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31283/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31284/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31285/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31286/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [31287/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [31288/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [31289/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [31290/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [31291/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [31292/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [31293/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [31294/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [31295/50000], Train Loss: 8088941.5000, Val Loss: 5298621.5000\n",
      "Epoch [31296/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [31297/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [31298/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [31299/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [31300/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [31301/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [31302/50000], Train Loss: 8088941.0000, Val Loss: 5298656.0000\n",
      "Epoch [31303/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [31304/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31305/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31306/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31307/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [31308/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [31309/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [31310/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [31311/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [31312/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [31313/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [31314/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [31315/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [31316/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [31317/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [31318/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [31319/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [31320/50000], Train Loss: 8088941.5000, Val Loss: 5298608.0000\n",
      "Epoch [31321/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [31322/50000], Train Loss: 8088942.5000, Val Loss: 5298610.5000\n",
      "Epoch [31323/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [31324/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [31325/50000], Train Loss: 8088941.5000, Val Loss: 5298670.0000\n",
      "Epoch [31326/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [31327/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [31328/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31329/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31330/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31331/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31332/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [31333/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [31334/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [31335/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [31336/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [31337/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31338/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31339/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [31340/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [31341/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31342/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31343/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31344/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31345/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31346/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31347/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31348/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31349/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31350/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31351/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [31352/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31353/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [31354/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31355/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31356/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [31357/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31358/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31359/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [31360/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31361/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [31362/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31363/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [31364/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [31365/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [31366/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [31367/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31368/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [31369/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [31370/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31371/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31372/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [31373/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [31374/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31375/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [31376/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [31377/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31378/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31379/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31380/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31381/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [31382/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [31383/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [31384/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31385/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31386/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [31387/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31388/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31389/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31390/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [31391/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [31392/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31393/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [31394/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [31395/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [31396/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [31397/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [31398/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [31399/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [31400/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [31401/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [31402/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [31403/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [31404/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [31405/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [31406/50000], Train Loss: 8088942.5000, Val Loss: 5298594.0000\n",
      "Epoch [31407/50000], Train Loss: 8088941.5000, Val Loss: 5298706.0000\n",
      "Epoch [31408/50000], Train Loss: 8088942.5000, Val Loss: 5298570.5000\n",
      "Epoch [31409/50000], Train Loss: 8088942.5000, Val Loss: 5298730.0000\n",
      "Epoch [31410/50000], Train Loss: 8088942.0000, Val Loss: 5298548.5000\n",
      "Epoch [31411/50000], Train Loss: 8088942.0000, Val Loss: 5298744.5000\n",
      "Epoch [31412/50000], Train Loss: 8088942.0000, Val Loss: 5298552.0000\n",
      "Epoch [31413/50000], Train Loss: 8088942.0000, Val Loss: 5298718.5000\n",
      "Epoch [31414/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [31415/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [31416/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [31417/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [31418/50000], Train Loss: 8088941.5000, Val Loss: 5298701.5000\n",
      "Epoch [31419/50000], Train Loss: 8088942.0000, Val Loss: 5298584.0000\n",
      "Epoch [31420/50000], Train Loss: 8088942.0000, Val Loss: 5298695.0000\n",
      "Epoch [31421/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [31422/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [31423/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [31424/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [31425/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [31426/50000], Train Loss: 8088942.5000, Val Loss: 5298607.0000\n",
      "Epoch [31427/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [31428/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [31429/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [31430/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [31431/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [31432/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [31433/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [31434/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [31435/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31436/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [31437/50000], Train Loss: 8088941.5000, Val Loss: 5298662.0000\n",
      "Epoch [31438/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [31439/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [31440/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [31441/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [31442/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [31443/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [31444/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [31445/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31446/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31447/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31448/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [31449/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [31450/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [31451/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31452/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [31453/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31454/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31455/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [31456/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [31457/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [31458/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [31459/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31460/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31461/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31462/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31463/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [31464/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31465/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31466/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31467/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31468/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31469/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31470/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31471/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31472/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [31473/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31474/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [31475/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31476/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31477/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [31478/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31479/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [31480/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31481/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31482/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [31483/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31484/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31485/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31486/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31487/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [31488/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31489/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31490/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31491/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [31492/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31493/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31494/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31495/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31496/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31497/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31498/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [31499/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31500/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [31501/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31502/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31503/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [31504/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31505/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31506/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [31507/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31508/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31509/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31510/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [31511/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31512/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31513/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31514/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31515/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31516/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31517/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [31518/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31519/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31520/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31521/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [31522/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31523/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31524/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31525/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [31526/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31527/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31528/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31529/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [31530/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31531/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31532/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31533/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31534/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31535/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31536/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31537/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31538/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [31539/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31540/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31541/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31542/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [31543/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31544/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31545/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31546/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [31547/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31548/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31549/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31550/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [31551/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31552/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31553/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [31554/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31555/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31556/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31557/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31558/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31559/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [31560/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31561/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31562/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31563/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [31564/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31565/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31566/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31567/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31568/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31569/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31570/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31571/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31572/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31573/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31574/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31575/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [31576/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31577/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [31578/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31579/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31580/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31581/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [31582/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31583/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31584/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [31585/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31586/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [31587/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31588/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [31589/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31590/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [31591/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31592/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31593/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31594/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [31595/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31596/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31597/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31598/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31599/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [31600/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31601/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31602/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31603/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [31604/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31605/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [31606/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [31607/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31608/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [31609/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31610/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31611/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31612/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [31613/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [31614/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [31615/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [31616/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [31617/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [31618/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [31619/50000], Train Loss: 8088942.0000, Val Loss: 5298588.5000\n",
      "Epoch [31620/50000], Train Loss: 8088941.5000, Val Loss: 5298713.0000\n",
      "Epoch [31621/50000], Train Loss: 8088942.0000, Val Loss: 5298561.0000\n",
      "Epoch [31622/50000], Train Loss: 8088942.0000, Val Loss: 5298740.5000\n",
      "Epoch [31623/50000], Train Loss: 8088942.0000, Val Loss: 5298537.5000\n",
      "Epoch [31624/50000], Train Loss: 8088942.5000, Val Loss: 5298753.0000\n",
      "Epoch [31625/50000], Train Loss: 8088941.5000, Val Loss: 5298546.0000\n",
      "Epoch [31626/50000], Train Loss: 8088942.0000, Val Loss: 5298717.0000\n",
      "Epoch [31627/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [31628/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [31629/50000], Train Loss: 8088941.0000, Val Loss: 5298683.0000\n",
      "Epoch [31630/50000], Train Loss: 8088941.5000, Val Loss: 5298582.5000\n",
      "Epoch [31631/50000], Train Loss: 8088941.5000, Val Loss: 5298713.0000\n",
      "Epoch [31632/50000], Train Loss: 8088942.0000, Val Loss: 5298584.0000\n",
      "Epoch [31633/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [31634/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [31635/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [31636/50000], Train Loss: 8088942.5000, Val Loss: 5298686.0000\n",
      "Epoch [31637/50000], Train Loss: 8088942.5000, Val Loss: 5298594.0000\n",
      "Epoch [31638/50000], Train Loss: 8088941.5000, Val Loss: 5298689.5000\n",
      "Epoch [31639/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [31640/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [31641/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [31642/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [31643/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [31644/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [31645/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [31646/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31647/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [31648/50000], Train Loss: 8088941.0000, Val Loss: 5298667.5000\n",
      "Epoch [31649/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [31650/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [31651/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [31652/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [31653/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31654/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [31655/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [31656/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [31657/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [31658/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [31659/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [31660/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [31661/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [31662/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [31663/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31664/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [31665/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [31666/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [31667/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31668/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31669/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31670/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31671/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31672/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31673/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [31674/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31675/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31676/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31677/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31678/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31679/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [31680/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31681/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31682/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [31683/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31684/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31685/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31686/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [31687/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [31688/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31689/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31690/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31691/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [31692/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31693/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31694/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31695/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31696/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31697/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31698/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [31699/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [31700/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31701/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31702/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [31703/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31704/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31705/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31706/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31707/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31708/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [31709/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31710/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31711/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [31712/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31713/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31714/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31715/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31716/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31717/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [31718/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31719/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31720/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [31721/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31722/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31723/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31724/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31725/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31726/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [31727/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31728/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31729/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [31730/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31731/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31732/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31733/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31734/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31735/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31736/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31737/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31738/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [31739/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31740/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [31741/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31742/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31743/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [31744/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31745/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [31746/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [31747/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [31748/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31749/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [31750/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [31751/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [31752/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [31753/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [31754/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31755/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [31756/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [31757/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [31758/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [31759/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [31760/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [31761/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [31762/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [31763/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31764/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31765/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [31766/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31767/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [31768/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [31769/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [31770/50000], Train Loss: 8088941.5000, Val Loss: 5298629.5000\n",
      "Epoch [31771/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [31772/50000], Train Loss: 8088941.5000, Val Loss: 5298631.5000\n",
      "Epoch [31773/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [31774/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [31775/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [31776/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31777/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [31778/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [31779/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [31780/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [31781/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [31782/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [31783/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [31784/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [31785/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [31786/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [31787/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [31788/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [31789/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31790/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31791/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [31792/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31793/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [31794/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [31795/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [31796/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [31797/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [31798/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [31799/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [31800/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [31801/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [31802/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [31803/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [31804/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [31805/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [31806/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [31807/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [31808/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [31809/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [31810/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31811/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [31812/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31813/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [31814/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [31815/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [31816/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [31817/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [31818/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [31819/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [31820/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [31821/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [31822/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31823/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [31824/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [31825/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [31826/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [31827/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [31828/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [31829/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [31830/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [31831/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [31832/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [31833/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [31834/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [31835/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [31836/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [31837/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [31838/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [31839/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [31840/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [31841/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [31842/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [31843/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [31844/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [31845/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [31846/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [31847/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [31848/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [31849/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [31850/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [31851/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [31852/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [31853/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [31854/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31855/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [31856/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [31857/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [31858/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [31859/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [31860/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [31861/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [31862/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [31863/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [31864/50000], Train Loss: 8088941.5000, Val Loss: 5298629.5000\n",
      "Epoch [31865/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [31866/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [31867/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [31868/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [31869/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [31870/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [31871/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [31872/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [31873/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [31874/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [31875/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [31876/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [31877/50000], Train Loss: 8088942.5000, Val Loss: 5298680.0000\n",
      "Epoch [31878/50000], Train Loss: 8088942.5000, Val Loss: 5298605.0000\n",
      "Epoch [31879/50000], Train Loss: 8088942.5000, Val Loss: 5298687.0000\n",
      "Epoch [31880/50000], Train Loss: 8088941.5000, Val Loss: 5298599.5000\n",
      "Epoch [31881/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [31882/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [31883/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [31884/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [31885/50000], Train Loss: 8088941.5000, Val Loss: 5298681.5000\n",
      "Epoch [31886/50000], Train Loss: 8088941.5000, Val Loss: 5298615.0000\n",
      "Epoch [31887/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [31888/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [31889/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [31890/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [31891/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [31892/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [31893/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [31894/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [31895/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [31896/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [31897/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [31898/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [31899/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31900/50000], Train Loss: 8088941.0000, Val Loss: 5298641.5000\n",
      "Epoch [31901/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [31902/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [31903/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [31904/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [31905/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [31906/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31907/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [31908/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31909/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31910/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31911/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [31912/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31913/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [31914/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31915/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31916/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [31917/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31918/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31919/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31920/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31921/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31922/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [31923/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [31924/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [31925/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [31926/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31927/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [31928/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31929/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [31930/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31931/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31932/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31933/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [31934/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31935/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [31936/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31937/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [31938/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [31939/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [31940/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [31941/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [31942/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31943/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [31944/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [31945/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [31946/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [31947/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [31948/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [31949/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [31950/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [31951/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [31952/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [31953/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [31954/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [31955/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [31956/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [31957/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [31958/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [31959/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [31960/50000], Train Loss: 8088941.5000, Val Loss: 5298676.5000\n",
      "Epoch [31961/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [31962/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [31963/50000], Train Loss: 8088941.5000, Val Loss: 5298601.5000\n",
      "Epoch [31964/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [31965/50000], Train Loss: 8088941.5000, Val Loss: 5298589.5000\n",
      "Epoch [31966/50000], Train Loss: 8088942.0000, Val Loss: 5298704.5000\n",
      "Epoch [31967/50000], Train Loss: 8088942.0000, Val Loss: 5298581.5000\n",
      "Epoch [31968/50000], Train Loss: 8088942.0000, Val Loss: 5298707.5000\n",
      "Epoch [31969/50000], Train Loss: 8088942.0000, Val Loss: 5298584.5000\n",
      "Epoch [31970/50000], Train Loss: 8088943.0000, Val Loss: 5298697.0000\n",
      "Epoch [31971/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [31972/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [31973/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [31974/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [31975/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [31976/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [31977/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [31978/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [31979/50000], Train Loss: 8088941.5000, Val Loss: 5298671.0000\n",
      "Epoch [31980/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [31981/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [31982/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [31983/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [31984/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [31985/50000], Train Loss: 8088942.5000, Val Loss: 5298634.0000\n",
      "Epoch [31986/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [31987/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [31988/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [31989/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [31990/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [31991/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [31992/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [31993/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [31994/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [31995/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [31996/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [31997/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [31998/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [31999/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [32000/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32001/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32002/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32003/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [32004/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32005/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [32006/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [32007/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [32008/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [32009/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [32010/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [32011/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32012/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32013/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [32014/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32015/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [32016/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [32017/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [32018/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [32019/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [32020/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [32021/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32022/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32023/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32024/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32025/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32026/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [32027/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [32028/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32029/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32030/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [32031/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [32032/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [32033/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32034/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [32035/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [32036/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [32037/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [32038/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [32039/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [32040/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [32041/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [32042/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [32043/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [32044/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [32045/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [32046/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [32047/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32048/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [32049/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [32050/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [32051/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [32052/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32053/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [32054/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32055/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32056/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32057/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32058/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32059/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32060/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32061/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32062/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [32063/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32064/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32065/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [32066/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32067/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [32068/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [32069/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [32070/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32071/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [32072/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [32073/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [32074/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [32075/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32076/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [32077/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [32078/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [32079/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [32080/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [32081/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [32082/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [32083/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32084/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32085/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [32086/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32087/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32088/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [32089/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [32090/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32091/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [32092/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32093/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [32094/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [32095/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [32096/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [32097/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [32098/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [32099/50000], Train Loss: 8088941.5000, Val Loss: 5298675.0000\n",
      "Epoch [32100/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [32101/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [32102/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [32103/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [32104/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32105/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [32106/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [32107/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [32108/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32109/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [32110/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [32111/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [32112/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [32113/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [32114/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [32115/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [32116/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [32117/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [32118/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [32119/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [32120/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [32121/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32122/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [32123/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [32124/50000], Train Loss: 8088942.5000, Val Loss: 5298672.5000\n",
      "Epoch [32125/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [32126/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [32127/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [32128/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [32129/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [32130/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [32131/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [32132/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32133/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [32134/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32135/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32136/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32137/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32138/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [32139/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [32140/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32141/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [32142/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32143/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [32144/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [32145/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [32146/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [32147/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [32148/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [32149/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [32150/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [32151/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [32152/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [32153/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [32154/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [32155/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32156/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32157/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [32158/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [32159/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [32160/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [32161/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32162/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [32163/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32164/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [32165/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [32166/50000], Train Loss: 8088941.5000, Val Loss: 5298630.5000\n",
      "Epoch [32167/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32168/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32169/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [32170/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [32171/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [32172/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [32173/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32174/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32175/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32176/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32177/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32178/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [32179/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [32180/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32181/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [32182/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [32183/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [32184/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [32185/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [32186/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32187/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [32188/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32189/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32190/50000], Train Loss: 8088941.5000, Val Loss: 5298629.5000\n",
      "Epoch [32191/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32192/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32193/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32194/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32195/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32196/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32197/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32198/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [32199/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [32200/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [32201/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32202/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [32203/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [32204/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [32205/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32206/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32207/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [32208/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [32209/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32210/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [32211/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32212/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32213/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [32214/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [32215/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [32216/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [32217/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [32218/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [32219/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [32220/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [32221/50000], Train Loss: 8088941.5000, Val Loss: 5298629.5000\n",
      "Epoch [32222/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32223/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [32224/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [32225/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [32226/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [32227/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [32228/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [32229/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [32230/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [32231/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [32232/50000], Train Loss: 8088941.5000, Val Loss: 5298671.0000\n",
      "Epoch [32233/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [32234/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32235/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32236/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [32237/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [32238/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [32239/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [32240/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [32241/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [32242/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32243/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32244/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32245/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32246/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32247/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32248/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32249/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32250/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [32251/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [32252/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [32253/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32254/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32255/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [32256/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32257/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [32258/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32259/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [32260/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [32261/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32262/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [32263/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32264/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32265/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32266/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32267/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32268/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [32269/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [32270/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [32271/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32272/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32273/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [32274/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [32275/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [32276/50000], Train Loss: 8088941.5000, Val Loss: 5298617.0000\n",
      "Epoch [32277/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [32278/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [32279/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [32280/50000], Train Loss: 8088941.5000, Val Loss: 5298601.5000\n",
      "Epoch [32281/50000], Train Loss: 8088941.5000, Val Loss: 5298693.5000\n",
      "Epoch [32282/50000], Train Loss: 8088941.5000, Val Loss: 5298588.5000\n",
      "Epoch [32283/50000], Train Loss: 8088941.5000, Val Loss: 5298706.0000\n",
      "Epoch [32284/50000], Train Loss: 8088942.0000, Val Loss: 5298579.0000\n",
      "Epoch [32285/50000], Train Loss: 8088942.0000, Val Loss: 5298710.0000\n",
      "Epoch [32286/50000], Train Loss: 8088942.0000, Val Loss: 5298581.5000\n",
      "Epoch [32287/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [32288/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [32289/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [32290/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [32291/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [32292/50000], Train Loss: 8088941.0000, Val Loss: 5298663.0000\n",
      "Epoch [32293/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32294/50000], Train Loss: 8088941.5000, Val Loss: 5298679.0000\n",
      "Epoch [32295/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [32296/50000], Train Loss: 8088942.0000, Val Loss: 5298685.0000\n",
      "Epoch [32297/50000], Train Loss: 8088941.5000, Val Loss: 5298607.0000\n",
      "Epoch [32298/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32299/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [32300/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [32301/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [32302/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32303/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32304/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [32305/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [32306/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [32307/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32308/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [32309/50000], Train Loss: 8088942.5000, Val Loss: 5298645.5000\n",
      "Epoch [32310/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32311/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [32312/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [32313/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32314/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [32315/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [32316/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [32317/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [32318/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32319/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32320/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [32321/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32322/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32323/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [32324/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [32325/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32326/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32327/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [32328/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [32329/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [32330/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [32331/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [32332/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [32333/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [32334/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [32335/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [32336/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32337/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [32338/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [32339/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [32340/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [32341/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [32342/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [32343/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [32344/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [32345/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [32346/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32347/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [32348/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32349/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [32350/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [32351/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32352/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [32353/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [32354/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32355/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [32356/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [32357/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [32358/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32359/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [32360/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [32361/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [32362/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [32363/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [32364/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [32365/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [32366/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32367/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32368/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [32369/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [32370/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32371/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32372/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32373/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32374/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32375/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [32376/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [32377/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [32378/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [32379/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32380/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [32381/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [32382/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32383/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32384/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32385/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32386/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [32387/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [32388/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32389/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [32390/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [32391/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32392/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [32393/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [32394/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [32395/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [32396/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [32397/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32398/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32399/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [32400/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32401/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32402/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32403/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32404/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32405/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [32406/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [32407/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [32408/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [32409/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [32410/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32411/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32412/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [32413/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [32414/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [32415/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [32416/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [32417/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [32418/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32419/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32420/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32421/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32422/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [32423/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [32424/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [32425/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [32426/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [32427/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32428/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [32429/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32430/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32431/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [32432/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [32433/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32434/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [32435/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32436/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [32437/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [32438/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [32439/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [32440/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [32441/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [32442/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [32443/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [32444/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [32445/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [32446/50000], Train Loss: 8088941.5000, Val Loss: 5298667.5000\n",
      "Epoch [32447/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [32448/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32449/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32450/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [32451/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [32452/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [32453/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [32454/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [32455/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [32456/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32457/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [32458/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [32459/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32460/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [32461/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32462/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32463/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32464/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32465/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32466/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [32467/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [32468/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [32469/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [32470/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32471/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [32472/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [32473/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [32474/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [32475/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [32476/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32477/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [32478/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32479/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32480/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32481/50000], Train Loss: 8088941.5000, Val Loss: 5298631.5000\n",
      "Epoch [32482/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [32483/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [32484/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32485/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [32486/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [32487/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [32488/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32489/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [32490/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [32491/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32492/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [32493/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [32494/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32495/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [32496/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [32497/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [32498/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [32499/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [32500/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [32501/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [32502/50000], Train Loss: 8088942.5000, Val Loss: 5298676.5000\n",
      "Epoch [32503/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [32504/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32505/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [32506/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32507/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [32508/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [32509/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [32510/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [32511/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32512/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [32513/50000], Train Loss: 8088942.5000, Val Loss: 5298626.5000\n",
      "Epoch [32514/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32515/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32516/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [32517/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [32518/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32519/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [32520/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32521/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [32522/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [32523/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [32524/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [32525/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [32526/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [32527/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [32528/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32529/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [32530/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32531/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [32532/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [32533/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32534/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [32535/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32536/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [32537/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [32538/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [32539/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [32540/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [32541/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [32542/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32543/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [32544/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32545/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [32546/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32547/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [32548/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32549/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [32550/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32551/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32552/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [32553/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [32554/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32555/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32556/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32557/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [32558/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [32559/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [32560/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [32561/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [32562/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [32563/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32564/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [32565/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [32566/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [32567/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32568/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32569/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [32570/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [32571/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32572/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [32573/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [32574/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [32575/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32576/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32577/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32578/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [32579/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32580/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [32581/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [32582/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32583/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [32584/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [32585/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [32586/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [32587/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [32588/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [32589/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [32590/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [32591/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [32592/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32593/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [32594/50000], Train Loss: 8088941.5000, Val Loss: 5298629.5000\n",
      "Epoch [32595/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32596/50000], Train Loss: 8088941.5000, Val Loss: 5298630.5000\n",
      "Epoch [32597/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32598/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32599/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [32600/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32601/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [32602/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [32603/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [32604/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32605/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32606/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32607/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32608/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [32609/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [32610/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [32611/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [32612/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [32613/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [32614/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [32615/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [32616/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [32617/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [32618/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [32619/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [32620/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [32621/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [32622/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [32623/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [32624/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32625/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32626/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32627/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32628/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32629/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32630/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32631/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [32632/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [32633/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [32634/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [32635/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [32636/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32637/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32638/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [32639/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32640/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32641/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32642/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32643/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32644/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [32645/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [32646/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [32647/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [32648/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [32649/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32650/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32651/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [32652/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [32653/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32654/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [32655/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [32656/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32657/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32658/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [32659/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [32660/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [32661/50000], Train Loss: 8088941.5000, Val Loss: 5298676.5000\n",
      "Epoch [32662/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [32663/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [32664/50000], Train Loss: 8088941.5000, Val Loss: 5298608.0000\n",
      "Epoch [32665/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [32666/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [32667/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [32668/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [32669/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [32670/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32671/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [32672/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32673/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [32674/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [32675/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32676/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [32677/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [32678/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [32679/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [32680/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [32681/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32682/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [32683/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32684/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [32685/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [32686/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [32687/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [32688/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32689/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [32690/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32691/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32692/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [32693/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [32694/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32695/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [32696/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [32697/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [32698/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [32699/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32700/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [32701/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32702/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [32703/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [32704/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [32705/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [32706/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [32707/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [32708/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [32709/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [32710/50000], Train Loss: 8088941.5000, Val Loss: 5298671.0000\n",
      "Epoch [32711/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [32712/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32713/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [32714/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [32715/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [32716/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [32717/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [32718/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [32719/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [32720/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32721/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32722/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [32723/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32724/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [32725/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [32726/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [32727/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [32728/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32729/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [32730/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [32731/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [32732/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [32733/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [32734/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [32735/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [32736/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [32737/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32738/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [32739/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32740/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32741/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32742/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32743/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [32744/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32745/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [32746/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [32747/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32748/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [32749/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32750/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32751/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32752/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [32753/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [32754/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32755/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32756/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32757/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32758/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32759/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32760/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32761/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32762/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32763/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [32764/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32765/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [32766/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [32767/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32768/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32769/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [32770/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32771/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [32772/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [32773/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [32774/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [32775/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32776/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [32777/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [32778/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [32779/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [32780/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [32781/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32782/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [32783/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32784/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [32785/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32786/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32787/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [32788/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [32789/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [32790/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [32791/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32792/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [32793/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [32794/50000], Train Loss: 8088941.5000, Val Loss: 5298630.5000\n",
      "Epoch [32795/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32796/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32797/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32798/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [32799/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32800/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [32801/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32802/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [32803/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32804/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [32805/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32806/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [32807/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32808/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [32809/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32810/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32811/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32812/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32813/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32814/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32815/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32816/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32817/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32818/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [32819/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32820/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32821/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [32822/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [32823/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [32824/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [32825/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [32826/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [32827/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [32828/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [32829/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32830/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32831/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32832/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32833/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [32834/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32835/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [32836/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [32837/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32838/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32839/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [32840/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [32841/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [32842/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [32843/50000], Train Loss: 8088942.5000, Val Loss: 5298672.5000\n",
      "Epoch [32844/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [32845/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [32846/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [32847/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [32848/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [32849/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [32850/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [32851/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32852/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32853/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32854/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32855/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [32856/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32857/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [32858/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [32859/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32860/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [32861/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32862/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32863/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32864/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [32865/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [32866/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32867/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32868/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32869/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32870/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32871/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [32872/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32873/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [32874/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32875/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [32876/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32877/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [32878/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [32879/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [32880/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32881/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [32882/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32883/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [32884/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [32885/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [32886/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [32887/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32888/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [32889/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [32890/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [32891/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32892/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [32893/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [32894/50000], Train Loss: 8088942.5000, Val Loss: 5298626.5000\n",
      "Epoch [32895/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [32896/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [32897/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [32898/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [32899/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [32900/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [32901/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [32902/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [32903/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32904/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [32905/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [32906/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [32907/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [32908/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [32909/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [32910/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [32911/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [32912/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [32913/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32914/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [32915/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [32916/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [32917/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [32918/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [32919/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [32920/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [32921/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [32922/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [32923/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [32924/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [32925/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [32926/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [32927/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [32928/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [32929/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [32930/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [32931/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [32932/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [32933/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [32934/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [32935/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [32936/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [32937/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [32938/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [32939/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [32940/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [32941/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32942/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32943/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [32944/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [32945/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [32946/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [32947/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32948/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [32949/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32950/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [32951/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32952/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32953/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [32954/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [32955/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32956/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [32957/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [32958/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [32959/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [32960/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [32961/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [32962/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32963/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32964/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32965/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [32966/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [32967/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32968/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [32969/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [32970/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [32971/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [32972/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [32973/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [32974/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32975/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [32976/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [32977/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [32978/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [32979/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [32980/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [32981/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [32982/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [32983/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32984/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [32985/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [32986/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [32987/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [32988/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [32989/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [32990/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [32991/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [32992/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [32993/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [32994/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [32995/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [32996/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [32997/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [32998/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [32999/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [33000/50000], Train Loss: 8088941.5000, Val Loss: 5298669.5000\n",
      "Epoch [33001/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [33002/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [33003/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33004/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [33005/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [33006/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [33007/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33008/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [33009/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33010/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33011/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [33012/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33013/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33014/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33015/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33016/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [33017/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [33018/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33019/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [33020/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [33021/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [33022/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [33023/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [33024/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [33025/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [33026/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [33027/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [33028/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [33029/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [33030/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [33031/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [33032/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [33033/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [33034/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33035/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [33036/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [33037/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [33038/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33039/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [33040/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [33041/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [33042/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33043/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [33044/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [33045/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [33046/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [33047/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [33048/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [33049/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [33050/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [33051/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33052/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33053/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [33054/50000], Train Loss: 8088941.5000, Val Loss: 5298631.5000\n",
      "Epoch [33055/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [33056/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [33057/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [33058/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [33059/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [33060/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33061/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33062/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [33063/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [33064/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [33065/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [33066/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [33067/50000], Train Loss: 8088941.5000, Val Loss: 5298675.0000\n",
      "Epoch [33068/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [33069/50000], Train Loss: 8088942.5000, Val Loss: 5298676.5000\n",
      "Epoch [33070/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [33071/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [33072/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [33073/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [33074/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33075/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [33076/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [33077/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33078/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33079/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33080/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33081/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [33082/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [33083/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [33084/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33085/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [33086/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [33087/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33088/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33089/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [33090/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [33091/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [33092/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33093/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [33094/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [33095/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [33096/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [33097/50000], Train Loss: 8088942.5000, Val Loss: 5298604.5000\n",
      "Epoch [33098/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [33099/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [33100/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [33101/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [33102/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [33103/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [33104/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [33105/50000], Train Loss: 8088941.5000, Val Loss: 5298611.0000\n",
      "Epoch [33106/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [33107/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [33108/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33109/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [33110/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [33111/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [33112/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [33113/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [33114/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [33115/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [33116/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [33117/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [33118/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [33119/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [33120/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [33121/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33122/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33123/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [33124/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33125/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33126/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33127/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33128/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [33129/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33130/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [33131/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33132/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33133/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33134/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33135/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [33136/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33137/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [33138/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33139/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33140/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [33141/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33142/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33143/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33144/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [33145/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [33146/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [33147/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [33148/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [33149/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [33150/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [33151/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [33152/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [33153/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [33154/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [33155/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [33156/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [33157/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [33158/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33159/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [33160/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [33161/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [33162/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33163/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33164/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [33165/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [33166/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [33167/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [33168/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [33169/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [33170/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [33171/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [33172/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [33173/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [33174/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [33175/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [33176/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [33177/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [33178/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [33179/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33180/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [33181/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33182/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [33183/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33184/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [33185/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33186/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33187/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [33188/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [33189/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [33190/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33191/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33192/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [33193/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [33194/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33195/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33196/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33197/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33198/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33199/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [33200/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33201/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33202/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33203/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [33204/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [33205/50000], Train Loss: 8088942.5000, Val Loss: 5298672.5000\n",
      "Epoch [33206/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [33207/50000], Train Loss: 8088942.5000, Val Loss: 5298688.5000\n",
      "Epoch [33208/50000], Train Loss: 8088942.0000, Val Loss: 5298592.0000\n",
      "Epoch [33209/50000], Train Loss: 8088942.0000, Val Loss: 5298704.0000\n",
      "Epoch [33210/50000], Train Loss: 8088942.0000, Val Loss: 5298580.5000\n",
      "Epoch [33211/50000], Train Loss: 8088942.5000, Val Loss: 5298710.0000\n",
      "Epoch [33212/50000], Train Loss: 8088942.0000, Val Loss: 5298580.5000\n",
      "Epoch [33213/50000], Train Loss: 8088942.5000, Val Loss: 5298702.0000\n",
      "Epoch [33214/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [33215/50000], Train Loss: 8088942.5000, Val Loss: 5298680.0000\n",
      "Epoch [33216/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [33217/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [33218/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [33219/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [33220/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [33221/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [33222/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [33223/50000], Train Loss: 8088941.5000, Val Loss: 5298608.5000\n",
      "Epoch [33224/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [33225/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [33226/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [33227/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33228/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33229/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33230/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [33231/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [33232/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [33233/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [33234/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [33235/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [33236/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [33237/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33238/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [33239/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [33240/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [33241/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33242/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33243/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33244/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33245/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33246/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [33247/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33248/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33249/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33250/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33251/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [33252/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33253/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33254/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33255/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [33256/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33257/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [33258/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [33259/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33260/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [33261/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33262/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [33263/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33264/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33265/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33266/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [33267/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33268/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33269/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33270/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [33271/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33272/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [33273/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33274/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33275/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [33276/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33277/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33278/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33279/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [33280/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33281/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33282/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33283/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33284/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33285/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33286/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33287/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [33288/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33289/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33290/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33291/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [33292/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33293/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [33294/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33295/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33296/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33297/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33298/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33299/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33300/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [33301/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33302/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33303/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [33304/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [33305/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [33306/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33307/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33308/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33309/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [33310/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33311/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33312/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [33313/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [33314/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [33315/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [33316/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [33317/50000], Train Loss: 8088942.5000, Val Loss: 5298602.0000\n",
      "Epoch [33318/50000], Train Loss: 8088941.5000, Val Loss: 5298694.5000\n",
      "Epoch [33319/50000], Train Loss: 8088942.5000, Val Loss: 5298586.5000\n",
      "Epoch [33320/50000], Train Loss: 8088941.5000, Val Loss: 5298711.0000\n",
      "Epoch [33321/50000], Train Loss: 8088941.5000, Val Loss: 5298570.5000\n",
      "Epoch [33322/50000], Train Loss: 8088942.5000, Val Loss: 5298722.5000\n",
      "Epoch [33323/50000], Train Loss: 8088942.0000, Val Loss: 5298565.5000\n",
      "Epoch [33324/50000], Train Loss: 8088942.0000, Val Loss: 5298718.0000\n",
      "Epoch [33325/50000], Train Loss: 8088942.0000, Val Loss: 5298584.0000\n",
      "Epoch [33326/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [33327/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33328/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [33329/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [33330/50000], Train Loss: 8088942.0000, Val Loss: 5298603.0000\n",
      "Epoch [33331/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [33332/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [33333/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [33334/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [33335/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [33336/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33337/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33338/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [33339/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [33340/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [33341/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [33342/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [33343/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [33344/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33345/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [33346/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33347/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [33348/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33349/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [33350/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [33351/50000], Train Loss: 8088941.0000, Val Loss: 5298641.5000\n",
      "Epoch [33352/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33353/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [33354/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [33355/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33356/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [33357/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33358/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33359/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [33360/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [33361/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33362/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33363/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [33364/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [33365/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33366/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33367/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33368/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33369/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [33370/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33371/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [33372/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33373/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [33374/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33375/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [33376/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [33377/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33378/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33379/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [33380/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [33381/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33382/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33383/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [33384/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33385/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33386/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33387/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [33388/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [33389/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [33390/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [33391/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33392/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [33393/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33394/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [33395/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [33396/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33397/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33398/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33399/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [33400/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33401/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [33402/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33403/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [33404/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33405/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [33406/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [33407/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [33408/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33409/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33410/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33411/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33412/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33413/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [33414/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33415/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [33416/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33417/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33418/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [33419/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33420/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33421/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33422/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33423/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [33424/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33425/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33426/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33427/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [33428/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33429/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [33430/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33431/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33432/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [33433/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33434/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [33435/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [33436/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33437/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33438/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33439/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [33440/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33441/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [33442/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33443/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33444/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33445/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [33446/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [33447/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33448/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [33449/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33450/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33451/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [33452/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [33453/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [33454/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [33455/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [33456/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [33457/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [33458/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [33459/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [33460/50000], Train Loss: 8088942.0000, Val Loss: 5298695.0000\n",
      "Epoch [33461/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [33462/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [33463/50000], Train Loss: 8088942.5000, Val Loss: 5298599.5000\n",
      "Epoch [33464/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [33465/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [33466/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [33467/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [33468/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33469/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [33470/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33471/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [33472/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [33473/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [33474/50000], Train Loss: 8088941.5000, Val Loss: 5298619.5000\n",
      "Epoch [33475/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [33476/50000], Train Loss: 8088941.5000, Val Loss: 5298621.5000\n",
      "Epoch [33477/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [33478/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33479/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [33480/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [33481/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [33482/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [33483/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33484/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33485/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33486/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33487/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [33488/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33489/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33490/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [33491/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [33492/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [33493/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33494/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [33495/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33496/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [33497/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33498/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [33499/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [33500/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33501/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [33502/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33503/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [33504/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33505/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [33506/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33507/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33508/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [33509/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33510/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33511/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [33512/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33513/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33514/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33515/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33516/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [33517/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33518/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [33519/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [33520/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [33521/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33522/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33523/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33524/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [33525/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [33526/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33527/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [33528/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33529/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [33530/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [33531/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33532/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [33533/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [33534/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33535/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33536/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33537/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [33538/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33539/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33540/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33541/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33542/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33543/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33544/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33545/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33546/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33547/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33548/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33549/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33550/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33551/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33552/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33553/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [33554/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33555/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [33556/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [33557/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33558/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33559/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33560/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [33561/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [33562/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33563/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [33564/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33565/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [33566/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [33567/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [33568/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [33569/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [33570/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33571/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [33572/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [33573/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [33574/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [33575/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [33576/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [33577/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [33578/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [33579/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [33580/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [33581/50000], Train Loss: 8088942.5000, Val Loss: 5298595.0000\n",
      "Epoch [33582/50000], Train Loss: 8088942.0000, Val Loss: 5298704.5000\n",
      "Epoch [33583/50000], Train Loss: 8088942.0000, Val Loss: 5298572.5000\n",
      "Epoch [33584/50000], Train Loss: 8088942.0000, Val Loss: 5298729.0000\n",
      "Epoch [33585/50000], Train Loss: 8088942.5000, Val Loss: 5298549.0000\n",
      "Epoch [33586/50000], Train Loss: 8088942.0000, Val Loss: 5298745.0000\n",
      "Epoch [33587/50000], Train Loss: 8088942.0000, Val Loss: 5298547.0000\n",
      "Epoch [33588/50000], Train Loss: 8088942.0000, Val Loss: 5298726.0000\n",
      "Epoch [33589/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [33590/50000], Train Loss: 8088941.0000, Val Loss: 5298669.0000\n",
      "Epoch [33591/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [33592/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [33593/50000], Train Loss: 8088942.0000, Val Loss: 5298698.0000\n",
      "Epoch [33594/50000], Train Loss: 8088942.5000, Val Loss: 5298585.0000\n",
      "Epoch [33595/50000], Train Loss: 8088942.0000, Val Loss: 5298695.5000\n",
      "Epoch [33596/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [33597/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [33598/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [33599/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [33600/50000], Train Loss: 8088942.5000, Val Loss: 5298691.5000\n",
      "Epoch [33601/50000], Train Loss: 8088942.0000, Val Loss: 5298595.5000\n",
      "Epoch [33602/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [33603/50000], Train Loss: 8088941.5000, Val Loss: 5298624.5000\n",
      "Epoch [33604/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33605/50000], Train Loss: 8088941.5000, Val Loss: 5298664.0000\n",
      "Epoch [33606/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [33607/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [33608/50000], Train Loss: 8088941.5000, Val Loss: 5298612.5000\n",
      "Epoch [33609/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [33610/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [33611/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [33612/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [33613/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [33614/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [33615/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [33616/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33617/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [33618/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [33619/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [33620/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [33621/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [33622/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33623/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [33624/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [33625/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33626/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33627/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33628/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33629/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33630/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33631/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [33632/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33633/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [33634/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33635/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33636/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33637/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33638/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33639/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33640/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33641/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33642/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [33643/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33644/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [33645/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33646/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33647/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [33648/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33649/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [33650/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33651/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33652/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33653/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33654/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33655/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33656/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [33657/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [33658/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33659/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33660/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33661/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [33662/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [33663/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33664/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33665/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33666/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33667/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33668/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33669/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33670/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [33671/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [33672/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [33673/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33674/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [33675/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33676/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33677/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33678/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [33679/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33680/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [33681/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [33682/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [33683/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33684/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33685/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [33686/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [33687/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33688/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [33689/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33690/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33691/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [33692/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [33693/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [33694/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33695/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33696/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [33697/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33698/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [33699/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33700/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33701/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33702/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [33703/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [33704/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33705/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33706/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33707/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [33708/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33709/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33710/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33711/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33712/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [33713/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [33714/50000], Train Loss: 8088941.0000, Val Loss: 5298655.0000\n",
      "Epoch [33715/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [33716/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33717/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33718/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [33719/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33720/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [33721/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33722/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33723/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [33724/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33725/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33726/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33727/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33728/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33729/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33730/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33731/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33732/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33733/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33734/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33735/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [33736/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33737/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33738/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [33739/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [33740/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33741/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [33742/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33743/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33744/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33745/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [33746/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [33747/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [33748/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33749/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [33750/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [33751/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33752/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [33753/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33754/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [33755/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [33756/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [33757/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [33758/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33759/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33760/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [33761/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33762/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [33763/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [33764/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [33765/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [33766/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33767/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [33768/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33769/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [33770/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33771/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33772/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33773/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [33774/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33775/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33776/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [33777/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33778/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [33779/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33780/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [33781/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33782/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [33783/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [33784/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [33785/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [33786/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [33787/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [33788/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [33789/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [33790/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [33791/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [33792/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [33793/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [33794/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [33795/50000], Train Loss: 8088941.5000, Val Loss: 5298594.0000\n",
      "Epoch [33796/50000], Train Loss: 8088941.5000, Val Loss: 5298695.5000\n",
      "Epoch [33797/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [33798/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [33799/50000], Train Loss: 8088941.5000, Val Loss: 5298603.0000\n",
      "Epoch [33800/50000], Train Loss: 8088941.5000, Val Loss: 5298677.0000\n",
      "Epoch [33801/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [33802/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [33803/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33804/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33805/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33806/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [33807/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [33808/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [33809/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [33810/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [33811/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [33812/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33813/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [33814/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [33815/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [33816/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [33817/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [33818/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [33819/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [33820/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33821/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33822/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [33823/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [33824/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33825/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [33826/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33827/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33828/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33829/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [33830/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [33831/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [33832/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33833/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [33834/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [33835/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [33836/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [33837/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [33838/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33839/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33840/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [33841/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33842/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [33843/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33844/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33845/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33846/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [33847/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33848/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [33849/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [33850/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [33851/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33852/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33853/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [33854/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33855/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [33856/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [33857/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33858/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33859/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33860/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [33861/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33862/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [33863/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [33864/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33865/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [33866/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33867/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [33868/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33869/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33870/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [33871/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33872/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [33873/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [33874/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [33875/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [33876/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [33877/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [33878/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [33879/50000], Train Loss: 8088942.0000, Val Loss: 5298595.5000\n",
      "Epoch [33880/50000], Train Loss: 8088942.0000, Val Loss: 5298697.0000\n",
      "Epoch [33881/50000], Train Loss: 8088942.0000, Val Loss: 5298589.5000\n",
      "Epoch [33882/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [33883/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [33884/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [33885/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [33886/50000], Train Loss: 8088941.5000, Val Loss: 5298680.0000\n",
      "Epoch [33887/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [33888/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [33889/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33890/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33891/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [33892/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [33893/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [33894/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [33895/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [33896/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [33897/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [33898/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [33899/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [33900/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [33901/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [33902/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33903/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [33904/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [33905/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [33906/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [33907/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33908/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [33909/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33910/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [33911/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [33912/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [33913/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [33914/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [33915/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [33916/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33917/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [33918/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33919/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33920/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33921/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33922/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33923/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33924/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33925/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33926/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [33927/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33928/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33929/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33930/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33931/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33932/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33933/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33934/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33935/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [33936/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33937/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [33938/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [33939/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [33940/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [33941/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [33942/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [33943/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [33944/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [33945/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [33946/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33947/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [33948/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [33949/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33950/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33951/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [33952/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33953/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33954/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [33955/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33956/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [33957/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33958/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [33959/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [33960/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [33961/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [33962/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [33963/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [33964/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [33965/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [33966/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [33967/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [33968/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [33969/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [33970/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [33971/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [33972/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [33973/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [33974/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [33975/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [33976/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [33977/50000], Train Loss: 8088942.5000, Val Loss: 5298698.5000\n",
      "Epoch [33978/50000], Train Loss: 8088942.5000, Val Loss: 5298585.0000\n",
      "Epoch [33979/50000], Train Loss: 8088942.0000, Val Loss: 5298707.0000\n",
      "Epoch [33980/50000], Train Loss: 8088942.0000, Val Loss: 5298579.5000\n",
      "Epoch [33981/50000], Train Loss: 8088941.5000, Val Loss: 5298707.0000\n",
      "Epoch [33982/50000], Train Loss: 8088942.0000, Val Loss: 5298589.0000\n",
      "Epoch [33983/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [33984/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [33985/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [33986/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [33987/50000], Train Loss: 8088941.0000, Val Loss: 5298634.5000\n",
      "Epoch [33988/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [33989/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [33990/50000], Train Loss: 8088941.5000, Val Loss: 5298677.0000\n",
      "Epoch [33991/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [33992/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [33993/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [33994/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [33995/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [33996/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [33997/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [33998/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [33999/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [34000/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [34001/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [34002/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [34003/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [34004/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [34005/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34006/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [34007/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [34008/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34009/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [34010/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [34011/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [34012/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34013/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [34014/50000], Train Loss: 8088941.0000, Val Loss: 5298644.5000\n",
      "Epoch [34015/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [34016/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [34017/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [34018/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [34019/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [34020/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [34021/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [34022/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34023/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [34024/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34025/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34026/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [34027/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34028/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34029/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34030/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [34031/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34032/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [34033/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34034/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34035/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34036/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34037/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34038/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [34039/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [34040/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34041/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34042/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34043/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34044/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [34045/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34046/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34047/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34048/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [34049/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34050/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34051/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [34052/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34053/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34054/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34055/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [34056/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [34057/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34058/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34059/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [34060/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [34061/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [34062/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [34063/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [34064/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [34065/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [34066/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [34067/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [34068/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34069/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [34070/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34071/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34072/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34073/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [34074/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34075/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34076/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [34077/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34078/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [34079/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34080/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [34081/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [34082/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [34083/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [34084/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [34085/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [34086/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [34087/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [34088/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [34089/50000], Train Loss: 8088942.5000, Val Loss: 5298598.0000\n",
      "Epoch [34090/50000], Train Loss: 8088941.5000, Val Loss: 5298698.0000\n",
      "Epoch [34091/50000], Train Loss: 8088942.5000, Val Loss: 5298584.0000\n",
      "Epoch [34092/50000], Train Loss: 8088942.0000, Val Loss: 5298707.5000\n",
      "Epoch [34093/50000], Train Loss: 8088942.0000, Val Loss: 5298580.5000\n",
      "Epoch [34094/50000], Train Loss: 8088942.5000, Val Loss: 5298705.0000\n",
      "Epoch [34095/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [34096/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [34097/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [34098/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [34099/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34100/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34101/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [34102/50000], Train Loss: 8088941.5000, Val Loss: 5298624.5000\n",
      "Epoch [34103/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [34104/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [34105/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [34106/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [34107/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [34108/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [34109/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34110/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34111/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [34112/50000], Train Loss: 8088941.0000, Val Loss: 5298663.0000\n",
      "Epoch [34113/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [34114/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [34115/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [34116/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [34117/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [34118/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34119/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34120/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34121/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34122/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34123/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34124/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34125/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34126/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34127/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [34128/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34129/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34130/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34131/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34132/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34133/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34134/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34135/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34136/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [34137/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34138/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34139/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34140/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34141/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34142/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34143/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34144/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34145/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [34146/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34147/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34148/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34149/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34150/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34151/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34152/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34153/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34154/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [34155/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34156/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34157/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [34158/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [34159/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [34160/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [34161/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [34162/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [34163/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [34164/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [34165/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [34166/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [34167/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34168/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34169/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34170/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [34171/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [34172/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34173/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [34174/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34175/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34176/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [34177/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34178/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34179/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34180/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [34181/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34182/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34183/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34184/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [34185/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34186/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34187/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [34188/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34189/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34190/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [34191/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [34192/50000], Train Loss: 8088941.5000, Val Loss: 5298626.5000\n",
      "Epoch [34193/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [34194/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [34195/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [34196/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [34197/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [34198/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [34199/50000], Train Loss: 8088942.0000, Val Loss: 5298706.0000\n",
      "Epoch [34200/50000], Train Loss: 8088942.0000, Val Loss: 5298575.0000\n",
      "Epoch [34201/50000], Train Loss: 8088942.5000, Val Loss: 5298720.5000\n",
      "Epoch [34202/50000], Train Loss: 8088942.0000, Val Loss: 5298562.5000\n",
      "Epoch [34203/50000], Train Loss: 8088942.5000, Val Loss: 5298727.5000\n",
      "Epoch [34204/50000], Train Loss: 8088942.0000, Val Loss: 5298567.5000\n",
      "Epoch [34205/50000], Train Loss: 8088941.5000, Val Loss: 5298707.0000\n",
      "Epoch [34206/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [34207/50000], Train Loss: 8088941.5000, Val Loss: 5298663.0000\n",
      "Epoch [34208/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34209/50000], Train Loss: 8088941.5000, Val Loss: 5298617.5000\n",
      "Epoch [34210/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [34211/50000], Train Loss: 8088942.0000, Val Loss: 5298595.0000\n",
      "Epoch [34212/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [34213/50000], Train Loss: 8088941.5000, Val Loss: 5298603.0000\n",
      "Epoch [34214/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [34215/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34216/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [34217/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [34218/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [34219/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [34220/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [34221/50000], Train Loss: 8088941.5000, Val Loss: 5298664.5000\n",
      "Epoch [34222/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [34223/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34224/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [34225/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [34226/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [34227/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [34228/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [34229/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [34230/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34231/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [34232/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [34233/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [34234/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [34235/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [34236/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34237/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34238/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [34239/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [34240/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [34241/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [34242/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [34243/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34244/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34245/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [34246/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [34247/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [34248/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34249/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34250/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34251/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34252/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34253/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34254/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34255/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [34256/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34257/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34258/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34259/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34260/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [34261/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [34262/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [34263/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34264/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34265/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34266/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34267/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34268/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34269/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34270/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34271/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34272/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [34273/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34274/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [34275/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34276/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [34277/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34278/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34279/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34280/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [34281/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34282/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34283/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34284/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [34285/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34286/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34287/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [34288/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34289/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [34290/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34291/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34292/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34293/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34294/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34295/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [34296/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34297/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [34298/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34299/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [34300/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34301/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34302/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [34303/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34304/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34305/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34306/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34307/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34308/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34309/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34310/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34311/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [34312/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34313/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34314/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34315/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [34316/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34317/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34318/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34319/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34320/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34321/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [34322/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34323/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34324/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [34325/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34326/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34327/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34328/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34329/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34330/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34331/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34332/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34333/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34334/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [34335/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [34336/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [34337/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [34338/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [34339/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [34340/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [34341/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [34342/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [34343/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [34344/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [34345/50000], Train Loss: 8088942.5000, Val Loss: 5298633.5000\n",
      "Epoch [34346/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34347/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34348/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34349/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [34350/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34351/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34352/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34353/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [34354/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34355/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [34356/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34357/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34358/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [34359/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34360/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34361/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34362/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34363/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [34364/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34365/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34366/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34367/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [34368/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34369/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [34370/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34371/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34372/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34373/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34374/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [34375/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34376/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34377/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34378/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34379/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [34380/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [34381/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [34382/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [34383/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [34384/50000], Train Loss: 8088942.5000, Val Loss: 5298602.0000\n",
      "Epoch [34385/50000], Train Loss: 8088941.5000, Val Loss: 5298697.0000\n",
      "Epoch [34386/50000], Train Loss: 8088942.0000, Val Loss: 5298581.0000\n",
      "Epoch [34387/50000], Train Loss: 8088942.5000, Val Loss: 5298720.0000\n",
      "Epoch [34388/50000], Train Loss: 8088942.0000, Val Loss: 5298557.0000\n",
      "Epoch [34389/50000], Train Loss: 8088942.0000, Val Loss: 5298742.0000\n",
      "Epoch [34390/50000], Train Loss: 8088941.5000, Val Loss: 5298543.5000\n",
      "Epoch [34391/50000], Train Loss: 8088942.0000, Val Loss: 5298737.5000\n",
      "Epoch [34392/50000], Train Loss: 8088942.5000, Val Loss: 5298570.5000\n",
      "Epoch [34393/50000], Train Loss: 8088942.5000, Val Loss: 5298689.5000\n",
      "Epoch [34394/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [34395/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [34396/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [34397/50000], Train Loss: 8088941.5000, Val Loss: 5298584.0000\n",
      "Epoch [34398/50000], Train Loss: 8088942.0000, Val Loss: 5298704.5000\n",
      "Epoch [34399/50000], Train Loss: 8088942.5000, Val Loss: 5298595.0000\n",
      "Epoch [34400/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [34401/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [34402/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [34403/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [34404/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [34405/50000], Train Loss: 8088941.5000, Val Loss: 5298680.0000\n",
      "Epoch [34406/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [34407/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [34408/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34409/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [34410/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [34411/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [34412/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [34413/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [34414/50000], Train Loss: 8088941.0000, Val Loss: 5298644.5000\n",
      "Epoch [34415/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [34416/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [34417/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [34418/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [34419/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [34420/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34421/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34422/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [34423/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [34424/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [34425/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [34426/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [34427/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [34428/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34429/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34430/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [34431/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [34432/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [34433/50000], Train Loss: 8088941.5000, Val Loss: 5298654.5000\n",
      "Epoch [34434/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [34435/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34436/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34437/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [34438/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [34439/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34440/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [34441/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34442/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34443/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34444/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34445/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [34446/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [34447/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [34448/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [34449/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [34450/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [34451/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [34452/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [34453/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [34454/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34455/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34456/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34457/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34458/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [34459/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [34460/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [34461/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34462/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34463/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34464/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34465/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [34466/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [34467/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [34468/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34469/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [34470/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34471/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34472/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34473/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [34474/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34475/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34476/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [34477/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [34478/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [34479/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34480/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [34481/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [34482/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34483/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [34484/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34485/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34486/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [34487/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [34488/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [34489/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34490/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [34491/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34492/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34493/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34494/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [34495/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34496/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34497/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [34498/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34499/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34500/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34501/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [34502/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [34503/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [34504/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34505/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [34506/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34507/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34508/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34509/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34510/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [34511/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [34512/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [34513/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [34514/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [34515/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [34516/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [34517/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [34518/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [34519/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34520/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [34521/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [34522/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [34523/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34524/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34525/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [34526/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34527/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34528/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34529/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34530/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34531/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34532/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34533/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34534/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34535/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34536/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34537/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [34538/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34539/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [34540/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34541/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34542/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [34543/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34544/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34545/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34546/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [34547/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34548/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [34549/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34550/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34551/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34552/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [34553/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34554/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [34555/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [34556/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34557/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [34558/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [34559/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34560/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [34561/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34562/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34563/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34564/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34565/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [34566/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34567/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34568/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [34569/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34570/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [34571/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34572/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34573/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34574/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34575/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [34576/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34577/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34578/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34579/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34580/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34581/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34582/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34583/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34584/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [34585/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34586/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34587/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [34588/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34589/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34590/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34591/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [34592/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34593/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34594/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34595/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34596/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34597/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34598/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [34599/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34600/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [34601/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [34602/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [34603/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [34604/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [34605/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [34606/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [34607/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [34608/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [34609/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [34610/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [34611/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [34612/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [34613/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [34614/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [34615/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [34616/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [34617/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [34618/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [34619/50000], Train Loss: 8088941.5000, Val Loss: 5298713.5000\n",
      "Epoch [34620/50000], Train Loss: 8088942.0000, Val Loss: 5298562.5000\n",
      "Epoch [34621/50000], Train Loss: 8088942.5000, Val Loss: 5298739.0000\n",
      "Epoch [34622/50000], Train Loss: 8088941.5000, Val Loss: 5298541.5000\n",
      "Epoch [34623/50000], Train Loss: 8088942.0000, Val Loss: 5298749.0000\n",
      "Epoch [34624/50000], Train Loss: 8088942.0000, Val Loss: 5298552.5000\n",
      "Epoch [34625/50000], Train Loss: 8088942.0000, Val Loss: 5298710.0000\n",
      "Epoch [34626/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [34627/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34628/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [34629/50000], Train Loss: 8088942.5000, Val Loss: 5298586.5000\n",
      "Epoch [34630/50000], Train Loss: 8088941.5000, Val Loss: 5298710.0000\n",
      "Epoch [34631/50000], Train Loss: 8088942.0000, Val Loss: 5298585.0000\n",
      "Epoch [34632/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [34633/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34634/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [34635/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [34636/50000], Train Loss: 8088942.5000, Val Loss: 5298597.5000\n",
      "Epoch [34637/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [34638/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [34639/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [34640/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [34641/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [34642/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [34643/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [34644/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [34645/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34646/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34647/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [34648/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [34649/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [34650/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [34651/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34652/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [34653/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34654/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [34655/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [34656/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34657/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [34658/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [34659/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [34660/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [34661/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34662/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34663/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34664/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34665/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [34666/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [34667/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34668/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34669/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [34670/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34671/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34672/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34673/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34674/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [34675/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [34676/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34677/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [34678/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [34679/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34680/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34681/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34682/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34683/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34684/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34685/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34686/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [34687/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34688/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [34689/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [34690/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34691/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [34692/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [34693/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34694/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34695/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34696/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34697/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34698/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [34699/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34700/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34701/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34702/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34703/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34704/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34705/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34706/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34707/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34708/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34709/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34710/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [34711/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [34712/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34713/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34714/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [34715/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34716/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34717/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [34718/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [34719/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34720/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [34721/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34722/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34723/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [34724/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34725/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34726/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34727/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34728/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34729/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34730/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [34731/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34732/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34733/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [34734/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34735/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [34736/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34737/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34738/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34739/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [34740/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34741/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [34742/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [34743/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34744/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34745/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [34746/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34747/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34748/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34749/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34750/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [34751/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34752/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [34753/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [34754/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [34755/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34756/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34757/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34758/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [34759/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34760/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34761/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34762/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [34763/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [34764/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [34765/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34766/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [34767/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34768/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34769/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34770/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34771/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [34772/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34773/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [34774/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [34775/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [34776/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [34777/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [34778/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [34779/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [34780/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [34781/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [34782/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [34783/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [34784/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [34785/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [34786/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [34787/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [34788/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [34789/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34790/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [34791/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [34792/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34793/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [34794/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34795/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34796/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [34797/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [34798/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [34799/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [34800/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [34801/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [34802/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [34803/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [34804/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [34805/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [34806/50000], Train Loss: 8088942.5000, Val Loss: 5298603.0000\n",
      "Epoch [34807/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [34808/50000], Train Loss: 8088942.5000, Val Loss: 5298607.0000\n",
      "Epoch [34809/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [34810/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [34811/50000], Train Loss: 8088941.5000, Val Loss: 5298665.5000\n",
      "Epoch [34812/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [34813/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34814/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [34815/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34816/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [34817/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [34818/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [34819/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [34820/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [34821/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [34822/50000], Train Loss: 8088941.5000, Val Loss: 5298672.5000\n",
      "Epoch [34823/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [34824/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [34825/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [34826/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [34827/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34828/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [34829/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34830/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34831/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [34832/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34833/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [34834/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [34835/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [34836/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [34837/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [34838/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [34839/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [34840/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [34841/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [34842/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [34843/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [34844/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [34845/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [34846/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [34847/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34848/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [34849/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [34850/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [34851/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [34852/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [34853/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [34854/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [34855/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [34856/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [34857/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [34858/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [34859/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [34860/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [34861/50000], Train Loss: 8088941.5000, Val Loss: 5298621.5000\n",
      "Epoch [34862/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [34863/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [34864/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [34865/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [34866/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [34867/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34868/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [34869/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [34870/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [34871/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34872/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34873/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [34874/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [34875/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [34876/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [34877/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [34878/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [34879/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [34880/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [34881/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [34882/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [34883/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [34884/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [34885/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [34886/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [34887/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [34888/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [34889/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [34890/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [34891/50000], Train Loss: 8088942.5000, Val Loss: 5298659.0000\n",
      "Epoch [34892/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [34893/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34894/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34895/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34896/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [34897/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [34898/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [34899/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [34900/50000], Train Loss: 8088941.5000, Val Loss: 5298675.5000\n",
      "Epoch [34901/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [34902/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [34903/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [34904/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [34905/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [34906/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [34907/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [34908/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [34909/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34910/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34911/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [34912/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34913/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [34914/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [34915/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [34916/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34917/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [34918/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [34919/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [34920/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34921/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [34922/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34923/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34924/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [34925/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [34926/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [34927/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [34928/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [34929/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [34930/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [34931/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [34932/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [34933/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [34934/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [34935/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [34936/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [34937/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [34938/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [34939/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [34940/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34941/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34942/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [34943/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34944/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [34945/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [34946/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [34947/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [34948/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [34949/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34950/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [34951/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [34952/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34953/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34954/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [34955/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [34956/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34957/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [34958/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34959/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [34960/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [34961/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [34962/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [34963/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34964/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [34965/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [34966/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [34967/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [34968/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [34969/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [34970/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [34971/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [34972/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [34973/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [34974/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [34975/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [34976/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [34977/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [34978/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [34979/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [34980/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [34981/50000], Train Loss: 8088942.0000, Val Loss: 5298697.0000\n",
      "Epoch [34982/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [34983/50000], Train Loss: 8088941.5000, Val Loss: 5298715.0000\n",
      "Epoch [34984/50000], Train Loss: 8088942.0000, Val Loss: 5298565.5000\n",
      "Epoch [34985/50000], Train Loss: 8088942.0000, Val Loss: 5298729.0000\n",
      "Epoch [34986/50000], Train Loss: 8088942.5000, Val Loss: 5298560.5000\n",
      "Epoch [34987/50000], Train Loss: 8088942.0000, Val Loss: 5298720.0000\n",
      "Epoch [34988/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [34989/50000], Train Loss: 8088941.5000, Val Loss: 5298678.5000\n",
      "Epoch [34990/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [34991/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [34992/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [34993/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [34994/50000], Train Loss: 8088942.0000, Val Loss: 5298695.0000\n",
      "Epoch [34995/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [34996/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [34997/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [34998/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [34999/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [35000/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [35001/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [35002/50000], Train Loss: 8088941.5000, Val Loss: 5298606.0000\n",
      "Epoch [35003/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [35004/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [35005/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [35006/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [35007/50000], Train Loss: 8088941.5000, Val Loss: 5298623.0000\n",
      "Epoch [35008/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [35009/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [35010/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [35011/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [35012/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [35013/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [35014/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35015/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [35016/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35017/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [35018/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35019/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35020/50000], Train Loss: 8088941.0000, Val Loss: 5298655.0000\n",
      "Epoch [35021/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35022/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [35023/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35024/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [35025/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35026/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35027/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [35028/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35029/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35030/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35031/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [35032/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35033/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35034/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35035/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [35036/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35037/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35038/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35039/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [35040/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35041/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [35042/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35043/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [35044/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35045/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [35046/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [35047/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [35048/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [35049/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35050/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35051/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35052/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35053/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35054/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35055/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [35056/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35057/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [35058/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35059/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [35060/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [35061/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35062/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35063/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35064/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35065/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35066/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35067/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [35068/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35069/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [35070/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [35071/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [35072/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [35073/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35074/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35075/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [35076/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35077/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35078/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [35079/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35080/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [35081/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [35082/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [35083/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35084/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [35085/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [35086/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35087/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35088/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [35089/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [35090/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [35091/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [35092/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35093/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35094/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [35095/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35096/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35097/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [35098/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35099/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [35100/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [35101/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [35102/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [35103/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35104/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [35105/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [35106/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35107/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [35108/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35109/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35110/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [35111/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35112/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [35113/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [35114/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [35115/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35116/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [35117/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [35118/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35119/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35120/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [35121/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [35122/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35123/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35124/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35125/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [35126/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [35127/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [35128/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35129/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [35130/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35131/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35132/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [35133/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35134/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [35135/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35136/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35137/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35138/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35139/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [35140/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35141/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [35142/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [35143/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [35144/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [35145/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [35146/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [35147/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [35148/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [35149/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [35150/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [35151/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [35152/50000], Train Loss: 8088942.5000, Val Loss: 5298608.0000\n",
      "Epoch [35153/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [35154/50000], Train Loss: 8088942.5000, Val Loss: 5298601.5000\n",
      "Epoch [35155/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [35156/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [35157/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [35158/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [35159/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [35160/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [35161/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [35162/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [35163/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [35164/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [35165/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [35166/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35167/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35168/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [35169/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [35170/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [35171/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [35172/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [35173/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [35174/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [35175/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [35176/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35177/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35178/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [35179/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35180/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [35181/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [35182/50000], Train Loss: 8088941.5000, Val Loss: 5298617.0000\n",
      "Epoch [35183/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [35184/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [35185/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [35186/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [35187/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [35188/50000], Train Loss: 8088941.5000, Val Loss: 5298623.5000\n",
      "Epoch [35189/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [35190/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35191/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [35192/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [35193/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [35194/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [35195/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [35196/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [35197/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [35198/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [35199/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [35200/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [35201/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [35202/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [35203/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35204/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [35205/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [35206/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [35207/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [35208/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35209/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [35210/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [35211/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [35212/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [35213/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [35214/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [35215/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [35216/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [35217/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [35218/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35219/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35220/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [35221/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [35222/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35223/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [35224/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [35225/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35226/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [35227/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35228/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [35229/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [35230/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [35231/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [35232/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35233/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [35234/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35235/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [35236/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35237/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [35238/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35239/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [35240/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35241/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35242/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [35243/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [35244/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [35245/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35246/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [35247/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35248/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [35249/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35250/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [35251/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [35252/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [35253/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [35254/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [35255/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [35256/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [35257/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [35258/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [35259/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [35260/50000], Train Loss: 8088943.0000, Val Loss: 5298671.0000\n",
      "Epoch [35261/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [35262/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [35263/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [35264/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [35265/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [35266/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35267/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35268/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [35269/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35270/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [35271/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35272/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35273/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [35274/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [35275/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [35276/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [35277/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [35278/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [35279/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [35280/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [35281/50000], Train Loss: 8088941.5000, Val Loss: 5298618.0000\n",
      "Epoch [35282/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [35283/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [35284/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [35285/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [35286/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [35287/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [35288/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [35289/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35290/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [35291/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [35292/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [35293/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35294/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [35295/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [35296/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [35297/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35298/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [35299/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35300/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [35301/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [35302/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [35303/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [35304/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [35305/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [35306/50000], Train Loss: 8088942.5000, Val Loss: 5298678.0000\n",
      "Epoch [35307/50000], Train Loss: 8088942.5000, Val Loss: 5298609.0000\n",
      "Epoch [35308/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [35309/50000], Train Loss: 8088942.5000, Val Loss: 5298609.0000\n",
      "Epoch [35310/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [35311/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [35312/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [35313/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [35314/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [35315/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35316/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [35317/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35318/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35319/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [35320/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [35321/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [35322/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35323/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [35324/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35325/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [35326/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [35327/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [35328/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [35329/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [35330/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [35331/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [35332/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [35333/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [35334/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35335/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [35336/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35337/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [35338/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35339/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [35340/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35341/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35342/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35343/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35344/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [35345/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [35346/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [35347/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35348/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [35349/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35350/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [35351/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35352/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35353/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35354/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35355/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35356/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35357/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [35358/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35359/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35360/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [35361/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [35362/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35363/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [35364/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35365/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [35366/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35367/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35368/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35369/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [35370/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35371/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35372/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [35373/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [35374/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35375/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [35376/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35377/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35378/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35379/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35380/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [35381/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [35382/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [35383/50000], Train Loss: 8088942.5000, Val Loss: 5298604.5000\n",
      "Epoch [35384/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [35385/50000], Train Loss: 8088941.5000, Val Loss: 5298587.5000\n",
      "Epoch [35386/50000], Train Loss: 8088941.5000, Val Loss: 5298710.0000\n",
      "Epoch [35387/50000], Train Loss: 8088942.0000, Val Loss: 5298570.5000\n",
      "Epoch [35388/50000], Train Loss: 8088942.5000, Val Loss: 5298725.5000\n",
      "Epoch [35389/50000], Train Loss: 8088941.5000, Val Loss: 5298559.5000\n",
      "Epoch [35390/50000], Train Loss: 8088942.0000, Val Loss: 5298726.0000\n",
      "Epoch [35391/50000], Train Loss: 8088942.0000, Val Loss: 5298574.0000\n",
      "Epoch [35392/50000], Train Loss: 8088942.5000, Val Loss: 5298694.5000\n",
      "Epoch [35393/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [35394/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [35395/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [35396/50000], Train Loss: 8088942.5000, Val Loss: 5298597.5000\n",
      "Epoch [35397/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [35398/50000], Train Loss: 8088942.0000, Val Loss: 5298591.5000\n",
      "Epoch [35399/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [35400/50000], Train Loss: 8088941.5000, Val Loss: 5298618.0000\n",
      "Epoch [35401/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35402/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [35403/50000], Train Loss: 8088941.5000, Val Loss: 5298614.0000\n",
      "Epoch [35404/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [35405/50000], Train Loss: 8088942.5000, Val Loss: 5298603.0000\n",
      "Epoch [35406/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [35407/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [35408/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35409/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [35410/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [35411/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [35412/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [35413/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [35414/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [35415/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [35416/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35417/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35418/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [35419/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [35420/50000], Train Loss: 8088942.5000, Val Loss: 5298659.0000\n",
      "Epoch [35421/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35422/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35423/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [35424/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35425/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [35426/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [35427/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [35428/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35429/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35430/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35431/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [35432/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35433/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35434/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35435/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [35436/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [35437/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35438/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35439/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [35440/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [35441/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35442/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [35443/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35444/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [35445/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35446/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35447/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [35448/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35449/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35450/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [35451/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35452/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35453/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [35454/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [35455/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35456/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35457/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [35458/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35459/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [35460/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [35461/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35462/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35463/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [35464/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35465/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [35466/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [35467/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [35468/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35469/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35470/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [35471/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [35472/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35473/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35474/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35475/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35476/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35477/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35478/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [35479/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35480/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35481/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [35482/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35483/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [35484/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35485/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35486/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [35487/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [35488/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35489/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35490/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35491/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35492/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [35493/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35494/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [35495/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35496/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [35497/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35498/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35499/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [35500/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35501/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [35502/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35503/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35504/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [35505/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35506/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35507/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [35508/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35509/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35510/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [35511/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35512/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [35513/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35514/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [35515/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35516/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [35517/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [35518/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35519/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35520/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35521/50000], Train Loss: 8088941.5000, Val Loss: 5298659.0000\n",
      "Epoch [35522/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35523/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [35524/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35525/50000], Train Loss: 8088942.5000, Val Loss: 5298659.0000\n",
      "Epoch [35526/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [35527/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [35528/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35529/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [35530/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35531/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [35532/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [35533/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [35534/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [35535/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [35536/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [35537/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [35538/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [35539/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [35540/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [35541/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [35542/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35543/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35544/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [35545/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [35546/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35547/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [35548/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [35549/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35550/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [35551/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35552/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35553/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [35554/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [35555/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [35556/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [35557/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [35558/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [35559/50000], Train Loss: 8088943.0000, Val Loss: 5298673.0000\n",
      "Epoch [35560/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [35561/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [35562/50000], Train Loss: 8088941.5000, Val Loss: 5298623.0000\n",
      "Epoch [35563/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [35564/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35565/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [35566/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [35567/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [35568/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [35569/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35570/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [35571/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35572/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [35573/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35574/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [35575/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35576/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35577/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35578/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [35579/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35580/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [35581/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35582/50000], Train Loss: 8088942.5000, Val Loss: 5298634.0000\n",
      "Epoch [35583/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [35584/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35585/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [35586/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [35587/50000], Train Loss: 8088942.5000, Val Loss: 5298672.5000\n",
      "Epoch [35588/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [35589/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [35590/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [35591/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [35592/50000], Train Loss: 8088941.5000, Val Loss: 5298600.5000\n",
      "Epoch [35593/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [35594/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [35595/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [35596/50000], Train Loss: 8088941.5000, Val Loss: 5298613.0000\n",
      "Epoch [35597/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [35598/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [35599/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [35600/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35601/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35602/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [35603/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35604/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [35605/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [35606/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [35607/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [35608/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [35609/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [35610/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [35611/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [35612/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [35613/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35614/50000], Train Loss: 8088941.0000, Val Loss: 5298654.5000\n",
      "Epoch [35615/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [35616/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [35617/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [35618/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35619/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [35620/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35621/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [35622/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [35623/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [35624/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [35625/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [35626/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [35627/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [35628/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [35629/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35630/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35631/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [35632/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [35633/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [35634/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35635/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [35636/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [35637/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35638/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [35639/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35640/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35641/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35642/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [35643/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35644/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35645/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35646/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35647/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35648/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35649/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35650/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [35651/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35652/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35653/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35654/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35655/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35656/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [35657/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35658/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35659/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [35660/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35661/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [35662/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35663/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35664/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [35665/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [35666/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [35667/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35668/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35669/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [35670/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [35671/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [35672/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [35673/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [35674/50000], Train Loss: 8088941.5000, Val Loss: 5298614.0000\n",
      "Epoch [35675/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [35676/50000], Train Loss: 8088942.5000, Val Loss: 5298600.5000\n",
      "Epoch [35677/50000], Train Loss: 8088941.5000, Val Loss: 5298695.0000\n",
      "Epoch [35678/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [35679/50000], Train Loss: 8088941.5000, Val Loss: 5298707.0000\n",
      "Epoch [35680/50000], Train Loss: 8088942.0000, Val Loss: 5298576.0000\n",
      "Epoch [35681/50000], Train Loss: 8088942.0000, Val Loss: 5298715.5000\n",
      "Epoch [35682/50000], Train Loss: 8088942.0000, Val Loss: 5298572.5000\n",
      "Epoch [35683/50000], Train Loss: 8088942.0000, Val Loss: 5298712.0000\n",
      "Epoch [35684/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [35685/50000], Train Loss: 8088941.5000, Val Loss: 5298688.5000\n",
      "Epoch [35686/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [35687/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35688/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35689/50000], Train Loss: 8088941.5000, Val Loss: 5298614.0000\n",
      "Epoch [35690/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [35691/50000], Train Loss: 8088941.5000, Val Loss: 5298596.5000\n",
      "Epoch [35692/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [35693/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [35694/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [35695/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [35696/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [35697/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35698/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [35699/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [35700/50000], Train Loss: 8088941.5000, Val Loss: 5298615.0000\n",
      "Epoch [35701/50000], Train Loss: 8088943.0000, Val Loss: 5298675.0000\n",
      "Epoch [35702/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [35703/50000], Train Loss: 8088941.5000, Val Loss: 5298666.5000\n",
      "Epoch [35704/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35705/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [35706/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35707/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [35708/50000], Train Loss: 8088941.5000, Val Loss: 5298661.5000\n",
      "Epoch [35709/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [35710/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [35711/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [35712/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [35713/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35714/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [35715/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35716/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35717/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35718/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [35719/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35720/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35721/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35722/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [35723/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35724/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35725/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35726/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [35727/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35728/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35729/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35730/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35731/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [35732/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [35733/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [35734/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [35735/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35736/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35737/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35738/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [35739/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35740/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [35741/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35742/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [35743/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35744/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [35745/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [35746/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35747/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [35748/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [35749/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35750/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35751/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [35752/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35753/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [35754/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35755/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35756/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [35757/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35758/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35759/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [35760/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [35761/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [35762/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [35763/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [35764/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35765/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35766/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [35767/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35768/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [35769/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [35770/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [35771/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [35772/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35773/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35774/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35775/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35776/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [35777/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35778/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [35779/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35780/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [35781/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35782/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [35783/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35784/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35785/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [35786/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35787/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35788/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35789/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [35790/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35791/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [35792/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35793/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35794/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [35795/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35796/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [35797/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35798/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35799/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35800/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [35801/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35802/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35803/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [35804/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35805/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [35806/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35807/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35808/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35809/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [35810/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35811/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [35812/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35813/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35814/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [35815/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35816/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [35817/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [35818/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [35819/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [35820/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [35821/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35822/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [35823/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [35824/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35825/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [35826/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [35827/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [35828/50000], Train Loss: 8088942.5000, Val Loss: 5298612.5000\n",
      "Epoch [35829/50000], Train Loss: 8088942.5000, Val Loss: 5298683.5000\n",
      "Epoch [35830/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [35831/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [35832/50000], Train Loss: 8088942.0000, Val Loss: 5298578.5000\n",
      "Epoch [35833/50000], Train Loss: 8088941.5000, Val Loss: 5298718.5000\n",
      "Epoch [35834/50000], Train Loss: 8088942.0000, Val Loss: 5298563.0000\n",
      "Epoch [35835/50000], Train Loss: 8088942.0000, Val Loss: 5298729.0000\n",
      "Epoch [35836/50000], Train Loss: 8088942.5000, Val Loss: 5298561.0000\n",
      "Epoch [35837/50000], Train Loss: 8088942.0000, Val Loss: 5298718.0000\n",
      "Epoch [35838/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [35839/50000], Train Loss: 8088941.0000, Val Loss: 5298678.5000\n",
      "Epoch [35840/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [35841/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35842/50000], Train Loss: 8088941.0000, Val Loss: 5298676.5000\n",
      "Epoch [35843/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [35844/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [35845/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [35846/50000], Train Loss: 8088941.5000, Val Loss: 5298677.0000\n",
      "Epoch [35847/50000], Train Loss: 8088942.5000, Val Loss: 5298627.0000\n",
      "Epoch [35848/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [35849/50000], Train Loss: 8088941.5000, Val Loss: 5298662.0000\n",
      "Epoch [35850/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [35851/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [35852/50000], Train Loss: 8088941.5000, Val Loss: 5298607.0000\n",
      "Epoch [35853/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [35854/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [35855/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [35856/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [35857/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [35858/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [35859/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [35860/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [35861/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [35862/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [35863/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [35864/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35865/50000], Train Loss: 8088941.5000, Val Loss: 5298663.0000\n",
      "Epoch [35866/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [35867/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [35868/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35869/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [35870/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [35871/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [35872/50000], Train Loss: 8088941.5000, Val Loss: 5298663.0000\n",
      "Epoch [35873/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [35874/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [35875/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35876/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [35877/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35878/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35879/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [35880/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [35881/50000], Train Loss: 8088941.0000, Val Loss: 5298656.0000\n",
      "Epoch [35882/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35883/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [35884/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35885/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35886/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [35887/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [35888/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [35889/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [35890/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [35891/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35892/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [35893/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35894/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35895/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [35896/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35897/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [35898/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35899/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35900/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [35901/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35902/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35903/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [35904/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35905/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [35906/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35907/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35908/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [35909/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35910/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [35911/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35912/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35913/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [35914/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35915/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35916/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [35917/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35918/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [35919/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35920/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35921/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [35922/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35923/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [35924/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35925/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35926/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [35927/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35928/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [35929/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35930/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [35931/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35932/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [35933/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [35934/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [35935/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [35936/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35937/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35938/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35939/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [35940/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35941/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [35942/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35943/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [35944/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35945/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [35946/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35947/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35948/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [35949/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [35950/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [35951/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [35952/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [35953/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [35954/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [35955/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35956/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [35957/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [35958/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35959/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [35960/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [35961/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [35962/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [35963/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35964/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [35965/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [35966/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35967/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [35968/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [35969/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [35970/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [35971/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [35972/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [35973/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [35974/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [35975/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [35976/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [35977/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [35978/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [35979/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35980/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [35981/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [35982/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [35983/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [35984/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [35985/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [35986/50000], Train Loss: 8088941.5000, Val Loss: 5298687.0000\n",
      "Epoch [35987/50000], Train Loss: 8088941.5000, Val Loss: 5298598.0000\n",
      "Epoch [35988/50000], Train Loss: 8088941.5000, Val Loss: 5298695.0000\n",
      "Epoch [35989/50000], Train Loss: 8088942.0000, Val Loss: 5298591.5000\n",
      "Epoch [35990/50000], Train Loss: 8088942.0000, Val Loss: 5298698.5000\n",
      "Epoch [35991/50000], Train Loss: 8088942.5000, Val Loss: 5298592.0000\n",
      "Epoch [35992/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [35993/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [35994/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [35995/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [35996/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [35997/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [35998/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [35999/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [36000/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [36001/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [36002/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [36003/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [36004/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [36005/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [36006/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36007/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36008/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36009/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [36010/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [36011/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [36012/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36013/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36014/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [36015/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36016/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [36017/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [36018/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [36019/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [36020/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [36021/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [36022/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [36023/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [36024/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [36025/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36026/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [36027/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [36028/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [36029/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [36030/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [36031/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36032/50000], Train Loss: 8088941.0000, Val Loss: 5298644.5000\n",
      "Epoch [36033/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [36034/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36035/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36036/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [36037/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36038/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36039/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36040/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [36041/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [36042/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36043/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36044/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [36045/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [36046/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36047/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36048/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36049/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36050/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36051/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [36052/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [36053/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36054/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36055/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [36056/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36057/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36058/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36059/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36060/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36061/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [36062/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36063/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36064/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36065/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36066/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [36067/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36068/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [36069/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [36070/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36071/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36072/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36073/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [36074/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36075/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36076/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36077/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36078/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36079/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [36080/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [36081/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [36082/50000], Train Loss: 8088941.5000, Val Loss: 5298617.0000\n",
      "Epoch [36083/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [36084/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [36085/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [36086/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [36087/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [36088/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [36089/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [36090/50000], Train Loss: 8088941.5000, Val Loss: 5298594.0000\n",
      "Epoch [36091/50000], Train Loss: 8088941.5000, Val Loss: 5298695.5000\n",
      "Epoch [36092/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [36093/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [36094/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [36095/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [36096/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [36097/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [36098/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36099/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [36100/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [36101/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [36102/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [36103/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [36104/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [36105/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36106/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36107/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36108/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36109/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [36110/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36111/50000], Train Loss: 8088941.5000, Val Loss: 5298663.0000\n",
      "Epoch [36112/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [36113/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [36114/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [36115/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [36116/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [36117/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [36118/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36119/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36120/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36121/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [36122/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [36123/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [36124/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [36125/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [36126/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [36127/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36128/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [36129/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36130/50000], Train Loss: 8088941.0000, Val Loss: 5298644.5000\n",
      "Epoch [36131/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [36132/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36133/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36134/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [36135/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [36136/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36137/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [36138/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36139/50000], Train Loss: 8088941.0000, Val Loss: 5298655.0000\n",
      "Epoch [36140/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36141/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36142/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [36143/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [36144/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [36145/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [36146/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36147/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [36148/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36149/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36150/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36151/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36152/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36153/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [36154/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36155/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [36156/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36157/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36158/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36159/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36160/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [36161/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36162/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36163/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36164/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [36165/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36166/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36167/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [36168/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [36169/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36170/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36171/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [36172/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36173/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36174/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36175/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [36176/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36177/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36178/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [36179/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36180/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [36181/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36182/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [36183/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [36184/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36185/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [36186/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [36187/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [36188/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [36189/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [36190/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [36191/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [36192/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36193/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36194/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [36195/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36196/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36197/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36198/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36199/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36200/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36201/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36202/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [36203/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36204/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36205/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36206/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36207/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [36208/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [36209/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [36210/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [36211/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [36212/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [36213/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [36214/50000], Train Loss: 8088942.0000, Val Loss: 5298589.5000\n",
      "Epoch [36215/50000], Train Loss: 8088942.0000, Val Loss: 5298710.0000\n",
      "Epoch [36216/50000], Train Loss: 8088942.0000, Val Loss: 5298566.0000\n",
      "Epoch [36217/50000], Train Loss: 8088942.5000, Val Loss: 5298734.5000\n",
      "Epoch [36218/50000], Train Loss: 8088941.5000, Val Loss: 5298546.5000\n",
      "Epoch [36219/50000], Train Loss: 8088942.0000, Val Loss: 5298742.5000\n",
      "Epoch [36220/50000], Train Loss: 8088942.0000, Val Loss: 5298557.0000\n",
      "Epoch [36221/50000], Train Loss: 8088942.0000, Val Loss: 5298710.0000\n",
      "Epoch [36222/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [36223/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36224/50000], Train Loss: 8088941.5000, Val Loss: 5298669.0000\n",
      "Epoch [36225/50000], Train Loss: 8088941.5000, Val Loss: 5298596.5000\n",
      "Epoch [36226/50000], Train Loss: 8088942.0000, Val Loss: 5298705.0000\n",
      "Epoch [36227/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [36228/50000], Train Loss: 8088941.5000, Val Loss: 5298693.5000\n",
      "Epoch [36229/50000], Train Loss: 8088941.5000, Val Loss: 5298617.0000\n",
      "Epoch [36230/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36231/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [36232/50000], Train Loss: 8088942.0000, Val Loss: 5298603.0000\n",
      "Epoch [36233/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [36234/50000], Train Loss: 8088941.5000, Val Loss: 5298602.5000\n",
      "Epoch [36235/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [36236/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36237/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36238/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [36239/50000], Train Loss: 8088941.5000, Val Loss: 5298611.0000\n",
      "Epoch [36240/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [36241/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [36242/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [36243/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36244/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36245/50000], Train Loss: 8088941.0000, Val Loss: 5298666.5000\n",
      "Epoch [36246/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [36247/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [36248/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36249/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36250/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36251/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36252/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [36253/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [36254/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [36255/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [36256/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36257/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36258/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [36259/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [36260/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36261/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [36262/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [36263/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36264/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36265/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36266/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36267/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [36268/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [36269/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [36270/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36271/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [36272/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36273/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36274/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36275/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36276/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [36277/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36278/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36279/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36280/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36281/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36282/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36283/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36284/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36285/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [36286/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [36287/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [36288/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [36289/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [36290/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36291/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36292/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [36293/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [36294/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36295/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36296/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36297/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [36298/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36299/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36300/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36301/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36302/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [36303/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36304/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36305/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [36306/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [36307/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [36308/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36309/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36310/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [36311/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36312/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [36313/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [36314/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [36315/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36316/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [36317/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36318/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [36319/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36320/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [36321/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [36322/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36323/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36324/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [36325/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36326/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36327/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [36328/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36329/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36330/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [36331/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [36332/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36333/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36334/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36335/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36336/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36337/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36338/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36339/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36340/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36341/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36342/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36343/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36344/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36345/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36346/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36347/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [36348/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36349/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36350/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36351/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36352/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [36353/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36354/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [36355/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36356/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36357/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [36358/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36359/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36360/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36361/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [36362/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [36363/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [36364/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36365/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [36366/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36367/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36368/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [36369/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [36370/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [36371/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [36372/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [36373/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [36374/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [36375/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [36376/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36377/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [36378/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [36379/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [36380/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36381/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36382/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36383/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36384/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [36385/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [36386/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36387/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [36388/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [36389/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [36390/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [36391/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [36392/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [36393/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [36394/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [36395/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [36396/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [36397/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [36398/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [36399/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36400/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [36401/50000], Train Loss: 8088941.5000, Val Loss: 5298631.5000\n",
      "Epoch [36402/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [36403/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36404/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [36405/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [36406/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [36407/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36408/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [36409/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [36410/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [36411/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [36412/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [36413/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [36414/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [36415/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [36416/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [36417/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [36418/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [36419/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [36420/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [36421/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [36422/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [36423/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [36424/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [36425/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [36426/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [36427/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36428/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [36429/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36430/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36431/50000], Train Loss: 8088941.5000, Val Loss: 5298664.5000\n",
      "Epoch [36432/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [36433/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [36434/50000], Train Loss: 8088942.5000, Val Loss: 5298608.0000\n",
      "Epoch [36435/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [36436/50000], Train Loss: 8088941.5000, Val Loss: 5298606.0000\n",
      "Epoch [36437/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [36438/50000], Train Loss: 8088941.5000, Val Loss: 5298607.0000\n",
      "Epoch [36439/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [36440/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [36441/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [36442/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [36443/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [36444/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [36445/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36446/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36447/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36448/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [36449/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [36450/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [36451/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [36452/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [36453/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36454/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [36455/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36456/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36457/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36458/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36459/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36460/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [36461/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [36462/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36463/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36464/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36465/50000], Train Loss: 8088941.0000, Val Loss: 5298661.5000\n",
      "Epoch [36466/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [36467/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [36468/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [36469/50000], Train Loss: 8088941.5000, Val Loss: 5298675.0000\n",
      "Epoch [36470/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [36471/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [36472/50000], Train Loss: 8088942.5000, Val Loss: 5298610.5000\n",
      "Epoch [36473/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [36474/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [36475/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [36476/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [36477/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [36478/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36479/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36480/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [36481/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [36482/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [36483/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36484/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [36485/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [36486/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [36487/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [36488/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [36489/50000], Train Loss: 8088942.5000, Val Loss: 5298612.5000\n",
      "Epoch [36490/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [36491/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [36492/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [36493/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [36494/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [36495/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36496/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36497/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [36498/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36499/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36500/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36501/50000], Train Loss: 8088941.0000, Val Loss: 5298661.5000\n",
      "Epoch [36502/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [36503/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [36504/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [36505/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [36506/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [36507/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [36508/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [36509/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36510/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [36511/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36512/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36513/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36514/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36515/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36516/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36517/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36518/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36519/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [36520/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [36521/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36522/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [36523/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [36524/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36525/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36526/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [36527/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36528/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36529/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36530/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36531/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36532/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [36533/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36534/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36535/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36536/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36537/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36538/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [36539/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36540/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [36541/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36542/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [36543/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [36544/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36545/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [36546/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [36547/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36548/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36549/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [36550/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36551/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36552/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36553/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [36554/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [36555/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [36556/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [36557/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36558/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36559/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [36560/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36561/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36562/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [36563/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [36564/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [36565/50000], Train Loss: 8088941.5000, Val Loss: 5298613.0000\n",
      "Epoch [36566/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [36567/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [36568/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [36569/50000], Train Loss: 8088942.0000, Val Loss: 5298578.5000\n",
      "Epoch [36570/50000], Train Loss: 8088941.5000, Val Loss: 5298718.5000\n",
      "Epoch [36571/50000], Train Loss: 8088942.0000, Val Loss: 5298563.0000\n",
      "Epoch [36572/50000], Train Loss: 8088942.0000, Val Loss: 5298729.0000\n",
      "Epoch [36573/50000], Train Loss: 8088942.5000, Val Loss: 5298562.5000\n",
      "Epoch [36574/50000], Train Loss: 8088942.5000, Val Loss: 5298715.0000\n",
      "Epoch [36575/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [36576/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [36577/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36578/50000], Train Loss: 8088941.5000, Val Loss: 5298621.5000\n",
      "Epoch [36579/50000], Train Loss: 8088942.5000, Val Loss: 5298684.5000\n",
      "Epoch [36580/50000], Train Loss: 8088942.5000, Val Loss: 5298594.5000\n",
      "Epoch [36581/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [36582/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [36583/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [36584/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36585/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36586/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [36587/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [36588/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [36589/50000], Train Loss: 8088941.5000, Val Loss: 5298608.0000\n",
      "Epoch [36590/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [36591/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [36592/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [36593/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36594/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36595/50000], Train Loss: 8088941.0000, Val Loss: 5298664.5000\n",
      "Epoch [36596/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [36597/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [36598/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36599/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [36600/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [36601/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36602/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [36603/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [36604/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36605/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36606/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36607/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [36608/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36609/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [36610/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36611/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36612/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36613/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [36614/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36615/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [36616/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [36617/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36618/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [36619/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36620/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36621/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36622/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [36623/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36624/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36625/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [36626/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36627/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [36628/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36629/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36630/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [36631/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36632/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [36633/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36634/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36635/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [36636/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36637/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36638/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [36639/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36640/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [36641/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36642/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36643/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [36644/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36645/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [36646/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36647/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36648/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [36649/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36650/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36651/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36652/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [36653/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36654/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36655/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36656/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [36657/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36658/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [36659/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36660/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36661/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36662/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [36663/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36664/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36665/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [36666/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [36667/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36668/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [36669/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [36670/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36671/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [36672/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36673/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36674/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [36675/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36676/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36677/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36678/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [36679/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36680/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [36681/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36682/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [36683/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36684/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36685/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36686/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36687/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [36688/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [36689/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [36690/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [36691/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [36692/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [36693/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [36694/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36695/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [36696/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [36697/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [36698/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [36699/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [36700/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36701/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [36702/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36703/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [36704/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36705/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [36706/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [36707/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [36708/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36709/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36710/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36711/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36712/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [36713/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [36714/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36715/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [36716/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36717/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36718/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [36719/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [36720/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [36721/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36722/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [36723/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [36724/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [36725/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [36726/50000], Train Loss: 8088943.0000, Val Loss: 5298677.0000\n",
      "Epoch [36727/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [36728/50000], Train Loss: 8088942.5000, Val Loss: 5298683.0000\n",
      "Epoch [36729/50000], Train Loss: 8088941.5000, Val Loss: 5298603.0000\n",
      "Epoch [36730/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [36731/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [36732/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [36733/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [36734/50000], Train Loss: 8088942.5000, Val Loss: 5298690.5000\n",
      "Epoch [36735/50000], Train Loss: 8088941.5000, Val Loss: 5298601.5000\n",
      "Epoch [36736/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [36737/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [36738/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [36739/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36740/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [36741/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [36742/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [36743/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [36744/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [36745/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [36746/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [36747/50000], Train Loss: 8088941.5000, Val Loss: 5298671.0000\n",
      "Epoch [36748/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [36749/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [36750/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [36751/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36752/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36753/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [36754/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [36755/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [36756/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [36757/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [36758/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [36759/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [36760/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [36761/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [36762/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [36763/50000], Train Loss: 8088941.5000, Val Loss: 5298621.0000\n",
      "Epoch [36764/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [36765/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [36766/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [36767/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36768/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [36769/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36770/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36771/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [36772/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [36773/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36774/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36775/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [36776/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36777/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [36778/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [36779/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [36780/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [36781/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [36782/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [36783/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [36784/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [36785/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [36786/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36787/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [36788/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36789/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [36790/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36791/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [36792/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [36793/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36794/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36795/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [36796/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [36797/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [36798/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [36799/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [36800/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36801/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [36802/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36803/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [36804/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36805/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [36806/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [36807/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [36808/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [36809/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [36810/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [36811/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [36812/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [36813/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36814/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36815/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36816/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [36817/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36818/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36819/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [36820/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [36821/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [36822/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36823/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [36824/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36825/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36826/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36827/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36828/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36829/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [36830/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36831/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [36832/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [36833/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36834/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [36835/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [36836/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [36837/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36838/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36839/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36840/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [36841/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [36842/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36843/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [36844/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36845/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [36846/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [36847/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [36848/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [36849/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [36850/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [36851/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [36852/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [36853/50000], Train Loss: 8088941.5000, Val Loss: 5298600.5000\n",
      "Epoch [36854/50000], Train Loss: 8088941.5000, Val Loss: 5298693.5000\n",
      "Epoch [36855/50000], Train Loss: 8088941.5000, Val Loss: 5298592.0000\n",
      "Epoch [36856/50000], Train Loss: 8088942.0000, Val Loss: 5298700.5000\n",
      "Epoch [36857/50000], Train Loss: 8088942.0000, Val Loss: 5298585.0000\n",
      "Epoch [36858/50000], Train Loss: 8088942.0000, Val Loss: 5298706.0000\n",
      "Epoch [36859/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [36860/50000], Train Loss: 8088941.5000, Val Loss: 5298702.0000\n",
      "Epoch [36861/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [36862/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [36863/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [36864/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [36865/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [36866/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36867/50000], Train Loss: 8088941.0000, Val Loss: 5298669.0000\n",
      "Epoch [36868/50000], Train Loss: 8088941.5000, Val Loss: 5298611.5000\n",
      "Epoch [36869/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [36870/50000], Train Loss: 8088941.5000, Val Loss: 5298609.0000\n",
      "Epoch [36871/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [36872/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [36873/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [36874/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [36875/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [36876/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [36877/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [36878/50000], Train Loss: 8088941.0000, Val Loss: 5298661.5000\n",
      "Epoch [36879/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [36880/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [36881/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [36882/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [36883/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [36884/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [36885/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [36886/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [36887/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [36888/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [36889/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [36890/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [36891/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [36892/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [36893/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [36894/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [36895/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [36896/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [36897/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [36898/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36899/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [36900/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [36901/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [36902/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [36903/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36904/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [36905/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36906/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [36907/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36908/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [36909/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [36910/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36911/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [36912/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36913/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36914/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [36915/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36916/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36917/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36918/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [36919/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [36920/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [36921/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [36922/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [36923/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [36924/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36925/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36926/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36927/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [36928/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36929/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [36930/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [36931/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36932/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [36933/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36934/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36935/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [36936/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [36937/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36938/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [36939/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [36940/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [36941/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [36942/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36943/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [36945/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36946/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36947/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [36948/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36949/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [36950/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36951/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [36952/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36953/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [36954/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [36955/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [36956/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [36957/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36958/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [36959/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [36960/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [36961/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [36962/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [36963/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [36964/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [36965/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [36966/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [36967/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [36968/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [36969/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [36970/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [36971/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [36972/50000], Train Loss: 8088941.0000, Val Loss: 5298661.5000\n",
      "Epoch [36973/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [36974/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [36975/50000], Train Loss: 8088942.5000, Val Loss: 5298611.5000\n",
      "Epoch [36976/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [36977/50000], Train Loss: 8088941.5000, Val Loss: 5298597.5000\n",
      "Epoch [36978/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [36979/50000], Train Loss: 8088942.0000, Val Loss: 5298579.5000\n",
      "Epoch [36980/50000], Train Loss: 8088941.5000, Val Loss: 5298718.0000\n",
      "Epoch [36981/50000], Train Loss: 8088942.0000, Val Loss: 5298563.0000\n",
      "Epoch [36982/50000], Train Loss: 8088942.5000, Val Loss: 5298728.0000\n",
      "Epoch [36983/50000], Train Loss: 8088941.5000, Val Loss: 5298563.0000\n",
      "Epoch [36984/50000], Train Loss: 8088941.5000, Val Loss: 5298716.5000\n",
      "Epoch [36985/50000], Train Loss: 8088942.5000, Val Loss: 5298590.0000\n",
      "Epoch [36986/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [36987/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [36988/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [36989/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [36990/50000], Train Loss: 8088942.5000, Val Loss: 5298599.5000\n",
      "Epoch [36991/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [36992/50000], Train Loss: 8088942.5000, Val Loss: 5298597.5000\n",
      "Epoch [36993/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [36994/50000], Train Loss: 8088941.5000, Val Loss: 5298623.5000\n",
      "Epoch [36995/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [36996/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [36997/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [36998/50000], Train Loss: 8088942.5000, Val Loss: 5298676.5000\n",
      "Epoch [36999/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [37000/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [37001/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [37002/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [37003/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37004/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [37005/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37006/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37007/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [37008/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37009/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37010/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37011/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37012/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [37013/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37014/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [37015/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [37016/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37017/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [37018/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37019/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [37020/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37021/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [37022/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37023/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37024/50000], Train Loss: 8088942.5000, Val Loss: 5298633.5000\n",
      "Epoch [37025/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37026/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37027/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [37028/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [37029/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [37030/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [37031/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37032/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37033/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [37034/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37035/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [37036/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [37037/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [37038/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [37039/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37040/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [37041/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37042/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37043/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [37044/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37045/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [37046/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37047/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37048/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37049/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [37050/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [37051/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37052/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37053/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37054/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37055/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37056/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [37057/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37058/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37059/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37060/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [37061/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [37062/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37063/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37064/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37065/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37066/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37067/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37068/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37069/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37070/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [37071/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37072/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [37073/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37074/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37075/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37076/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37077/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37078/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [37079/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37080/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37081/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37082/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37083/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [37084/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37085/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [37086/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37087/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [37088/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37089/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37090/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [37091/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [37092/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37093/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [37094/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37095/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [37096/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37097/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [37098/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37099/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37100/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [37101/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37102/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [37103/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37104/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [37105/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37106/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37107/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [37108/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37109/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37110/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [37111/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [37112/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [37113/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37114/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [37115/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37116/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [37117/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37118/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [37119/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37120/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [37121/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [37122/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [37123/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [37124/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37125/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [37126/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [37127/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37128/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [37129/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [37130/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [37131/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [37132/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [37133/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [37134/50000], Train Loss: 8088942.5000, Val Loss: 5298687.0000\n",
      "Epoch [37135/50000], Train Loss: 8088941.5000, Val Loss: 5298595.0000\n",
      "Epoch [37136/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [37137/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [37138/50000], Train Loss: 8088941.5000, Val Loss: 5298711.0000\n",
      "Epoch [37139/50000], Train Loss: 8088941.5000, Val Loss: 5298577.5000\n",
      "Epoch [37140/50000], Train Loss: 8088942.0000, Val Loss: 5298707.5000\n",
      "Epoch [37141/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [37142/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [37143/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [37144/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [37145/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [37146/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [37147/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [37148/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [37149/50000], Train Loss: 8088942.5000, Val Loss: 5298684.5000\n",
      "Epoch [37150/50000], Train Loss: 8088942.5000, Val Loss: 5298602.5000\n",
      "Epoch [37151/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [37152/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [37153/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [37154/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37155/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [37156/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37157/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [37158/50000], Train Loss: 8088941.0000, Val Loss: 5298662.0000\n",
      "Epoch [37159/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [37160/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [37161/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [37162/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [37163/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37164/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [37165/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37166/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37167/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37168/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37169/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [37170/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [37171/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37172/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [37173/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37174/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [37175/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [37176/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [37177/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37178/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [37179/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [37180/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37181/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [37182/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [37183/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37184/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37185/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37186/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37187/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37188/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37189/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37190/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37191/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37192/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37193/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [37194/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37195/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [37196/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [37197/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [37198/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [37199/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37200/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37201/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [37202/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [37203/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37204/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [37205/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37206/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [37207/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [37208/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37209/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [37210/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37211/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37212/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [37213/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37214/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37215/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37216/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37217/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37218/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [37219/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [37220/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [37221/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [37222/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [37223/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [37224/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [37225/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [37226/50000], Train Loss: 8088941.5000, Val Loss: 5298608.0000\n",
      "Epoch [37227/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [37228/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [37229/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [37230/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [37231/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [37232/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37233/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [37234/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [37235/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37236/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37237/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [37238/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37239/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [37240/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [37241/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [37242/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [37243/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [37244/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [37245/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [37246/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [37247/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [37248/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [37249/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37250/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [37251/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [37252/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [37253/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [37254/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [37255/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37256/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37257/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [37258/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37259/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37260/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [37261/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37262/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [37263/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [37264/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37265/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37266/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37267/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37268/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [37269/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37270/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [37271/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [37272/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [37273/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [37274/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [37275/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [37276/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [37277/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [37278/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [37279/50000], Train Loss: 8088942.5000, Val Loss: 5298679.0000\n",
      "Epoch [37280/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [37281/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [37282/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [37283/50000], Train Loss: 8088941.5000, Val Loss: 5298693.5000\n",
      "Epoch [37284/50000], Train Loss: 8088941.5000, Val Loss: 5298594.5000\n",
      "Epoch [37285/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [37286/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [37287/50000], Train Loss: 8088941.5000, Val Loss: 5298683.0000\n",
      "Epoch [37288/50000], Train Loss: 8088941.5000, Val Loss: 5298613.0000\n",
      "Epoch [37289/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [37290/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [37291/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [37292/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37293/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [37294/50000], Train Loss: 8088941.5000, Val Loss: 5298663.0000\n",
      "Epoch [37295/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [37296/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [37297/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [37298/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [37299/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [37300/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37301/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [37302/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [37303/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37304/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37305/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [37306/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37307/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37308/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [37309/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37310/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [37311/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37312/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37313/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37314/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37315/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37316/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [37317/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37318/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37319/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37320/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37321/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37322/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37323/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37324/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37325/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [37326/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [37327/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37328/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [37329/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [37330/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [37331/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [37332/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37333/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37334/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37335/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [37336/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37337/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37338/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [37339/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [37340/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37341/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [37342/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37343/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [37344/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37345/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37346/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37347/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [37348/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37349/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [37350/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37351/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [37352/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37353/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37354/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [37355/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37356/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37357/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [37358/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [37359/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [37360/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37361/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [37362/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [37363/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [37364/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [37365/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [37366/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [37367/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [37368/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [37369/50000], Train Loss: 8088942.0000, Val Loss: 5298594.5000\n",
      "Epoch [37370/50000], Train Loss: 8088942.0000, Val Loss: 5298698.0000\n",
      "Epoch [37371/50000], Train Loss: 8088942.5000, Val Loss: 5298587.5000\n",
      "Epoch [37372/50000], Train Loss: 8088941.5000, Val Loss: 5298703.0000\n",
      "Epoch [37373/50000], Train Loss: 8088942.0000, Val Loss: 5298584.0000\n",
      "Epoch [37374/50000], Train Loss: 8088942.0000, Val Loss: 5298704.0000\n",
      "Epoch [37375/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [37376/50000], Train Loss: 8088941.0000, Val Loss: 5298695.0000\n",
      "Epoch [37377/50000], Train Loss: 8088942.0000, Val Loss: 5298603.0000\n",
      "Epoch [37378/50000], Train Loss: 8088941.5000, Val Loss: 5298672.5000\n",
      "Epoch [37379/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [37380/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37381/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [37382/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [37383/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [37384/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [37385/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [37386/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [37387/50000], Train Loss: 8088943.0000, Val Loss: 5298665.5000\n",
      "Epoch [37388/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [37389/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [37390/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [37391/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37392/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37393/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [37394/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [37395/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [37396/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37397/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [37398/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [37399/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37400/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [37401/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37402/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [37403/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37404/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37405/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37406/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [37407/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [37408/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37409/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37410/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37411/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37412/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37413/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [37414/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37415/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37416/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37417/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [37418/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [37419/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [37420/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [37421/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [37422/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [37423/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [37424/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [37425/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [37426/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [37427/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [37428/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [37429/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [37430/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [37431/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [37432/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [37433/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [37434/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [37435/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [37436/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [37437/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [37438/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37439/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37440/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [37441/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37442/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37443/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [37444/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [37445/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37446/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [37447/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [37448/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [37449/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [37450/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37451/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [37452/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [37453/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [37454/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [37455/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [37456/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [37457/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [37458/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [37459/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [37460/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [37461/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37462/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [37463/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [37464/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37465/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37466/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [37467/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [37468/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37469/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37470/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37471/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37472/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [37473/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [37474/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [37475/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [37476/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [37477/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [37478/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [37479/50000], Train Loss: 8088941.5000, Val Loss: 5298687.0000\n",
      "Epoch [37480/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [37481/50000], Train Loss: 8088941.5000, Val Loss: 5298693.5000\n",
      "Epoch [37482/50000], Train Loss: 8088941.5000, Val Loss: 5298596.5000\n",
      "Epoch [37483/50000], Train Loss: 8088942.5000, Val Loss: 5298689.5000\n",
      "Epoch [37484/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [37485/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [37486/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [37487/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37488/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37489/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [37490/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37491/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37492/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [37493/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [37494/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [37495/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [37496/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37497/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37498/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37499/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [37500/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [37501/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [37502/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [37503/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [37504/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [37505/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [37506/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37507/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37508/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37509/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [37510/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37511/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37512/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37513/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [37514/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [37515/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [37516/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37517/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37518/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [37519/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37520/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [37521/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37522/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37523/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [37524/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37525/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [37526/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [37527/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37528/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37529/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [37530/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [37531/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37532/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37533/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [37534/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37535/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37536/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37537/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37538/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [37539/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [37540/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37541/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37542/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [37543/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [37544/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [37545/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [37546/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [37547/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [37548/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [37549/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [37550/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [37551/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [37552/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [37553/50000], Train Loss: 8088942.0000, Val Loss: 5298585.0000\n",
      "Epoch [37554/50000], Train Loss: 8088942.0000, Val Loss: 5298703.0000\n",
      "Epoch [37555/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [37556/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [37557/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [37558/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [37559/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37560/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [37561/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [37562/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [37563/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [37564/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [37565/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [37566/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [37567/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [37568/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37569/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [37570/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [37571/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [37572/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [37573/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [37574/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [37575/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37576/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [37577/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [37578/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [37579/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [37580/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [37581/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37582/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37583/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [37584/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37585/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [37586/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [37587/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [37588/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37589/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [37590/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [37591/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [37592/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37593/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [37594/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [37595/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [37596/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37597/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [37598/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [37599/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [37600/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [37601/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [37602/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37603/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [37604/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37605/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [37606/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37607/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [37608/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37609/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [37610/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37611/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [37612/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37613/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37614/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37615/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [37616/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [37617/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [37618/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [37619/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [37620/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37621/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37622/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [37623/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [37624/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [37625/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [37626/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [37627/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [37628/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [37629/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [37630/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [37631/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [37632/50000], Train Loss: 8088943.0000, Val Loss: 5298671.0000\n",
      "Epoch [37633/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [37634/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [37635/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37636/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [37637/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [37638/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37639/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37640/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37641/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [37642/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37643/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [37644/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37645/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [37646/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [37647/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [37648/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [37649/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [37650/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [37651/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [37652/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [37653/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [37654/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [37655/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [37656/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [37657/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [37658/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [37659/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [37660/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [37661/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [37662/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [37663/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [37664/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37665/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [37666/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37667/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [37668/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [37669/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [37670/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [37671/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [37672/50000], Train Loss: 8088941.5000, Val Loss: 5298659.0000\n",
      "Epoch [37673/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [37674/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [37675/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [37676/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [37677/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [37678/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [37679/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [37680/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [37681/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [37682/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [37683/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [37684/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [37685/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [37686/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [37687/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [37688/50000], Train Loss: 8088941.5000, Val Loss: 5298670.0000\n",
      "Epoch [37689/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [37690/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [37691/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [37692/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [37693/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [37694/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [37695/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37696/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [37697/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37698/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [37699/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [37700/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [37701/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37702/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [37703/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [37704/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [37705/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [37706/50000], Train Loss: 8088941.5000, Val Loss: 5298675.5000\n",
      "Epoch [37707/50000], Train Loss: 8088942.5000, Val Loss: 5298609.0000\n",
      "Epoch [37708/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [37709/50000], Train Loss: 8088941.5000, Val Loss: 5298603.0000\n",
      "Epoch [37710/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [37711/50000], Train Loss: 8088941.5000, Val Loss: 5298601.5000\n",
      "Epoch [37712/50000], Train Loss: 8088941.5000, Val Loss: 5298685.5000\n",
      "Epoch [37713/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [37714/50000], Train Loss: 8088941.5000, Val Loss: 5298678.5000\n",
      "Epoch [37715/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [37716/50000], Train Loss: 8088941.5000, Val Loss: 5298666.5000\n",
      "Epoch [37717/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [37718/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [37719/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37720/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37721/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [37722/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [37723/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [37724/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [37725/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [37726/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [37727/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [37728/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37729/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [37730/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [37731/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37732/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37733/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37734/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [37735/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [37736/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [37737/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [37738/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [37739/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37740/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37741/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [37742/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [37743/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [37744/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37745/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [37746/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [37747/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [37748/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37749/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [37750/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [37751/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [37752/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [37753/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [37754/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [37755/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [37756/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [37757/50000], Train Loss: 8088941.5000, Val Loss: 5298619.5000\n",
      "Epoch [37758/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [37759/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [37760/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [37761/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [37762/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [37763/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [37764/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [37765/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [37766/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [37767/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [37768/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [37769/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [37770/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [37771/50000], Train Loss: 8088941.5000, Val Loss: 5298621.0000\n",
      "Epoch [37772/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [37773/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [37774/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [37775/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [37776/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [37777/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37778/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37779/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37780/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37781/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37782/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37783/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37784/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37785/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37786/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37787/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37788/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [37789/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [37790/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37791/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37792/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [37793/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [37794/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [37795/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37796/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [37797/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37798/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37799/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37800/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [37801/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [37802/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37803/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [37804/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [37805/50000], Train Loss: 8088941.0000, Val Loss: 5298655.5000\n",
      "Epoch [37806/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [37807/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [37808/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37809/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [37810/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37811/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [37812/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37813/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37814/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37815/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37816/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37817/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37818/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37819/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37820/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37821/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37822/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [37823/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37824/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37825/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [37826/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37827/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [37828/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [37829/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [37830/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37831/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [37832/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [37833/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [37834/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [37835/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [37836/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [37837/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [37838/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [37839/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [37840/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [37841/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [37842/50000], Train Loss: 8088942.0000, Val Loss: 5298708.5000\n",
      "Epoch [37843/50000], Train Loss: 8088942.0000, Val Loss: 5298571.5000\n",
      "Epoch [37844/50000], Train Loss: 8088942.0000, Val Loss: 5298725.0000\n",
      "Epoch [37845/50000], Train Loss: 8088942.5000, Val Loss: 5298559.5000\n",
      "Epoch [37846/50000], Train Loss: 8088942.0000, Val Loss: 5298729.0000\n",
      "Epoch [37847/50000], Train Loss: 8088942.5000, Val Loss: 5298567.5000\n",
      "Epoch [37848/50000], Train Loss: 8088941.5000, Val Loss: 5298706.0000\n",
      "Epoch [37849/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [37850/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37851/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [37852/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [37853/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [37854/50000], Train Loss: 8088942.5000, Val Loss: 5298588.5000\n",
      "Epoch [37855/50000], Train Loss: 8088941.5000, Val Loss: 5298695.5000\n",
      "Epoch [37856/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [37857/50000], Train Loss: 8088941.5000, Val Loss: 5298664.0000\n",
      "Epoch [37858/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37859/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [37860/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [37861/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [37862/50000], Train Loss: 8088942.5000, Val Loss: 5298678.0000\n",
      "Epoch [37863/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [37864/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [37865/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37866/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [37867/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [37868/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [37869/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [37870/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [37871/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37872/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37873/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [37874/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [37875/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [37876/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [37877/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [37878/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [37879/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37880/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37881/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [37882/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [37883/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [37884/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [37885/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37886/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37887/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [37888/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37889/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [37890/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [37891/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [37892/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [37893/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37894/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37895/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37896/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [37897/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [37898/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [37899/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [37900/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [37901/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37902/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37903/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [37904/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37905/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [37906/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [37907/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37908/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37909/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37910/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [37911/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37912/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [37913/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37914/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37915/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [37916/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [37917/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37918/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37919/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37920/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [37921/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37922/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37923/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37924/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37925/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37926/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [37927/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [37928/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [37929/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [37930/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [37931/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37932/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37933/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [37934/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37935/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37936/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37937/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37938/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [37939/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [37940/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [37941/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [37942/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37943/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [37944/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [37945/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [37946/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37947/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [37948/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37949/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [37950/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37951/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [37952/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37953/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37954/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [37955/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37956/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [37957/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [37958/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [37959/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [37960/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37961/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [37962/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [37963/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [37964/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [37965/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [37966/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [37967/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [37968/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37969/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37970/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [37971/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [37972/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37973/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [37974/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [37975/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [37976/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [37977/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [37978/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [37979/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [37980/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [37981/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [37982/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [37983/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [37984/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [37985/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [37986/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [37987/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [37988/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [37989/50000], Train Loss: 8088942.5000, Val Loss: 5298683.5000\n",
      "Epoch [37990/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [37991/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [37992/50000], Train Loss: 8088941.5000, Val Loss: 5298596.5000\n",
      "Epoch [37993/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [37994/50000], Train Loss: 8088941.5000, Val Loss: 5298600.5000\n",
      "Epoch [37995/50000], Train Loss: 8088941.5000, Val Loss: 5298683.0000\n",
      "Epoch [37996/50000], Train Loss: 8088941.5000, Val Loss: 5298611.5000\n",
      "Epoch [37997/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [37998/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [37999/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [38000/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38001/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38002/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [38003/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [38004/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [38005/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [38006/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [38007/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [38008/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [38009/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [38010/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38011/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [38012/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [38013/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38014/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [38015/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38016/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [38017/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [38018/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [38019/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38020/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38021/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38022/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38023/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38024/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38025/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38026/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [38027/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [38028/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [38029/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38030/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [38031/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38032/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38033/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38034/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38035/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38036/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38037/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38038/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38039/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38040/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38041/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38042/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38043/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38044/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38045/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [38046/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38047/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [38048/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38049/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38050/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38051/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38052/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38053/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38054/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [38055/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38056/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [38057/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38058/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38059/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38060/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [38061/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38062/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38063/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [38064/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38065/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [38066/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38067/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38068/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38069/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [38070/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38071/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [38072/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [38073/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38074/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [38075/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [38076/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [38077/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [38078/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [38079/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [38080/50000], Train Loss: 8088942.5000, Val Loss: 5298688.5000\n",
      "Epoch [38081/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [38082/50000], Train Loss: 8088942.0000, Val Loss: 5298710.0000\n",
      "Epoch [38083/50000], Train Loss: 8088942.0000, Val Loss: 5298566.0000\n",
      "Epoch [38084/50000], Train Loss: 8088942.0000, Val Loss: 5298734.5000\n",
      "Epoch [38085/50000], Train Loss: 8088941.5000, Val Loss: 5298548.5000\n",
      "Epoch [38086/50000], Train Loss: 8088941.5000, Val Loss: 5298739.5000\n",
      "Epoch [38087/50000], Train Loss: 8088941.5000, Val Loss: 5298559.5000\n",
      "Epoch [38088/50000], Train Loss: 8088942.0000, Val Loss: 5298707.0000\n",
      "Epoch [38089/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [38090/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38091/50000], Train Loss: 8088941.5000, Val Loss: 5298675.0000\n",
      "Epoch [38092/50000], Train Loss: 8088942.0000, Val Loss: 5298592.0000\n",
      "Epoch [38093/50000], Train Loss: 8088942.0000, Val Loss: 5298707.0000\n",
      "Epoch [38094/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [38095/50000], Train Loss: 8088941.5000, Val Loss: 5298691.5000\n",
      "Epoch [38096/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [38097/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38098/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [38099/50000], Train Loss: 8088942.5000, Val Loss: 5298600.5000\n",
      "Epoch [38100/50000], Train Loss: 8088941.5000, Val Loss: 5298693.5000\n",
      "Epoch [38101/50000], Train Loss: 8088941.5000, Val Loss: 5298602.0000\n",
      "Epoch [38102/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [38103/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38104/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38105/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [38106/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [38107/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [38108/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [38109/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [38110/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38111/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [38112/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [38113/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [38114/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [38115/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38116/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [38117/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38118/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [38119/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [38120/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [38121/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [38122/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38123/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38124/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [38125/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38126/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38127/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38128/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [38129/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38130/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38131/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38132/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38133/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [38134/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38135/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [38136/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [38137/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38138/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38139/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [38140/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38141/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38142/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38143/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38144/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [38145/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [38146/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38147/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38148/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38149/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38150/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [38151/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38152/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38153/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38154/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38155/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38156/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38157/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [38158/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38159/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38160/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38161/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38162/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [38163/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38164/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [38165/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38166/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38167/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38168/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38169/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38170/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38171/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38172/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38173/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38174/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38175/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [38176/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38177/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38178/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38179/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38180/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [38181/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38182/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38183/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [38184/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38185/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38186/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38187/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38188/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38189/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [38190/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38191/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [38192/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38193/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [38194/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38195/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [38196/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38197/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38198/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [38199/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38200/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38201/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38202/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38203/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38204/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38205/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [38206/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38207/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38208/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38209/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38210/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [38211/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38212/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [38213/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38214/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38215/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [38216/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38217/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38218/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [38219/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38220/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38221/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [38222/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [38223/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38224/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38225/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38226/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38227/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [38228/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38229/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38230/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38231/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [38232/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38233/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [38234/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38235/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38236/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38237/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38238/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38239/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38240/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [38241/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [38242/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [38243/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [38244/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [38245/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [38246/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [38247/50000], Train Loss: 8088942.5000, Val Loss: 5298633.5000\n",
      "Epoch [38248/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38249/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [38250/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38251/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38252/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [38253/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38254/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38255/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38256/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [38257/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38258/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [38259/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38260/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38261/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38262/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38263/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38264/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [38265/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38266/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38267/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [38268/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38269/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [38270/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38271/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38272/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38273/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [38274/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38275/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38276/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38277/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [38278/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [38279/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38280/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [38281/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [38282/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38283/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [38284/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [38285/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [38286/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [38287/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [38288/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [38289/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [38290/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [38291/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [38292/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [38293/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [38294/50000], Train Loss: 8088942.5000, Val Loss: 5298611.5000\n",
      "Epoch [38295/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [38296/50000], Train Loss: 8088941.5000, Val Loss: 5298597.5000\n",
      "Epoch [38297/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [38298/50000], Train Loss: 8088942.0000, Val Loss: 5298580.5000\n",
      "Epoch [38299/50000], Train Loss: 8088942.5000, Val Loss: 5298716.5000\n",
      "Epoch [38300/50000], Train Loss: 8088942.5000, Val Loss: 5298566.0000\n",
      "Epoch [38301/50000], Train Loss: 8088942.5000, Val Loss: 5298725.0000\n",
      "Epoch [38302/50000], Train Loss: 8088941.5000, Val Loss: 5298567.0000\n",
      "Epoch [38303/50000], Train Loss: 8088941.5000, Val Loss: 5298712.0000\n",
      "Epoch [38304/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [38305/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [38306/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [38307/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [38308/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [38309/50000], Train Loss: 8088942.5000, Val Loss: 5298600.5000\n",
      "Epoch [38310/50000], Train Loss: 8088941.5000, Val Loss: 5298691.5000\n",
      "Epoch [38311/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [38312/50000], Train Loss: 8088941.5000, Val Loss: 5298678.0000\n",
      "Epoch [38313/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [38314/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [38315/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [38316/50000], Train Loss: 8088941.5000, Val Loss: 5298623.0000\n",
      "Epoch [38317/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [38318/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [38319/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [38320/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [38321/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [38322/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38323/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [38324/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [38325/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [38326/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [38327/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [38328/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [38329/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38330/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [38331/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38332/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38333/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [38334/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [38335/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [38336/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38337/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38338/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38339/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [38340/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38341/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [38342/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38343/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38344/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38345/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38346/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [38347/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38348/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38349/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38350/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [38351/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [38352/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38353/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [38354/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38355/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [38356/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38357/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38358/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38359/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38360/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [38361/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38362/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [38363/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [38364/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38365/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38366/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38367/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38368/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38369/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [38370/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [38371/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [38372/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [38373/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38374/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38375/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [38376/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38377/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38378/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [38379/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38380/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38381/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [38382/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38383/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38384/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38385/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38386/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [38387/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38388/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [38389/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [38390/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [38391/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [38392/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38393/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38394/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38395/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [38396/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38397/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38398/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [38399/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38400/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38401/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [38402/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38403/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38404/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38405/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [38406/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38407/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [38408/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [38409/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38410/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [38411/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38412/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [38413/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38414/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38415/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [38416/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38417/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38418/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38419/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [38420/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [38421/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38422/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [38423/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38424/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38425/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38426/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [38427/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38428/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38429/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38430/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38431/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [38432/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38433/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38434/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38435/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38436/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [38437/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38438/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [38439/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38440/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38441/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38442/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38443/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38444/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [38445/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [38446/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38447/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [38448/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [38449/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [38450/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [38451/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [38452/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [38453/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [38454/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [38455/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [38456/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [38457/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [38458/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [38459/50000], Train Loss: 8088942.0000, Val Loss: 5298706.0000\n",
      "Epoch [38460/50000], Train Loss: 8088942.0000, Val Loss: 5298577.5000\n",
      "Epoch [38461/50000], Train Loss: 8088942.0000, Val Loss: 5298715.5000\n",
      "Epoch [38462/50000], Train Loss: 8088942.0000, Val Loss: 5298571.5000\n",
      "Epoch [38463/50000], Train Loss: 8088942.0000, Val Loss: 5298715.0000\n",
      "Epoch [38464/50000], Train Loss: 8088942.0000, Val Loss: 5298581.5000\n",
      "Epoch [38465/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [38466/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [38467/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [38468/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [38469/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [38470/50000], Train Loss: 8088942.5000, Val Loss: 5298680.0000\n",
      "Epoch [38471/50000], Train Loss: 8088942.5000, Val Loss: 5298601.5000\n",
      "Epoch [38472/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [38473/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [38474/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [38475/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [38476/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [38477/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38478/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [38479/50000], Train Loss: 8088941.0000, Val Loss: 5298664.5000\n",
      "Epoch [38480/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [38481/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [38482/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [38483/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [38484/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38485/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38486/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [38487/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [38488/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [38489/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [38490/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [38491/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38492/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [38493/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38494/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38495/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38496/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38497/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [38498/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [38499/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [38500/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38501/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38502/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38503/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [38504/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38505/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38506/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [38507/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [38508/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [38509/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [38510/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38511/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38512/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [38513/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38514/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [38515/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38516/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38517/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38518/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38519/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38520/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38521/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38522/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [38523/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38524/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38525/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38526/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [38527/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38528/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38529/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38530/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [38531/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38532/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38533/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38534/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38535/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [38536/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38537/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [38538/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [38539/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38540/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38541/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38542/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [38543/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [38544/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [38545/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38546/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38547/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [38548/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38549/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38550/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [38551/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38552/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38553/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38554/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38555/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38556/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38557/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [38558/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38559/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38560/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38561/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38562/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [38563/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [38564/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [38565/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [38566/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38567/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [38568/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38569/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [38570/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [38571/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38572/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38573/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38574/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [38575/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38576/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [38577/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [38578/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38579/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38580/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38581/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38582/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38583/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38584/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38585/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38586/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [38587/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [38588/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [38589/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [38590/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [38591/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [38592/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [38593/50000], Train Loss: 8088942.5000, Val Loss: 5298603.0000\n",
      "Epoch [38594/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [38595/50000], Train Loss: 8088941.5000, Val Loss: 5298592.5000\n",
      "Epoch [38596/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [38597/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [38598/50000], Train Loss: 8088941.5000, Val Loss: 5298703.0000\n",
      "Epoch [38599/50000], Train Loss: 8088942.0000, Val Loss: 5298588.5000\n",
      "Epoch [38600/50000], Train Loss: 8088941.5000, Val Loss: 5298694.5000\n",
      "Epoch [38601/50000], Train Loss: 8088942.5000, Val Loss: 5298603.0000\n",
      "Epoch [38602/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [38603/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [38604/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [38605/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38606/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [38607/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [38608/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [38609/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [38610/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [38611/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [38612/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [38613/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [38614/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38615/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [38616/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38617/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38618/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [38619/50000], Train Loss: 8088941.5000, Val Loss: 5298626.0000\n",
      "Epoch [38620/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [38621/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [38622/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [38623/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38624/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38625/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38626/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38627/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38628/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [38629/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38630/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38631/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38632/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38633/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38634/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38635/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [38636/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38637/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [38638/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [38639/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38640/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38641/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38642/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38643/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [38644/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38645/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38646/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38647/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38648/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38649/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38650/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [38651/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38652/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38653/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38654/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38655/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [38656/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38657/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38658/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38659/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [38660/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38661/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [38662/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38663/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38664/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38665/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38666/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [38667/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38668/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38669/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [38670/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [38671/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [38672/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [38673/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [38674/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [38675/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [38676/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [38677/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38678/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [38679/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38680/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38681/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38682/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38683/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38684/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38685/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [38686/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38687/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38688/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38689/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38690/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [38691/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38692/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [38693/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [38694/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38695/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [38696/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [38697/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [38698/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [38699/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [38700/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [38701/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [38702/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38703/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [38704/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [38705/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38706/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [38707/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38708/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38709/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38710/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38711/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38712/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38713/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38714/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38715/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38716/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38717/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38718/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [38719/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38720/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [38721/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [38722/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38723/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [38724/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [38725/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [38726/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [38727/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [38728/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [38729/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [38730/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [38731/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [38732/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [38733/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [38734/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [38735/50000], Train Loss: 8088942.5000, Val Loss: 5298690.5000\n",
      "Epoch [38736/50000], Train Loss: 8088941.5000, Val Loss: 5298584.0000\n",
      "Epoch [38737/50000], Train Loss: 8088942.0000, Val Loss: 5298719.0000\n",
      "Epoch [38738/50000], Train Loss: 8088942.5000, Val Loss: 5298552.5000\n",
      "Epoch [38739/50000], Train Loss: 8088942.0000, Val Loss: 5298752.5000\n",
      "Epoch [38740/50000], Train Loss: 8088941.5000, Val Loss: 5298526.5000\n",
      "Epoch [38741/50000], Train Loss: 8088942.0000, Val Loss: 5298760.5000\n",
      "Epoch [38742/50000], Train Loss: 8088942.0000, Val Loss: 5298547.0000\n",
      "Epoch [38743/50000], Train Loss: 8088942.0000, Val Loss: 5298705.0000\n",
      "Epoch [38744/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [38745/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [38746/50000], Train Loss: 8088941.5000, Val Loss: 5298704.5000\n",
      "Epoch [38747/50000], Train Loss: 8088942.0000, Val Loss: 5298571.5000\n",
      "Epoch [38748/50000], Train Loss: 8088942.0000, Val Loss: 5298710.0000\n",
      "Epoch [38749/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [38750/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [38751/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [38752/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [38753/50000], Train Loss: 8088942.0000, Val Loss: 5298696.5000\n",
      "Epoch [38754/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [38755/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [38756/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [38757/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [38758/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [38759/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [38760/50000], Train Loss: 8088941.5000, Val Loss: 5298672.5000\n",
      "Epoch [38761/50000], Train Loss: 8088942.5000, Val Loss: 5298634.0000\n",
      "Epoch [38762/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38763/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [38764/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [38765/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [38766/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [38767/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38768/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [38769/50000], Train Loss: 8088941.5000, Val Loss: 5298623.0000\n",
      "Epoch [38770/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [38771/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [38772/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38773/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38774/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [38775/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [38776/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38777/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [38778/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38779/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38780/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [38781/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [38782/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38783/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38784/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38785/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38786/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38787/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38788/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38789/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [38790/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [38791/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38792/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38793/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [38794/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38795/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38796/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38797/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38798/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [38799/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [38800/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38801/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [38802/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [38803/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38804/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [38805/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38806/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38807/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38808/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38809/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38810/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38811/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38812/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38813/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38814/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38815/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [38816/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [38817/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38818/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38819/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38820/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38821/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38822/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38823/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38824/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38825/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38826/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [38827/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38828/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38829/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [38830/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38831/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [38832/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38833/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38834/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38835/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38836/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38837/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38838/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38839/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [38840/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38841/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38842/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38843/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [38844/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38845/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38846/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38847/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [38848/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [38849/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38850/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [38851/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [38852/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [38853/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38854/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [38855/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38856/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38857/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38858/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38859/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38860/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [38861/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38862/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38863/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38864/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38865/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38866/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [38867/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [38868/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38869/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38870/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [38871/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38872/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [38873/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38874/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38875/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38876/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38877/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38878/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [38879/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38880/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38881/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38882/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38883/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [38884/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38885/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [38886/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38887/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [38888/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38889/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38890/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38891/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [38892/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38893/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38894/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [38895/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [38896/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [38897/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38898/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [38899/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [38900/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38901/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [38902/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38903/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [38904/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38905/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [38906/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [38907/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [38908/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [38909/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38910/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38911/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [38912/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38913/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [38914/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [38915/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [38916/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [38917/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38918/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38919/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38920/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38921/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38922/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [38923/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38924/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38925/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38926/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38927/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38928/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [38929/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [38930/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [38931/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [38932/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [38933/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [38934/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [38935/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [38936/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [38937/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [38938/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38939/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38940/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38941/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [38942/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [38943/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38944/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38945/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38946/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38947/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38948/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38949/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38950/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38951/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [38952/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [38953/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38954/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [38955/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [38956/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38957/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [38958/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38959/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [38960/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [38961/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [38962/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [38963/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38964/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [38965/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38966/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [38967/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38968/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [38969/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38970/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [38971/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38972/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [38973/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38974/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38975/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38976/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38977/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38978/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38979/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [38980/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38981/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [38982/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38983/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38984/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [38985/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [38986/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38987/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38988/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [38989/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [38990/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [38991/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [38992/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38993/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [38994/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [38995/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [38996/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [38997/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [38998/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [38999/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [39000/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39001/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39002/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39003/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [39004/50000], Train Loss: 8088941.0000, Val Loss: 5298664.5000\n",
      "Epoch [39005/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [39006/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [39007/50000], Train Loss: 8088942.5000, Val Loss: 5298590.5000\n",
      "Epoch [39008/50000], Train Loss: 8088942.0000, Val Loss: 5298714.0000\n",
      "Epoch [39009/50000], Train Loss: 8088942.0000, Val Loss: 5298557.0000\n",
      "Epoch [39010/50000], Train Loss: 8088942.0000, Val Loss: 5298750.0000\n",
      "Epoch [39011/50000], Train Loss: 8088942.0000, Val Loss: 5298525.5000\n",
      "Epoch [39012/50000], Train Loss: 8088942.5000, Val Loss: 5298766.5000\n",
      "Epoch [39013/50000], Train Loss: 8088942.0000, Val Loss: 5298536.5000\n",
      "Epoch [39014/50000], Train Loss: 8088942.0000, Val Loss: 5298719.0000\n",
      "Epoch [39015/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [39016/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [39017/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [39018/50000], Train Loss: 8088942.0000, Val Loss: 5298567.5000\n",
      "Epoch [39019/50000], Train Loss: 8088941.5000, Val Loss: 5298720.0000\n",
      "Epoch [39020/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [39021/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [39022/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [39023/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [39024/50000], Train Loss: 8088942.0000, Val Loss: 5298702.0000\n",
      "Epoch [39025/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [39026/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [39027/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39028/50000], Train Loss: 8088941.5000, Val Loss: 5298620.5000\n",
      "Epoch [39029/50000], Train Loss: 8088942.5000, Val Loss: 5298683.0000\n",
      "Epoch [39030/50000], Train Loss: 8088941.5000, Val Loss: 5298606.0000\n",
      "Epoch [39031/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [39032/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [39033/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39034/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [39035/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [39036/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [39037/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [39038/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39039/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [39040/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [39041/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [39042/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39043/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [39044/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [39045/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39046/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [39047/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39048/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39049/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [39050/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [39051/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39052/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [39053/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [39054/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39055/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39056/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [39057/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39058/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [39059/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39060/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39061/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39062/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39063/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [39064/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39065/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39066/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [39067/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39068/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [39069/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [39070/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39071/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39072/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39073/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39074/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39075/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39076/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39077/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39078/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [39079/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39080/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39081/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [39082/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39083/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [39084/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39085/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39086/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [39087/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [39088/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39089/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39090/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [39091/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39092/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39093/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [39094/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39095/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39096/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39097/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39098/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39099/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39100/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39101/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39102/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39103/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [39104/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39105/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39106/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [39107/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39108/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39109/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39110/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39111/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39112/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39113/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39114/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [39115/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39116/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39117/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39118/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39119/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39120/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39121/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [39122/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39123/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39124/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39125/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39126/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [39127/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39128/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [39129/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39130/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39131/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39132/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39133/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39134/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39135/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39136/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [39137/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39138/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39139/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [39140/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [39141/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39142/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39143/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39144/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [39145/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [39146/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39147/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [39148/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39149/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [39150/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [39151/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [39152/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39153/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [39154/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39155/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [39156/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39157/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39158/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39159/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [39160/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39161/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [39162/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39163/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39164/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [39165/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39166/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39167/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39168/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39169/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39170/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39171/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39172/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39173/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [39174/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39175/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [39176/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [39177/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39178/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39179/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [39180/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39181/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [39182/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39183/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39184/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [39185/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39186/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [39187/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39188/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39189/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39190/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39191/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39192/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [39193/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39194/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [39195/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39196/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39197/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [39198/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39199/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39200/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39201/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39202/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39203/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39204/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [39205/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39206/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [39207/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39208/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39209/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [39210/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39211/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [39212/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39213/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39214/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [39215/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39216/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39217/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39218/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39219/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39220/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39221/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39222/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [39223/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39224/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39225/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39226/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [39227/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39228/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39229/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39230/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39231/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [39232/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39233/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [39234/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [39235/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39236/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [39237/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39238/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [39239/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39240/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [39241/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [39242/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [39243/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39244/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39245/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [39246/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [39247/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [39248/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39249/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [39250/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [39251/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [39252/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [39253/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [39254/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [39255/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [39256/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [39257/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [39258/50000], Train Loss: 8088942.5000, Val Loss: 5298612.5000\n",
      "Epoch [39259/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [39260/50000], Train Loss: 8088941.5000, Val Loss: 5298602.0000\n",
      "Epoch [39261/50000], Train Loss: 8088941.5000, Val Loss: 5298691.5000\n",
      "Epoch [39262/50000], Train Loss: 8088942.0000, Val Loss: 5298592.0000\n",
      "Epoch [39263/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [39264/50000], Train Loss: 8088942.0000, Val Loss: 5298585.0000\n",
      "Epoch [39265/50000], Train Loss: 8088942.0000, Val Loss: 5298704.5000\n",
      "Epoch [39266/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [39267/50000], Train Loss: 8088941.5000, Val Loss: 5298697.0000\n",
      "Epoch [39268/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [39269/50000], Train Loss: 8088941.5000, Val Loss: 5298678.0000\n",
      "Epoch [39270/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [39271/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39272/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39273/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [39274/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [39275/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [39276/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [39277/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [39278/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [39279/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [39280/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [39281/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [39282/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [39283/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [39284/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39285/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39286/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [39287/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [39288/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [39289/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [39290/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [39291/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [39292/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [39293/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [39294/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [39295/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [39296/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [39297/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [39298/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [39299/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [39300/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39301/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [39302/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [39303/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39304/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [39305/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [39306/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39307/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39308/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [39309/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39310/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [39311/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [39312/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [39313/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [39314/50000], Train Loss: 8088941.0000, Val Loss: 5298659.0000\n",
      "Epoch [39315/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39316/50000], Train Loss: 8088942.5000, Val Loss: 5298659.0000\n",
      "Epoch [39317/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [39318/50000], Train Loss: 8088941.0000, Val Loss: 5298655.5000\n",
      "Epoch [39319/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [39320/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [39321/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39322/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [39323/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [39324/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39325/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39326/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [39327/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39328/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39329/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [39330/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [39331/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [39332/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39333/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39334/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [39335/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39336/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39337/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [39338/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [39339/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [39340/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [39341/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [39342/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39343/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [39344/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [39345/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [39346/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39347/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [39348/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [39349/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [39350/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [39351/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [39352/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39353/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [39354/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39355/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [39356/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39357/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [39358/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39359/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [39360/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39361/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [39362/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [39363/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [39364/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39365/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39366/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39367/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39368/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [39369/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39370/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [39371/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39372/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39373/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39374/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [39375/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [39376/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [39377/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [39378/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [39379/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [39380/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [39381/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [39382/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [39383/50000], Train Loss: 8088942.0000, Val Loss: 5298595.0000\n",
      "Epoch [39384/50000], Train Loss: 8088942.0000, Val Loss: 5298702.0000\n",
      "Epoch [39385/50000], Train Loss: 8088942.0000, Val Loss: 5298577.5000\n",
      "Epoch [39386/50000], Train Loss: 8088942.0000, Val Loss: 5298720.5000\n",
      "Epoch [39387/50000], Train Loss: 8088942.0000, Val Loss: 5298561.0000\n",
      "Epoch [39388/50000], Train Loss: 8088941.5000, Val Loss: 5298730.0000\n",
      "Epoch [39389/50000], Train Loss: 8088942.0000, Val Loss: 5298563.0000\n",
      "Epoch [39390/50000], Train Loss: 8088942.0000, Val Loss: 5298713.0000\n",
      "Epoch [39391/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [39392/50000], Train Loss: 8088941.5000, Val Loss: 5298672.5000\n",
      "Epoch [39393/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [39394/50000], Train Loss: 8088941.0000, Val Loss: 5298627.5000\n",
      "Epoch [39395/50000], Train Loss: 8088942.5000, Val Loss: 5298678.0000\n",
      "Epoch [39396/50000], Train Loss: 8088942.5000, Val Loss: 5298600.5000\n",
      "Epoch [39397/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [39398/50000], Train Loss: 8088941.5000, Val Loss: 5298602.5000\n",
      "Epoch [39399/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [39400/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [39401/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39402/50000], Train Loss: 8088941.5000, Val Loss: 5298661.5000\n",
      "Epoch [39403/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [39404/50000], Train Loss: 8088941.5000, Val Loss: 5298678.5000\n",
      "Epoch [39405/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [39406/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [39407/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [39408/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [39409/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [39410/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [39411/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [39412/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [39413/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [39414/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [39415/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [39416/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39417/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [39418/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [39419/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [39420/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [39421/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39422/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39423/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39424/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39425/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39426/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [39427/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39428/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39429/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [39430/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39431/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39432/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [39433/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39434/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [39435/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39436/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39437/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39438/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39439/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39440/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39441/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [39442/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39443/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [39444/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39445/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [39446/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39447/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [39448/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [39449/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [39450/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39451/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [39452/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [39453/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39454/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39455/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39456/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39457/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39458/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39459/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39460/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39461/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [39462/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39463/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [39464/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39465/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39466/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39467/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [39468/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [39469/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [39470/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39471/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39472/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [39473/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [39474/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [39475/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39476/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [39477/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39478/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [39479/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39480/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39481/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39482/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39483/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [39484/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [39485/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [39486/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39487/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39488/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39489/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [39490/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39491/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39492/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39493/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39494/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [39495/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [39496/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [39497/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [39498/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [39499/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [39500/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [39501/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [39502/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [39503/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [39504/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39505/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39506/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [39507/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [39508/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [39509/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [39510/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [39511/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [39512/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [39513/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [39514/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [39515/50000], Train Loss: 8088941.5000, Val Loss: 5298620.5000\n",
      "Epoch [39516/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [39517/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [39518/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [39519/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39520/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [39521/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [39522/50000], Train Loss: 8088941.5000, Val Loss: 5298654.5000\n",
      "Epoch [39523/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [39524/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39525/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [39526/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39527/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39528/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39529/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39530/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39531/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39532/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39533/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39534/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [39535/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39536/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39537/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39538/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [39539/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39540/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [39541/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [39542/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [39543/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39544/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39545/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39546/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [39547/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39548/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39549/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39550/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39551/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39552/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39553/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [39554/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39555/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39556/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [39557/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [39558/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39559/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [39560/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39561/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [39562/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39563/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [39564/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39565/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [39566/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39567/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39568/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39569/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [39570/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [39571/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [39572/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [39573/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [39574/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [39575/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [39576/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [39577/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [39578/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [39579/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [39580/50000], Train Loss: 8088942.5000, Val Loss: 5298683.5000\n",
      "Epoch [39581/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [39582/50000], Train Loss: 8088941.5000, Val Loss: 5298699.5000\n",
      "Epoch [39583/50000], Train Loss: 8088942.0000, Val Loss: 5298581.0000\n",
      "Epoch [39584/50000], Train Loss: 8088942.5000, Val Loss: 5298715.5000\n",
      "Epoch [39585/50000], Train Loss: 8088942.0000, Val Loss: 5298566.0000\n",
      "Epoch [39586/50000], Train Loss: 8088942.0000, Val Loss: 5298727.5000\n",
      "Epoch [39587/50000], Train Loss: 8088942.0000, Val Loss: 5298562.5000\n",
      "Epoch [39588/50000], Train Loss: 8088942.5000, Val Loss: 5298718.5000\n",
      "Epoch [39589/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [39590/50000], Train Loss: 8088941.5000, Val Loss: 5298680.0000\n",
      "Epoch [39591/50000], Train Loss: 8088942.5000, Val Loss: 5298634.0000\n",
      "Epoch [39592/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [39593/50000], Train Loss: 8088941.0000, Val Loss: 5298677.0000\n",
      "Epoch [39594/50000], Train Loss: 8088942.5000, Val Loss: 5298598.0000\n",
      "Epoch [39595/50000], Train Loss: 8088942.0000, Val Loss: 5298695.5000\n",
      "Epoch [39596/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [39597/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [39598/50000], Train Loss: 8088941.5000, Val Loss: 5298625.0000\n",
      "Epoch [39599/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [39600/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [39601/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [39602/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [39603/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [39604/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [39605/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [39606/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39607/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39608/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [39609/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [39610/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [39611/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [39612/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [39613/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39614/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [39615/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [39616/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [39617/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [39618/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [39619/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [39620/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39621/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [39622/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [39623/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [39624/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [39625/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [39626/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [39627/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [39628/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39629/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [39630/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [39631/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [39632/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39633/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [39634/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [39635/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39636/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [39637/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [39638/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39639/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39640/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [39641/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39642/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39643/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [39644/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39645/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39646/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [39647/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39648/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39649/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [39650/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39651/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39652/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39653/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39654/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39655/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [39656/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39657/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39658/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39659/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39660/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39661/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [39662/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [39663/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [39664/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39665/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39666/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39667/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [39668/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [39669/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [39670/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39671/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39672/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [39673/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39674/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [39675/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39676/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39677/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [39678/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39679/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39680/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39681/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [39682/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39683/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [39684/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39685/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39686/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39687/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39688/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [39689/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39690/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39691/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39692/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [39693/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39694/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [39695/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [39696/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39697/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [39698/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [39699/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39700/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39701/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39702/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39703/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [39704/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [39705/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [39706/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [39707/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [39708/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [39709/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [39710/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [39711/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [39712/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [39713/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [39714/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [39715/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [39716/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [39717/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39718/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [39719/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [39720/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [39721/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39722/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39723/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [39724/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [39725/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [39726/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [39727/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [39728/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [39729/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [39730/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [39731/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [39732/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [39733/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [39734/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [39735/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [39736/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [39737/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [39738/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [39739/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [39740/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [39741/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [39742/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [39743/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [39744/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [39745/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [39746/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [39747/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [39748/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [39749/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [39750/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [39751/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [39752/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [39753/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [39754/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [39755/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39756/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [39757/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [39758/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [39759/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [39760/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [39761/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [39762/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [39763/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [39764/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [39765/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [39766/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [39767/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [39768/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [39769/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [39770/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [39771/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [39772/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [39773/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [39774/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [39775/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [39776/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39777/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39778/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [39779/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39780/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [39781/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [39782/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39783/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39784/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [39785/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [39786/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [39787/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [39788/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [39789/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [39790/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [39791/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [39792/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [39793/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [39794/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [39795/50000], Train Loss: 8088941.5000, Val Loss: 5298686.0000\n",
      "Epoch [39796/50000], Train Loss: 8088942.5000, Val Loss: 5298600.5000\n",
      "Epoch [39797/50000], Train Loss: 8088941.5000, Val Loss: 5298689.5000\n",
      "Epoch [39798/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [39799/50000], Train Loss: 8088941.5000, Val Loss: 5298684.5000\n",
      "Epoch [39800/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [39801/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [39802/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [39803/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [39804/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [39805/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [39806/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [39807/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [39808/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [39809/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [39810/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [39811/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [39812/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [39813/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [39814/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [39815/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [39816/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [39817/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39818/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [39819/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [39820/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [39821/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [39822/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [39823/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [39824/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39825/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [39826/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39827/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [39828/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39829/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [39830/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39831/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [39832/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39833/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39834/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [39835/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [39836/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [39837/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39838/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [39839/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39840/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39841/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39842/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [39843/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39844/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39845/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39846/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39847/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39848/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39849/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39850/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39851/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39852/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [39853/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39854/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [39855/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [39856/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39857/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [39858/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [39859/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [39860/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [39861/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39862/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [39863/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39864/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39865/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39866/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39867/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [39868/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [39869/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [39870/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [39871/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [39872/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [39873/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [39874/50000], Train Loss: 8088942.5000, Val Loss: 5298608.0000\n",
      "Epoch [39875/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [39876/50000], Train Loss: 8088942.5000, Val Loss: 5298600.5000\n",
      "Epoch [39877/50000], Train Loss: 8088941.5000, Val Loss: 5298691.5000\n",
      "Epoch [39878/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [39879/50000], Train Loss: 8088942.0000, Val Loss: 5298698.5000\n",
      "Epoch [39880/50000], Train Loss: 8088942.5000, Val Loss: 5298588.5000\n",
      "Epoch [39881/50000], Train Loss: 8088941.5000, Val Loss: 5298699.5000\n",
      "Epoch [39882/50000], Train Loss: 8088942.0000, Val Loss: 5298592.0000\n",
      "Epoch [39883/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [39884/50000], Train Loss: 8088942.0000, Val Loss: 5298602.5000\n",
      "Epoch [39885/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [39886/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [39887/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [39888/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [39889/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [39890/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [39891/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [39892/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [39893/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [39894/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [39895/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [39896/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [39897/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [39898/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [39899/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [39900/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [39901/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [39902/50000], Train Loss: 8088941.0000, Val Loss: 5298641.0000\n",
      "Epoch [39903/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39904/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [39905/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [39906/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [39907/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [39908/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [39909/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [39910/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39911/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39912/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39913/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39914/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [39915/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [39916/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [39917/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [39918/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [39919/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [39920/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [39921/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39922/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [39923/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39924/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39925/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [39926/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [39927/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [39928/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [39929/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [39930/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39931/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39932/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39933/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39934/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [39935/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [39936/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [39937/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [39938/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [39939/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [39940/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [39941/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [39942/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39943/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39944/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39945/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [39946/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39947/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [39948/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39949/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [39950/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39951/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39952/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39953/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39954/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [39955/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39956/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39957/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [39958/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39959/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [39960/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [39961/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [39962/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [39963/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [39964/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [39965/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [39966/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [39967/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [39968/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [39969/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39970/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [39971/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39972/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [39973/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39974/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [39975/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [39976/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [39977/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [39978/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [39979/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [39980/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [39981/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [39982/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [39983/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [39984/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [39985/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [39986/50000], Train Loss: 8088941.5000, Val Loss: 5298602.5000\n",
      "Epoch [39987/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [39988/50000], Train Loss: 8088941.5000, Val Loss: 5298592.5000\n",
      "Epoch [39989/50000], Train Loss: 8088942.0000, Val Loss: 5298700.5000\n",
      "Epoch [39990/50000], Train Loss: 8088942.0000, Val Loss: 5298585.0000\n",
      "Epoch [39991/50000], Train Loss: 8088942.0000, Val Loss: 5298704.5000\n",
      "Epoch [39992/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [39993/50000], Train Loss: 8088941.5000, Val Loss: 5298697.0000\n",
      "Epoch [39994/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [39995/50000], Train Loss: 8088941.5000, Val Loss: 5298675.5000\n",
      "Epoch [39996/50000], Train Loss: 8088942.5000, Val Loss: 5298626.5000\n",
      "Epoch [39997/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [39998/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [39999/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [40000/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [40001/50000], Train Loss: 8088942.0000, Val Loss: 5298609.0000\n",
      "Epoch [40002/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [40003/50000], Train Loss: 8088941.5000, Val Loss: 5298610.0000\n",
      "Epoch [40004/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [40005/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [40006/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [40007/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40008/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [40009/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [40010/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [40011/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [40012/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40013/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [40014/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [40015/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [40016/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [40017/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [40018/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40019/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40020/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [40021/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40022/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [40023/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [40024/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [40025/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [40026/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40027/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [40028/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40029/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [40030/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40031/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [40032/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40033/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40034/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40035/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40036/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [40037/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40038/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [40039/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40040/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [40041/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40042/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40043/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [40044/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40045/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40046/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [40047/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40048/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [40049/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40050/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [40051/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [40052/50000], Train Loss: 8088941.5000, Val Loss: 5298627.0000\n",
      "Epoch [40053/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [40054/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40055/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [40056/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40057/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [40058/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40059/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [40060/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [40061/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40062/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [40063/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40064/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [40065/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40066/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [40067/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [40068/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [40069/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [40070/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [40071/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [40072/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [40073/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [40074/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [40075/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [40076/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [40077/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [40078/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [40079/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [40080/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [40081/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [40082/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [40083/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [40084/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [40085/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [40086/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40087/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40088/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [40089/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [40090/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [40091/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [40092/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [40093/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [40094/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [40095/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40096/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [40097/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40098/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [40099/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [40100/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40101/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40102/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40103/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [40104/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40105/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [40106/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [40107/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [40108/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40109/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40110/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [40111/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [40112/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40113/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40114/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40115/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40116/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [40117/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40118/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [40119/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40120/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40121/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [40122/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [40123/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40124/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40125/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40126/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40127/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [40128/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40129/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [40130/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40131/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40132/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [40133/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [40134/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40135/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40136/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40137/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40138/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [40139/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [40140/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40141/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [40142/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [40143/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [40144/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [40145/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [40146/50000], Train Loss: 8088941.5000, Val Loss: 5298597.5000\n",
      "Epoch [40147/50000], Train Loss: 8088942.0000, Val Loss: 5298698.5000\n",
      "Epoch [40148/50000], Train Loss: 8088942.5000, Val Loss: 5298582.5000\n",
      "Epoch [40149/50000], Train Loss: 8088941.5000, Val Loss: 5298711.5000\n",
      "Epoch [40150/50000], Train Loss: 8088941.5000, Val Loss: 5298573.5000\n",
      "Epoch [40151/50000], Train Loss: 8088942.0000, Val Loss: 5298715.5000\n",
      "Epoch [40152/50000], Train Loss: 8088942.0000, Val Loss: 5298575.0000\n",
      "Epoch [40153/50000], Train Loss: 8088942.5000, Val Loss: 5298705.0000\n",
      "Epoch [40154/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [40155/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [40156/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [40157/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [40158/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [40159/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [40160/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [40161/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [40162/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [40163/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [40164/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [40165/50000], Train Loss: 8088942.5000, Val Loss: 5298627.0000\n",
      "Epoch [40166/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [40167/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [40168/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [40169/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [40170/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [40171/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [40172/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [40173/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [40174/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [40175/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40176/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [40177/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [40178/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40179/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [40180/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [40181/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [40182/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40183/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [40184/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [40185/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40186/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [40187/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40188/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40189/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40190/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40191/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40192/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [40193/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [40194/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40195/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [40196/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40197/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [40198/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [40199/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40200/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [40201/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40202/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40203/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40204/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [40205/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40206/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40207/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [40208/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40209/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [40210/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40211/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40212/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [40213/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40214/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40215/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [40216/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [40217/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40218/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [40219/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [40220/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40221/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40222/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [40223/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40224/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [40225/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [40226/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40227/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [40228/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40229/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40230/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40231/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40232/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [40233/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40234/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [40235/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40236/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40237/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40238/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [40239/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40240/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40241/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [40242/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40243/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40244/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40245/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [40246/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40247/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [40248/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40249/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40250/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [40251/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40252/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [40253/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40254/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40255/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40256/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [40257/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40258/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40259/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [40260/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40261/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [40262/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40263/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40264/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40265/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [40266/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40267/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [40268/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40269/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40270/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [40271/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40272/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [40273/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40274/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [40275/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40276/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [40277/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [40278/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40279/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [40280/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40281/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40282/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [40283/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40284/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40285/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [40286/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40287/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [40288/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40289/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40290/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [40291/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40292/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [40293/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [40294/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [40295/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [40296/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [40297/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [40298/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [40299/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [40300/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [40301/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [40302/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [40303/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [40304/50000], Train Loss: 8088941.5000, Val Loss: 5298597.5000\n",
      "Epoch [40305/50000], Train Loss: 8088942.0000, Val Loss: 5298702.5000\n",
      "Epoch [40306/50000], Train Loss: 8088942.0000, Val Loss: 5298573.5000\n",
      "Epoch [40307/50000], Train Loss: 8088942.0000, Val Loss: 5298729.0000\n",
      "Epoch [40308/50000], Train Loss: 8088942.5000, Val Loss: 5298546.5000\n",
      "Epoch [40309/50000], Train Loss: 8088942.0000, Val Loss: 5298751.0000\n",
      "Epoch [40310/50000], Train Loss: 8088942.0000, Val Loss: 5298537.5000\n",
      "Epoch [40311/50000], Train Loss: 8088941.5000, Val Loss: 5298736.5000\n",
      "Epoch [40312/50000], Train Loss: 8088942.0000, Val Loss: 5298580.5000\n",
      "Epoch [40313/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [40314/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [40315/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [40316/50000], Train Loss: 8088942.0000, Val Loss: 5298712.5000\n",
      "Epoch [40317/50000], Train Loss: 8088942.0000, Val Loss: 5298570.5000\n",
      "Epoch [40318/50000], Train Loss: 8088942.5000, Val Loss: 5298706.0000\n",
      "Epoch [40319/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [40320/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [40321/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [40322/50000], Train Loss: 8088942.0000, Val Loss: 5298594.5000\n",
      "Epoch [40323/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [40324/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [40325/50000], Train Loss: 8088941.5000, Val Loss: 5298664.5000\n",
      "Epoch [40326/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [40327/50000], Train Loss: 8088941.5000, Val Loss: 5298613.0000\n",
      "Epoch [40328/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [40329/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [40330/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [40331/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [40332/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [40333/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [40334/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [40335/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [40336/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [40337/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [40338/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40339/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40340/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [40341/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [40342/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [40343/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40344/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [40345/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [40346/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40347/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [40348/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40349/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40350/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [40351/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [40352/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [40353/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40354/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [40355/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [40356/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40357/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40358/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40359/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40360/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [40361/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [40362/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40363/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [40364/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40365/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [40366/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [40367/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [40368/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40369/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40370/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [40371/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [40372/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40373/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [40374/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40375/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40376/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [40377/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40378/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40379/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40380/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [40381/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40382/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40383/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [40384/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40385/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40386/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [40387/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40388/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40389/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40390/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [40391/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40392/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40393/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [40394/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40395/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40396/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [40397/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40398/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40399/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [40400/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40401/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40402/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [40403/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40404/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40405/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [40406/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40407/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40408/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40409/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [40410/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40411/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40412/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40413/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [40414/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40415/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40416/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [40417/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40418/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40419/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40420/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [40421/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [40422/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40423/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40424/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40425/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40426/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40427/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [40428/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40429/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [40430/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40431/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [40432/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40433/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40434/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40435/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [40436/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40437/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [40438/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40439/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40440/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40441/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40442/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40443/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [40444/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40445/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40446/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40447/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40448/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [40449/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40450/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40451/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40452/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40453/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40454/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [40455/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [40456/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40457/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40458/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [40459/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [40460/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [40461/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [40462/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [40463/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [40464/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40465/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40466/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40467/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [40468/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40469/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [40470/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40471/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [40472/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [40473/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [40474/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [40475/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [40476/50000], Train Loss: 8088942.5000, Val Loss: 5298634.0000\n",
      "Epoch [40477/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40478/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40479/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [40480/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [40481/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [40482/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [40483/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [40484/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [40485/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [40486/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40487/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40488/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [40489/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [40490/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40491/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40492/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [40493/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40494/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40495/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40496/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [40497/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [40498/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [40499/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40500/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40501/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [40502/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [40503/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [40504/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40505/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [40506/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [40507/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [40508/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [40509/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40510/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [40511/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [40512/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [40513/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [40514/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [40515/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40516/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [40517/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40518/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [40519/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [40520/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [40521/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [40522/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [40523/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [40524/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40525/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40526/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [40527/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [40528/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [40529/50000], Train Loss: 8088941.5000, Val Loss: 5298620.5000\n",
      "Epoch [40530/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [40531/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40532/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40533/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [40534/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [40535/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [40536/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [40537/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [40538/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [40539/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [40540/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [40541/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [40542/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40543/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [40544/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40545/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40546/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40547/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40548/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [40549/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40550/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40551/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [40552/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40553/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40554/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40555/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [40556/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [40557/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40558/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [40559/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [40560/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [40561/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [40562/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [40563/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [40564/50000], Train Loss: 8088942.5000, Val Loss: 5298612.5000\n",
      "Epoch [40565/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [40566/50000], Train Loss: 8088941.5000, Val Loss: 5298602.0000\n",
      "Epoch [40567/50000], Train Loss: 8088941.5000, Val Loss: 5298693.5000\n",
      "Epoch [40568/50000], Train Loss: 8088941.5000, Val Loss: 5298590.0000\n",
      "Epoch [40569/50000], Train Loss: 8088942.0000, Val Loss: 5298705.0000\n",
      "Epoch [40570/50000], Train Loss: 8088942.0000, Val Loss: 5298579.5000\n",
      "Epoch [40571/50000], Train Loss: 8088941.5000, Val Loss: 5298709.5000\n",
      "Epoch [40572/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [40573/50000], Train Loss: 8088941.5000, Val Loss: 5298697.0000\n",
      "Epoch [40574/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [40575/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [40576/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40577/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [40578/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [40579/50000], Train Loss: 8088941.5000, Val Loss: 5298608.5000\n",
      "Epoch [40580/50000], Train Loss: 8088942.5000, Val Loss: 5298688.5000\n",
      "Epoch [40581/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [40582/50000], Train Loss: 8088941.5000, Val Loss: 5298689.5000\n",
      "Epoch [40583/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [40584/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [40585/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [40586/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [40587/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40588/50000], Train Loss: 8088941.5000, Val Loss: 5298625.5000\n",
      "Epoch [40589/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [40590/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [40591/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [40592/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [40593/50000], Train Loss: 8088941.5000, Val Loss: 5298667.5000\n",
      "Epoch [40594/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40595/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [40596/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40597/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [40598/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40599/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [40600/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [40601/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [40602/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40603/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [40604/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [40605/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40606/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40607/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [40608/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [40609/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40610/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40611/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [40612/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [40613/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [40614/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [40615/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40616/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40617/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [40618/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40619/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [40620/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40621/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40622/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40623/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [40624/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [40625/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [40626/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40627/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [40628/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [40629/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40630/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40631/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40632/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40633/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [40634/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40635/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40636/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40637/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40638/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [40639/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40640/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [40641/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40642/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [40643/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40644/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40645/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40646/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [40647/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40648/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [40649/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [40650/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40651/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [40652/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40653/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40654/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40655/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40656/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40657/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [40658/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40659/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [40660/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40661/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [40662/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40663/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40664/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40665/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [40666/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40667/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [40668/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40669/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40670/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [40671/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [40672/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40673/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [40674/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40675/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [40676/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [40677/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [40678/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40679/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [40680/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [40681/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [40682/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [40683/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [40684/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [40685/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [40686/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [40687/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [40688/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [40689/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [40690/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [40691/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [40692/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [40693/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [40694/50000], Train Loss: 8088941.5000, Val Loss: 5298605.0000\n",
      "Epoch [40695/50000], Train Loss: 8088942.5000, Val Loss: 5298687.0000\n",
      "Epoch [40696/50000], Train Loss: 8088941.5000, Val Loss: 5298600.5000\n",
      "Epoch [40697/50000], Train Loss: 8088941.5000, Val Loss: 5298688.5000\n",
      "Epoch [40698/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [40699/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [40700/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [40701/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [40702/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40703/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [40704/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40705/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40706/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [40707/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [40708/50000], Train Loss: 8088941.5000, Val Loss: 5298661.5000\n",
      "Epoch [40709/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [40710/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [40711/50000], Train Loss: 8088941.5000, Val Loss: 5298618.0000\n",
      "Epoch [40712/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [40713/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [40714/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [40715/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [40716/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [40717/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [40718/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40719/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40720/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40721/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40722/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [40723/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [40724/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [40725/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [40726/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40727/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [40728/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40729/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40730/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40731/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40732/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40733/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40734/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40735/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40736/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40737/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40738/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40739/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40740/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40741/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40742/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40743/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40744/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40745/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40746/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40747/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40748/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40749/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40750/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40751/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40752/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40753/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40754/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40755/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40756/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40757/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40758/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40759/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40760/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40761/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40762/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40763/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40764/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40765/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40766/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [40767/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40768/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40769/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [40770/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40771/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [40772/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40773/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [40774/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40775/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [40776/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [40777/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [40778/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [40779/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [40780/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [40781/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [40782/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [40783/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [40784/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [40785/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [40786/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [40787/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [40788/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [40789/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [40790/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [40791/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [40792/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40793/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [40794/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [40795/50000], Train Loss: 8088941.0000, Val Loss: 5298655.5000\n",
      "Epoch [40796/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [40797/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40798/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [40799/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40800/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40801/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40802/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40803/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [40804/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40805/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40806/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40807/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [40808/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [40809/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40810/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [40811/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40812/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [40813/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40814/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40815/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40816/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40817/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40818/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [40819/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [40820/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [40821/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [40822/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [40823/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40824/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [40825/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [40826/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [40827/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [40828/50000], Train Loss: 8088941.5000, Val Loss: 5298602.0000\n",
      "Epoch [40829/50000], Train Loss: 8088941.5000, Val Loss: 5298693.5000\n",
      "Epoch [40830/50000], Train Loss: 8088941.5000, Val Loss: 5298589.0000\n",
      "Epoch [40831/50000], Train Loss: 8088942.0000, Val Loss: 5298706.5000\n",
      "Epoch [40832/50000], Train Loss: 8088942.0000, Val Loss: 5298578.0000\n",
      "Epoch [40833/50000], Train Loss: 8088942.0000, Val Loss: 5298712.5000\n",
      "Epoch [40834/50000], Train Loss: 8088942.0000, Val Loss: 5298578.5000\n",
      "Epoch [40835/50000], Train Loss: 8088941.5000, Val Loss: 5298703.0000\n",
      "Epoch [40836/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [40837/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [40838/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [40839/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [40840/50000], Train Loss: 8088941.5000, Val Loss: 5298661.5000\n",
      "Epoch [40841/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [40842/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [40843/50000], Train Loss: 8088942.5000, Val Loss: 5298600.5000\n",
      "Epoch [40844/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [40845/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [40846/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [40847/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40848/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [40849/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40850/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [40851/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [40852/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [40853/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [40854/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [40855/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [40856/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [40857/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40858/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [40859/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [40860/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [40861/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [40862/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [40863/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [40864/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [40865/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [40866/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40867/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40868/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40869/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [40870/50000], Train Loss: 8088942.5000, Val Loss: 5298634.0000\n",
      "Epoch [40871/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [40872/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [40873/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [40874/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [40875/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40876/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [40877/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [40878/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40879/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [40880/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40881/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [40882/50000], Train Loss: 8088941.5000, Val Loss: 5298659.0000\n",
      "Epoch [40883/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [40884/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [40885/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [40886/50000], Train Loss: 8088941.5000, Val Loss: 5298654.5000\n",
      "Epoch [40887/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [40888/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40889/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40890/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40891/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [40892/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40893/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [40894/50000], Train Loss: 8088941.5000, Val Loss: 5298661.5000\n",
      "Epoch [40895/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [40896/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [40897/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [40898/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [40899/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [40900/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [40901/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [40902/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [40903/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [40904/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [40905/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [40906/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [40907/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40908/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [40909/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [40910/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [40911/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [40912/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40913/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [40914/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [40915/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [40916/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40917/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [40918/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40919/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [40920/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [40921/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [40922/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [40923/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [40924/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [40925/50000], Train Loss: 8088941.5000, Val Loss: 5298620.5000\n",
      "Epoch [40926/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [40927/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [40928/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [40929/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [40930/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [40931/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [40932/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40933/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40934/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40935/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [40936/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [40937/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40938/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [40939/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [40940/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [40941/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [40942/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40943/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [40944/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [40945/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40946/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40947/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [40948/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40949/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [40950/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40951/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [40952/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40953/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [40954/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [40955/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [40956/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [40957/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40958/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [40959/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40960/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40961/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [40962/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40963/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [40964/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40965/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [40966/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40967/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40968/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40969/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [40970/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40971/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40972/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [40973/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40974/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [40975/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [40976/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40977/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [40978/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [40979/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [40980/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [40981/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [40982/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [40983/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [40984/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [40985/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [40986/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [40987/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [40988/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [40989/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [40990/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [40991/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [40992/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [40993/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [40994/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [40995/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [40996/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [40997/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [40998/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [40999/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [41000/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41001/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [41002/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [41003/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [41004/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [41005/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [41006/50000], Train Loss: 8088942.0000, Val Loss: 5298712.5000\n",
      "Epoch [41007/50000], Train Loss: 8088942.0000, Val Loss: 5298562.5000\n",
      "Epoch [41008/50000], Train Loss: 8088942.5000, Val Loss: 5298739.5000\n",
      "Epoch [41009/50000], Train Loss: 8088942.0000, Val Loss: 5298539.5000\n",
      "Epoch [41010/50000], Train Loss: 8088942.0000, Val Loss: 5298750.5000\n",
      "Epoch [41011/50000], Train Loss: 8088942.5000, Val Loss: 5298550.0000\n",
      "Epoch [41012/50000], Train Loss: 8088942.0000, Val Loss: 5298713.0000\n",
      "Epoch [41013/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [41014/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41015/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [41016/50000], Train Loss: 8088942.5000, Val Loss: 5298580.5000\n",
      "Epoch [41017/50000], Train Loss: 8088942.5000, Val Loss: 5298717.5000\n",
      "Epoch [41018/50000], Train Loss: 8088942.0000, Val Loss: 5298579.0000\n",
      "Epoch [41019/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [41020/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [41021/50000], Train Loss: 8088941.5000, Val Loss: 5298625.5000\n",
      "Epoch [41022/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [41023/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [41024/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [41025/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [41026/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [41027/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [41028/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [41029/50000], Train Loss: 8088942.5000, Val Loss: 5298683.5000\n",
      "Epoch [41030/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [41031/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41032/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41033/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [41034/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [41035/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [41036/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [41037/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41038/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41039/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41040/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [41041/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41042/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [41043/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [41044/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [41045/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41046/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [41047/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41048/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41049/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41050/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [41051/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41052/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [41053/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41054/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41055/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [41056/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [41057/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [41058/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41059/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41060/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [41061/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [41062/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41063/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41064/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [41065/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41066/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41067/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41068/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [41069/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41070/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [41071/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41072/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41073/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41074/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41075/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41076/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [41077/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41078/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41079/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [41080/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [41081/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [41082/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [41083/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41084/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41085/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41086/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41087/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41088/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41089/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41090/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41091/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41092/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41093/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [41094/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41095/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [41096/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41097/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41098/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41099/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41100/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41101/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41102/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41103/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41104/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [41105/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41106/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [41107/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [41108/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41109/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [41110/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [41111/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41112/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41113/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [41114/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41115/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41116/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41117/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41118/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41119/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [41120/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41121/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41122/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41123/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [41124/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41125/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41126/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41127/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41128/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41129/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41130/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [41131/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41132/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41133/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41134/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41135/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41136/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [41137/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41138/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41139/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41140/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [41141/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41142/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41143/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41144/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [41145/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41146/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41147/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [41148/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41149/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [41150/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41151/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41152/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41153/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41154/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41155/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41156/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41157/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41158/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41159/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41160/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41161/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41162/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [41163/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41164/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [41165/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [41166/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41167/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [41168/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41169/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41170/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41171/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41172/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [41173/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41174/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41175/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41176/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41177/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [41178/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41179/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [41180/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41181/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [41182/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [41183/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41184/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [41185/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41186/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41187/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [41188/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41189/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41190/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [41191/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41192/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41193/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [41194/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41195/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [41196/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41197/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [41198/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [41199/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41200/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [41201/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41202/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41203/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41204/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [41205/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41206/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41207/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [41208/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41209/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [41210/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41211/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41212/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41213/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41214/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [41215/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41216/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41217/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41218/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [41219/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [41220/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [41221/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41222/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [41223/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [41224/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41225/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [41226/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [41227/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41228/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [41229/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [41230/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [41231/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41232/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41233/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [41234/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41235/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [41236/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41237/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41238/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41239/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41240/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41241/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41242/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41243/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41244/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41245/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [41246/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41247/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41248/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41249/50000], Train Loss: 8088941.5000, Val Loss: 5298626.5000\n",
      "Epoch [41250/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [41251/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [41252/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [41253/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [41254/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [41255/50000], Train Loss: 8088941.5000, Val Loss: 5298583.5000\n",
      "Epoch [41256/50000], Train Loss: 8088942.0000, Val Loss: 5298719.0000\n",
      "Epoch [41257/50000], Train Loss: 8088942.5000, Val Loss: 5298557.0000\n",
      "Epoch [41258/50000], Train Loss: 8088942.0000, Val Loss: 5298742.0000\n",
      "Epoch [41259/50000], Train Loss: 8088941.5000, Val Loss: 5298543.5000\n",
      "Epoch [41260/50000], Train Loss: 8088942.0000, Val Loss: 5298737.5000\n",
      "Epoch [41261/50000], Train Loss: 8088942.5000, Val Loss: 5298570.5000\n",
      "Epoch [41262/50000], Train Loss: 8088942.5000, Val Loss: 5298688.5000\n",
      "Epoch [41263/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [41264/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [41265/50000], Train Loss: 8088942.5000, Val Loss: 5298695.0000\n",
      "Epoch [41266/50000], Train Loss: 8088942.0000, Val Loss: 5298577.5000\n",
      "Epoch [41267/50000], Train Loss: 8088942.0000, Val Loss: 5298713.0000\n",
      "Epoch [41268/50000], Train Loss: 8088942.0000, Val Loss: 5298588.5000\n",
      "Epoch [41269/50000], Train Loss: 8088941.5000, Val Loss: 5298676.5000\n",
      "Epoch [41270/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41271/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [41272/50000], Train Loss: 8088942.5000, Val Loss: 5298687.0000\n",
      "Epoch [41273/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [41274/50000], Train Loss: 8088941.5000, Val Loss: 5298689.5000\n",
      "Epoch [41275/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [41276/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [41277/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [41278/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [41279/50000], Train Loss: 8088942.5000, Val Loss: 5298684.5000\n",
      "Epoch [41280/50000], Train Loss: 8088942.5000, Val Loss: 5298608.0000\n",
      "Epoch [41281/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [41282/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41283/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41284/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [41285/50000], Train Loss: 8088942.5000, Val Loss: 5298612.5000\n",
      "Epoch [41286/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [41287/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [41288/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41289/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41290/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [41291/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [41292/50000], Train Loss: 8088941.5000, Val Loss: 5298623.0000\n",
      "Epoch [41293/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41294/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41295/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41296/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41297/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41298/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41299/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [41300/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [41301/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41302/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [41303/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [41304/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [41305/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41306/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [41307/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [41308/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41309/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41310/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41311/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41312/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41313/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41314/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41315/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [41316/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41317/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [41318/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [41319/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41320/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41321/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [41322/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41323/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41324/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [41325/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41326/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41327/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41328/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41329/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41330/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41331/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41332/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41333/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41334/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41335/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41336/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41337/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41338/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41339/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41340/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41341/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [41342/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [41343/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41344/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41345/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [41346/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41347/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41348/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41349/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [41350/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41351/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [41352/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41353/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41354/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [41355/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [41356/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [41357/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41358/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41359/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [41360/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41361/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41362/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [41363/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41364/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41365/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [41366/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41367/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41368/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41369/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41370/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41371/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41372/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41373/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [41374/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41375/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41376/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41377/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41378/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41379/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41380/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41381/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [41382/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41383/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41384/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41385/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41386/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41387/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41388/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [41389/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41390/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41391/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41392/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41393/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41394/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [41395/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41396/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41397/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41398/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [41399/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41400/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41401/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41402/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41403/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41404/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [41405/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41406/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41407/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41408/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41409/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41410/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [41411/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41412/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41413/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41414/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [41415/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41416/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [41417/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [41418/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41419/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [41420/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41421/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41422/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [41423/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41424/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41425/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41426/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41427/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41428/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41429/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [41430/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41431/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41432/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [41433/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41434/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41435/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41436/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [41437/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41438/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41439/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41440/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [41441/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41442/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41443/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41444/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [41445/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41446/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41447/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41448/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [41449/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [41450/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41451/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [41452/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41453/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [41454/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [41455/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [41456/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [41457/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [41458/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [41459/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [41460/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [41461/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [41462/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [41463/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [41464/50000], Train Loss: 8088942.5000, Val Loss: 5298626.5000\n",
      "Epoch [41465/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [41466/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [41467/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [41468/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41469/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [41470/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41471/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41472/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [41473/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [41474/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [41475/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [41476/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [41477/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [41478/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [41479/50000], Train Loss: 8088942.5000, Val Loss: 5298604.5000\n",
      "Epoch [41480/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [41481/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [41482/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [41483/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [41484/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [41485/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [41486/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [41487/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [41488/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [41489/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [41490/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [41491/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [41492/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41493/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [41494/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41495/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [41496/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [41497/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [41498/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [41499/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [41500/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [41501/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [41502/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [41503/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [41504/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41505/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [41506/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41507/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41508/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [41509/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [41510/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [41511/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [41512/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41513/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41514/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41515/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [41516/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [41517/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [41518/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41519/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41520/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41521/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41522/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [41523/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [41524/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [41525/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [41526/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [41527/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [41528/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [41529/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41530/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41531/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41532/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41533/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41534/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [41535/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [41536/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [41537/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [41538/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41539/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [41540/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [41541/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [41542/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [41543/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [41544/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [41545/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [41546/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [41547/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [41548/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [41549/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [41550/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [41551/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [41552/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [41553/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41554/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41555/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41556/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41557/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41558/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [41559/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41560/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41561/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [41562/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41563/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [41564/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [41565/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [41566/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [41567/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41568/50000], Train Loss: 8088941.5000, Val Loss: 5298629.5000\n",
      "Epoch [41569/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41570/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41571/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41572/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [41573/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [41574/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [41575/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [41576/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [41577/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [41578/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [41579/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [41580/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [41581/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [41582/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [41583/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [41584/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [41585/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [41586/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [41587/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [41588/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [41589/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [41590/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [41591/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [41592/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [41593/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [41594/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41595/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41596/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [41597/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [41598/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [41599/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41600/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [41601/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41602/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41603/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [41604/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [41605/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [41606/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [41607/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [41608/50000], Train Loss: 8088942.5000, Val Loss: 5298685.0000\n",
      "Epoch [41609/50000], Train Loss: 8088941.5000, Val Loss: 5298602.5000\n",
      "Epoch [41610/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [41611/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [41612/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [41613/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [41614/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [41615/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [41616/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [41617/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41618/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41619/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [41620/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41621/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [41622/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [41623/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [41624/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [41625/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [41626/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [41627/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [41628/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [41629/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [41630/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [41631/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [41632/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [41633/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [41634/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [41635/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [41636/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41637/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41638/50000], Train Loss: 8088941.5000, Val Loss: 5298631.5000\n",
      "Epoch [41639/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [41640/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41641/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [41642/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41643/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [41644/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [41645/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [41646/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [41647/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41648/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41649/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41650/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [41651/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [41652/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [41653/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [41654/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41655/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41656/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41657/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [41658/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [41659/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [41660/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41661/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [41662/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41663/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [41664/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [41665/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [41666/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [41667/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [41668/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [41669/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [41670/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [41671/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [41672/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [41673/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [41674/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [41675/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41676/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [41677/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41678/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [41679/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41680/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41681/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41682/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [41683/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41684/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [41685/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [41686/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [41687/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [41688/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [41689/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [41690/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [41691/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [41692/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [41693/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [41694/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [41695/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41696/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41697/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41698/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41699/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41700/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41701/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41702/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [41703/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [41704/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [41705/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [41706/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [41707/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [41708/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [41709/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [41710/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [41711/50000], Train Loss: 8088941.5000, Val Loss: 5298607.0000\n",
      "Epoch [41712/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [41713/50000], Train Loss: 8088942.5000, Val Loss: 5298608.0000\n",
      "Epoch [41714/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [41715/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [41716/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [41717/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [41718/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [41719/50000], Train Loss: 8088942.5000, Val Loss: 5298629.5000\n",
      "Epoch [41720/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41721/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41722/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41723/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [41724/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [41725/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [41726/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [41727/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41728/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41729/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41730/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41731/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41732/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [41733/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [41734/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [41735/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [41736/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [41737/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [41738/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [41739/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [41740/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41741/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [41742/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41743/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41744/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41745/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41746/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41747/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [41748/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [41749/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41750/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41751/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41752/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41753/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41754/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [41755/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41756/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41757/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41758/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41759/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41760/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41761/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41762/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [41763/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41764/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [41765/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41766/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [41767/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [41768/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [41769/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [41770/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [41771/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [41772/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [41773/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [41774/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [41775/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [41776/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [41777/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [41778/50000], Train Loss: 8088941.5000, Val Loss: 5298595.0000\n",
      "Epoch [41779/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [41780/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [41781/50000], Train Loss: 8088941.5000, Val Loss: 5298682.0000\n",
      "Epoch [41782/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [41783/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [41784/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [41785/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41786/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [41787/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41788/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [41789/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [41790/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [41791/50000], Train Loss: 8088941.5000, Val Loss: 5298621.5000\n",
      "Epoch [41792/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [41793/50000], Train Loss: 8088942.5000, Val Loss: 5298627.0000\n",
      "Epoch [41794/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [41795/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41796/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41797/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41798/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41799/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41800/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41801/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41802/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41803/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [41804/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41805/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [41806/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41807/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41808/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41809/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41810/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41811/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41812/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [41813/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [41814/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [41815/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [41816/50000], Train Loss: 8088941.0000, Val Loss: 5298655.5000\n",
      "Epoch [41817/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [41818/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41819/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41820/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41821/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41822/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41823/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [41824/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41825/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41826/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41827/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [41828/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41829/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41830/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [41831/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41832/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41833/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [41834/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41835/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41836/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41837/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41838/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41839/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41840/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [41841/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41842/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [41843/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41844/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [41845/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41846/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [41847/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41848/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41849/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41850/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [41851/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [41852/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [41853/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [41854/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [41855/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41856/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41857/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [41858/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [41859/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41860/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41861/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41862/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [41863/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41864/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41865/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41866/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41867/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41868/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41869/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41870/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41871/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41872/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41873/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [41874/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41875/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [41876/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41877/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [41878/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [41879/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41880/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41881/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41882/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [41883/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [41884/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [41885/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [41886/50000], Train Loss: 8088942.5000, Val Loss: 5298608.0000\n",
      "Epoch [41887/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [41888/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [41889/50000], Train Loss: 8088942.0000, Val Loss: 5298708.5000\n",
      "Epoch [41890/50000], Train Loss: 8088942.0000, Val Loss: 5298570.5000\n",
      "Epoch [41891/50000], Train Loss: 8088942.5000, Val Loss: 5298726.0000\n",
      "Epoch [41892/50000], Train Loss: 8088941.5000, Val Loss: 5298557.5000\n",
      "Epoch [41893/50000], Train Loss: 8088942.5000, Val Loss: 5298731.0000\n",
      "Epoch [41894/50000], Train Loss: 8088942.0000, Val Loss: 5298565.0000\n",
      "Epoch [41895/50000], Train Loss: 8088942.0000, Val Loss: 5298709.0000\n",
      "Epoch [41896/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [41897/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [41898/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [41899/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [41900/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [41901/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [41902/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [41903/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [41904/50000], Train Loss: 8088941.5000, Val Loss: 5298665.5000\n",
      "Epoch [41905/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [41906/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [41907/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [41908/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [41909/50000], Train Loss: 8088942.5000, Val Loss: 5298680.0000\n",
      "Epoch [41910/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [41911/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [41912/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41913/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41914/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [41915/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [41916/50000], Train Loss: 8088942.5000, Val Loss: 5298672.5000\n",
      "Epoch [41917/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [41918/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [41919/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41920/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41921/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41922/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [41923/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [41924/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [41925/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [41926/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41927/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [41928/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [41929/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [41930/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [41931/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [41932/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41933/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41934/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [41935/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41936/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [41937/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41938/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [41939/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [41940/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [41941/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41942/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41943/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41945/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41946/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41947/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [41948/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41949/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [41950/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41951/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41952/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [41953/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41954/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41955/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [41956/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [41957/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [41958/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [41959/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41960/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41961/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41962/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41963/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41964/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41965/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41966/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41967/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41968/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [41969/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [41970/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [41971/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41972/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41973/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [41974/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41975/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41976/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [41977/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41978/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41979/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [41980/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [41981/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41982/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41983/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [41984/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [41985/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41986/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [41987/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [41988/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [41989/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [41990/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [41991/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [41992/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [41993/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [41994/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [41995/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [41996/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [41997/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [41998/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [41999/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42000/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42001/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [42002/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [42003/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [42004/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42005/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42006/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [42007/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [42008/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [42009/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42010/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [42011/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [42012/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [42013/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [42014/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [42015/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [42016/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [42017/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [42018/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [42019/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [42020/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [42021/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [42022/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42023/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42024/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42025/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [42026/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [42027/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [42028/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [42029/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [42030/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [42031/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [42032/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [42033/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [42034/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [42035/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [42036/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [42037/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [42038/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [42039/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [42040/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42041/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42042/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [42043/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [42044/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42045/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [42046/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42047/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42048/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42049/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [42050/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [42051/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [42052/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42053/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42054/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42055/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42056/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [42057/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [42058/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42059/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42060/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42061/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [42062/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42063/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [42064/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42065/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [42066/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42067/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [42068/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42069/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [42070/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42071/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42072/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42073/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [42074/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42075/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42076/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42077/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [42078/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42079/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [42080/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [42081/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42082/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [42083/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42084/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [42085/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42086/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42087/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [42088/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42089/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [42090/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42091/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [42092/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [42093/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [42094/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [42095/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [42096/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [42097/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [42098/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [42099/50000], Train Loss: 8088942.5000, Val Loss: 5298596.5000\n",
      "Epoch [42100/50000], Train Loss: 8088942.5000, Val Loss: 5298701.0000\n",
      "Epoch [42101/50000], Train Loss: 8088942.0000, Val Loss: 5298579.5000\n",
      "Epoch [42102/50000], Train Loss: 8088941.5000, Val Loss: 5298717.5000\n",
      "Epoch [42103/50000], Train Loss: 8088942.0000, Val Loss: 5298564.5000\n",
      "Epoch [42104/50000], Train Loss: 8088942.0000, Val Loss: 5298728.0000\n",
      "Epoch [42105/50000], Train Loss: 8088941.5000, Val Loss: 5298562.5000\n",
      "Epoch [42106/50000], Train Loss: 8088942.5000, Val Loss: 5298715.5000\n",
      "Epoch [42107/50000], Train Loss: 8088942.0000, Val Loss: 5298592.0000\n",
      "Epoch [42108/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [42109/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42110/50000], Train Loss: 8088941.5000, Val Loss: 5298620.5000\n",
      "Epoch [42111/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [42112/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [42113/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [42114/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [42115/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [42116/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [42117/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [42118/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [42119/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [42120/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [42121/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [42122/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [42123/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42124/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42125/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [42126/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [42127/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [42128/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [42129/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [42130/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [42131/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [42132/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42133/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [42134/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [42135/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [42136/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [42137/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [42138/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [42139/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42140/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42141/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [42142/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [42143/50000], Train Loss: 8088941.0000, Val Loss: 5298655.0000\n",
      "Epoch [42144/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42145/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42146/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [42147/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [42148/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [42149/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [42150/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42151/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42152/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [42153/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42154/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [42155/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42156/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42157/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [42158/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42159/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42160/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [42161/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [42162/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42163/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [42164/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42165/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42166/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [42167/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [42168/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42169/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [42170/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42171/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [42172/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42173/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [42174/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42175/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [42176/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42177/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42178/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42179/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42180/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42181/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42182/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [42183/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [42184/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42185/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42186/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42187/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42188/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [42189/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [42190/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42191/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [42192/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42193/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42194/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42195/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [42196/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42197/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [42198/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42199/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [42200/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42201/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [42202/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42203/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42204/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [42205/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42206/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42207/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [42208/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42209/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42210/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [42211/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42212/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [42213/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [42214/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42215/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42216/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [42217/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [42218/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [42219/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42220/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42221/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [42222/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42223/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42224/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [42225/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42226/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [42227/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42228/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42229/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42230/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42231/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42232/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [42233/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [42234/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42235/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42236/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42237/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42238/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [42239/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [42240/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42241/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42242/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [42243/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [42244/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42245/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [42246/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42247/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [42248/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42249/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [42250/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42251/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [42252/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42253/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [42254/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42255/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [42256/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42257/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [42258/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [42259/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [42260/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [42261/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42262/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [42263/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [42264/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42265/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [42266/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [42267/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [42268/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [42269/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [42270/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42271/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42272/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42273/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [42274/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42275/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42276/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [42277/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42278/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [42279/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42280/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42281/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [42282/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [42283/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42284/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [42285/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42286/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42287/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42288/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [42289/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [42290/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42291/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42292/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [42293/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42294/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42295/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [42296/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42297/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42298/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [42299/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [42300/50000], Train Loss: 8088941.0000, Val Loss: 5298661.5000\n",
      "Epoch [42301/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [42302/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [42303/50000], Train Loss: 8088942.5000, Val Loss: 5298611.5000\n",
      "Epoch [42304/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [42305/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [42306/50000], Train Loss: 8088942.0000, Val Loss: 5298703.0000\n",
      "Epoch [42307/50000], Train Loss: 8088942.0000, Val Loss: 5298572.5000\n",
      "Epoch [42308/50000], Train Loss: 8088942.0000, Val Loss: 5298731.0000\n",
      "Epoch [42309/50000], Train Loss: 8088942.0000, Val Loss: 5298545.5000\n",
      "Epoch [42310/50000], Train Loss: 8088942.0000, Val Loss: 5298751.0000\n",
      "Epoch [42311/50000], Train Loss: 8088942.0000, Val Loss: 5298539.5000\n",
      "Epoch [42312/50000], Train Loss: 8088942.0000, Val Loss: 5298733.5000\n",
      "Epoch [42313/50000], Train Loss: 8088941.5000, Val Loss: 5298585.0000\n",
      "Epoch [42314/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [42315/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [42316/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [42317/50000], Train Loss: 8088942.5000, Val Loss: 5298710.0000\n",
      "Epoch [42318/50000], Train Loss: 8088942.0000, Val Loss: 5298575.0000\n",
      "Epoch [42319/50000], Train Loss: 8088942.5000, Val Loss: 5298701.0000\n",
      "Epoch [42320/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [42321/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42322/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [42323/50000], Train Loss: 8088942.0000, Val Loss: 5298595.0000\n",
      "Epoch [42324/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [42325/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [42326/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [42327/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42328/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [42329/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [42330/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [42331/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [42332/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [42333/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [42334/50000], Train Loss: 8088941.5000, Val Loss: 5298663.5000\n",
      "Epoch [42335/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [42336/50000], Train Loss: 8088943.0000, Val Loss: 5298675.0000\n",
      "Epoch [42337/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [42338/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42339/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42340/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [42341/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [42342/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [42343/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [42344/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42345/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [42346/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [42347/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [42348/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [42349/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [42350/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [42351/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42352/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [42353/50000], Train Loss: 8088941.0000, Val Loss: 5298659.0000\n",
      "Epoch [42354/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [42355/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42356/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [42357/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42358/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [42359/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [42360/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42361/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [42362/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [42363/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [42364/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42365/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [42366/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42367/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42368/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42369/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [42370/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42371/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42372/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42373/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42374/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [42375/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [42376/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42377/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [42378/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42379/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [42380/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [42381/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [42382/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [42383/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [42384/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42385/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [42386/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [42387/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [42388/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [42389/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [42390/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42391/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [42392/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [42393/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42394/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [42395/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42396/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [42397/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42398/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [42399/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [42400/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42401/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42402/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [42403/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42404/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42405/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [42406/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42407/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [42408/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42409/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [42410/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42411/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [42412/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42413/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42414/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [42415/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [42416/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42417/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42418/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42419/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [42420/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [42421/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42422/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42423/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [42424/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [42425/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [42426/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42427/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [42428/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [42429/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42430/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [42431/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42432/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [42433/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42434/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42435/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [42436/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42437/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42438/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [42439/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42440/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [42441/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42442/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [42443/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [42444/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42445/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42446/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [42447/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42448/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [42449/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [42450/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42451/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [42452/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42453/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42454/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42455/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42456/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42457/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42458/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42459/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42460/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [42461/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42462/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [42463/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [42464/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42465/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42466/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [42467/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42468/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [42469/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42470/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42471/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [42472/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42473/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [42474/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [42475/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42476/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [42477/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [42478/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [42479/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [42480/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42481/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42482/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [42483/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [42484/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [42485/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [42486/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42487/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42488/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42489/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [42490/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [42491/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42492/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [42493/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [42494/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42495/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42496/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42497/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42498/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42499/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42500/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [42501/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42502/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42503/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42504/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [42505/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42506/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42507/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42508/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42509/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [42510/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42511/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [42512/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [42513/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42514/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [42515/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42516/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [42517/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42518/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [42519/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42520/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [42521/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [42522/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [42523/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42524/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [42525/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [42526/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [42527/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [42528/50000], Train Loss: 8088941.5000, Val Loss: 5298627.0000\n",
      "Epoch [42529/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [42530/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [42531/50000], Train Loss: 8088942.5000, Val Loss: 5298672.5000\n",
      "Epoch [42532/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [42533/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [42534/50000], Train Loss: 8088941.5000, Val Loss: 5298603.0000\n",
      "Epoch [42535/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [42536/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [42537/50000], Train Loss: 8088942.0000, Val Loss: 5298702.5000\n",
      "Epoch [42538/50000], Train Loss: 8088942.0000, Val Loss: 5298581.5000\n",
      "Epoch [42539/50000], Train Loss: 8088942.0000, Val Loss: 5298710.0000\n",
      "Epoch [42540/50000], Train Loss: 8088942.0000, Val Loss: 5298578.5000\n",
      "Epoch [42541/50000], Train Loss: 8088941.5000, Val Loss: 5298706.0000\n",
      "Epoch [42542/50000], Train Loss: 8088942.5000, Val Loss: 5298590.5000\n",
      "Epoch [42543/50000], Train Loss: 8088942.0000, Val Loss: 5298686.0000\n",
      "Epoch [42544/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [42545/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42546/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42547/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [42548/50000], Train Loss: 8088942.5000, Val Loss: 5298672.5000\n",
      "Epoch [42549/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [42550/50000], Train Loss: 8088942.5000, Val Loss: 5298678.5000\n",
      "Epoch [42551/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [42552/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [42553/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [42554/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [42555/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42556/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [42557/50000], Train Loss: 8088941.0000, Val Loss: 5298664.5000\n",
      "Epoch [42558/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [42559/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [42560/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [42561/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [42562/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [42563/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42564/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [42565/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [42566/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [42567/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [42568/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [42569/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [42570/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [42571/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [42572/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [42573/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [42574/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [42575/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42576/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [42577/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [42578/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [42579/50000], Train Loss: 8088941.0000, Val Loss: 5298661.5000\n",
      "Epoch [42580/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [42581/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [42582/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [42583/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [42584/50000], Train Loss: 8088942.5000, Val Loss: 5298626.5000\n",
      "Epoch [42585/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [42586/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [42587/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42588/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [42589/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42590/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42591/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [42592/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42593/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [42594/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [42595/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [42596/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [42597/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [42598/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [42599/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42600/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42601/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [42602/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [42603/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42604/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42605/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [42606/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [42607/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [42608/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [42609/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [42610/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [42611/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [42612/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [42613/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [42614/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [42615/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [42616/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [42617/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [42618/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [42619/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [42620/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [42621/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [42622/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42623/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [42624/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42625/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [42626/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42627/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42628/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [42629/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [42630/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [42631/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [42632/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [42633/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [42634/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [42635/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [42636/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [42637/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [42638/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [42639/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [42640/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [42641/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42642/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [42643/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [42644/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [42645/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [42646/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [42647/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42648/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [42649/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [42650/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [42651/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [42652/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [42653/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [42654/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [42655/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [42656/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [42657/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [42658/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [42659/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [42660/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [42661/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42662/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [42663/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42664/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [42665/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42666/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [42667/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42668/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [42669/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [42670/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [42671/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [42672/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [42673/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [42674/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [42675/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [42676/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [42677/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [42678/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [42679/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [42680/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [42681/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [42682/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [42683/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [42684/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [42685/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [42686/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [42687/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [42688/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [42689/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [42690/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [42691/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [42692/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [42693/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42694/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [42695/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [42696/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [42697/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [42698/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [42699/50000], Train Loss: 8088941.5000, Val Loss: 5298620.5000\n",
      "Epoch [42700/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [42701/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [42702/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [42703/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [42704/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [42705/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [42706/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [42707/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [42708/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [42709/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [42710/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [42711/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [42712/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [42713/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42714/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [42715/50000], Train Loss: 8088942.5000, Val Loss: 5298634.0000\n",
      "Epoch [42716/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42717/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [42718/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42719/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [42720/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [42721/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [42722/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [42723/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42724/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [42725/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [42726/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [42727/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [42728/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [42729/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [42730/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [42731/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [42732/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [42733/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [42734/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [42735/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [42736/50000], Train Loss: 8088941.5000, Val Loss: 5298607.0000\n",
      "Epoch [42737/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [42738/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [42739/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [42740/50000], Train Loss: 8088941.5000, Val Loss: 5298608.5000\n",
      "Epoch [42741/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [42742/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [42743/50000], Train Loss: 8088941.5000, Val Loss: 5298668.5000\n",
      "Epoch [42744/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [42745/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [42746/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [42747/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42748/50000], Train Loss: 8088941.0000, Val Loss: 5298654.5000\n",
      "Epoch [42749/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [42750/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [42751/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [42752/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [42753/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [42754/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [42755/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [42756/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [42757/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [42758/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [42759/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [42760/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [42761/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [42762/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [42763/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [42764/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42765/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [42766/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [42767/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [42768/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [42769/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [42770/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [42771/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [42772/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [42773/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [42774/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [42775/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [42776/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [42777/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [42778/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [42779/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42780/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [42781/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42782/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [42783/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [42784/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [42785/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42786/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [42787/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [42788/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [42789/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [42790/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [42791/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [42792/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [42793/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [42794/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [42795/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [42796/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [42797/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [42798/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [42799/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [42800/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [42801/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [42802/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [42803/50000], Train Loss: 8088941.0000, Val Loss: 5298655.5000\n",
      "Epoch [42804/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [42805/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42806/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [42807/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [42808/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [42809/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [42810/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42811/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42812/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [42813/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [42814/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [42815/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [42816/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [42817/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [42818/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [42819/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [42820/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [42821/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [42822/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [42823/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [42824/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [42825/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [42826/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [42827/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [42828/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [42829/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [42830/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [42831/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42832/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [42833/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42834/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [42835/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [42836/50000], Train Loss: 8088941.0000, Val Loss: 5298659.0000\n",
      "Epoch [42837/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42838/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [42839/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [42840/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [42841/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [42842/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [42843/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [42844/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [42845/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [42846/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [42847/50000], Train Loss: 8088942.5000, Val Loss: 5298607.0000\n",
      "Epoch [42848/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [42849/50000], Train Loss: 8088942.5000, Val Loss: 5298601.5000\n",
      "Epoch [42850/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [42851/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [42852/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [42853/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [42854/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [42855/50000], Train Loss: 8088941.5000, Val Loss: 5298606.0000\n",
      "Epoch [42856/50000], Train Loss: 8088941.5000, Val Loss: 5298677.0000\n",
      "Epoch [42857/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [42858/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [42859/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [42860/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [42861/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [42862/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [42863/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [42864/50000], Train Loss: 8088941.5000, Val Loss: 5298626.5000\n",
      "Epoch [42865/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [42866/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [42867/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [42868/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [42869/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [42870/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [42871/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [42872/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42873/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [42874/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [42875/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [42876/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [42877/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [42878/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [42879/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [42880/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [42881/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [42882/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [42883/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [42884/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [42885/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42886/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [42887/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [42888/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42889/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [42890/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42891/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [42892/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [42893/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42894/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42895/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42896/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42897/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42898/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [42899/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [42900/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42901/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [42902/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [42903/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42904/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [42905/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [42906/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [42907/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [42908/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [42909/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [42910/50000], Train Loss: 8088941.5000, Val Loss: 5298659.0000\n",
      "Epoch [42911/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [42912/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [42913/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [42914/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [42915/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [42916/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [42917/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [42918/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [42919/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [42920/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [42921/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [42922/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [42923/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [42924/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [42925/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [42926/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [42927/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [42928/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [42929/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [42930/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [42931/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [42932/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [42933/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [42934/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [42935/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [42936/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [42937/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42938/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [42939/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [42940/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [42941/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [42942/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [42943/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [42944/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [42945/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [42946/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [42947/50000], Train Loss: 8088941.5000, Val Loss: 5298617.0000\n",
      "Epoch [42948/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [42949/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [42950/50000], Train Loss: 8088942.5000, Val Loss: 5298678.5000\n",
      "Epoch [42951/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [42952/50000], Train Loss: 8088942.5000, Val Loss: 5298675.5000\n",
      "Epoch [42953/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [42954/50000], Train Loss: 8088941.5000, Val Loss: 5298669.0000\n",
      "Epoch [42955/50000], Train Loss: 8088941.5000, Val Loss: 5298625.0000\n",
      "Epoch [42956/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [42957/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [42958/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [42959/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [42960/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [42961/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [42962/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [42963/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [42964/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [42965/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [42966/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [42967/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [42968/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [42969/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [42970/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [42971/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [42972/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [42973/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [42974/50000], Train Loss: 8088941.5000, Val Loss: 5298618.5000\n",
      "Epoch [42975/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [42976/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [42977/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [42978/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [42979/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [42980/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [42981/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [42982/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [42983/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42984/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [42985/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [42986/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [42987/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [42988/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [42989/50000], Train Loss: 8088941.0000, Val Loss: 5298659.0000\n",
      "Epoch [42990/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [42991/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [42992/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [42993/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [42994/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [42995/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [42996/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [42997/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [42998/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [42999/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [43000/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [43001/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [43002/50000], Train Loss: 8088942.5000, Val Loss: 5298611.5000\n",
      "Epoch [43003/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [43004/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [43005/50000], Train Loss: 8088943.0000, Val Loss: 5298671.5000\n",
      "Epoch [43006/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [43007/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [43008/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [43009/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [43010/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [43011/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43012/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [43013/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [43014/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [43015/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [43016/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43017/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [43018/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [43019/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [43020/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [43021/50000], Train Loss: 8088941.5000, Val Loss: 5298627.0000\n",
      "Epoch [43022/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [43023/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [43024/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [43025/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [43026/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [43027/50000], Train Loss: 8088941.5000, Val Loss: 5298618.0000\n",
      "Epoch [43028/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [43029/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [43030/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [43031/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [43032/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [43033/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [43034/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [43035/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [43036/50000], Train Loss: 8088941.0000, Val Loss: 5298655.5000\n",
      "Epoch [43037/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [43038/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43039/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [43040/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [43041/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [43042/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [43043/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [43044/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [43045/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [43046/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [43047/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [43048/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [43049/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [43050/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [43051/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [43052/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [43053/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [43054/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [43055/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [43056/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [43057/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [43058/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [43059/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [43060/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43061/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [43062/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [43063/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [43064/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [43065/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43066/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [43067/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [43068/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [43069/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [43070/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [43071/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [43072/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [43073/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [43074/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [43075/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43076/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [43077/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [43078/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [43079/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [43080/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43081/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43082/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43083/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43084/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43085/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43086/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43087/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43088/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43089/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [43090/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [43091/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43092/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [43093/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43094/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [43095/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43096/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43097/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43098/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43099/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43100/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43101/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [43102/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43103/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [43104/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43105/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43106/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43107/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43108/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43109/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43110/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [43111/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43112/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [43113/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [43114/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43115/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43116/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43117/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43118/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43119/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43120/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43121/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43122/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43123/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43124/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43125/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [43126/50000], Train Loss: 8088941.0000, Val Loss: 5298664.5000\n",
      "Epoch [43127/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [43128/50000], Train Loss: 8088941.5000, Val Loss: 5298683.0000\n",
      "Epoch [43129/50000], Train Loss: 8088941.5000, Val Loss: 5298594.0000\n",
      "Epoch [43130/50000], Train Loss: 8088941.5000, Val Loss: 5298710.0000\n",
      "Epoch [43131/50000], Train Loss: 8088942.0000, Val Loss: 5298560.5000\n",
      "Epoch [43132/50000], Train Loss: 8088942.0000, Val Loss: 5298750.0000\n",
      "Epoch [43133/50000], Train Loss: 8088942.0000, Val Loss: 5298521.0000\n",
      "Epoch [43134/50000], Train Loss: 8088942.0000, Val Loss: 5298775.5000\n",
      "Epoch [43135/50000], Train Loss: 8088943.0000, Val Loss: 5298525.5000\n",
      "Epoch [43136/50000], Train Loss: 8088942.0000, Val Loss: 5298729.0000\n",
      "Epoch [43137/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [43138/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [43139/50000], Train Loss: 8088942.5000, Val Loss: 5298712.0000\n",
      "Epoch [43140/50000], Train Loss: 8088942.0000, Val Loss: 5298554.5000\n",
      "Epoch [43141/50000], Train Loss: 8088942.0000, Val Loss: 5298727.5000\n",
      "Epoch [43142/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [43143/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [43144/50000], Train Loss: 8088942.5000, Val Loss: 5298683.0000\n",
      "Epoch [43145/50000], Train Loss: 8088941.5000, Val Loss: 5298579.5000\n",
      "Epoch [43146/50000], Train Loss: 8088941.5000, Val Loss: 5298710.0000\n",
      "Epoch [43147/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [43148/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [43149/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [43150/50000], Train Loss: 8088942.5000, Val Loss: 5298596.5000\n",
      "Epoch [43151/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [43152/50000], Train Loss: 8088941.5000, Val Loss: 5298613.0000\n",
      "Epoch [43153/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43154/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [43155/50000], Train Loss: 8088942.5000, Val Loss: 5298601.5000\n",
      "Epoch [43156/50000], Train Loss: 8088942.0000, Val Loss: 5298686.0000\n",
      "Epoch [43157/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [43158/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43159/50000], Train Loss: 8088941.5000, Val Loss: 5298669.0000\n",
      "Epoch [43160/50000], Train Loss: 8088941.5000, Val Loss: 5298609.0000\n",
      "Epoch [43161/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [43162/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [43163/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43164/50000], Train Loss: 8088941.0000, Val Loss: 5298665.5000\n",
      "Epoch [43165/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [43166/50000], Train Loss: 8088943.0000, Val Loss: 5298670.0000\n",
      "Epoch [43167/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [43168/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43169/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [43170/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [43171/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [43172/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [43173/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43174/50000], Train Loss: 8088941.0000, Val Loss: 5298659.0000\n",
      "Epoch [43175/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [43176/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [43177/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [43178/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43179/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [43180/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [43181/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [43182/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [43183/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43184/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [43185/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [43186/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [43187/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [43188/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43189/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43190/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [43191/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [43192/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43193/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43194/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [43195/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [43196/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43197/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43198/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43199/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43200/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [43201/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43202/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43203/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [43204/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [43205/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [43206/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43207/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [43208/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43209/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [43210/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43211/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43212/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43213/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43214/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43215/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43216/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [43217/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43218/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43219/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [43220/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [43221/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [43222/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43223/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43224/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43225/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43226/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43227/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43228/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43229/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43230/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43231/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43232/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43233/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43234/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43235/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43236/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43237/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43238/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43239/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43240/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43241/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43242/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43243/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43244/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43245/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [43246/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43247/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43248/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43249/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [43250/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43251/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [43252/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43253/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43254/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43255/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43256/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43257/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [43258/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43259/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43260/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43261/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43262/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43263/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43264/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43265/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43266/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43267/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43268/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43269/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43270/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43271/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [43272/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43273/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43274/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [43275/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43276/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43277/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43278/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43279/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43280/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [43281/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43282/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43283/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [43284/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43285/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43286/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [43287/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43288/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43289/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [43290/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43291/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43292/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43293/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43294/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43295/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43296/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43297/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43298/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43299/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43300/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43301/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43302/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43303/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43304/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [43305/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43306/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43307/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43308/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43309/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43310/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43311/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43312/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43313/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43314/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43315/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43316/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43317/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43318/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43319/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [43320/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43321/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43322/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43323/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43324/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43325/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43326/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43327/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43328/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43329/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43330/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43331/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43332/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43333/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43334/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [43335/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43336/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43337/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43338/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43339/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43340/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43341/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43342/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43343/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43344/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43345/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [43346/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43347/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43348/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43349/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [43350/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43351/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [43352/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43353/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43354/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43355/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43356/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43357/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43358/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43359/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43360/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [43361/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [43362/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43363/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [43364/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [43365/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43366/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [43367/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43368/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43369/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43370/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43371/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43372/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43373/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [43374/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43375/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43376/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [43377/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43378/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43379/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43380/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43381/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43382/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43383/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43384/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43385/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43386/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [43387/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43388/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43389/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43390/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43391/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43392/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43393/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [43394/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43395/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43396/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [43397/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43398/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [43399/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [43400/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [43401/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [43402/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [43403/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [43404/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [43405/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43406/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [43407/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43408/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43409/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [43410/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [43411/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [43412/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43413/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [43414/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [43415/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [43416/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [43417/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [43418/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [43419/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [43420/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43421/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [43422/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [43423/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [43424/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [43425/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [43426/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43427/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [43428/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [43429/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [43430/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43431/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43432/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43433/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43434/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43435/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43436/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43437/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43438/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43439/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43440/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43441/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [43442/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [43443/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43444/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [43445/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [43446/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [43447/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43448/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [43449/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [43450/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [43451/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [43452/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [43453/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [43454/50000], Train Loss: 8088941.5000, Val Loss: 5298587.5000\n",
      "Epoch [43455/50000], Train Loss: 8088941.5000, Val Loss: 5298712.5000\n",
      "Epoch [43456/50000], Train Loss: 8088942.0000, Val Loss: 5298563.0000\n",
      "Epoch [43457/50000], Train Loss: 8088942.0000, Val Loss: 5298738.5000\n",
      "Epoch [43458/50000], Train Loss: 8088942.0000, Val Loss: 5298541.5000\n",
      "Epoch [43459/50000], Train Loss: 8088942.0000, Val Loss: 5298747.5000\n",
      "Epoch [43460/50000], Train Loss: 8088942.0000, Val Loss: 5298553.5000\n",
      "Epoch [43461/50000], Train Loss: 8088942.0000, Val Loss: 5298707.5000\n",
      "Epoch [43462/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [43463/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [43464/50000], Train Loss: 8088942.5000, Val Loss: 5298683.5000\n",
      "Epoch [43465/50000], Train Loss: 8088942.0000, Val Loss: 5298584.0000\n",
      "Epoch [43466/50000], Train Loss: 8088942.0000, Val Loss: 5298711.0000\n",
      "Epoch [43467/50000], Train Loss: 8088941.5000, Val Loss: 5298586.5000\n",
      "Epoch [43468/50000], Train Loss: 8088941.5000, Val Loss: 5298682.0000\n",
      "Epoch [43469/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [43470/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [43471/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [43472/50000], Train Loss: 8088941.5000, Val Loss: 5298600.5000\n",
      "Epoch [43473/50000], Train Loss: 8088941.5000, Val Loss: 5298683.5000\n",
      "Epoch [43474/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [43475/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43476/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [43477/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [43478/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [43479/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [43480/50000], Train Loss: 8088941.5000, Val Loss: 5298663.0000\n",
      "Epoch [43481/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43482/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [43483/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [43484/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [43485/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [43486/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [43487/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [43488/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [43489/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [43490/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [43491/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [43492/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [43493/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43494/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [43495/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [43496/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [43497/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [43498/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [43499/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43500/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43501/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [43502/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [43503/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [43504/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43505/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43506/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [43507/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43508/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [43509/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43510/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [43511/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [43512/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [43513/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43514/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43515/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [43516/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43517/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43518/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43519/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43520/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [43521/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [43522/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43523/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43524/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43525/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [43526/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43527/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [43528/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [43529/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43530/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43531/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [43532/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [43533/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [43534/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43535/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [43536/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43537/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43538/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43539/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43540/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43541/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [43542/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43543/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [43544/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43545/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [43546/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [43547/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [43548/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43549/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [43550/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43551/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43552/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43553/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43554/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43555/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [43556/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43557/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43558/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43559/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43560/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43561/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43562/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43563/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43564/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43565/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43566/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43567/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43568/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43569/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43570/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43571/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [43572/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43573/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43574/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43575/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43576/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43577/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [43578/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43579/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43580/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43581/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43582/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43583/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43584/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43585/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43586/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [43587/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43588/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43589/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [43590/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43591/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43592/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43593/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43594/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43595/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43596/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [43597/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43598/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43599/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43600/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43601/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [43602/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43603/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43604/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43605/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43606/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43607/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [43608/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [43609/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43610/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [43611/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43612/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43613/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43614/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43615/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43616/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43617/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43618/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43619/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [43620/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [43621/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43622/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [43623/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [43624/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43625/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [43626/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43627/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43628/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43629/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43630/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [43631/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43632/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [43633/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43634/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43635/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [43636/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43637/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43638/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43639/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43640/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43641/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43642/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43643/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [43644/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43645/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43646/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [43647/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43648/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [43649/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43650/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [43651/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [43652/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [43653/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [43654/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [43655/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [43656/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [43657/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [43658/50000], Train Loss: 8088942.5000, Val Loss: 5298611.5000\n",
      "Epoch [43659/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [43660/50000], Train Loss: 8088942.5000, Val Loss: 5298600.5000\n",
      "Epoch [43661/50000], Train Loss: 8088941.5000, Val Loss: 5298695.5000\n",
      "Epoch [43662/50000], Train Loss: 8088942.0000, Val Loss: 5298584.5000\n",
      "Epoch [43663/50000], Train Loss: 8088942.0000, Val Loss: 5298711.5000\n",
      "Epoch [43664/50000], Train Loss: 8088941.5000, Val Loss: 5298571.5000\n",
      "Epoch [43665/50000], Train Loss: 8088942.0000, Val Loss: 5298719.0000\n",
      "Epoch [43666/50000], Train Loss: 8088942.5000, Val Loss: 5298571.5000\n",
      "Epoch [43667/50000], Train Loss: 8088942.0000, Val Loss: 5298709.5000\n",
      "Epoch [43668/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [43669/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [43670/50000], Train Loss: 8088942.5000, Val Loss: 5298633.5000\n",
      "Epoch [43671/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [43672/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [43673/50000], Train Loss: 8088942.5000, Val Loss: 5298601.5000\n",
      "Epoch [43674/50000], Train Loss: 8088942.0000, Val Loss: 5298696.0000\n",
      "Epoch [43675/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [43676/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [43677/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [43678/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [43679/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43680/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [43681/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [43682/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [43683/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [43684/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [43685/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [43686/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43687/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [43688/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [43689/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [43690/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [43691/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [43692/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [43693/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43694/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43695/50000], Train Loss: 8088941.0000, Val Loss: 5298655.0000\n",
      "Epoch [43696/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [43697/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [43698/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [43699/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43700/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [43701/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43702/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43703/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43704/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43705/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [43706/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [43707/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [43708/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43709/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [43710/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [43711/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43712/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43713/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43714/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [43715/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43716/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [43717/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [43718/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [43719/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [43720/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [43721/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [43722/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43723/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43724/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43725/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43726/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43727/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43728/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43729/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43730/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43731/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43732/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43733/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43734/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43735/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43736/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43737/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43738/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43739/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43740/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43741/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43742/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43743/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [43744/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43745/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43746/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43747/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43748/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43749/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43750/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43751/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43752/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [43753/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43754/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43755/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [43756/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43757/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43758/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43759/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43760/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43761/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43762/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43763/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43764/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43765/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [43766/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43767/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43768/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43769/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43770/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43771/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43772/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43773/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43774/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43775/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43776/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [43777/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43778/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43779/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43780/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43781/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43782/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [43783/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43784/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43785/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43786/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [43787/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43788/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43789/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [43790/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43791/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [43792/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43793/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [43794/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43795/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [43796/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43797/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43798/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [43799/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43800/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [43801/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [43802/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [43803/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [43804/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [43805/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [43806/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [43807/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [43808/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [43809/50000], Train Loss: 8088941.5000, Val Loss: 5298677.5000\n",
      "Epoch [43810/50000], Train Loss: 8088942.5000, Val Loss: 5298607.0000\n",
      "Epoch [43811/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [43812/50000], Train Loss: 8088942.0000, Val Loss: 5298594.5000\n",
      "Epoch [43813/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [43814/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [43815/50000], Train Loss: 8088941.5000, Val Loss: 5298710.0000\n",
      "Epoch [43816/50000], Train Loss: 8088942.0000, Val Loss: 5298577.5000\n",
      "Epoch [43817/50000], Train Loss: 8088942.0000, Val Loss: 5298709.5000\n",
      "Epoch [43818/50000], Train Loss: 8088942.0000, Val Loss: 5298585.0000\n",
      "Epoch [43819/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [43820/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [43821/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43822/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [43823/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [43824/50000], Train Loss: 8088942.5000, Val Loss: 5298679.0000\n",
      "Epoch [43825/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [43826/50000], Train Loss: 8088942.0000, Val Loss: 5298695.5000\n",
      "Epoch [43827/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [43828/50000], Train Loss: 8088941.5000, Val Loss: 5298687.0000\n",
      "Epoch [43829/50000], Train Loss: 8088941.5000, Val Loss: 5298615.0000\n",
      "Epoch [43830/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43831/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43832/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [43833/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [43834/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [43835/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [43836/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [43837/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [43838/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [43839/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [43840/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43841/50000], Train Loss: 8088941.5000, Val Loss: 5298622.0000\n",
      "Epoch [43842/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [43843/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [43844/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [43845/50000], Train Loss: 8088942.5000, Val Loss: 5298630.5000\n",
      "Epoch [43846/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43847/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43848/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [43849/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [43850/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [43851/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43852/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [43853/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [43854/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [43855/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [43856/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43857/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [43858/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [43859/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [43860/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [43861/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [43862/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [43863/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43864/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [43865/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [43866/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [43867/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [43868/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [43869/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [43870/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [43871/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [43872/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43873/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [43874/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [43875/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [43876/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [43877/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [43878/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [43879/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [43880/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [43881/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [43882/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [43883/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43884/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [43885/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [43886/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [43887/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43888/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [43889/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [43890/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [43891/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43892/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [43893/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [43894/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [43895/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43896/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [43897/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [43898/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [43899/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [43900/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [43901/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [43902/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [43903/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [43904/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [43905/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [43906/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43907/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [43908/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [43909/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43910/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [43911/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [43912/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [43913/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [43914/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [43915/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [43916/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [43917/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [43918/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [43919/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [43920/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [43921/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [43922/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [43923/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [43924/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [43925/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [43926/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [43927/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [43928/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [43929/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [43930/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [43931/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [43932/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [43933/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [43934/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [43935/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [43936/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [43937/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [43938/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [43939/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [43940/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [43941/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [43942/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [43943/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [43944/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [43945/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43946/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [43947/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [43948/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [43949/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [43950/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [43951/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [43952/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [43953/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [43954/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [43955/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [43956/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [43957/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [43958/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [43959/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [43960/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [43961/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [43962/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [43963/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43964/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [43965/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [43966/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [43967/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [43968/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [43969/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [43970/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [43971/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [43972/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [43973/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [43974/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [43975/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [43976/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [43977/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [43978/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [43979/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [43980/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [43981/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [43982/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [43983/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [43984/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [43985/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [43986/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [43987/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [43988/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [43989/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [43990/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [43991/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [43992/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [43993/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [43994/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [43995/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [43996/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [43997/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [43998/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [43999/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [44000/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [44001/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [44002/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [44003/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [44004/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [44005/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44006/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44007/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [44008/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44009/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44010/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44011/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [44012/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [44013/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44014/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [44015/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44016/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44017/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [44018/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [44019/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44020/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44021/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44022/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44023/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44024/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [44025/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [44026/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44027/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44028/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [44029/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44030/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44031/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [44032/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44033/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44034/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [44035/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44036/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44037/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [44038/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [44039/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [44040/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [44041/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [44042/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44043/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [44044/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [44045/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [44046/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [44047/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [44048/50000], Train Loss: 8088941.5000, Val Loss: 5298600.5000\n",
      "Epoch [44049/50000], Train Loss: 8088942.0000, Val Loss: 5298695.5000\n",
      "Epoch [44050/50000], Train Loss: 8088942.0000, Val Loss: 5298584.5000\n",
      "Epoch [44051/50000], Train Loss: 8088943.0000, Val Loss: 5298712.5000\n",
      "Epoch [44052/50000], Train Loss: 8088942.0000, Val Loss: 5298569.5000\n",
      "Epoch [44053/50000], Train Loss: 8088942.5000, Val Loss: 5298722.0000\n",
      "Epoch [44054/50000], Train Loss: 8088942.0000, Val Loss: 5298567.5000\n",
      "Epoch [44055/50000], Train Loss: 8088941.5000, Val Loss: 5298714.5000\n",
      "Epoch [44056/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [44057/50000], Train Loss: 8088941.5000, Val Loss: 5298683.0000\n",
      "Epoch [44058/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [44059/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44060/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [44061/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [44062/50000], Train Loss: 8088941.5000, Val Loss: 5298694.5000\n",
      "Epoch [44063/50000], Train Loss: 8088942.5000, Val Loss: 5298594.0000\n",
      "Epoch [44064/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [44065/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [44066/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44067/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [44068/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [44069/50000], Train Loss: 8088942.5000, Val Loss: 5298676.5000\n",
      "Epoch [44070/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [44071/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [44072/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [44073/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44074/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [44075/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [44076/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [44077/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [44078/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [44079/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [44080/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [44081/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44082/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44083/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44084/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44085/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [44086/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [44087/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [44088/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [44089/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [44090/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [44091/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44092/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44093/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [44094/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [44095/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [44096/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44097/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [44098/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44099/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [44100/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [44101/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44102/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44103/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [44104/50000], Train Loss: 8088942.5000, Val Loss: 5298630.5000\n",
      "Epoch [44105/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44106/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44107/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [44108/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [44109/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [44110/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [44111/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [44112/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44113/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [44114/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44115/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [44116/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [44117/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44118/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44119/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [44120/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44121/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44122/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44123/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [44124/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [44125/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [44126/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [44127/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44128/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44129/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [44130/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44131/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [44132/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44133/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [44134/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [44135/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [44136/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44137/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44138/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44139/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44140/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [44141/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [44142/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [44143/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44144/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44145/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44146/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44147/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44148/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [44149/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [44150/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [44151/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [44152/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [44153/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [44154/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [44155/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [44156/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [44157/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44158/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44159/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [44160/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [44161/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [44162/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44163/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [44164/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [44165/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [44166/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [44167/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [44168/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [44169/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44170/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44171/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44172/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44173/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44174/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [44175/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44176/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [44177/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [44178/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44179/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44180/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [44181/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44182/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [44183/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [44184/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [44185/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [44186/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [44187/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [44188/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [44189/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [44190/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [44191/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [44192/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [44193/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [44194/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [44195/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [44196/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [44197/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [44198/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [44199/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [44200/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [44201/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [44202/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [44203/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [44204/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [44205/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44206/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [44207/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44208/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44209/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [44210/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [44211/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44212/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44213/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [44214/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44215/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [44216/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [44217/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [44218/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [44219/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [44220/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [44221/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [44222/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [44223/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [44224/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [44225/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [44226/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44227/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [44228/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44229/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44230/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [44231/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44232/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44233/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [44234/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44235/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [44236/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [44237/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44238/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [44239/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44240/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [44241/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44242/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [44243/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [44244/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44245/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [44246/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [44247/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [44248/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [44249/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [44250/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [44251/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [44252/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [44253/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [44254/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [44255/50000], Train Loss: 8088942.5000, Val Loss: 5298683.0000\n",
      "Epoch [44256/50000], Train Loss: 8088941.5000, Val Loss: 5298602.5000\n",
      "Epoch [44257/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [44258/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [44259/50000], Train Loss: 8088941.5000, Val Loss: 5298687.0000\n",
      "Epoch [44260/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [44261/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [44262/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [44263/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [44264/50000], Train Loss: 8088942.5000, Val Loss: 5298629.5000\n",
      "Epoch [44265/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44266/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [44267/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [44268/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [44269/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [44270/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [44271/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [44272/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [44273/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [44274/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [44275/50000], Train Loss: 8088941.5000, Val Loss: 5298610.0000\n",
      "Epoch [44276/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [44277/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [44278/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [44279/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [44280/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44281/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44282/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [44283/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [44284/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44285/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [44286/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [44287/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [44288/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [44289/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [44290/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [44291/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44292/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [44293/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44294/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [44295/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44296/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44297/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [44298/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [44299/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44300/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [44301/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [44302/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44303/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44304/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44305/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44306/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [44307/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44308/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44309/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [44310/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44311/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44312/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44313/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [44314/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [44315/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [44316/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [44317/50000], Train Loss: 8088942.5000, Val Loss: 5298680.5000\n",
      "Epoch [44318/50000], Train Loss: 8088941.5000, Val Loss: 5298608.5000\n",
      "Epoch [44319/50000], Train Loss: 8088942.5000, Val Loss: 5298678.0000\n",
      "Epoch [44320/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [44321/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [44322/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [44323/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [44324/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [44325/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [44326/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [44327/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [44328/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [44329/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [44330/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44331/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44332/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [44333/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44334/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [44335/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [44336/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [44337/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [44338/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44339/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44340/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [44341/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [44342/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [44343/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [44344/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [44345/50000], Train Loss: 8088942.0000, Val Loss: 5298606.0000\n",
      "Epoch [44346/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [44347/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [44348/50000], Train Loss: 8088942.0000, Val Loss: 5298693.0000\n",
      "Epoch [44349/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [44350/50000], Train Loss: 8088942.5000, Val Loss: 5298689.5000\n",
      "Epoch [44351/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [44352/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [44353/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [44354/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [44355/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44356/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44357/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [44358/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [44359/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [44360/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [44361/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [44362/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [44363/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [44364/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [44365/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44366/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44367/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44368/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [44369/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [44370/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [44371/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [44372/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44373/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44374/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [44375/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [44376/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44377/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44378/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44379/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [44380/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44381/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [44382/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44383/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [44384/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [44385/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44386/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44387/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [44388/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [44389/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [44390/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [44391/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44392/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44393/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [44394/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44395/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44396/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [44397/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44398/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [44399/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [44400/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [44401/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [44402/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44403/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44404/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44405/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [44406/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [44407/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [44408/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44409/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44410/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44411/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [44412/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44413/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44414/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [44415/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44416/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [44417/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [44418/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [44419/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [44420/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [44421/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [44422/50000], Train Loss: 8088941.5000, Val Loss: 5298676.5000\n",
      "Epoch [44423/50000], Train Loss: 8088942.0000, Val Loss: 5298609.0000\n",
      "Epoch [44424/50000], Train Loss: 8088942.0000, Val Loss: 5298685.0000\n",
      "Epoch [44425/50000], Train Loss: 8088941.5000, Val Loss: 5298599.5000\n",
      "Epoch [44426/50000], Train Loss: 8088942.0000, Val Loss: 5298693.0000\n",
      "Epoch [44427/50000], Train Loss: 8088942.0000, Val Loss: 5298593.0000\n",
      "Epoch [44428/50000], Train Loss: 8088942.0000, Val Loss: 5298697.0000\n",
      "Epoch [44429/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [44430/50000], Train Loss: 8088942.0000, Val Loss: 5298698.0000\n",
      "Epoch [44431/50000], Train Loss: 8088942.5000, Val Loss: 5298592.5000\n",
      "Epoch [44432/50000], Train Loss: 8088942.0000, Val Loss: 5298692.5000\n",
      "Epoch [44433/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [44434/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [44435/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [44436/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [44437/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44438/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [44439/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [44440/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44441/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [44442/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [44443/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [44444/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [44445/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [44446/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [44447/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [44448/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44449/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [44450/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44451/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44452/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [44453/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44454/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44455/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [44456/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [44457/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [44458/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [44459/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [44460/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44461/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [44462/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [44463/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44464/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [44465/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44466/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [44467/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [44468/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44469/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [44470/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [44471/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [44472/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44473/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [44474/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44475/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44476/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [44477/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44478/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [44479/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [44480/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [44481/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [44482/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44483/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44484/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44485/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [44486/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [44487/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [44488/50000], Train Loss: 8088941.5000, Val Loss: 5298629.5000\n",
      "Epoch [44489/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [44490/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44491/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [44492/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [44493/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [44494/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [44495/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [44496/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [44497/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [44498/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [44499/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [44500/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [44501/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [44502/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [44503/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [44504/50000], Train Loss: 8088941.5000, Val Loss: 5298603.0000\n",
      "Epoch [44505/50000], Train Loss: 8088941.5000, Val Loss: 5298685.5000\n",
      "Epoch [44506/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [44507/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [44508/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [44509/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [44510/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [44511/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [44512/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [44513/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [44514/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [44515/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [44516/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [44517/50000], Train Loss: 8088941.5000, Val Loss: 5298631.5000\n",
      "Epoch [44518/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [44519/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [44520/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [44521/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [44522/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44523/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [44524/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44525/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44526/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [44527/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44528/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [44529/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [44530/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44531/50000], Train Loss: 8088942.5000, Val Loss: 5298645.5000\n",
      "Epoch [44532/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [44533/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44534/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44535/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [44536/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44537/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [44538/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [44539/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44540/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [44541/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [44542/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [44543/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [44544/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [44545/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [44546/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [44547/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [44548/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [44549/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44550/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [44551/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44552/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44553/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44554/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44555/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44556/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [44557/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [44558/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [44559/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [44560/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [44561/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [44562/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [44563/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [44564/50000], Train Loss: 8088942.5000, Val Loss: 5298680.0000\n",
      "Epoch [44565/50000], Train Loss: 8088942.5000, Val Loss: 5298605.0000\n",
      "Epoch [44566/50000], Train Loss: 8088942.5000, Val Loss: 5298687.0000\n",
      "Epoch [44567/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [44568/50000], Train Loss: 8088941.5000, Val Loss: 5298695.0000\n",
      "Epoch [44569/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [44570/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [44571/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [44572/50000], Train Loss: 8088941.0000, Val Loss: 5298698.0000\n",
      "Epoch [44573/50000], Train Loss: 8088942.5000, Val Loss: 5298596.5000\n",
      "Epoch [44574/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [44575/50000], Train Loss: 8088941.5000, Val Loss: 5298616.0000\n",
      "Epoch [44576/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [44577/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44578/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44579/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44580/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [44581/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [44582/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [44583/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [44584/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [44585/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [44586/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [44587/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [44588/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44589/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [44590/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44591/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44592/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [44593/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [44594/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [44595/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [44596/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [44597/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44598/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [44599/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [44600/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [44601/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [44602/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [44603/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [44604/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44605/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [44606/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [44607/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [44608/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [44609/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [44610/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44611/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44612/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [44613/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44614/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [44615/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [44616/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44617/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [44618/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44619/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44620/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44621/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44622/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [44623/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [44624/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [44625/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [44626/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [44627/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [44628/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [44629/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [44630/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44631/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [44632/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44633/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [44634/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [44635/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44636/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44637/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44638/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44639/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [44640/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [44641/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [44642/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [44643/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [44644/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [44645/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [44646/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [44647/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [44648/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [44649/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [44650/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [44651/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44652/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44653/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44654/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [44655/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [44656/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44657/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44658/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44659/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [44660/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [44661/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [44662/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44663/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [44664/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [44665/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [44666/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [44667/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [44668/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [44669/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [44670/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [44671/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [44672/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [44673/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [44674/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [44675/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [44676/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [44677/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [44678/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [44679/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44680/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [44681/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [44682/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44683/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [44684/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [44685/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44686/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [44687/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [44688/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44689/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44690/50000], Train Loss: 8088941.5000, Val Loss: 5298629.5000\n",
      "Epoch [44691/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [44692/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [44693/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [44694/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [44695/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [44696/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [44697/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [44698/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [44699/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [44700/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [44701/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [44702/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [44703/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [44704/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [44705/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [44706/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [44707/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [44708/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [44709/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44710/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44711/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44712/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [44713/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [44714/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44715/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [44716/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44717/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [44718/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [44719/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44720/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [44721/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [44722/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [44723/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [44724/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [44725/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [44726/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [44727/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [44728/50000], Train Loss: 8088941.5000, Val Loss: 5298670.5000\n",
      "Epoch [44729/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [44730/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [44731/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [44732/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [44733/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [44734/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44735/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [44736/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [44737/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [44738/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44739/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44740/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44741/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44742/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44743/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44744/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44745/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44746/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [44747/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [44748/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [44749/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [44750/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44751/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [44752/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [44753/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [44754/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [44755/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [44756/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [44757/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [44758/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [44759/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [44760/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [44761/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [44762/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [44763/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [44764/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [44765/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [44766/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [44767/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [44768/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [44769/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [44770/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [44771/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44772/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44773/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44774/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44775/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44776/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [44777/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44778/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44779/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [44780/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44781/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [44782/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [44783/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [44784/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [44785/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44786/50000], Train Loss: 8088941.5000, Val Loss: 5298630.5000\n",
      "Epoch [44787/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44788/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [44789/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [44790/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [44791/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [44792/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [44793/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [44794/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [44795/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [44796/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [44797/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [44798/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [44799/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [44800/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [44801/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [44802/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [44803/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [44804/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [44805/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [44806/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [44807/50000], Train Loss: 8088942.5000, Val Loss: 5298677.5000\n",
      "Epoch [44808/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [44809/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [44810/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [44811/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [44812/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [44813/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [44814/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44815/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [44816/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44817/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [44818/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [44819/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [44820/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44821/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44822/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [44823/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [44824/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [44825/50000], Train Loss: 8088941.5000, Val Loss: 5298630.5000\n",
      "Epoch [44826/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [44827/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [44828/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [44829/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [44830/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [44831/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [44832/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [44833/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [44834/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [44835/50000], Train Loss: 8088942.5000, Val Loss: 5298611.5000\n",
      "Epoch [44836/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [44837/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [44838/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [44839/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [44840/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [44841/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [44842/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [44843/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [44844/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [44845/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44846/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [44847/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [44848/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [44849/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [44850/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [44851/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [44852/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [44853/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [44854/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [44855/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [44856/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [44857/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [44858/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [44859/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [44860/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [44861/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44862/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [44863/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44864/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [44865/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [44866/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [44867/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [44868/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [44869/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [44870/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [44871/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [44872/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [44873/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [44874/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [44875/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [44876/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [44877/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [44878/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [44879/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [44880/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [44881/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44882/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [44883/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44884/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [44885/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [44886/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [44887/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44888/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44889/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44890/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [44891/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [44892/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44893/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44894/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [44895/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [44896/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [44897/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44898/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [44899/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [44900/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [44901/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44902/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [44903/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [44904/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [44905/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [44906/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [44907/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [44908/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [44909/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [44910/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44911/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44912/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44913/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44914/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [44915/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [44916/50000], Train Loss: 8088941.5000, Val Loss: 5298631.5000\n",
      "Epoch [44917/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [44918/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [44919/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [44920/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [44921/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [44922/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [44923/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [44924/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [44925/50000], Train Loss: 8088942.5000, Val Loss: 5298685.5000\n",
      "Epoch [44926/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [44927/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [44928/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [44929/50000], Train Loss: 8088942.0000, Val Loss: 5298697.0000\n",
      "Epoch [44930/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [44931/50000], Train Loss: 8088942.0000, Val Loss: 5298698.0000\n",
      "Epoch [44932/50000], Train Loss: 8088942.5000, Val Loss: 5298592.5000\n",
      "Epoch [44933/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [44934/50000], Train Loss: 8088942.0000, Val Loss: 5298602.5000\n",
      "Epoch [44935/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [44936/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [44937/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [44938/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [44939/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [44940/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [44941/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [44942/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [44943/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [44944/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [44945/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [44946/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [44947/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [44948/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [44949/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [44950/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [44951/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [44952/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [44953/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44954/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [44955/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44956/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44957/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [44958/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [44959/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [44960/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [44961/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [44962/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [44963/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [44964/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [44965/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [44966/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [44967/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44968/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [44969/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44970/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [44971/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [44972/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [44973/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [44974/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [44975/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [44976/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [44977/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [44978/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [44979/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [44980/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [44981/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [44982/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44983/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [44984/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [44985/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [44986/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [44987/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [44988/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [44989/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [44990/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [44991/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [44992/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [44993/50000], Train Loss: 8088942.5000, Val Loss: 5298679.0000\n",
      "Epoch [44994/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [44995/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [44996/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [44997/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [44998/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [44999/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45000/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45001/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45002/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45003/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [45004/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [45005/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [45006/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [45007/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [45008/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [45009/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [45010/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [45011/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [45012/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [45013/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [45014/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45015/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45016/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [45017/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45018/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [45019/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [45020/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [45021/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [45022/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [45023/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [45024/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [45025/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [45026/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [45027/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [45028/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [45029/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [45030/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [45031/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [45032/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45033/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [45034/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45035/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [45036/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [45037/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [45038/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [45039/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [45040/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [45041/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [45042/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [45043/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [45044/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45045/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [45046/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45047/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [45048/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [45049/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [45050/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [45051/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [45052/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [45053/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [45054/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [45055/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [45056/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [45057/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [45058/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [45059/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [45060/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45061/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45062/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [45063/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [45064/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45065/50000], Train Loss: 8088942.5000, Val Loss: 5298645.5000\n",
      "Epoch [45066/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45067/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45068/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45069/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45070/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [45071/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45072/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [45073/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45074/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [45075/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45076/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [45077/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [45078/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [45079/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [45080/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [45081/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45082/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45083/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [45084/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [45085/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45086/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [45087/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45088/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45089/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45090/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [45091/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45092/50000], Train Loss: 8088942.5000, Val Loss: 5298645.5000\n",
      "Epoch [45093/50000], Train Loss: 8088942.0000, Val Loss: 5298645.5000\n",
      "Epoch [45094/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45095/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45096/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [45097/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [45098/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [45099/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [45100/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45101/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [45102/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [45103/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [45104/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [45105/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [45106/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [45107/50000], Train Loss: 8088942.0000, Val Loss: 5298685.0000\n",
      "Epoch [45108/50000], Train Loss: 8088941.5000, Val Loss: 5298597.5000\n",
      "Epoch [45109/50000], Train Loss: 8088942.0000, Val Loss: 5298698.5000\n",
      "Epoch [45110/50000], Train Loss: 8088942.5000, Val Loss: 5298582.5000\n",
      "Epoch [45111/50000], Train Loss: 8088941.5000, Val Loss: 5298713.0000\n",
      "Epoch [45112/50000], Train Loss: 8088942.0000, Val Loss: 5298570.5000\n",
      "Epoch [45113/50000], Train Loss: 8088942.5000, Val Loss: 5298719.0000\n",
      "Epoch [45114/50000], Train Loss: 8088942.5000, Val Loss: 5298573.5000\n",
      "Epoch [45115/50000], Train Loss: 8088942.0000, Val Loss: 5298705.0000\n",
      "Epoch [45116/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [45117/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [45118/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45119/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [45120/50000], Train Loss: 8088942.5000, Val Loss: 5298677.5000\n",
      "Epoch [45121/50000], Train Loss: 8088942.5000, Val Loss: 5298604.5000\n",
      "Epoch [45122/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [45123/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [45124/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [45125/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [45126/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45127/50000], Train Loss: 8088941.0000, Val Loss: 5298662.0000\n",
      "Epoch [45128/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [45129/50000], Train Loss: 8088942.5000, Val Loss: 5298677.5000\n",
      "Epoch [45130/50000], Train Loss: 8088942.5000, Val Loss: 5298612.5000\n",
      "Epoch [45131/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [45132/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [45133/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [45134/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [45135/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [45136/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [45137/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [45138/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [45139/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [45140/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [45141/50000], Train Loss: 8088942.5000, Val Loss: 5298630.5000\n",
      "Epoch [45142/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45143/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [45144/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [45145/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [45146/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [45147/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [45148/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [45149/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [45150/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [45151/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [45152/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45153/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45154/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [45155/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [45156/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [45157/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45158/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [45159/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [45160/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [45161/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [45162/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [45163/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [45164/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45165/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [45166/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [45167/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45168/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45169/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45170/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [45171/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45172/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [45173/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45174/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [45175/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45176/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45177/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [45178/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [45179/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [45180/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45181/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45182/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [45183/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [45184/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [45185/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [45186/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [45187/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [45188/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45189/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [45190/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45191/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45192/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45193/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45194/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [45195/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45196/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45197/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45198/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [45199/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45200/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [45201/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [45202/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [45203/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [45204/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45205/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [45206/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45207/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45208/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [45209/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [45210/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45211/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [45212/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45213/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [45214/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45215/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [45216/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45217/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45218/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45219/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [45220/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45221/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45222/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [45223/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45224/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [45225/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45226/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [45227/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [45228/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [45229/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [45230/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [45231/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [45232/50000], Train Loss: 8088942.5000, Val Loss: 5298667.0000\n",
      "Epoch [45233/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [45234/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [45235/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [45236/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [45237/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [45238/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [45239/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [45240/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [45241/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [45242/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45243/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45244/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45245/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [45246/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45247/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45248/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [45249/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [45250/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [45251/50000], Train Loss: 8088942.5000, Val Loss: 5298673.0000\n",
      "Epoch [45252/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [45253/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [45254/50000], Train Loss: 8088942.5000, Val Loss: 5298598.0000\n",
      "Epoch [45255/50000], Train Loss: 8088942.0000, Val Loss: 5298695.0000\n",
      "Epoch [45256/50000], Train Loss: 8088942.0000, Val Loss: 5298591.5000\n",
      "Epoch [45257/50000], Train Loss: 8088942.0000, Val Loss: 5298698.0000\n",
      "Epoch [45258/50000], Train Loss: 8088942.5000, Val Loss: 5298592.5000\n",
      "Epoch [45259/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [45260/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [45261/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [45262/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [45263/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [45264/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [45265/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45266/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [45267/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [45268/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [45269/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [45270/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [45271/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45272/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45273/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45274/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [45275/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45276/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [45277/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [45278/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [45279/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [45280/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [45281/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45282/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45283/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [45284/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45285/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45286/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [45287/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [45288/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [45289/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [45290/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45291/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [45292/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [45293/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45294/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45295/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45296/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [45297/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [45298/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [45299/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [45300/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45301/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45302/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [45303/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [45304/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45305/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [45306/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45307/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45308/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45309/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45310/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [45311/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [45312/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [45313/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [45314/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45315/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [45316/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [45317/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45318/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45319/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45320/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45321/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [45322/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [45323/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [45324/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [45325/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [45326/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [45327/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [45328/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [45329/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45330/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45331/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45332/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45333/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [45334/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [45335/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [45336/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [45337/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45338/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45339/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45340/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [45341/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45342/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [45343/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45344/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45345/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45346/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [45347/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45348/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [45349/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45350/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45351/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [45352/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45353/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [45354/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45355/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45356/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [45357/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [45358/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [45359/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [45360/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [45361/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [45362/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [45363/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [45364/50000], Train Loss: 8088942.5000, Val Loss: 5298675.5000\n",
      "Epoch [45365/50000], Train Loss: 8088942.5000, Val Loss: 5298607.0000\n",
      "Epoch [45366/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [45367/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [45368/50000], Train Loss: 8088941.5000, Val Loss: 5298702.0000\n",
      "Epoch [45369/50000], Train Loss: 8088942.0000, Val Loss: 5298579.5000\n",
      "Epoch [45370/50000], Train Loss: 8088941.5000, Val Loss: 5298713.0000\n",
      "Epoch [45371/50000], Train Loss: 8088942.0000, Val Loss: 5298574.0000\n",
      "Epoch [45372/50000], Train Loss: 8088942.5000, Val Loss: 5298712.5000\n",
      "Epoch [45373/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [45374/50000], Train Loss: 8088941.5000, Val Loss: 5298694.5000\n",
      "Epoch [45375/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [45376/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [45377/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45378/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [45379/50000], Train Loss: 8088941.5000, Val Loss: 5298669.0000\n",
      "Epoch [45380/50000], Train Loss: 8088941.5000, Val Loss: 5298611.0000\n",
      "Epoch [45381/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [45382/50000], Train Loss: 8088942.5000, Val Loss: 5298607.0000\n",
      "Epoch [45383/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [45384/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [45385/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [45386/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [45387/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [45388/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [45389/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [45390/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [45391/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [45392/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [45393/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [45394/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [45395/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45396/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [45397/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [45398/50000], Train Loss: 8088941.5000, Val Loss: 5298627.0000\n",
      "Epoch [45399/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [45400/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [45401/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [45402/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45403/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45404/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45405/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45406/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45407/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45408/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [45409/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45410/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45411/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [45412/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45413/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [45414/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45415/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45416/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [45417/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45418/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45419/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45420/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45421/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45422/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45423/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45424/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [45425/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45426/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45427/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45428/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45429/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45430/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [45431/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45432/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45433/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45434/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [45435/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45436/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45437/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45438/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [45439/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [45440/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45441/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45442/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45443/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [45444/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45445/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45446/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [45447/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45448/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45449/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45450/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45451/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45452/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45453/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [45454/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45455/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45456/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [45457/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45458/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45459/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [45460/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45461/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45462/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45463/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [45464/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45465/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45466/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [45467/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45468/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45469/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45470/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45471/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [45472/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [45473/50000], Train Loss: 8088941.0000, Val Loss: 5298659.0000\n",
      "Epoch [45474/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [45475/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [45476/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [45477/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [45478/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [45479/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [45480/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [45481/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [45482/50000], Train Loss: 8088941.5000, Val Loss: 5298617.5000\n",
      "Epoch [45483/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [45484/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [45485/50000], Train Loss: 8088943.0000, Val Loss: 5298674.0000\n",
      "Epoch [45486/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [45487/50000], Train Loss: 8088943.0000, Val Loss: 5298673.0000\n",
      "Epoch [45488/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [45489/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [45490/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [45491/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [45492/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [45493/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [45494/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [45495/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [45496/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [45497/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [45498/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [45499/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [45500/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [45501/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [45502/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [45503/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [45504/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [45505/50000], Train Loss: 8088941.5000, Val Loss: 5298670.5000\n",
      "Epoch [45506/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [45507/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [45508/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45509/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [45510/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45511/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [45512/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45513/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [45514/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [45515/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45516/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45517/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [45518/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [45519/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45520/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [45521/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45522/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45523/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45524/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45525/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [45526/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45527/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45528/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45529/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [45530/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45531/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [45532/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [45533/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [45534/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [45535/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [45536/50000], Train Loss: 8088941.5000, Val Loss: 5298618.5000\n",
      "Epoch [45537/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [45538/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [45539/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [45540/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [45541/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [45542/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [45543/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [45544/50000], Train Loss: 8088941.5000, Val Loss: 5298597.5000\n",
      "Epoch [45545/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [45546/50000], Train Loss: 8088941.5000, Val Loss: 5298595.0000\n",
      "Epoch [45547/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [45548/50000], Train Loss: 8088942.0000, Val Loss: 5298600.5000\n",
      "Epoch [45549/50000], Train Loss: 8088941.5000, Val Loss: 5298682.0000\n",
      "Epoch [45550/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [45551/50000], Train Loss: 8088941.5000, Val Loss: 5298662.0000\n",
      "Epoch [45552/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45553/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45554/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [45555/50000], Train Loss: 8088941.5000, Val Loss: 5298623.5000\n",
      "Epoch [45556/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [45557/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [45558/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [45559/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [45560/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [45561/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [45562/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [45563/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [45564/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [45565/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45566/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45567/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [45568/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [45569/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [45570/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [45571/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [45572/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45573/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45574/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [45575/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [45576/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [45577/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45578/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45579/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45580/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45581/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45582/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [45583/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45584/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45585/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45586/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45587/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45588/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45589/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45590/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45591/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45592/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45593/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45594/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45595/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [45596/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [45597/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45598/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45599/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45600/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45601/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45602/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [45603/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45604/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45605/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [45606/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45607/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [45608/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45609/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [45610/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45611/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [45612/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45613/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [45614/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [45615/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45616/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [45617/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45618/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45619/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45620/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45621/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45622/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [45623/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45624/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [45625/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45626/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [45627/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45628/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [45629/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45630/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [45631/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45632/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45633/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [45634/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45635/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [45636/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [45637/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [45638/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [45639/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [45640/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [45641/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [45642/50000], Train Loss: 8088942.5000, Val Loss: 5298594.5000\n",
      "Epoch [45643/50000], Train Loss: 8088942.0000, Val Loss: 5298704.0000\n",
      "Epoch [45644/50000], Train Loss: 8088942.0000, Val Loss: 5298575.0000\n",
      "Epoch [45645/50000], Train Loss: 8088942.5000, Val Loss: 5298721.5000\n",
      "Epoch [45646/50000], Train Loss: 8088941.5000, Val Loss: 5298560.5000\n",
      "Epoch [45647/50000], Train Loss: 8088942.0000, Val Loss: 5298731.0000\n",
      "Epoch [45648/50000], Train Loss: 8088942.0000, Val Loss: 5298561.0000\n",
      "Epoch [45649/50000], Train Loss: 8088942.0000, Val Loss: 5298715.0000\n",
      "Epoch [45650/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [45651/50000], Train Loss: 8088941.5000, Val Loss: 5298669.0000\n",
      "Epoch [45652/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [45653/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [45654/50000], Train Loss: 8088942.5000, Val Loss: 5298688.5000\n",
      "Epoch [45655/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [45656/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [45657/50000], Train Loss: 8088941.5000, Val Loss: 5298607.0000\n",
      "Epoch [45658/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [45659/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45660/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [45661/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [45662/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [45663/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [45664/50000], Train Loss: 8088941.5000, Val Loss: 5298615.0000\n",
      "Epoch [45665/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [45666/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45667/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45668/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [45669/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [45670/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [45671/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [45672/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [45673/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45674/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45675/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45676/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45677/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45678/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45679/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45680/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45681/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [45682/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [45683/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [45684/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [45685/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45686/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45687/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45688/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45689/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [45690/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45691/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45692/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [45693/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45694/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45695/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [45696/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45697/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45698/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45699/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [45700/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45701/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45702/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45703/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45704/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [45705/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [45706/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45707/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [45708/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45709/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [45710/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45711/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45712/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45713/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [45714/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45715/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [45716/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45717/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [45718/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45719/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45720/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [45721/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45722/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [45723/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45724/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [45725/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [45726/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45727/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45728/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45729/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45730/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45731/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45732/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45733/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45734/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45735/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45736/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45737/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45738/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45739/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45740/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45741/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45742/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45743/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [45744/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45745/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [45746/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45747/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45748/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [45749/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45750/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45751/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [45752/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [45753/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45754/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [45755/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [45756/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45757/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45758/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45759/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45760/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [45761/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45762/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45763/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45764/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [45765/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45766/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [45767/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45768/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45769/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45770/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45771/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45772/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [45773/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [45774/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [45775/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [45776/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [45777/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [45778/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [45779/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [45780/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [45781/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [45782/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [45783/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [45784/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [45785/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [45786/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [45787/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [45788/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45789/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [45790/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45791/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [45792/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45793/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [45794/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45795/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [45796/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45797/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45798/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [45799/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [45800/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [45801/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [45802/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [45803/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45804/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45805/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45806/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45807/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45808/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45809/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45810/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45811/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [45812/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [45813/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45814/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45815/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [45816/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [45817/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45818/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [45819/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45820/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45821/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45822/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45823/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45824/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45825/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45826/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [45827/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45828/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45829/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [45830/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45831/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45832/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [45833/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [45834/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [45835/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [45836/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [45837/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [45838/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [45839/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [45840/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [45841/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [45842/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [45843/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [45844/50000], Train Loss: 8088942.5000, Val Loss: 5298595.0000\n",
      "Epoch [45845/50000], Train Loss: 8088942.0000, Val Loss: 5298704.0000\n",
      "Epoch [45846/50000], Train Loss: 8088942.0000, Val Loss: 5298573.5000\n",
      "Epoch [45847/50000], Train Loss: 8088942.0000, Val Loss: 5298726.0000\n",
      "Epoch [45848/50000], Train Loss: 8088942.0000, Val Loss: 5298553.5000\n",
      "Epoch [45849/50000], Train Loss: 8088942.0000, Val Loss: 5298739.5000\n",
      "Epoch [45850/50000], Train Loss: 8088941.5000, Val Loss: 5298552.5000\n",
      "Epoch [45851/50000], Train Loss: 8088942.0000, Val Loss: 5298722.5000\n",
      "Epoch [45852/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [45853/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [45854/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45855/50000], Train Loss: 8088941.5000, Val Loss: 5298608.0000\n",
      "Epoch [45856/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [45857/50000], Train Loss: 8088942.0000, Val Loss: 5298581.5000\n",
      "Epoch [45858/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [45859/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [45860/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [45861/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [45862/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [45863/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [45864/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [45865/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [45866/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [45867/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [45868/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [45869/50000], Train Loss: 8088941.5000, Val Loss: 5298619.5000\n",
      "Epoch [45870/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [45871/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [45872/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [45873/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45874/50000], Train Loss: 8088941.0000, Val Loss: 5298637.0000\n",
      "Epoch [45875/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [45876/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [45877/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [45878/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [45879/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [45880/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45881/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45882/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [45883/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [45884/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [45885/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [45886/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45887/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [45888/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [45889/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [45890/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45891/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [45892/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45893/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [45894/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [45895/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45896/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [45897/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [45898/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45899/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [45900/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45901/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45902/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45903/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45904/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [45905/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45906/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45907/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45908/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45909/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45910/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45911/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45912/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45913/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45914/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45915/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45916/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45917/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45918/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45919/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45920/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [45921/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [45922/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [45923/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45924/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [45925/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45926/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45927/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45928/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45929/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [45930/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [45931/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45932/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [45933/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45934/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [45935/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45936/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45937/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45938/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [45939/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45940/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [45941/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45942/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [45943/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45944/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45945/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45946/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45947/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [45948/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45949/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45950/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45951/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45952/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45953/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45954/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45955/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [45956/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [45957/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45958/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [45959/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [45960/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45961/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [45962/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [45963/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [45964/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [45965/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [45966/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [45967/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [45968/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [45969/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45970/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45971/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45972/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45973/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [45974/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45975/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45976/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45977/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45978/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45979/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45980/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45981/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45982/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [45983/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [45984/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [45985/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [45986/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [45987/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [45988/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [45989/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [45990/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [45991/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [45992/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [45993/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [45994/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [45995/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [45996/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [45997/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [45998/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [45999/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46000/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46001/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [46002/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [46003/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [46004/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [46005/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [46006/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [46007/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [46008/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [46009/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [46010/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46011/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [46012/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46013/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46014/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46015/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46016/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46017/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46018/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46019/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46020/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46021/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46022/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46023/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46024/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46025/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46026/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46027/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [46028/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [46029/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [46030/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46031/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [46032/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46033/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46034/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46035/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46036/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46037/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46038/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46039/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46040/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46041/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46042/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46043/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46044/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [46045/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [46046/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [46047/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [46048/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [46049/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [46050/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [46051/50000], Train Loss: 8088942.5000, Val Loss: 5298690.5000\n",
      "Epoch [46052/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [46053/50000], Train Loss: 8088941.5000, Val Loss: 5298714.5000\n",
      "Epoch [46054/50000], Train Loss: 8088942.0000, Val Loss: 5298562.5000\n",
      "Epoch [46055/50000], Train Loss: 8088942.5000, Val Loss: 5298736.5000\n",
      "Epoch [46056/50000], Train Loss: 8088942.0000, Val Loss: 5298546.5000\n",
      "Epoch [46057/50000], Train Loss: 8088942.0000, Val Loss: 5298739.5000\n",
      "Epoch [46058/50000], Train Loss: 8088942.0000, Val Loss: 5298562.5000\n",
      "Epoch [46059/50000], Train Loss: 8088942.5000, Val Loss: 5298703.0000\n",
      "Epoch [46060/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [46061/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46062/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [46063/50000], Train Loss: 8088942.5000, Val Loss: 5298592.5000\n",
      "Epoch [46064/50000], Train Loss: 8088942.0000, Val Loss: 5298703.0000\n",
      "Epoch [46065/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [46066/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [46067/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [46068/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [46069/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [46070/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [46071/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [46072/50000], Train Loss: 8088941.5000, Val Loss: 5298614.0000\n",
      "Epoch [46073/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [46074/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46075/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [46076/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [46077/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [46078/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [46079/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [46080/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [46081/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46082/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [46083/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [46084/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [46085/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [46086/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [46087/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46088/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46089/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [46090/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [46091/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [46092/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46093/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46094/50000], Train Loss: 8088941.0000, Val Loss: 5298643.5000\n",
      "Epoch [46095/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [46096/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46097/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46098/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46099/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46100/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46101/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46102/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46103/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46104/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46105/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [46106/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46107/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46108/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46109/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [46110/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46111/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46112/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46113/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [46114/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [46115/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46116/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [46117/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [46118/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46119/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [46120/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46121/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46122/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [46123/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [46124/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46125/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46126/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [46127/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46128/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46129/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46130/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46131/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [46132/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46133/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46134/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [46135/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46136/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46137/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46138/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [46139/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [46140/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46141/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46142/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [46143/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46144/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46145/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46146/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [46147/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46148/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46149/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46150/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46151/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46152/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [46153/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46154/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46155/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46156/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [46157/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46158/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46159/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [46160/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46161/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46162/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46163/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46164/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [46165/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46166/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46167/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [46168/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46169/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [46170/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46171/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [46172/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46173/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [46174/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46175/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46176/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [46177/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46178/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46179/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46180/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46181/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46182/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46183/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46184/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46185/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [46186/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46187/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46188/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46189/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [46190/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46191/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46192/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46193/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46194/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46195/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [46196/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46197/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46198/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [46199/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46200/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46201/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46202/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46203/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46204/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46205/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46206/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46207/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [46208/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [46209/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [46210/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [46211/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [46212/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [46213/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [46214/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [46215/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [46216/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [46217/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [46218/50000], Train Loss: 8088941.5000, Val Loss: 5298671.0000\n",
      "Epoch [46219/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [46220/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [46221/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [46222/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [46223/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [46224/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [46225/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46226/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [46227/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [46228/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46229/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46230/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [46231/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [46232/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [46233/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [46234/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [46235/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [46236/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [46237/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [46238/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [46239/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [46240/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [46241/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [46242/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [46243/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [46244/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [46245/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [46246/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [46247/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [46248/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [46249/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [46250/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [46251/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46252/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46253/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [46254/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [46255/50000], Train Loss: 8088942.5000, Val Loss: 5298638.0000\n",
      "Epoch [46256/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46257/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [46258/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [46259/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [46260/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [46261/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [46262/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [46263/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [46264/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [46265/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [46266/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [46267/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [46268/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [46269/50000], Train Loss: 8088942.5000, Val Loss: 5298626.5000\n",
      "Epoch [46270/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [46271/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [46272/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46273/50000], Train Loss: 8088941.5000, Val Loss: 5298638.0000\n",
      "Epoch [46274/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46275/50000], Train Loss: 8088942.0000, Val Loss: 5298638.0000\n",
      "Epoch [46276/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46277/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46278/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46279/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46280/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46281/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46282/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46283/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46284/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [46285/50000], Train Loss: 8088941.5000, Val Loss: 5298645.5000\n",
      "Epoch [46286/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46287/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46288/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46289/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46290/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46291/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46292/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46293/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [46294/50000], Train Loss: 8088941.5000, Val Loss: 5298630.5000\n",
      "Epoch [46295/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [46296/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [46297/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [46298/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [46299/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [46300/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [46301/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [46302/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [46303/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [46304/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [46305/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [46306/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [46307/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [46308/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [46309/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [46310/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [46311/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [46312/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [46313/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [46314/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46315/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [46316/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [46317/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [46318/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [46319/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [46320/50000], Train Loss: 8088942.0000, Val Loss: 5298622.0000\n",
      "Epoch [46321/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [46322/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [46323/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [46324/50000], Train Loss: 8088942.0000, Val Loss: 5298613.0000\n",
      "Epoch [46325/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [46326/50000], Train Loss: 8088942.5000, Val Loss: 5298612.5000\n",
      "Epoch [46327/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [46328/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [46329/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [46330/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [46331/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [46332/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [46333/50000], Train Loss: 8088942.0000, Val Loss: 5298667.0000\n",
      "Epoch [46334/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [46335/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [46336/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [46337/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [46338/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [46339/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [46340/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [46341/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [46342/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [46343/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [46344/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [46345/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46346/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [46347/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46348/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46349/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [46350/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [46351/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [46352/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [46353/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [46354/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [46355/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [46356/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [46357/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [46358/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [46359/50000], Train Loss: 8088942.5000, Val Loss: 5298607.0000\n",
      "Epoch [46360/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [46361/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [46362/50000], Train Loss: 8088942.0000, Val Loss: 5298678.0000\n",
      "Epoch [46363/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [46364/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [46365/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [46366/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [46367/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [46368/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [46369/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [46370/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46371/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [46372/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46373/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46374/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46375/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46376/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [46377/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [46378/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [46379/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [46380/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46381/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [46382/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [46383/50000], Train Loss: 8088941.5000, Val Loss: 5298631.5000\n",
      "Epoch [46384/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [46385/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [46386/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [46387/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [46388/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [46389/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [46390/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [46391/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [46392/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [46393/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [46394/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [46395/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [46396/50000], Train Loss: 8088942.0000, Val Loss: 5298667.5000\n",
      "Epoch [46397/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [46398/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [46399/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [46400/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [46401/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46402/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46403/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46404/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46405/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46406/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [46407/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [46408/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46409/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46410/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [46411/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [46412/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [46413/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [46414/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [46415/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [46416/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [46417/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [46418/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [46419/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [46420/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [46421/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [46422/50000], Train Loss: 8088942.5000, Val Loss: 5298627.5000\n",
      "Epoch [46423/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [46424/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46425/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46426/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46427/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46428/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [46429/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [46430/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [46431/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [46432/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [46433/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [46434/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [46435/50000], Train Loss: 8088941.5000, Val Loss: 5298608.0000\n",
      "Epoch [46436/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [46437/50000], Train Loss: 8088941.5000, Val Loss: 5298610.0000\n",
      "Epoch [46438/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [46439/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [46440/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [46441/50000], Train Loss: 8088942.5000, Val Loss: 5298630.5000\n",
      "Epoch [46442/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46443/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46444/50000], Train Loss: 8088941.0000, Val Loss: 5298643.5000\n",
      "Epoch [46445/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [46446/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [46447/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [46448/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [46449/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [46450/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [46451/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [46452/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [46453/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [46454/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [46455/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [46456/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [46457/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [46458/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [46459/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [46460/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [46461/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [46462/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46463/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [46464/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [46465/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46466/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46467/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46468/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46469/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [46470/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [46471/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [46472/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [46473/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [46474/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [46475/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [46476/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [46477/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [46478/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [46479/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [46480/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [46481/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [46482/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [46483/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [46484/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46485/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [46486/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [46487/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [46488/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46489/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46490/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [46491/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46492/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [46493/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46494/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46495/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46496/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46497/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46498/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46499/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46500/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [46501/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [46502/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [46503/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [46504/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [46505/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [46506/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46507/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46508/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46509/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [46510/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [46511/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [46512/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [46513/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [46514/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [46515/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [46516/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [46517/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [46518/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [46519/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [46520/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46521/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46522/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [46523/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [46524/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46525/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [46526/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [46527/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [46528/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [46529/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [46530/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [46531/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [46532/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [46533/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [46534/50000], Train Loss: 8088942.0000, Val Loss: 5298697.0000\n",
      "Epoch [46535/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [46536/50000], Train Loss: 8088941.5000, Val Loss: 5298715.0000\n",
      "Epoch [46537/50000], Train Loss: 8088942.0000, Val Loss: 5298565.5000\n",
      "Epoch [46538/50000], Train Loss: 8088942.0000, Val Loss: 5298729.0000\n",
      "Epoch [46539/50000], Train Loss: 8088942.5000, Val Loss: 5298560.5000\n",
      "Epoch [46540/50000], Train Loss: 8088942.0000, Val Loss: 5298720.0000\n",
      "Epoch [46541/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [46542/50000], Train Loss: 8088941.5000, Val Loss: 5298678.5000\n",
      "Epoch [46543/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46544/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [46545/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [46546/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [46547/50000], Train Loss: 8088942.0000, Val Loss: 5298695.0000\n",
      "Epoch [46548/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [46549/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [46550/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [46551/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [46552/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [46553/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [46554/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [46555/50000], Train Loss: 8088942.0000, Val Loss: 5298602.5000\n",
      "Epoch [46556/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [46557/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [46558/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46559/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46560/50000], Train Loss: 8088941.5000, Val Loss: 5298626.0000\n",
      "Epoch [46561/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [46562/50000], Train Loss: 8088941.5000, Val Loss: 5298619.5000\n",
      "Epoch [46563/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [46564/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [46565/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46566/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46567/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [46568/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [46569/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [46570/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [46571/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [46572/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [46573/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46574/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [46575/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [46576/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [46577/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [46578/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [46579/50000], Train Loss: 8088941.0000, Val Loss: 5298655.0000\n",
      "Epoch [46580/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46581/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46582/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46583/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46584/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [46585/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [46586/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [46587/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46588/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46589/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46590/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46591/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46592/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [46593/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [46594/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [46595/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46596/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [46597/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46598/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46599/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46600/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46601/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [46602/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46603/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46604/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [46605/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46606/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46607/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46608/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46609/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46610/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46611/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [46612/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46613/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [46614/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46615/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46616/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [46617/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46618/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46619/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46620/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46621/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46622/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46623/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46624/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46625/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46626/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46627/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46628/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [46629/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46630/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46631/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [46632/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46633/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46634/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46635/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46636/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46637/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [46638/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46639/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46640/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [46641/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46642/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46643/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46644/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [46645/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46646/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46647/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46648/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46649/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46650/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [46651/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46652/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46653/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46654/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46655/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46656/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46657/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46658/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46659/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46660/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46661/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46662/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46663/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46664/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46665/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [46666/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46667/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46668/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46669/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46670/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46671/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [46672/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46673/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46674/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [46675/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46676/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46677/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46678/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [46679/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46680/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [46681/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46682/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46683/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [46684/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46685/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46686/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46687/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [46688/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [46689/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46690/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46691/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46692/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46693/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46694/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46695/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46696/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46697/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46698/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46699/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [46700/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [46701/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [46702/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46703/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [46704/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46705/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [46706/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46707/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46708/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46709/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46710/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46711/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46712/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46713/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [46714/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46715/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [46716/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46717/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46718/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46719/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46720/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46721/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46722/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46723/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [46724/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [46725/50000], Train Loss: 8088941.0000, Val Loss: 5298661.5000\n",
      "Epoch [46726/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [46727/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [46728/50000], Train Loss: 8088942.5000, Val Loss: 5298611.5000\n",
      "Epoch [46729/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [46730/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [46731/50000], Train Loss: 8088942.0000, Val Loss: 5298703.0000\n",
      "Epoch [46732/50000], Train Loss: 8088942.0000, Val Loss: 5298572.5000\n",
      "Epoch [46733/50000], Train Loss: 8088942.0000, Val Loss: 5298731.0000\n",
      "Epoch [46734/50000], Train Loss: 8088942.0000, Val Loss: 5298544.5000\n",
      "Epoch [46735/50000], Train Loss: 8088942.0000, Val Loss: 5298753.0000\n",
      "Epoch [46736/50000], Train Loss: 8088942.5000, Val Loss: 5298535.5000\n",
      "Epoch [46737/50000], Train Loss: 8088942.0000, Val Loss: 5298739.0000\n",
      "Epoch [46738/50000], Train Loss: 8088941.5000, Val Loss: 5298578.5000\n",
      "Epoch [46739/50000], Train Loss: 8088941.5000, Val Loss: 5298671.0000\n",
      "Epoch [46740/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [46741/50000], Train Loss: 8088942.0000, Val Loss: 5298595.5000\n",
      "Epoch [46742/50000], Train Loss: 8088942.0000, Val Loss: 5298715.0000\n",
      "Epoch [46743/50000], Train Loss: 8088942.0000, Val Loss: 5298568.5000\n",
      "Epoch [46744/50000], Train Loss: 8088942.0000, Val Loss: 5298706.5000\n",
      "Epoch [46745/50000], Train Loss: 8088942.0000, Val Loss: 5298610.5000\n",
      "Epoch [46746/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [46747/50000], Train Loss: 8088941.5000, Val Loss: 5298677.0000\n",
      "Epoch [46748/50000], Train Loss: 8088942.5000, Val Loss: 5298592.0000\n",
      "Epoch [46749/50000], Train Loss: 8088942.0000, Val Loss: 5298700.5000\n",
      "Epoch [46750/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [46751/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [46752/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46753/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [46754/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [46755/50000], Train Loss: 8088942.0000, Val Loss: 5298602.5000\n",
      "Epoch [46756/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [46757/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46758/50000], Train Loss: 8088942.5000, Val Loss: 5298632.0000\n",
      "Epoch [46759/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [46760/50000], Train Loss: 8088942.0000, Val Loss: 5298610.0000\n",
      "Epoch [46761/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [46762/50000], Train Loss: 8088942.5000, Val Loss: 5298631.0000\n",
      "Epoch [46763/50000], Train Loss: 8088941.0000, Val Loss: 5298640.0000\n",
      "Epoch [46764/50000], Train Loss: 8088941.0000, Val Loss: 5298664.5000\n",
      "Epoch [46765/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [46766/50000], Train Loss: 8088943.0000, Val Loss: 5298674.0000\n",
      "Epoch [46767/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [46768/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [46769/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [46770/50000], Train Loss: 8088942.5000, Val Loss: 5298625.5000\n",
      "Epoch [46771/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [46772/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [46773/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [46774/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46775/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [46776/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [46777/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [46778/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [46779/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [46780/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [46781/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46782/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46783/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46784/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46785/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46786/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46787/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46788/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46789/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46790/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46791/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46792/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [46793/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46794/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46795/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46796/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46797/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46798/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46799/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46800/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46801/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46802/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46803/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46804/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46805/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46806/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46807/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46808/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [46809/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46810/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46811/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46812/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [46813/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46814/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46815/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46816/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [46817/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [46818/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46819/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46820/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46821/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [46822/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46823/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [46824/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46825/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [46826/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46827/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [46828/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46829/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [46830/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46831/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46832/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46833/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [46834/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46835/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46836/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46837/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [46838/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46839/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [46840/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [46841/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [46842/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [46843/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46844/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [46845/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [46846/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46847/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46848/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [46849/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46850/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46851/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46852/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [46853/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46854/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46855/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46856/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46857/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [46858/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [46859/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [46860/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46861/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [46862/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46863/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46864/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46865/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [46866/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46867/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [46868/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46869/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [46870/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46871/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46872/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46873/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46874/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46875/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46876/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46877/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [46878/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46879/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [46880/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46881/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [46882/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [46883/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [46884/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [46885/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [46886/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [46887/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [46888/50000], Train Loss: 8088941.0000, Val Loss: 5298656.0000\n",
      "Epoch [46889/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [46890/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [46891/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [46892/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [46893/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46894/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [46895/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [46896/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [46897/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [46898/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [46899/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46900/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46901/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46902/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [46903/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [46904/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [46905/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [46906/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [46907/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [46908/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [46909/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [46910/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [46911/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [46912/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46913/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46914/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46915/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [46916/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [46917/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [46918/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [46919/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [46920/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [46921/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [46922/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46923/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [46924/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46925/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [46926/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [46927/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [46928/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46929/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46930/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46931/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46932/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [46933/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46934/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46935/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [46936/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [46937/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [46938/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [46939/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46940/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [46941/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46942/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [46943/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [46944/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46945/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [46946/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46947/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [46948/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46949/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [46950/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [46951/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [46952/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [46953/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [46954/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [46955/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [46956/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [46957/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [46958/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [46959/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [46960/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [46961/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [46962/50000], Train Loss: 8088943.0000, Val Loss: 5298675.5000\n",
      "Epoch [46963/50000], Train Loss: 8088942.5000, Val Loss: 5298610.5000\n",
      "Epoch [46964/50000], Train Loss: 8088942.5000, Val Loss: 5298679.0000\n",
      "Epoch [46965/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [46966/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [46967/50000], Train Loss: 8088941.5000, Val Loss: 5298608.5000\n",
      "Epoch [46968/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [46969/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [46970/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [46971/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [46972/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [46973/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [46974/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [46975/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [46976/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [46977/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [46978/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [46979/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [46980/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [46981/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [46982/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [46983/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [46984/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [46985/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [46986/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [46987/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [46988/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [46989/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [46990/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [46991/50000], Train Loss: 8088942.5000, Val Loss: 5298677.0000\n",
      "Epoch [46992/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [46993/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [46994/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [46995/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [46996/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [46997/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [46998/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [46999/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47000/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [47001/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47002/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [47003/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47004/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [47005/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47006/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47007/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [47008/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [47009/50000], Train Loss: 8088941.5000, Val Loss: 5298619.5000\n",
      "Epoch [47010/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [47011/50000], Train Loss: 8088941.5000, Val Loss: 5298621.5000\n",
      "Epoch [47012/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [47013/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [47014/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [47015/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [47016/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [47017/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [47018/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [47019/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [47020/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47021/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47022/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [47023/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [47024/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [47025/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [47026/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [47027/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [47028/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [47029/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47030/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [47031/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [47032/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [47033/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [47034/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [47035/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [47036/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [47037/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [47038/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [47039/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [47040/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [47041/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47042/50000], Train Loss: 8088941.0000, Val Loss: 5298655.5000\n",
      "Epoch [47043/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [47044/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47045/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [47046/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [47047/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [47048/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [47049/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47050/50000], Train Loss: 8088941.0000, Val Loss: 5298659.0000\n",
      "Epoch [47051/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47052/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [47053/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [47054/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [47055/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47056/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47057/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [47058/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [47059/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [47060/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [47061/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [47062/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47063/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [47064/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [47065/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [47066/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47067/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47068/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [47069/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47070/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [47071/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [47072/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [47073/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47074/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [47075/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [47076/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [47077/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [47078/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [47079/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47080/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [47081/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [47082/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [47083/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47084/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47085/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [47086/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [47087/50000], Train Loss: 8088941.5000, Val Loss: 5298618.0000\n",
      "Epoch [47088/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [47089/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [47090/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [47091/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [47092/50000], Train Loss: 8088943.0000, Val Loss: 5298674.0000\n",
      "Epoch [47093/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [47094/50000], Train Loss: 8088943.0000, Val Loss: 5298672.5000\n",
      "Epoch [47095/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [47096/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [47097/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [47098/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [47099/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [47100/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47101/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [47102/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47103/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47104/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [47105/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47106/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47107/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [47108/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [47109/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47110/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [47111/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [47112/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [47113/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [47114/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [47115/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [47116/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [47117/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [47118/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47119/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [47120/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [47121/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [47122/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [47123/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [47124/50000], Train Loss: 8088941.5000, Val Loss: 5298674.0000\n",
      "Epoch [47125/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [47126/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [47127/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [47128/50000], Train Loss: 8088942.5000, Val Loss: 5298676.5000\n",
      "Epoch [47129/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [47130/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [47131/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [47132/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [47133/50000], Train Loss: 8088941.5000, Val Loss: 5298623.5000\n",
      "Epoch [47134/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [47135/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47136/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [47137/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [47138/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [47139/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [47140/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [47141/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [47142/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [47143/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47144/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [47145/50000], Train Loss: 8088942.0000, Val Loss: 5298671.0000\n",
      "Epoch [47146/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [47147/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [47148/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [47149/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [47150/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [47151/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [47152/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [47153/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [47154/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [47155/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [47156/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47157/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [47158/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [47159/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [47160/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [47161/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47162/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47163/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [47164/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47165/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47166/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [47167/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47168/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [47169/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [47170/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [47171/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47172/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [47173/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47174/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [47175/50000], Train Loss: 8088941.5000, Val Loss: 5298617.0000\n",
      "Epoch [47176/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [47177/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [47178/50000], Train Loss: 8088941.5000, Val Loss: 5298671.5000\n",
      "Epoch [47179/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [47180/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [47181/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [47182/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [47183/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [47184/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47185/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [47186/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [47187/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47188/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47189/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [47190/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47191/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47192/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [47193/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [47194/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [47195/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [47196/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [47197/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47198/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [47199/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [47200/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47201/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [47202/50000], Train Loss: 8088941.5000, Val Loss: 5298649.0000\n",
      "Epoch [47203/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47204/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [47205/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [47206/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [47207/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47208/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [47209/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [47210/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [47211/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47212/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47213/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [47214/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [47215/50000], Train Loss: 8088941.5000, Val Loss: 5298617.0000\n",
      "Epoch [47216/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [47217/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [47218/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [47219/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [47220/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [47221/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [47222/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [47223/50000], Train Loss: 8088942.5000, Val Loss: 5298594.0000\n",
      "Epoch [47224/50000], Train Loss: 8088941.5000, Val Loss: 5298694.5000\n",
      "Epoch [47225/50000], Train Loss: 8088942.5000, Val Loss: 5298598.0000\n",
      "Epoch [47226/50000], Train Loss: 8088941.5000, Val Loss: 5298687.0000\n",
      "Epoch [47227/50000], Train Loss: 8088941.5000, Val Loss: 5298608.5000\n",
      "Epoch [47228/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [47229/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [47230/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [47231/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47232/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [47233/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [47234/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [47235/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47236/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [47237/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47238/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [47239/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [47240/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47241/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [47242/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [47243/50000], Train Loss: 8088941.0000, Val Loss: 5298644.5000\n",
      "Epoch [47244/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [47245/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [47246/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [47247/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [47248/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [47249/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47250/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [47251/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47252/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [47253/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [47254/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [47255/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47256/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [47257/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [47258/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [47259/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [47260/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47261/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [47262/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47263/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [47264/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [47265/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47266/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47267/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47268/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [47269/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47270/50000], Train Loss: 8088942.5000, Val Loss: 5298656.0000\n",
      "Epoch [47271/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47272/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [47273/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47274/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [47275/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [47276/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [47277/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47278/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [47279/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [47280/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [47281/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [47282/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47283/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [47284/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [47285/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [47286/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47287/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [47288/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47289/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [47290/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [47291/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [47292/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [47293/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47294/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [47295/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [47296/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47297/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47298/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47299/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [47300/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [47301/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [47302/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [47303/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [47304/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [47305/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [47306/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [47307/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47308/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47309/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [47310/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [47311/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [47312/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [47313/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [47314/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [47315/50000], Train Loss: 8088941.5000, Val Loss: 5298603.0000\n",
      "Epoch [47316/50000], Train Loss: 8088941.5000, Val Loss: 5298688.5000\n",
      "Epoch [47317/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [47318/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [47319/50000], Train Loss: 8088941.5000, Val Loss: 5298596.5000\n",
      "Epoch [47320/50000], Train Loss: 8088942.5000, Val Loss: 5298690.5000\n",
      "Epoch [47321/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [47322/50000], Train Loss: 8088941.5000, Val Loss: 5298680.5000\n",
      "Epoch [47323/50000], Train Loss: 8088941.5000, Val Loss: 5298615.5000\n",
      "Epoch [47324/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [47325/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [47326/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [47327/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47328/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47329/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [47330/50000], Train Loss: 8088941.5000, Val Loss: 5298626.0000\n",
      "Epoch [47331/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [47332/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [47333/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [47334/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [47335/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [47336/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [47337/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [47338/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [47339/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [47340/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [47341/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47342/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [47343/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [47344/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [47345/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [47346/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [47347/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [47348/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [47349/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [47350/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [47351/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [47352/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [47353/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47354/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [47355/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47356/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [47357/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [47358/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47359/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [47360/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [47361/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47362/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47363/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [47364/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [47365/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47366/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47367/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [47368/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47369/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47370/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47371/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47372/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47373/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47374/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47375/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47376/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47377/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [47378/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [47379/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [47380/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [47381/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47382/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47383/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47384/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [47385/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47386/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [47387/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [47388/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [47389/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47390/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47391/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47392/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47393/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47394/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47395/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [47396/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [47397/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [47398/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [47399/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [47400/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [47401/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [47402/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [47403/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [47404/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [47405/50000], Train Loss: 8088941.5000, Val Loss: 5298606.0000\n",
      "Epoch [47406/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [47407/50000], Train Loss: 8088942.5000, Val Loss: 5298604.5000\n",
      "Epoch [47408/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [47409/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [47410/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [47411/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [47412/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [47413/50000], Train Loss: 8088942.5000, Val Loss: 5298624.5000\n",
      "Epoch [47414/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [47415/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47416/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [47417/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [47418/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47419/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [47420/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47421/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [47422/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [47423/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [47424/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47425/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [47426/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [47427/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47428/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [47429/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [47430/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47431/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [47432/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [47433/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47434/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [47435/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47436/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [47437/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [47438/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47439/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47440/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [47441/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [47442/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [47443/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [47444/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [47445/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47446/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [47447/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47448/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47449/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [47450/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47451/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47452/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47453/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47454/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47455/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47456/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47457/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47458/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47459/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [47460/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47461/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [47462/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [47463/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47464/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47465/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [47466/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [47467/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [47468/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47469/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47470/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47471/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [47472/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [47473/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47474/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [47475/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [47476/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47477/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [47478/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47479/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [47480/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [47481/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [47482/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [47483/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [47484/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [47485/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [47486/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [47487/50000], Train Loss: 8088942.0000, Val Loss: 5298592.0000\n",
      "Epoch [47488/50000], Train Loss: 8088942.0000, Val Loss: 5298706.0000\n",
      "Epoch [47489/50000], Train Loss: 8088942.5000, Val Loss: 5298573.5000\n",
      "Epoch [47490/50000], Train Loss: 8088942.0000, Val Loss: 5298725.0000\n",
      "Epoch [47491/50000], Train Loss: 8088942.5000, Val Loss: 5298557.0000\n",
      "Epoch [47492/50000], Train Loss: 8088942.0000, Val Loss: 5298735.0000\n",
      "Epoch [47493/50000], Train Loss: 8088942.0000, Val Loss: 5298559.5000\n",
      "Epoch [47494/50000], Train Loss: 8088942.5000, Val Loss: 5298714.5000\n",
      "Epoch [47495/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [47496/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [47497/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [47498/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [47499/50000], Train Loss: 8088942.0000, Val Loss: 5298696.5000\n",
      "Epoch [47500/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [47501/50000], Train Loss: 8088941.5000, Val Loss: 5298695.5000\n",
      "Epoch [47502/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [47503/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [47504/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47505/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [47506/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [47507/50000], Train Loss: 8088941.5000, Val Loss: 5298601.5000\n",
      "Epoch [47508/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [47509/50000], Train Loss: 8088941.5000, Val Loss: 5298617.0000\n",
      "Epoch [47510/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [47511/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47512/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47513/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [47514/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [47515/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [47516/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [47517/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47518/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [47519/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47520/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [47521/50000], Train Loss: 8088942.5000, Val Loss: 5298622.0000\n",
      "Epoch [47522/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [47523/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [47524/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47525/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47526/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [47527/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [47528/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [47529/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [47530/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [47531/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [47532/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47533/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [47534/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [47535/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [47536/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [47537/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47538/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [47539/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [47540/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47541/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [47542/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [47543/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [47544/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [47545/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [47546/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47547/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [47548/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47549/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47550/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47551/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47552/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [47553/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [47554/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [47555/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47556/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47557/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [47558/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [47559/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47560/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47561/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [47562/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47563/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [47564/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [47565/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [47566/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [47567/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47568/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [47569/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47570/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [47571/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47572/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47573/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47574/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47575/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [47576/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47577/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47578/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47579/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [47580/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [47581/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [47582/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [47583/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47584/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [47585/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47586/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47587/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47588/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47589/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47590/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [47591/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [47592/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47593/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47594/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [47595/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47596/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47597/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [47598/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [47599/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47600/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47601/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [47602/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47603/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47604/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47605/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47606/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [47607/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [47608/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47609/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47610/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [47611/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [47612/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47613/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47614/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [47615/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [47616/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [47617/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47618/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [47619/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47620/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47621/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47622/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [47623/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47624/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47625/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47626/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [47627/50000], Train Loss: 8088941.5000, Val Loss: 5298660.5000\n",
      "Epoch [47628/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47629/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [47630/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [47631/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [47632/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [47633/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [47634/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47635/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [47636/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [47637/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [47638/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [47639/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [47640/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [47641/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47642/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47643/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [47644/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47645/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47646/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47647/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47648/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [47649/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47650/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [47651/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [47652/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [47653/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [47654/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [47655/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47656/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [47657/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47658/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [47659/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47660/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [47661/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47662/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47663/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [47664/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [47665/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47666/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [47667/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [47668/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47669/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47670/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47671/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [47672/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [47673/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [47674/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [47675/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47676/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47677/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [47678/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47679/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47680/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [47681/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47682/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47683/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [47684/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [47685/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [47686/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [47687/50000], Train Loss: 8088942.0000, Val Loss: 5298621.5000\n",
      "Epoch [47688/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [47689/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [47690/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [47691/50000], Train Loss: 8088941.5000, Val Loss: 5298598.0000\n",
      "Epoch [47692/50000], Train Loss: 8088942.0000, Val Loss: 5298699.0000\n",
      "Epoch [47693/50000], Train Loss: 8088941.5000, Val Loss: 5298579.5000\n",
      "Epoch [47694/50000], Train Loss: 8088941.5000, Val Loss: 5298719.0000\n",
      "Epoch [47695/50000], Train Loss: 8088942.5000, Val Loss: 5298562.0000\n",
      "Epoch [47696/50000], Train Loss: 8088942.5000, Val Loss: 5298730.0000\n",
      "Epoch [47697/50000], Train Loss: 8088942.0000, Val Loss: 5298562.0000\n",
      "Epoch [47698/50000], Train Loss: 8088942.5000, Val Loss: 5298715.5000\n",
      "Epoch [47699/50000], Train Loss: 8088942.0000, Val Loss: 5298591.5000\n",
      "Epoch [47700/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [47701/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47702/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [47703/50000], Train Loss: 8088942.5000, Val Loss: 5298678.0000\n",
      "Epoch [47704/50000], Train Loss: 8088942.5000, Val Loss: 5298600.5000\n",
      "Epoch [47705/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [47706/50000], Train Loss: 8088941.5000, Val Loss: 5298601.5000\n",
      "Epoch [47707/50000], Train Loss: 8088942.0000, Val Loss: 5298678.5000\n",
      "Epoch [47708/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47709/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [47710/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [47711/50000], Train Loss: 8088941.5000, Val Loss: 5298619.5000\n",
      "Epoch [47712/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [47713/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [47714/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [47715/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [47716/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47717/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [47718/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47719/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [47720/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [47721/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [47722/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [47723/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [47724/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [47725/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [47726/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [47727/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [47728/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [47729/50000], Train Loss: 8088942.5000, Val Loss: 5298626.0000\n",
      "Epoch [47730/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [47731/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [47732/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47733/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47734/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [47735/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47736/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [47737/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [47738/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47739/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [47740/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [47741/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [47742/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47743/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47744/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [47745/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47746/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47747/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [47748/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47749/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47750/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47751/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47752/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47753/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [47754/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47755/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [47756/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [47757/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47758/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [47759/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47760/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [47761/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [47762/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47763/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47764/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [47765/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [47766/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47767/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47768/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [47769/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47770/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47771/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47772/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47773/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47774/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [47775/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [47776/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [47777/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47778/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [47779/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [47780/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [47781/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [47782/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47783/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47784/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [47785/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47786/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [47787/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47788/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47789/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [47790/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47791/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [47792/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47793/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [47794/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47795/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [47796/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47797/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47798/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [47799/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47800/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47801/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [47802/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [47803/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [47804/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47805/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [47806/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [47807/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [47808/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47809/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [47810/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [47811/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [47812/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47813/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [47814/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [47815/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47816/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [47817/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47818/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47819/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47820/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [47821/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [47822/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [47823/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [47824/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47825/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47826/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47827/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47828/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [47829/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47830/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47831/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47832/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [47833/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47834/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47835/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [47836/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47837/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [47838/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [47839/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47840/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [47841/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47842/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [47843/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [47844/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47845/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [47846/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [47847/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [47848/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [47849/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [47850/50000], Train Loss: 8088941.5000, Val Loss: 5298602.0000\n",
      "Epoch [47851/50000], Train Loss: 8088941.5000, Val Loss: 5298691.5000\n",
      "Epoch [47852/50000], Train Loss: 8088942.0000, Val Loss: 5298591.5000\n",
      "Epoch [47853/50000], Train Loss: 8088942.0000, Val Loss: 5298704.0000\n",
      "Epoch [47854/50000], Train Loss: 8088942.0000, Val Loss: 5298578.5000\n",
      "Epoch [47855/50000], Train Loss: 8088941.5000, Val Loss: 5298715.0000\n",
      "Epoch [47856/50000], Train Loss: 8088942.0000, Val Loss: 5298571.5000\n",
      "Epoch [47857/50000], Train Loss: 8088942.0000, Val Loss: 5298715.0000\n",
      "Epoch [47858/50000], Train Loss: 8088942.0000, Val Loss: 5298581.5000\n",
      "Epoch [47859/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [47860/50000], Train Loss: 8088941.5000, Val Loss: 5298614.0000\n",
      "Epoch [47861/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [47862/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [47863/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [47864/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [47865/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [47866/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [47867/50000], Train Loss: 8088941.5000, Val Loss: 5298608.0000\n",
      "Epoch [47868/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [47869/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [47870/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [47871/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47872/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [47873/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [47874/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [47875/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [47876/50000], Train Loss: 8088941.5000, Val Loss: 5298621.0000\n",
      "Epoch [47877/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [47878/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [47879/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [47880/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [47881/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47882/50000], Train Loss: 8088941.5000, Val Loss: 5298651.5000\n",
      "Epoch [47883/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [47884/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [47885/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [47886/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [47887/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [47888/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [47889/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [47890/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [47891/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47892/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47893/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47894/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [47895/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47896/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [47897/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47898/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47899/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47900/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47901/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [47902/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47903/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47904/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [47905/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47906/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [47907/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47908/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [47909/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47910/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47911/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [47912/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [47913/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47914/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47915/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [47916/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47917/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47918/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [47919/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [47920/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47921/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47922/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47923/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [47924/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [47925/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47926/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47927/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [47928/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [47929/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [47930/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [47931/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [47932/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47933/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [47934/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47935/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47936/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [47937/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [47938/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47939/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [47940/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [47941/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [47942/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [47943/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [47944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [47945/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [47946/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [47947/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47948/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [47949/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47950/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47951/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [47952/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [47953/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [47954/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47955/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47956/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47957/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [47958/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [47959/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [47960/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [47961/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [47962/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47963/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [47964/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [47965/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47966/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [47967/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [47968/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [47969/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [47970/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [47971/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [47972/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [47973/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [47974/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [47975/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [47976/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [47977/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [47978/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [47979/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [47980/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [47981/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [47982/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [47983/50000], Train Loss: 8088942.0000, Val Loss: 5298681.5000\n",
      "Epoch [47984/50000], Train Loss: 8088941.5000, Val Loss: 5298600.5000\n",
      "Epoch [47985/50000], Train Loss: 8088941.5000, Val Loss: 5298696.0000\n",
      "Epoch [47986/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [47987/50000], Train Loss: 8088941.5000, Val Loss: 5298706.0000\n",
      "Epoch [47988/50000], Train Loss: 8088942.0000, Val Loss: 5298581.5000\n",
      "Epoch [47989/50000], Train Loss: 8088942.0000, Val Loss: 5298704.5000\n",
      "Epoch [47990/50000], Train Loss: 8088942.0000, Val Loss: 5298592.0000\n",
      "Epoch [47991/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [47992/50000], Train Loss: 8088942.0000, Val Loss: 5298616.0000\n",
      "Epoch [47993/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [47994/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [47995/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [47996/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [47997/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [47998/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [47999/50000], Train Loss: 8088942.0000, Val Loss: 5298612.5000\n",
      "Epoch [48000/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [48001/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [48002/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [48003/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [48004/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48005/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48006/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [48007/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [48008/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [48009/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [48010/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48011/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48012/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [48013/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48014/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [48015/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [48016/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48017/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48018/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48019/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48020/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48021/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48022/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [48023/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48024/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48025/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [48026/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48027/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [48028/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48029/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48030/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48031/50000], Train Loss: 8088941.0000, Val Loss: 5298646.5000\n",
      "Epoch [48032/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48033/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [48034/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48035/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48036/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [48037/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48038/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48039/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48040/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [48041/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48042/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [48043/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [48044/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48045/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [48046/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48047/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [48048/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48049/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48050/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [48051/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48052/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48053/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48054/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [48055/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [48056/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48057/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [48058/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48059/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48060/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48061/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [48062/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48063/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48064/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48065/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48066/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [48067/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48068/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48069/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48070/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48071/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [48072/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48073/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [48074/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48075/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [48076/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48077/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48078/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48079/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [48080/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [48081/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48082/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [48083/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [48084/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [48085/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [48086/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [48087/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [48088/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [48089/50000], Train Loss: 8088942.5000, Val Loss: 5298601.5000\n",
      "Epoch [48090/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [48091/50000], Train Loss: 8088941.5000, Val Loss: 5298590.0000\n",
      "Epoch [48092/50000], Train Loss: 8088942.0000, Val Loss: 5298704.5000\n",
      "Epoch [48093/50000], Train Loss: 8088942.5000, Val Loss: 5298580.5000\n",
      "Epoch [48094/50000], Train Loss: 8088942.5000, Val Loss: 5298710.0000\n",
      "Epoch [48095/50000], Train Loss: 8088942.0000, Val Loss: 5298579.5000\n",
      "Epoch [48096/50000], Train Loss: 8088941.5000, Val Loss: 5298704.0000\n",
      "Epoch [48097/50000], Train Loss: 8088942.0000, Val Loss: 5298594.0000\n",
      "Epoch [48098/50000], Train Loss: 8088941.5000, Val Loss: 5298682.0000\n",
      "Epoch [48099/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [48100/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [48101/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48102/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [48103/50000], Train Loss: 8088941.5000, Val Loss: 5298682.0000\n",
      "Epoch [48104/50000], Train Loss: 8088942.5000, Val Loss: 5298604.5000\n",
      "Epoch [48105/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [48106/50000], Train Loss: 8088941.5000, Val Loss: 5298611.0000\n",
      "Epoch [48107/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [48108/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [48109/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [48110/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48111/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [48112/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [48113/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [48114/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [48115/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [48116/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48117/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [48118/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48119/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48120/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [48121/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48122/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [48123/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [48124/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [48125/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [48126/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [48127/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48128/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48129/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [48130/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48131/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48132/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48133/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [48134/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48135/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [48136/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48137/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48138/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [48139/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [48140/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48141/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48142/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48143/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48144/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48145/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48146/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [48147/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48148/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48149/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48150/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48151/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48152/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [48153/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48154/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48155/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48156/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [48157/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48158/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48159/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48160/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48161/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48162/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [48163/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [48164/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [48165/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48166/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [48167/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48168/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48169/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48170/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48171/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [48172/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48173/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [48174/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [48175/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [48176/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [48177/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48178/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [48179/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48180/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [48181/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [48182/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48183/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48184/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48185/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [48186/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48187/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [48188/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [48189/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48190/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48191/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48192/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [48193/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48194/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48195/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48196/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48197/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [48198/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [48199/50000], Train Loss: 8088942.5000, Val Loss: 5298669.5000\n",
      "Epoch [48200/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [48201/50000], Train Loss: 8088942.0000, Val Loss: 5298677.5000\n",
      "Epoch [48202/50000], Train Loss: 8088942.5000, Val Loss: 5298610.0000\n",
      "Epoch [48203/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [48204/50000], Train Loss: 8088941.5000, Val Loss: 5298606.0000\n",
      "Epoch [48205/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [48206/50000], Train Loss: 8088941.5000, Val Loss: 5298599.5000\n",
      "Epoch [48207/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [48208/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [48209/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [48210/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [48211/50000], Train Loss: 8088941.5000, Val Loss: 5298683.0000\n",
      "Epoch [48212/50000], Train Loss: 8088941.5000, Val Loss: 5298610.0000\n",
      "Epoch [48213/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [48214/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [48215/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [48216/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [48217/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [48218/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [48219/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48220/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [48221/50000], Train Loss: 8088942.5000, Val Loss: 5298633.5000\n",
      "Epoch [48222/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [48223/50000], Train Loss: 8088942.5000, Val Loss: 5298630.5000\n",
      "Epoch [48224/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48225/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [48226/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48227/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [48228/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48229/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [48230/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [48231/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48232/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [48233/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48234/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48235/50000], Train Loss: 8088942.5000, Val Loss: 5298643.0000\n",
      "Epoch [48236/50000], Train Loss: 8088941.5000, Val Loss: 5298642.5000\n",
      "Epoch [48237/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [48238/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48239/50000], Train Loss: 8088942.5000, Val Loss: 5298655.5000\n",
      "Epoch [48240/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [48241/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [48242/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48243/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48244/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48245/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [48246/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [48247/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [48248/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [48249/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48250/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48251/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48252/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48253/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48254/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [48255/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [48256/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [48257/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [48258/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [48259/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [48260/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [48261/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [48262/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [48263/50000], Train Loss: 8088942.5000, Val Loss: 5298620.5000\n",
      "Epoch [48264/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [48265/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [48266/50000], Train Loss: 8088941.5000, Val Loss: 5298673.0000\n",
      "Epoch [48267/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [48268/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [48269/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [48270/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [48271/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [48272/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [48273/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [48274/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [48275/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [48276/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48277/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48278/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48279/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48280/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48281/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48282/50000], Train Loss: 8088942.0000, Val Loss: 5298629.5000\n",
      "Epoch [48283/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [48284/50000], Train Loss: 8088942.5000, Val Loss: 5298618.5000\n",
      "Epoch [48285/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [48286/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [48287/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [48288/50000], Train Loss: 8088941.5000, Val Loss: 5298605.0000\n",
      "Epoch [48289/50000], Train Loss: 8088942.5000, Val Loss: 5298686.0000\n",
      "Epoch [48290/50000], Train Loss: 8088942.5000, Val Loss: 5298601.5000\n",
      "Epoch [48291/50000], Train Loss: 8088942.0000, Val Loss: 5298688.5000\n",
      "Epoch [48292/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [48293/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [48294/50000], Train Loss: 8088942.0000, Val Loss: 5298607.0000\n",
      "Epoch [48295/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [48296/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [48297/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [48298/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [48299/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48300/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48301/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [48302/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48303/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [48304/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [48305/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [48306/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [48307/50000], Train Loss: 8088942.0000, Val Loss: 5298630.5000\n",
      "Epoch [48308/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [48309/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [48310/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48311/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [48312/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48313/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [48314/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48315/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48316/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [48317/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [48318/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48319/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [48320/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [48321/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48322/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48323/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [48324/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [48325/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48326/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [48327/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [48328/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [48329/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48330/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [48331/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48332/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48333/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48334/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48335/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48336/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [48337/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48338/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [48339/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48340/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [48341/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48342/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [48343/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48344/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48345/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48346/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48347/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48348/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48349/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [48350/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48351/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48352/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48353/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48354/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48355/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48356/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [48357/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [48358/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48359/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48360/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [48361/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [48362/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [48363/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [48364/50000], Train Loss: 8088942.5000, Val Loss: 5298688.5000\n",
      "Epoch [48365/50000], Train Loss: 8088942.0000, Val Loss: 5298591.5000\n",
      "Epoch [48366/50000], Train Loss: 8088942.0000, Val Loss: 5298707.0000\n",
      "Epoch [48367/50000], Train Loss: 8088942.0000, Val Loss: 5298573.5000\n",
      "Epoch [48368/50000], Train Loss: 8088942.0000, Val Loss: 5298721.0000\n",
      "Epoch [48369/50000], Train Loss: 8088942.0000, Val Loss: 5298565.0000\n",
      "Epoch [48370/50000], Train Loss: 8088942.0000, Val Loss: 5298722.0000\n",
      "Epoch [48371/50000], Train Loss: 8088942.0000, Val Loss: 5298574.5000\n",
      "Epoch [48372/50000], Train Loss: 8088942.5000, Val Loss: 5298699.5000\n",
      "Epoch [48373/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [48374/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [48375/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48376/50000], Train Loss: 8088941.5000, Val Loss: 5298617.5000\n",
      "Epoch [48377/50000], Train Loss: 8088942.5000, Val Loss: 5298687.0000\n",
      "Epoch [48378/50000], Train Loss: 8088941.5000, Val Loss: 5298594.0000\n",
      "Epoch [48379/50000], Train Loss: 8088941.5000, Val Loss: 5298695.0000\n",
      "Epoch [48380/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [48381/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [48382/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48383/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [48384/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [48385/50000], Train Loss: 8088942.0000, Val Loss: 5298615.0000\n",
      "Epoch [48386/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [48387/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [48388/50000], Train Loss: 8088942.0000, Val Loss: 5298669.5000\n",
      "Epoch [48389/50000], Train Loss: 8088942.0000, Val Loss: 5298631.5000\n",
      "Epoch [48390/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [48391/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [48392/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [48393/50000], Train Loss: 8088942.5000, Val Loss: 5298667.5000\n",
      "Epoch [48394/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [48395/50000], Train Loss: 8088942.5000, Val Loss: 5298664.5000\n",
      "Epoch [48396/50000], Train Loss: 8088942.5000, Val Loss: 5298631.5000\n",
      "Epoch [48397/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [48398/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [48399/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48400/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [48401/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48402/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [48403/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [48404/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [48405/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48406/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48407/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48408/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48409/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [48410/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48411/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [48412/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [48413/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [48414/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48415/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48416/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48417/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48418/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [48419/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [48420/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [48421/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48422/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48423/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48424/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [48425/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48426/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [48427/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48428/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48429/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48430/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48431/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [48432/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [48433/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48434/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [48435/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48436/50000], Train Loss: 8088941.5000, Val Loss: 5298656.0000\n",
      "Epoch [48437/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [48438/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48439/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48440/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48441/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48442/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [48443/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [48444/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48445/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [48446/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [48447/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48448/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48449/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [48450/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48451/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [48452/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48453/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48454/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48455/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [48456/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [48457/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48458/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48459/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48460/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48461/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [48462/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48463/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48464/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48465/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48466/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48467/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48468/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48469/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48470/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [48471/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48472/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48473/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [48474/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48475/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48476/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [48477/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [48478/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [48479/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48480/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [48481/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48482/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48483/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48484/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [48485/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [48486/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [48487/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48488/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48489/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48490/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48491/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [48492/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48493/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48494/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48495/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [48496/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [48497/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48498/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [48499/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48500/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48501/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48502/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48503/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [48504/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48505/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48506/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48507/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [48508/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48509/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48510/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48511/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48512/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48513/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [48514/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48515/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [48516/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48517/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [48518/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48519/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [48520/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [48521/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48522/50000], Train Loss: 8088941.5000, Val Loss: 5298657.5000\n",
      "Epoch [48523/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [48524/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48525/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [48526/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [48527/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48528/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [48529/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48530/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [48531/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [48532/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [48533/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [48534/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [48535/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [48536/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [48537/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [48538/50000], Train Loss: 8088942.0000, Val Loss: 5298706.5000\n",
      "Epoch [48539/50000], Train Loss: 8088942.0000, Val Loss: 5298573.5000\n",
      "Epoch [48540/50000], Train Loss: 8088942.0000, Val Loss: 5298722.5000\n",
      "Epoch [48541/50000], Train Loss: 8088942.0000, Val Loss: 5298563.0000\n",
      "Epoch [48542/50000], Train Loss: 8088942.0000, Val Loss: 5298722.5000\n",
      "Epoch [48543/50000], Train Loss: 8088942.0000, Val Loss: 5298575.0000\n",
      "Epoch [48544/50000], Train Loss: 8088942.5000, Val Loss: 5298697.0000\n",
      "Epoch [48545/50000], Train Loss: 8088942.0000, Val Loss: 5298614.0000\n",
      "Epoch [48546/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48547/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [48548/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [48549/50000], Train Loss: 8088942.5000, Val Loss: 5298694.5000\n",
      "Epoch [48550/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [48551/50000], Train Loss: 8088941.5000, Val Loss: 5298699.5000\n",
      "Epoch [48552/50000], Train Loss: 8088942.0000, Val Loss: 5298599.5000\n",
      "Epoch [48553/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [48554/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [48555/50000], Train Loss: 8088942.5000, Val Loss: 5298633.5000\n",
      "Epoch [48556/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [48557/50000], Train Loss: 8088942.5000, Val Loss: 5298609.0000\n",
      "Epoch [48558/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [48559/50000], Train Loss: 8088941.5000, Val Loss: 5298614.0000\n",
      "Epoch [48560/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [48561/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [48562/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48563/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48564/50000], Train Loss: 8088941.5000, Val Loss: 5298621.5000\n",
      "Epoch [48565/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [48566/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [48567/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [48568/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48569/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [48570/50000], Train Loss: 8088941.5000, Val Loss: 5298654.5000\n",
      "Epoch [48571/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [48572/50000], Train Loss: 8088942.0000, Val Loss: 5298663.5000\n",
      "Epoch [48573/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [48574/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48575/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [48576/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48577/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48578/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [48579/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [48580/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [48581/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48582/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [48583/50000], Train Loss: 8088941.5000, Val Loss: 5298653.0000\n",
      "Epoch [48584/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48585/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48586/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48587/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [48588/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [48589/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [48590/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48591/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48592/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48593/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [48594/50000], Train Loss: 8088941.5000, Val Loss: 5298630.0000\n",
      "Epoch [48595/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [48596/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48597/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [48598/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48599/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48600/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [48601/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48602/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48603/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [48604/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [48605/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [48606/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [48607/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48608/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [48609/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48610/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [48611/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48612/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [48613/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48614/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48615/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48616/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [48617/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48618/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48619/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [48620/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48621/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [48622/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48623/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [48624/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48625/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [48626/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48627/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48628/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [48629/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48630/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [48631/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48632/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48633/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48634/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48635/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [48636/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [48637/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [48638/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48639/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48640/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48641/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [48642/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48643/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48644/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48645/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [48646/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [48647/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48648/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [48649/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48650/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48651/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48652/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48653/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48654/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48655/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [48656/50000], Train Loss: 8088942.5000, Val Loss: 5298633.5000\n",
      "Epoch [48657/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [48658/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [48659/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [48660/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [48661/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [48662/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [48663/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [48664/50000], Train Loss: 8088941.5000, Val Loss: 5298620.5000\n",
      "Epoch [48665/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [48666/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [48667/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [48668/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [48669/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [48670/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [48671/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48672/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48673/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48674/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48675/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48676/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [48677/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [48678/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [48679/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48680/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [48681/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [48682/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [48683/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [48684/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [48685/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [48686/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [48687/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [48688/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [48689/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48690/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [48691/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [48692/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [48693/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [48694/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48695/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48696/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48697/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [48698/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [48699/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [48700/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [48701/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [48702/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [48703/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [48704/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [48705/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [48706/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [48707/50000], Train Loss: 8088941.5000, Val Loss: 5298620.5000\n",
      "Epoch [48708/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [48709/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [48710/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [48711/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48712/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48713/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [48714/50000], Train Loss: 8088941.0000, Val Loss: 5298657.5000\n",
      "Epoch [48715/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [48716/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [48717/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [48718/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [48719/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [48720/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [48721/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [48722/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [48723/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [48724/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [48725/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48726/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [48727/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [48728/50000], Train Loss: 8088942.0000, Val Loss: 5298664.0000\n",
      "Epoch [48729/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [48730/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [48731/50000], Train Loss: 8088942.5000, Val Loss: 5298621.5000\n",
      "Epoch [48732/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [48733/50000], Train Loss: 8088941.5000, Val Loss: 5298617.5000\n",
      "Epoch [48734/50000], Train Loss: 8088942.0000, Val Loss: 5298672.5000\n",
      "Epoch [48735/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [48736/50000], Train Loss: 8088943.0000, Val Loss: 5298675.0000\n",
      "Epoch [48737/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [48738/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [48739/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [48740/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [48741/50000], Train Loss: 8088941.5000, Val Loss: 5298604.5000\n",
      "Epoch [48742/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [48743/50000], Train Loss: 8088942.5000, Val Loss: 5298604.5000\n",
      "Epoch [48744/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [48745/50000], Train Loss: 8088942.0000, Val Loss: 5298608.0000\n",
      "Epoch [48746/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [48747/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [48748/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [48749/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [48750/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48751/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48752/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48753/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [48754/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [48755/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [48756/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48757/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [48758/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48759/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48760/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [48761/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [48762/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [48763/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48764/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48765/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48766/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48767/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48768/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [48769/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48770/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48771/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48772/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [48773/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [48774/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [48775/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [48776/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [48777/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48778/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48779/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [48780/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48781/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48782/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [48783/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48784/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [48785/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48786/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48787/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48788/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [48789/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48790/50000], Train Loss: 8088941.5000, Val Loss: 5298661.0000\n",
      "Epoch [48791/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [48792/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [48793/50000], Train Loss: 8088942.5000, Val Loss: 5298611.5000\n",
      "Epoch [48794/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [48795/50000], Train Loss: 8088941.5000, Val Loss: 5298599.5000\n",
      "Epoch [48796/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [48797/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [48798/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [48799/50000], Train Loss: 8088942.0000, Val Loss: 5298589.0000\n",
      "Epoch [48800/50000], Train Loss: 8088942.0000, Val Loss: 5298697.0000\n",
      "Epoch [48801/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [48802/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [48803/50000], Train Loss: 8088942.5000, Val Loss: 5298616.0000\n",
      "Epoch [48804/50000], Train Loss: 8088941.5000, Val Loss: 5298663.0000\n",
      "Epoch [48805/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48806/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [48807/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [48808/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48809/50000], Train Loss: 8088942.5000, Val Loss: 5298663.5000\n",
      "Epoch [48810/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [48811/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [48812/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [48813/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [48814/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [48815/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48816/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48817/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48818/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48819/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48820/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48821/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48822/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48823/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48824/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [48825/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48826/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48827/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48828/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48829/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48830/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48831/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48832/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48833/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [48834/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48835/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48836/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48837/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48838/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48839/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48840/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48841/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48842/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [48843/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48844/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48845/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [48846/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48847/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [48848/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48849/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48850/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48851/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48852/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48853/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [48854/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48855/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48856/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [48857/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [48858/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [48859/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48860/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [48861/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [48862/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [48863/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48864/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [48865/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48866/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [48867/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48868/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [48869/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48870/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [48871/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48872/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [48873/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [48874/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [48875/50000], Train Loss: 8088941.5000, Val Loss: 5298632.0000\n",
      "Epoch [48876/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [48877/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48878/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48879/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48880/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48881/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [48882/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48883/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [48884/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48885/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48886/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48887/50000], Train Loss: 8088942.5000, Val Loss: 5298655.0000\n",
      "Epoch [48888/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [48889/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [48890/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [48891/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [48892/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [48893/50000], Train Loss: 8088942.5000, Val Loss: 5298672.5000\n",
      "Epoch [48894/50000], Train Loss: 8088942.5000, Val Loss: 5298611.0000\n",
      "Epoch [48895/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [48896/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [48897/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [48898/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [48899/50000], Train Loss: 8088941.5000, Val Loss: 5298712.5000\n",
      "Epoch [48900/50000], Train Loss: 8088942.0000, Val Loss: 5298572.5000\n",
      "Epoch [48901/50000], Train Loss: 8088942.0000, Val Loss: 5298717.0000\n",
      "Epoch [48902/50000], Train Loss: 8088942.0000, Val Loss: 5298575.0000\n",
      "Epoch [48903/50000], Train Loss: 8088942.5000, Val Loss: 5298705.0000\n",
      "Epoch [48904/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [48905/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [48906/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [48907/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48908/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [48909/50000], Train Loss: 8088941.5000, Val Loss: 5298607.0000\n",
      "Epoch [48910/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [48911/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [48912/50000], Train Loss: 8088942.0000, Val Loss: 5298685.5000\n",
      "Epoch [48913/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [48914/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [48915/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48916/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [48917/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [48918/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [48919/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [48920/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [48921/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [48922/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [48923/50000], Train Loss: 8088941.5000, Val Loss: 5298649.5000\n",
      "Epoch [48924/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [48925/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [48926/50000], Train Loss: 8088942.5000, Val Loss: 5298657.0000\n",
      "Epoch [48927/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [48928/50000], Train Loss: 8088941.0000, Val Loss: 5298657.0000\n",
      "Epoch [48929/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [48930/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48931/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48932/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [48933/50000], Train Loss: 8088942.5000, Val Loss: 5298649.0000\n",
      "Epoch [48934/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48935/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48936/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [48937/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [48938/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48939/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [48940/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48941/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48942/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48943/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [48944/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [48945/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [48946/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48947/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [48948/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48949/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [48950/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [48951/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48952/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48953/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48954/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48955/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48956/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48957/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48958/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [48959/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48960/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48961/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [48962/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48963/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48964/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48965/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48966/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48967/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48968/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48969/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48970/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [48971/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [48972/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48973/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48974/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [48975/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [48976/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [48977/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48978/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [48979/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [48980/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48981/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [48982/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48983/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [48984/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48985/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [48986/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [48987/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [48988/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [48989/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [48990/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [48991/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [48992/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [48993/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [48994/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [48995/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [48996/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [48997/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [48998/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [48999/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [49000/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [49001/50000], Train Loss: 8088941.5000, Val Loss: 5298627.5000\n",
      "Epoch [49002/50000], Train Loss: 8088942.5000, Val Loss: 5298664.0000\n",
      "Epoch [49003/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [49004/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [49005/50000], Train Loss: 8088941.5000, Val Loss: 5298617.0000\n",
      "Epoch [49006/50000], Train Loss: 8088942.0000, Val Loss: 5298676.5000\n",
      "Epoch [49007/50000], Train Loss: 8088942.0000, Val Loss: 5298609.0000\n",
      "Epoch [49008/50000], Train Loss: 8088942.0000, Val Loss: 5298683.0000\n",
      "Epoch [49009/50000], Train Loss: 8088941.5000, Val Loss: 5298602.0000\n",
      "Epoch [49010/50000], Train Loss: 8088941.5000, Val Loss: 5298689.5000\n",
      "Epoch [49011/50000], Train Loss: 8088942.0000, Val Loss: 5298598.0000\n",
      "Epoch [49012/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [49013/50000], Train Loss: 8088941.5000, Val Loss: 5298600.5000\n",
      "Epoch [49014/50000], Train Loss: 8088941.5000, Val Loss: 5298683.0000\n",
      "Epoch [49015/50000], Train Loss: 8088941.5000, Val Loss: 5298614.0000\n",
      "Epoch [49016/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [49017/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [49018/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [49019/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49020/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [49021/50000], Train Loss: 8088941.0000, Val Loss: 5298665.0000\n",
      "Epoch [49022/50000], Train Loss: 8088942.0000, Val Loss: 5298618.0000\n",
      "Epoch [49023/50000], Train Loss: 8088942.0000, Val Loss: 5298673.0000\n",
      "Epoch [49024/50000], Train Loss: 8088942.5000, Val Loss: 5298614.0000\n",
      "Epoch [49025/50000], Train Loss: 8088942.0000, Val Loss: 5298674.0000\n",
      "Epoch [49026/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [49027/50000], Train Loss: 8088942.0000, Val Loss: 5298668.5000\n",
      "Epoch [49028/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [49029/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [49030/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [49031/50000], Train Loss: 8088941.0000, Val Loss: 5298641.5000\n",
      "Epoch [49032/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [49033/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [49034/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [49035/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [49036/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [49037/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [49038/50000], Train Loss: 8088942.0000, Val Loss: 5298664.5000\n",
      "Epoch [49039/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [49040/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [49041/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [49042/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [49043/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [49044/50000], Train Loss: 8088941.0000, Val Loss: 5298654.5000\n",
      "Epoch [49045/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [49046/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49047/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49048/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49049/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49050/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [49051/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49052/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49053/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49054/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [49055/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49056/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [49057/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49058/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49059/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [49060/50000], Train Loss: 8088941.5000, Val Loss: 5298648.0000\n",
      "Epoch [49061/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49062/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49063/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [49064/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [49065/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49066/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [49067/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [49068/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49069/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49070/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [49071/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [49072/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49073/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49074/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [49075/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [49076/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49077/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49078/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [49079/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49080/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49081/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49082/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [49083/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49084/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49085/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49086/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49087/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49088/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [49089/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [49090/50000], Train Loss: 8088941.5000, Val Loss: 5298626.5000\n",
      "Epoch [49091/50000], Train Loss: 8088942.0000, Val Loss: 5298665.5000\n",
      "Epoch [49092/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [49093/50000], Train Loss: 8088942.5000, Val Loss: 5298672.5000\n",
      "Epoch [49094/50000], Train Loss: 8088942.5000, Val Loss: 5298611.5000\n",
      "Epoch [49095/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [49096/50000], Train Loss: 8088941.5000, Val Loss: 5298603.0000\n",
      "Epoch [49097/50000], Train Loss: 8088942.0000, Val Loss: 5298690.5000\n",
      "Epoch [49098/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [49099/50000], Train Loss: 8088942.0000, Val Loss: 5298701.0000\n",
      "Epoch [49100/50000], Train Loss: 8088942.0000, Val Loss: 5298582.5000\n",
      "Epoch [49101/50000], Train Loss: 8088941.5000, Val Loss: 5298708.5000\n",
      "Epoch [49102/50000], Train Loss: 8088942.0000, Val Loss: 5298581.0000\n",
      "Epoch [49103/50000], Train Loss: 8088942.5000, Val Loss: 5298704.0000\n",
      "Epoch [49104/50000], Train Loss: 8088942.0000, Val Loss: 5298592.5000\n",
      "Epoch [49105/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [49106/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [49107/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49108/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [49109/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [49110/50000], Train Loss: 8088942.5000, Val Loss: 5298678.0000\n",
      "Epoch [49111/50000], Train Loss: 8088942.5000, Val Loss: 5298605.0000\n",
      "Epoch [49112/50000], Train Loss: 8088942.5000, Val Loss: 5298684.5000\n",
      "Epoch [49113/50000], Train Loss: 8088942.5000, Val Loss: 5298608.5000\n",
      "Epoch [49114/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [49115/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49116/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [49117/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49118/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49119/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [49120/50000], Train Loss: 8088942.5000, Val Loss: 5298618.0000\n",
      "Epoch [49121/50000], Train Loss: 8088942.0000, Val Loss: 5298670.5000\n",
      "Epoch [49122/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [49123/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [49124/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [49125/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49126/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49127/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [49128/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [49129/50000], Train Loss: 8088942.5000, Val Loss: 5298629.0000\n",
      "Epoch [49130/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [49131/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [49132/50000], Train Loss: 8088942.5000, Val Loss: 5298659.0000\n",
      "Epoch [49133/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [49134/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49135/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [49136/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [49137/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49138/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49139/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49140/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [49141/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49142/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49143/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49144/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [49145/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [49146/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [49147/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [49148/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [49149/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [49150/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49151/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [49152/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49153/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49154/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [49155/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [49156/50000], Train Loss: 8088941.0000, Val Loss: 5298661.5000\n",
      "Epoch [49157/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [49158/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [49159/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [49160/50000], Train Loss: 8088942.5000, Val Loss: 5298658.5000\n",
      "Epoch [49161/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [49162/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49163/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [49164/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49165/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49166/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49167/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [49168/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49169/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [49170/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [49171/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [49172/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [49173/50000], Train Loss: 8088941.5000, Val Loss: 5298658.5000\n",
      "Epoch [49174/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49175/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [49176/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [49177/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [49178/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [49179/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [49180/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [49181/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [49182/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [49183/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [49184/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49185/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [49186/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49187/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49188/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49189/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [49190/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49191/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49192/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [49193/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49194/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49195/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49196/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [49197/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49198/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49199/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49200/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49201/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49202/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49203/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49204/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49205/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49206/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [49207/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49208/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49209/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49210/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [49211/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49212/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [49213/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49214/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49215/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49216/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49217/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49218/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49219/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [49220/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49221/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49222/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [49223/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [49224/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [49225/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49226/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49227/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [49228/50000], Train Loss: 8088942.5000, Val Loss: 5298633.0000\n",
      "Epoch [49229/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [49230/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [49231/50000], Train Loss: 8088942.5000, Val Loss: 5298675.0000\n",
      "Epoch [49232/50000], Train Loss: 8088942.0000, Val Loss: 5298605.0000\n",
      "Epoch [49233/50000], Train Loss: 8088942.5000, Val Loss: 5298693.5000\n",
      "Epoch [49234/50000], Train Loss: 8088941.5000, Val Loss: 5298585.0000\n",
      "Epoch [49235/50000], Train Loss: 8088942.0000, Val Loss: 5298714.0000\n",
      "Epoch [49236/50000], Train Loss: 8088942.0000, Val Loss: 5298565.5000\n",
      "Epoch [49237/50000], Train Loss: 8088942.0000, Val Loss: 5298729.0000\n",
      "Epoch [49238/50000], Train Loss: 8088942.5000, Val Loss: 5298560.5000\n",
      "Epoch [49239/50000], Train Loss: 8088942.0000, Val Loss: 5298719.0000\n",
      "Epoch [49240/50000], Train Loss: 8088942.5000, Val Loss: 5298587.5000\n",
      "Epoch [49241/50000], Train Loss: 8088941.5000, Val Loss: 5298678.0000\n",
      "Epoch [49242/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [49243/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49244/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [49245/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [49246/50000], Train Loss: 8088942.0000, Val Loss: 5298694.5000\n",
      "Epoch [49247/50000], Train Loss: 8088942.0000, Val Loss: 5298597.5000\n",
      "Epoch [49248/50000], Train Loss: 8088942.0000, Val Loss: 5298680.5000\n",
      "Epoch [49249/50000], Train Loss: 8088941.5000, Val Loss: 5298623.5000\n",
      "Epoch [49250/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [49251/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [49252/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [49253/50000], Train Loss: 8088942.0000, Val Loss: 5298679.0000\n",
      "Epoch [49254/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [49255/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [49256/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [49257/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [49258/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49259/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [49260/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [49261/50000], Train Loss: 8088941.5000, Val Loss: 5298627.0000\n",
      "Epoch [49262/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [49263/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49264/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [49265/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [49266/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49267/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [49268/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [49269/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [49270/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49271/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49272/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49273/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49274/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [49275/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49276/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49277/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49278/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49279/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [49280/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49281/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49282/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49283/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49284/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [49285/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [49286/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49287/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49288/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [49289/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49290/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49291/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49292/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49293/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [49294/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49295/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [49296/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49297/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49298/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [49299/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49300/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49301/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49302/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49303/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [49304/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [49305/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49306/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49307/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49308/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49309/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49310/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [49311/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49312/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49313/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [49314/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49315/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [49316/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49317/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49318/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49319/50000], Train Loss: 8088942.5000, Val Loss: 5298654.5000\n",
      "Epoch [49320/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [49321/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [49322/50000], Train Loss: 8088941.5000, Val Loss: 5298633.5000\n",
      "Epoch [49323/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [49324/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [49325/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49326/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [49327/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49328/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49329/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [49330/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [49331/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49332/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [49333/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49334/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [49335/50000], Train Loss: 8088941.0000, Val Loss: 5298648.0000\n",
      "Epoch [49336/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49337/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49338/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49339/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [49340/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49341/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [49342/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49343/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49344/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49345/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49346/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [49347/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49348/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49349/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [49350/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49351/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49352/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49353/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [49354/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49355/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [49356/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49357/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49358/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49359/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49360/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [49361/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49362/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49363/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [49364/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49365/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [49366/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49367/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49368/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49369/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [49370/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [49371/50000], Train Loss: 8088941.0000, Val Loss: 5298661.5000\n",
      "Epoch [49372/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [49373/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [49374/50000], Train Loss: 8088941.5000, Val Loss: 5298615.0000\n",
      "Epoch [49375/50000], Train Loss: 8088943.0000, Val Loss: 5298679.0000\n",
      "Epoch [49376/50000], Train Loss: 8088942.0000, Val Loss: 5298604.5000\n",
      "Epoch [49377/50000], Train Loss: 8088942.0000, Val Loss: 5298689.5000\n",
      "Epoch [49378/50000], Train Loss: 8088942.0000, Val Loss: 5298594.5000\n",
      "Epoch [49379/50000], Train Loss: 8088942.0000, Val Loss: 5298697.0000\n",
      "Epoch [49380/50000], Train Loss: 8088942.0000, Val Loss: 5298588.5000\n",
      "Epoch [49381/50000], Train Loss: 8088941.5000, Val Loss: 5298701.0000\n",
      "Epoch [49382/50000], Train Loss: 8088942.0000, Val Loss: 5298589.0000\n",
      "Epoch [49383/50000], Train Loss: 8088942.0000, Val Loss: 5298697.0000\n",
      "Epoch [49384/50000], Train Loss: 8088942.0000, Val Loss: 5298596.5000\n",
      "Epoch [49385/50000], Train Loss: 8088942.0000, Val Loss: 5298683.5000\n",
      "Epoch [49386/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [49387/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [49388/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [49389/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [49390/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [49391/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [49392/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [49393/50000], Train Loss: 8088942.0000, Val Loss: 5298615.5000\n",
      "Epoch [49394/50000], Train Loss: 8088941.5000, Val Loss: 5298670.5000\n",
      "Epoch [49395/50000], Train Loss: 8088942.5000, Val Loss: 5298623.0000\n",
      "Epoch [49396/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [49397/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [49398/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49399/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49400/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [49401/50000], Train Loss: 8088942.0000, Val Loss: 5298656.0000\n",
      "Epoch [49402/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [49403/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [49404/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [49405/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [49406/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [49407/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [49408/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49409/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49410/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49411/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49412/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [49413/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [49414/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [49415/50000], Train Loss: 8088942.5000, Val Loss: 5298619.5000\n",
      "Epoch [49416/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [49417/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [49418/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [49419/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [49420/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49421/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [49422/50000], Train Loss: 8088941.5000, Val Loss: 5298640.0000\n",
      "Epoch [49423/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [49424/50000], Train Loss: 8088941.5000, Val Loss: 5298634.0000\n",
      "Epoch [49425/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [49426/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [49427/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [49428/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [49429/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [49430/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49431/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49432/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49433/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49434/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [49435/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49436/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [49437/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [49438/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49439/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [49440/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49441/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [49442/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [49443/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49444/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49445/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [49446/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49447/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [49448/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [49449/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49450/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [49451/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49452/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [49453/50000], Train Loss: 8088941.0000, Val Loss: 5298646.0000\n",
      "Epoch [49454/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49455/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49456/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [49457/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49458/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [49459/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [49460/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49461/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49462/50000], Train Loss: 8088942.5000, Val Loss: 5298650.0000\n",
      "Epoch [49463/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49464/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49465/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49466/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49467/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [49468/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49469/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [49470/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [49471/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [49472/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [49473/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [49474/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [49475/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49476/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [49477/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49478/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49479/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49480/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [49481/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49482/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49483/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49484/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [49485/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [49486/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [49487/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [49488/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [49489/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [49490/50000], Train Loss: 8088941.5000, Val Loss: 5298629.0000\n",
      "Epoch [49491/50000], Train Loss: 8088942.5000, Val Loss: 5298661.0000\n",
      "Epoch [49492/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [49493/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [49494/50000], Train Loss: 8088942.0000, Val Loss: 5298626.0000\n",
      "Epoch [49495/50000], Train Loss: 8088942.0000, Val Loss: 5298665.0000\n",
      "Epoch [49496/50000], Train Loss: 8088942.0000, Val Loss: 5298621.0000\n",
      "Epoch [49497/50000], Train Loss: 8088942.5000, Val Loss: 5298671.5000\n",
      "Epoch [49498/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [49499/50000], Train Loss: 8088942.5000, Val Loss: 5298684.5000\n",
      "Epoch [49500/50000], Train Loss: 8088942.5000, Val Loss: 5298595.5000\n",
      "Epoch [49501/50000], Train Loss: 8088942.0000, Val Loss: 5298702.0000\n",
      "Epoch [49502/50000], Train Loss: 8088942.0000, Val Loss: 5298577.5000\n",
      "Epoch [49503/50000], Train Loss: 8088942.0000, Val Loss: 5298720.0000\n",
      "Epoch [49504/50000], Train Loss: 8088942.0000, Val Loss: 5298563.0000\n",
      "Epoch [49505/50000], Train Loss: 8088942.0000, Val Loss: 5298727.5000\n",
      "Epoch [49506/50000], Train Loss: 8088942.0000, Val Loss: 5298566.0000\n",
      "Epoch [49507/50000], Train Loss: 8088942.0000, Val Loss: 5298711.5000\n",
      "Epoch [49508/50000], Train Loss: 8088941.5000, Val Loss: 5298596.5000\n",
      "Epoch [49509/50000], Train Loss: 8088942.5000, Val Loss: 5298669.0000\n",
      "Epoch [49510/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49511/50000], Train Loss: 8088942.0000, Val Loss: 5298619.5000\n",
      "Epoch [49512/50000], Train Loss: 8088942.0000, Val Loss: 5298687.0000\n",
      "Epoch [49513/50000], Train Loss: 8088941.5000, Val Loss: 5298592.0000\n",
      "Epoch [49514/50000], Train Loss: 8088942.0000, Val Loss: 5298697.0000\n",
      "Epoch [49515/50000], Train Loss: 8088942.0000, Val Loss: 5298601.5000\n",
      "Epoch [49516/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [49517/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49518/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [49519/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [49520/50000], Train Loss: 8088941.5000, Val Loss: 5298611.0000\n",
      "Epoch [49521/50000], Train Loss: 8088942.5000, Val Loss: 5298680.0000\n",
      "Epoch [49522/50000], Train Loss: 8088942.5000, Val Loss: 5298613.0000\n",
      "Epoch [49523/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [49524/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [49525/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49526/50000], Train Loss: 8088941.5000, Val Loss: 5298657.0000\n",
      "Epoch [49527/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [49528/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [49529/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [49530/50000], Train Loss: 8088942.5000, Val Loss: 5298660.5000\n",
      "Epoch [49531/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49532/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49533/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [49534/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [49535/50000], Train Loss: 8088942.0000, Val Loss: 5298669.0000\n",
      "Epoch [49536/50000], Train Loss: 8088941.5000, Val Loss: 5298621.5000\n",
      "Epoch [49537/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [49538/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49539/50000], Train Loss: 8088942.5000, Val Loss: 5298642.5000\n",
      "Epoch [49540/50000], Train Loss: 8088941.5000, Val Loss: 5298655.0000\n",
      "Epoch [49541/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49542/50000], Train Loss: 8088942.0000, Val Loss: 5298661.0000\n",
      "Epoch [49543/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [49544/50000], Train Loss: 8088941.5000, Val Loss: 5298650.0000\n",
      "Epoch [49545/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49546/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49547/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [49548/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49549/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [49550/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [49551/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [49552/50000], Train Loss: 8088941.5000, Val Loss: 5298641.5000\n",
      "Epoch [49553/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [49554/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49555/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [49556/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49557/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [49558/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49559/50000], Train Loss: 8088942.5000, Val Loss: 5298641.0000\n",
      "Epoch [49560/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49561/50000], Train Loss: 8088942.5000, Val Loss: 5298651.5000\n",
      "Epoch [49562/50000], Train Loss: 8088942.0000, Val Loss: 5298635.0000\n",
      "Epoch [49563/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [49564/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49565/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49566/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [49567/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [49568/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49569/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49570/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49571/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49572/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49573/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49574/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49575/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49576/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [49577/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49578/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49579/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49580/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [49581/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49582/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49583/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49584/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49585/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49586/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49587/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49588/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [49589/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49590/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49591/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49592/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [49593/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [49594/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49595/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [49596/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49597/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49598/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49599/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49600/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [49601/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49602/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49603/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [49604/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49605/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [49606/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49607/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [49608/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49609/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [49610/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49611/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49612/50000], Train Loss: 8088942.5000, Val Loss: 5298648.0000\n",
      "Epoch [49613/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49614/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49615/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49616/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49617/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49618/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49619/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49620/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49621/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [49622/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49623/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49624/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49625/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [49626/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49627/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49628/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49629/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49630/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49631/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [49632/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [49633/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49634/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [49635/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49636/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49637/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49638/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49639/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49640/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49641/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49642/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [49643/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [49644/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [49645/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49646/50000], Train Loss: 8088942.0000, Val Loss: 5298662.0000\n",
      "Epoch [49647/50000], Train Loss: 8088942.0000, Val Loss: 5298624.5000\n",
      "Epoch [49648/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [49649/50000], Train Loss: 8088942.0000, Val Loss: 5298620.5000\n",
      "Epoch [49650/50000], Train Loss: 8088942.5000, Val Loss: 5298670.5000\n",
      "Epoch [49651/50000], Train Loss: 8088942.5000, Val Loss: 5298615.5000\n",
      "Epoch [49652/50000], Train Loss: 8088941.5000, Val Loss: 5298675.0000\n",
      "Epoch [49653/50000], Train Loss: 8088942.0000, Val Loss: 5298611.0000\n",
      "Epoch [49654/50000], Train Loss: 8088942.5000, Val Loss: 5298680.0000\n",
      "Epoch [49655/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [49656/50000], Train Loss: 8088941.5000, Val Loss: 5298686.0000\n",
      "Epoch [49657/50000], Train Loss: 8088942.5000, Val Loss: 5298600.5000\n",
      "Epoch [49658/50000], Train Loss: 8088941.5000, Val Loss: 5298688.5000\n",
      "Epoch [49659/50000], Train Loss: 8088942.0000, Val Loss: 5298602.0000\n",
      "Epoch [49660/50000], Train Loss: 8088941.5000, Val Loss: 5298683.5000\n",
      "Epoch [49661/50000], Train Loss: 8088942.0000, Val Loss: 5298608.5000\n",
      "Epoch [49662/50000], Train Loss: 8088942.5000, Val Loss: 5298674.0000\n",
      "Epoch [49663/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [49664/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [49665/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49666/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49667/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [49668/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [49669/50000], Train Loss: 8088942.5000, Val Loss: 5298665.0000\n",
      "Epoch [49670/50000], Train Loss: 8088942.0000, Val Loss: 5298623.0000\n",
      "Epoch [49671/50000], Train Loss: 8088942.0000, Val Loss: 5298666.5000\n",
      "Epoch [49672/50000], Train Loss: 8088942.5000, Val Loss: 5298623.5000\n",
      "Epoch [49673/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [49674/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49675/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [49676/50000], Train Loss: 8088942.5000, Val Loss: 5298634.5000\n",
      "Epoch [49677/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49678/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49679/50000], Train Loss: 8088942.5000, Val Loss: 5298643.5000\n",
      "Epoch [49680/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49681/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49682/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49683/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49684/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49685/50000], Train Loss: 8088942.5000, Val Loss: 5298638.5000\n",
      "Epoch [49686/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49687/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49688/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [49689/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49690/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49691/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49692/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [49693/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [49694/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49695/50000], Train Loss: 8088942.5000, Val Loss: 5298652.5000\n",
      "Epoch [49696/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [49697/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49698/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49699/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [49700/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49701/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49702/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49703/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49704/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [49705/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [49706/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49707/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [49708/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49709/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [49710/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49711/50000], Train Loss: 8088942.0000, Val Loss: 5298634.5000\n",
      "Epoch [49712/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [49713/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49714/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49715/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49716/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49717/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [49718/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [49719/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49720/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49721/50000], Train Loss: 8088942.5000, Val Loss: 5298646.5000\n",
      "Epoch [49722/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49723/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49724/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49725/50000], Train Loss: 8088941.0000, Val Loss: 5298650.0000\n",
      "Epoch [49726/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [49727/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [49728/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [49729/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [49730/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [49731/50000], Train Loss: 8088942.5000, Val Loss: 5298671.0000\n",
      "Epoch [49732/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [49733/50000], Train Loss: 8088942.0000, Val Loss: 5298682.0000\n",
      "Epoch [49734/50000], Train Loss: 8088942.5000, Val Loss: 5298602.0000\n",
      "Epoch [49735/50000], Train Loss: 8088941.5000, Val Loss: 5298690.5000\n",
      "Epoch [49736/50000], Train Loss: 8088941.5000, Val Loss: 5298592.5000\n",
      "Epoch [49737/50000], Train Loss: 8088942.0000, Val Loss: 5298699.5000\n",
      "Epoch [49738/50000], Train Loss: 8088942.0000, Val Loss: 5298587.5000\n",
      "Epoch [49739/50000], Train Loss: 8088941.5000, Val Loss: 5298701.0000\n",
      "Epoch [49740/50000], Train Loss: 8088942.0000, Val Loss: 5298590.5000\n",
      "Epoch [49741/50000], Train Loss: 8088942.0000, Val Loss: 5298693.5000\n",
      "Epoch [49742/50000], Train Loss: 8088941.5000, Val Loss: 5298602.0000\n",
      "Epoch [49743/50000], Train Loss: 8088941.5000, Val Loss: 5298679.0000\n",
      "Epoch [49744/50000], Train Loss: 8088942.0000, Val Loss: 5298617.5000\n",
      "Epoch [49745/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [49746/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49747/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49748/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [49749/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [49750/50000], Train Loss: 8088942.0000, Val Loss: 5298661.5000\n",
      "Epoch [49751/50000], Train Loss: 8088942.0000, Val Loss: 5298625.5000\n",
      "Epoch [49752/50000], Train Loss: 8088942.0000, Val Loss: 5298663.0000\n",
      "Epoch [49753/50000], Train Loss: 8088942.0000, Val Loss: 5298626.5000\n",
      "Epoch [49754/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [49755/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [49756/50000], Train Loss: 8088941.0000, Val Loss: 5298654.0000\n",
      "Epoch [49757/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49758/50000], Train Loss: 8088941.0000, Val Loss: 5298641.5000\n",
      "Epoch [49759/50000], Train Loss: 8088942.5000, Val Loss: 5298653.0000\n",
      "Epoch [49760/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49761/50000], Train Loss: 8088942.5000, Val Loss: 5298666.5000\n",
      "Epoch [49762/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [49763/50000], Train Loss: 8088942.0000, Val Loss: 5298675.0000\n",
      "Epoch [49764/50000], Train Loss: 8088942.0000, Val Loss: 5298611.5000\n",
      "Epoch [49765/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [49766/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [49767/50000], Train Loss: 8088943.0000, Val Loss: 5298670.0000\n",
      "Epoch [49768/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [49769/50000], Train Loss: 8088942.5000, Val Loss: 5298657.5000\n",
      "Epoch [49770/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [49771/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49772/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49773/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49774/50000], Train Loss: 8088942.0000, Val Loss: 5298654.5000\n",
      "Epoch [49775/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [49776/50000], Train Loss: 8088941.0000, Val Loss: 5298660.5000\n",
      "Epoch [49777/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [49778/50000], Train Loss: 8088942.5000, Val Loss: 5298662.0000\n",
      "Epoch [49779/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [49780/50000], Train Loss: 8088942.5000, Val Loss: 5298659.0000\n",
      "Epoch [49781/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [49782/50000], Train Loss: 8088941.5000, Val Loss: 5298654.0000\n",
      "Epoch [49783/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49784/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49785/50000], Train Loss: 8088942.5000, Val Loss: 5298641.5000\n",
      "Epoch [49786/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [49787/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49788/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49789/50000], Train Loss: 8088942.0000, Val Loss: 5298653.0000\n",
      "Epoch [49790/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [49791/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [49792/50000], Train Loss: 8088942.5000, Val Loss: 5298630.0000\n",
      "Epoch [49793/50000], Train Loss: 8088942.0000, Val Loss: 5298659.0000\n",
      "Epoch [49794/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49795/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [49796/50000], Train Loss: 8088942.0000, Val Loss: 5298625.0000\n",
      "Epoch [49797/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [49798/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [49799/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [49800/50000], Train Loss: 8088942.5000, Val Loss: 5298617.0000\n",
      "Epoch [49801/50000], Train Loss: 8088942.0000, Val Loss: 5298671.5000\n",
      "Epoch [49802/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [49803/50000], Train Loss: 8088942.0000, Val Loss: 5298670.0000\n",
      "Epoch [49804/50000], Train Loss: 8088942.5000, Val Loss: 5298621.0000\n",
      "Epoch [49805/50000], Train Loss: 8088942.5000, Val Loss: 5298665.5000\n",
      "Epoch [49806/50000], Train Loss: 8088942.5000, Val Loss: 5298625.0000\n",
      "Epoch [49807/50000], Train Loss: 8088942.5000, Val Loss: 5298661.5000\n",
      "Epoch [49808/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49809/50000], Train Loss: 8088942.5000, Val Loss: 5298659.0000\n",
      "Epoch [49810/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [49811/50000], Train Loss: 8088942.0000, Val Loss: 5298657.5000\n",
      "Epoch [49812/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [49813/50000], Train Loss: 8088941.5000, Val Loss: 5298655.5000\n",
      "Epoch [49814/50000], Train Loss: 8088941.5000, Val Loss: 5298635.0000\n",
      "Epoch [49815/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49816/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49817/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49818/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49819/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49820/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49821/50000], Train Loss: 8088942.5000, Val Loss: 5298637.0000\n",
      "Epoch [49822/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49823/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49824/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49825/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [49826/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [49827/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49828/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [49829/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [49830/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [49831/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49832/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49833/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49834/50000], Train Loss: 8088942.5000, Val Loss: 5298647.5000\n",
      "Epoch [49835/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49836/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49837/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49838/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49839/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49840/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [49841/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49842/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49843/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49844/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49845/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [49846/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49847/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49848/50000], Train Loss: 8088942.0000, Val Loss: 5298648.0000\n",
      "Epoch [49849/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49850/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49851/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49852/50000], Train Loss: 8088942.5000, Val Loss: 5298646.0000\n",
      "Epoch [49853/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49854/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [49855/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49856/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49857/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49858/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49859/50000], Train Loss: 8088941.0000, Val Loss: 5298649.5000\n",
      "Epoch [49860/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [49861/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49862/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [49863/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49864/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [49865/50000], Train Loss: 8088942.5000, Val Loss: 5298654.0000\n",
      "Epoch [49866/50000], Train Loss: 8088942.0000, Val Loss: 5298633.5000\n",
      "Epoch [49867/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [49868/50000], Train Loss: 8088941.5000, Val Loss: 5298633.0000\n",
      "Epoch [49869/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [49870/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [49871/50000], Train Loss: 8088941.0000, Val Loss: 5298658.5000\n",
      "Epoch [49872/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [49873/50000], Train Loss: 8088942.0000, Val Loss: 5298660.5000\n",
      "Epoch [49874/50000], Train Loss: 8088942.0000, Val Loss: 5298627.0000\n",
      "Epoch [49875/50000], Train Loss: 8088942.5000, Val Loss: 5298663.0000\n",
      "Epoch [49876/50000], Train Loss: 8088942.0000, Val Loss: 5298623.5000\n",
      "Epoch [49877/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [49878/50000], Train Loss: 8088942.0000, Val Loss: 5298617.0000\n",
      "Epoch [49879/50000], Train Loss: 8088942.0000, Val Loss: 5298675.5000\n",
      "Epoch [49880/50000], Train Loss: 8088942.5000, Val Loss: 5298606.0000\n",
      "Epoch [49881/50000], Train Loss: 8088942.0000, Val Loss: 5298691.5000\n",
      "Epoch [49882/50000], Train Loss: 8088942.0000, Val Loss: 5298586.5000\n",
      "Epoch [49883/50000], Train Loss: 8088941.5000, Val Loss: 5298713.5000\n",
      "Epoch [49884/50000], Train Loss: 8088942.0000, Val Loss: 5298562.5000\n",
      "Epoch [49885/50000], Train Loss: 8088942.5000, Val Loss: 5298738.5000\n",
      "Epoch [49886/50000], Train Loss: 8088942.0000, Val Loss: 5298543.0000\n",
      "Epoch [49887/50000], Train Loss: 8088942.0000, Val Loss: 5298744.5000\n",
      "Epoch [49888/50000], Train Loss: 8088942.0000, Val Loss: 5298559.5000\n",
      "Epoch [49889/50000], Train Loss: 8088942.5000, Val Loss: 5298703.0000\n",
      "Epoch [49890/50000], Train Loss: 8088942.0000, Val Loss: 5298618.5000\n",
      "Epoch [49891/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [49892/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [49893/50000], Train Loss: 8088942.5000, Val Loss: 5298585.0000\n",
      "Epoch [49894/50000], Train Loss: 8088942.0000, Val Loss: 5298708.5000\n",
      "Epoch [49895/50000], Train Loss: 8088942.0000, Val Loss: 5298590.0000\n",
      "Epoch [49896/50000], Train Loss: 8088942.0000, Val Loss: 5298680.0000\n",
      "Epoch [49897/50000], Train Loss: 8088942.5000, Val Loss: 5298634.0000\n",
      "Epoch [49898/50000], Train Loss: 8088942.0000, Val Loss: 5298630.0000\n",
      "Epoch [49899/50000], Train Loss: 8088942.0000, Val Loss: 5298677.0000\n",
      "Epoch [49900/50000], Train Loss: 8088942.5000, Val Loss: 5298602.5000\n",
      "Epoch [49901/50000], Train Loss: 8088942.0000, Val Loss: 5298684.5000\n",
      "Epoch [49902/50000], Train Loss: 8088942.5000, Val Loss: 5298615.0000\n",
      "Epoch [49903/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [49904/50000], Train Loss: 8088942.0000, Val Loss: 5298647.5000\n",
      "Epoch [49905/50000], Train Loss: 8088942.0000, Val Loss: 5298627.5000\n",
      "Epoch [49906/50000], Train Loss: 8088942.5000, Val Loss: 5298670.0000\n",
      "Epoch [49907/50000], Train Loss: 8088942.5000, Val Loss: 5298617.5000\n",
      "Epoch [49908/50000], Train Loss: 8088942.5000, Val Loss: 5298668.5000\n",
      "Epoch [49909/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49910/50000], Train Loss: 8088942.5000, Val Loss: 5298649.5000\n",
      "Epoch [49911/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49912/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [49913/50000], Train Loss: 8088941.0000, Val Loss: 5298661.0000\n",
      "Epoch [49914/50000], Train Loss: 8088942.0000, Val Loss: 5298631.0000\n",
      "Epoch [49915/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [49916/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49917/50000], Train Loss: 8088942.0000, Val Loss: 5298634.0000\n",
      "Epoch [49918/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [49919/50000], Train Loss: 8088941.5000, Val Loss: 5298631.0000\n",
      "Epoch [49920/50000], Train Loss: 8088941.0000, Val Loss: 5298653.0000\n",
      "Epoch [49921/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49922/50000], Train Loss: 8088942.5000, Val Loss: 5298639.0000\n",
      "Epoch [49923/50000], Train Loss: 8088942.0000, Val Loss: 5298657.0000\n",
      "Epoch [49924/50000], Train Loss: 8088942.0000, Val Loss: 5298629.0000\n",
      "Epoch [49925/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [49926/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [49927/50000], Train Loss: 8088942.5000, Val Loss: 5298645.0000\n",
      "Epoch [49928/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49929/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [49930/50000], Train Loss: 8088942.0000, Val Loss: 5298658.5000\n",
      "Epoch [49931/50000], Train Loss: 8088942.0000, Val Loss: 5298632.0000\n",
      "Epoch [49932/50000], Train Loss: 8088941.5000, Val Loss: 5298652.5000\n",
      "Epoch [49933/50000], Train Loss: 8088941.5000, Val Loss: 5298643.0000\n",
      "Epoch [49934/50000], Train Loss: 8088941.5000, Val Loss: 5298639.0000\n",
      "Epoch [49935/50000], Train Loss: 8088942.0000, Val Loss: 5298654.0000\n",
      "Epoch [49936/50000], Train Loss: 8088942.0000, Val Loss: 5298633.0000\n",
      "Epoch [49937/50000], Train Loss: 8088942.0000, Val Loss: 5298655.0000\n",
      "Epoch [49938/50000], Train Loss: 8088942.0000, Val Loss: 5298637.0000\n",
      "Epoch [49939/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49940/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49941/50000], Train Loss: 8088942.5000, Val Loss: 5298635.0000\n",
      "Epoch [49942/50000], Train Loss: 8088942.0000, Val Loss: 5298655.5000\n",
      "Epoch [49943/50000], Train Loss: 8088941.5000, Val Loss: 5298634.5000\n",
      "Epoch [49944/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49945/50000], Train Loss: 8088941.5000, Val Loss: 5298644.5000\n",
      "Epoch [49946/50000], Train Loss: 8088941.5000, Val Loss: 5298641.0000\n",
      "Epoch [49947/50000], Train Loss: 8088942.0000, Val Loss: 5298651.5000\n",
      "Epoch [49948/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49949/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49950/50000], Train Loss: 8088941.5000, Val Loss: 5298638.5000\n",
      "Epoch [49951/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49952/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49953/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49954/50000], Train Loss: 8088941.0000, Val Loss: 5298651.5000\n",
      "Epoch [49955/50000], Train Loss: 8088942.0000, Val Loss: 5298636.5000\n",
      "Epoch [49956/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49957/50000], Train Loss: 8088941.5000, Val Loss: 5298637.0000\n",
      "Epoch [49958/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49959/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49960/50000], Train Loss: 8088942.5000, Val Loss: 5298644.5000\n",
      "Epoch [49961/50000], Train Loss: 8088941.5000, Val Loss: 5298646.5000\n",
      "Epoch [49962/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49963/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49964/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49965/50000], Train Loss: 8088941.5000, Val Loss: 5298646.0000\n",
      "Epoch [49966/50000], Train Loss: 8088942.0000, Val Loss: 5298643.5000\n",
      "Epoch [49967/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [49968/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49969/50000], Train Loss: 8088942.0000, Val Loss: 5298642.5000\n",
      "Epoch [49970/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49971/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49972/50000], Train Loss: 8088941.0000, Val Loss: 5298649.0000\n",
      "Epoch [49973/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49974/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49975/50000], Train Loss: 8088942.0000, Val Loss: 5298645.0000\n",
      "Epoch [49976/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49977/50000], Train Loss: 8088941.0000, Val Loss: 5298652.5000\n",
      "Epoch [49978/50000], Train Loss: 8088942.5000, Val Loss: 5298636.5000\n",
      "Epoch [49979/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49980/50000], Train Loss: 8088941.5000, Val Loss: 5298636.5000\n",
      "Epoch [49981/50000], Train Loss: 8088942.0000, Val Loss: 5298652.5000\n",
      "Epoch [49982/50000], Train Loss: 8088942.5000, Val Loss: 5298640.0000\n",
      "Epoch [49983/50000], Train Loss: 8088941.0000, Val Loss: 5298645.0000\n",
      "Epoch [49984/50000], Train Loss: 8088942.0000, Val Loss: 5298646.5000\n",
      "Epoch [49985/50000], Train Loss: 8088942.0000, Val Loss: 5298641.0000\n",
      "Epoch [49986/50000], Train Loss: 8088942.0000, Val Loss: 5298649.0000\n",
      "Epoch [49987/50000], Train Loss: 8088942.0000, Val Loss: 5298640.0000\n",
      "Epoch [49988/50000], Train Loss: 8088941.0000, Val Loss: 5298647.5000\n",
      "Epoch [49989/50000], Train Loss: 8088942.0000, Val Loss: 5298643.0000\n",
      "Epoch [49990/50000], Train Loss: 8088941.5000, Val Loss: 5298643.5000\n",
      "Epoch [49991/50000], Train Loss: 8088941.5000, Val Loss: 5298647.5000\n",
      "Epoch [49992/50000], Train Loss: 8088942.0000, Val Loss: 5298639.0000\n",
      "Epoch [49993/50000], Train Loss: 8088942.0000, Val Loss: 5298650.0000\n",
      "Epoch [49994/50000], Train Loss: 8088942.0000, Val Loss: 5298638.5000\n",
      "Epoch [49995/50000], Train Loss: 8088942.0000, Val Loss: 5298649.5000\n",
      "Epoch [49996/50000], Train Loss: 8088942.0000, Val Loss: 5298641.5000\n",
      "Epoch [49997/50000], Train Loss: 8088942.0000, Val Loss: 5298646.0000\n",
      "Epoch [49998/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n",
      "Epoch [49999/50000], Train Loss: 8088941.5000, Val Loss: 5298645.0000\n",
      "Epoch [50000/50000], Train Loss: 8088942.0000, Val Loss: 5298644.5000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train.unsqueeze(1))  # Add sequence length dimension\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_test.unsqueeze(1))  # Add sequence length dimension\n",
    "        val_loss = criterion(val_outputs, y_test)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ground truth and predictions to an Excel file\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test.unsqueeze(1)).numpy()\n",
    "    df = pd.DataFrame({'GroundTruth': y_test.squeeze().numpy(),\n",
    "                       'Prediction': predictions.squeeze()})\n",
    "    df.to_excel('results/LSTM.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: 6014.16796875\n",
      "Actual value: 4430.0\n"
     ]
    }
   ],
   "source": [
    "# Predict random data from the validation set\n",
    "random_index = np.random.randint(len(X_test))\n",
    "random_data = X_test[random_index].reshape(1, 1, -1)  # Reshape to match model input dimensions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model(random_data)\n",
    "    print(\"Predicted value:\", prediction.item())\n",
    "    print(\"Actual value:\", y_test[random_index].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), 'models/StateLSTM.pth')\n",
    "# Save the entire model\n",
    "torch.save(model, 'models/LSTM.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "253px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
